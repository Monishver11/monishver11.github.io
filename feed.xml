<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-16T01:12:12+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Understanding the Kernel Trick</title><link href="https://monishver11.github.io/blog/2025/kernel-trick/" rel="alternate" type="text/html" title="Understanding the Kernel Trick"/><published>2025-01-13T23:03:00+00:00</published><updated>2025-01-13T23:03:00+00:00</updated><id>https://monishver11.github.io/blog/2025/kernel-trick</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/kernel-trick/"><![CDATA[<p>When working with machine learning models, especially Support Vector Machines (SVMs), the idea of mapping data into a higher-dimensional space often comes into play. This mapping helps transform non-linearly separable data into a space where linear decision boundaries can be applied. But what happens when the dimensionality of the feature space becomes overwhelmingly large? This is where the <strong>kernel trick</strong> saves the day. In this post, we will explore the kernel trick, starting with SVMs, their reliance on feature mappings, and how inner products in feature space can be computed without ever explicitly constructing that space.</p> <hr/> <h4 id="svms-with-explicit-feature-maps"><strong>SVMs with Explicit Feature Maps</strong></h4> <p>To understand the kernel trick, let’s begin with SVMs. In the simplest case, an SVM aims to find a hyperplane that separates data into classes with the largest possible margin. To handle more complex data, we map the input data \(\mathbf{x}\) into a higher-dimensional feature space using a feature map \(\psi: X \to \mathbb{R}^d\). In this space, the SVM optimization problem can be written as:</p> \[\min_{\mathbf{w} \in \mathbb{R}^d} \frac{1}{2} \|\mathbf{w}\|^2 + \frac{c}{n} \sum_{i=1}^n \max(0, 1 - y_i \mathbf{w}^T \psi(\mathbf{x}_i)).\] <p>Here, \(\mathbf{w}\) is the weight vector, \(c\) is a regularization parameter, and \(y_i\) are the labels of the data points. While this approach works well for small \(d\), it becomes computationally expensive as \(d\) increases, especially when using high-degree polynomial mappings.</p> <p>To address this issue, we turn to a reformulation of the SVM problem, derived from <strong>Lagrangian duality</strong>.</p> <h4 id="the-svm-dual-problem"><strong>The SVM Dual Problem</strong></h4> <p>Through Lagrangian duality, the SVM optimization problem can be re-expressed as a dual problem:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \psi(\mathbf{x}_j)^T \psi(\mathbf{x}_i),\] <p>subject to:</p> \[\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \in \left[ 0, \frac{c}{n} \right] \quad \forall i.\] <p>Here, \(\alpha_i\) are the dual variables (Lagrange multipliers). Once the optimal \(\boldsymbol{\alpha}^*\) is obtained, the weight vector in the feature space can be reconstructed as:</p> \[\mathbf{w}^* = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i).\] <p>The decision function for a new input \(\mathbf{x}\) is given by:</p> \[\hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i)^T \psi(\mathbf{x}).\] <h5 id="observing-the-role-of-inner-products"><strong>Observing the Role of Inner Products</strong></h5> <p>An important observation here is that the feature map \(\psi(\mathbf{x})\) appears only through inner products of the form \(\psi(\mathbf{x}_j)^T \psi(\mathbf{x}_i)\). This means we don’t actually need the explicit feature representation \(\psi(\mathbf{x})\); instead, we just need the ability to compute these inner products efficiently.</p> <h4 id="computing-inner-products-in-practice"><strong>Computing Inner Products in Practice</strong></h4> <p>Let’s explore the kernel trick with an example.</p> <h5 id="example-degree-2-monomials"><strong>Example: Degree-2 Monomials</strong></h5> <p>Suppose we are working with 2D data points \(\mathbf{x} = (x_1, x_2)\). If we map the data into a space of degree-2 monomials, the feature map becomes:</p> \[\psi: \mathbb{R}^2 \to \mathbb{R}^6, \quad (x_1, x_2) \mapsto (1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, \sqrt{2}x_1x_2, x_2^2).\] <p>The inner product in the feature space is:</p> \[\psi(\mathbf{x})^T \psi(\mathbf{x}') = 1 + 2x_1x_1' + 2x_2x_2' + (x_1x_1')^2 + 2x_1x_2x_1'x_2' + (x_2x_2')^2.\] <p>Simplifying, we observe:</p> \[\psi(\mathbf{x})^T \psi(\mathbf{x}') = (1 + x_1x_1' + x_2x_2')^2 = (1 + \mathbf{x}^T \mathbf{x}')^2.\] <p>This shows that we can compute \(\psi(\mathbf{x})^T \psi(\mathbf{x}')\) directly from the original input space without explicitly constructing \(\psi(\mathbf{x})\)—a key insight behind the kernel trick.</p> <h5 id="general-case-monomials-up-to-degree-p"><strong>General Case: Monomials Up to Degree \(p\)</strong></h5> <p>For feature maps that produce monomials up to degree \(p\), the inner product generalizes as:</p> \[\psi(x)^T \psi(x') = (1 + x^T x')^p.\] <p>It is worth noting that the coefficients of the monomials in \(\psi(x)\) may vary depending on the specific feature map.</p> <h4 id="efficiency-of-the-kernel-trick-from-exponential-to-linear-complexity"><strong>Efficiency of the Kernel Trick: From Exponential to Linear Complexity</strong></h4> <p>One of the key advantages of the kernel trick is its ability to reduce the computational complexity of working with high-dimensional feature spaces. Let’s break this down:</p> <h5 id="explicit-computation-complexity"><strong>Explicit Computation Complexity</strong></h5> <p>When we map an input vector \(\mathbf{x} \in \mathbb{R}^d\) to a feature space with monomials up to degree \(p\), the dimensionality of the feature space increases significantly. Specifically:</p> <ul> <li> <p><strong>Feature Space Dimension</strong>: The number of features in the expansion is:</p> \[\binom{d + p}{p} = \frac{(d + p)!}{d! \, p!}.\] <p>For large \(p\) or \(d\), this grows rapidly and can quickly become computationally prohibitive.</p> </li> <li> <p><strong>Explicit Inner Product</strong>: Computing the inner product directly in this expanded space has a complexity of:</p> \[O\left(\binom{d + p}{p}\right),\] <p>which is exponential in \(p\) for fixed \(d\).</p> </li> </ul> <h5 id="implicit-computation-complexity"><strong>Implicit Computation Complexity</strong></h5> <p>Using the kernel trick, we avoid explicitly constructing the feature space. For a kernel function like:</p> \[k(\mathbf{x}, \mathbf{x}') = (1 + \mathbf{x}^T \mathbf{x}')^p,\] <p>the computation operates directly in the input space.</p> <ul> <li> <p><strong>Input Space Computation</strong>: Computing the kernel function involves:</p> <ol> <li><strong>Dot Product</strong>: \(\mathbf{x}^T \mathbf{x}'\) is computed in \(O(d)\).</li> <li><strong>Polynomial Evaluation</strong>: Raising this result to power \(p\) is done in constant time, independent of \(d\).</li> </ol> </li> </ul> <p>Thus, the complexity is reduced to:</p> \[O(d),\] <p>which is <strong>linear</strong> in the input dimensionality \(d\), regardless of \(p\).</p> <h5 id="why-this-matters"><strong>Why This Matters</strong></h5> <ul> <li><strong>Explicit Features</strong>: For high \(p\), the feature space grows exponentially, leading to a <strong>curse of dimensionality</strong> if explicit computation is used.</li> <li><strong>Implicit Kernel Computation</strong>: The kernel trick sidesteps the explicit feature space, allowing efficient computation even when the feature space is high-dimensional or infinite (e.g., with RBF kernels).</li> </ul> <p>This transformation from <strong>exponential</strong> to <strong>linear complexity</strong> is one of the core reasons kernel methods are powerful tools in machine learning.</p> <p><strong>Key Takeaway</strong> : The kernel trick enables efficient computation in high-dimensional feature spaces by directly working in the input space. This reduces the complexity from \(O\left(\binom{d + p}{p}\right)\) to \(O(d)\), making it feasible to apply machine learning methods to problems with high-degree polynomial or infinite-dimensional feature spaces.</p> <hr/> <h4 id="exploring-the-kernel-function"><strong>Exploring the Kernel Function</strong></h4> <p>To fully appreciate the kernel trick, we need to formalize the concept of the <strong>kernel function</strong>. In our earlier discussion, we introduced the idea of a feature map \(\psi: X \to \mathcal{H}\), which maps input data from the original space \(X\) to a higher-dimensional feature space \(\mathcal{H}\). The kernel function \(k\) corresponding to this feature map is defined as:</p> \[k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle,\] <p>where \(\langle \cdot, \cdot \rangle\) represents the inner product in \(\mathcal{H}\).</p> <h5 id="why-use-kernel-functions"><strong>Why Use Kernel Functions?</strong></h5> <p>At first glance, this notation might seem like a trivial restatement of the inner product, but it’s far more powerful. The key insight is that we can often evaluate \(k(\mathbf{x}, \mathbf{x}')\) directly, without explicitly computing \(\psi(\mathbf{x})\) and \(\psi(\mathbf{x}')\). This is crucial for efficiently working with high-dimensional or infinite-dimensional feature spaces. But this efficiency only applies to certain methods — those that can be <strong>kernelized</strong>.</p> <h4 id="kernelized-methods"><strong>Kernelized Methods</strong></h4> <p>A method is said to be <strong>kernelized</strong> if it uses the feature vectors \(\psi(\mathbf{x})\) only inside inner products of the form \(\langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle\). For such methods, we can replace these inner products with a kernel function \(k(\mathbf{x}, \mathbf{x}')\), avoiding explicit feature computation. This applies to both the optimization problem and the prediction function. Let’s revisit the SVM example to see kernelization in action.</p> <h5 id="kernelized-svm-dual-formulation"><strong>Kernelized SVM Dual Formulation</strong></h5> <p>Recall the dual problem for SVMs:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle,\] <p>subject to:</p> \[\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \in \left[ 0, \frac{c}{n} \right] \quad \forall i.\] <p><strong>Here’s the key</strong>: because every occurrence of \(\psi(\mathbf{x})\) is inside an inner product, we can replace \(\langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle\) with \(k(\mathbf{x}_i, \mathbf{x}_j)\), the kernel function. The resulting dual optimization problem becomes:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j),\] <p>subject to the same constraints.</p> <p>For predictions, the decision function can also be written in terms of the kernel:</p> \[\hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i k(\mathbf{x}_i, \mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i)^T \psi(\mathbf{x})\] <p>This reformulation is what allows SVMs to operate efficiently in high-dimensional spaces.</p> <h5 id="the-kernel-matrix"><strong>The Kernel Matrix</strong></h5> <p>A key component in kernelized methods is the <strong>kernel matrix</strong>, which encapsulates the pairwise kernel values for all data points. For a dataset \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}\), the kernel matrix \(\mathbf{K}\) is defined as:</p> \[\mathbf{K} = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}.\] <p>This \(n \times n\) matrix, also known as the <strong>Gram matrix</strong> in machine learning, summarizes all the information about the training data necessary for solving the kernelized optimization problem.</p> <p>For the kernelized SVM, we can replace \(\langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle\) with \(K_{ij}\), reducing the dual problem to:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K_{ij},\] <p>subject to the same constraints.</p> <p><strong>So, Given a kernelized ML algorithm</strong> (i.e., all \(\psi(x)\)’s show up as \(\langle \psi(x), \psi(x') \rangle\)) :</p> <ol> <li><strong>Flexibility</strong>: By substituting the kernel function, we can implicitly use very high-dimensional or even infinite-dimensional feature spaces.</li> <li><strong>Scalability</strong>: Once the kernel matrix is computed, the computational cost depends on the number of data points \(n\), rather than the dimension of the feature space \(d\).</li> <li><strong>Efficiency</strong>: For many kernels, \(k(\mathbf{x}, \mathbf{x}')\) can be computed without directly accessing the high-dimensional feature representation \(\psi(\mathbf{x})\), avoiding the \(O(d)\) dependence.</li> </ol> <p>These properties make kernelized methods invaluable when \(d \gg n\), a common scenario in machine learning tasks.</p> <p>The kernel trick revolutionizes how we think about high-dimensional data. Next, we will delve into popular kernel functions, their interpretations, and how to choose the right one for your problem.</p> <hr/> <h4 id="example-kernels"><strong>Example Kernels</strong></h4> <p>In many cases, it’s useful to think of the kernel function \(k(x, x')\) as a <strong>similarity score</strong> between the data points \(x\) and \(x'\). This perspective allows us to design similarity functions without explicitly considering the feature map.</p> <p>For example, we can create <strong>string kernels</strong> or <strong>graph kernels</strong>—functions that define similarity based on the structure of strings or graphs, respectively. The key question, however, is: <strong>How do we know that our kernel functions truly correspond to inner products in some feature space?</strong></p> <p>This is an essential consideration, as it ensures that the kernel method preserves the properties necessary for various machine learning algorithms to work effectively. Let’s break this down.</p> <h5 id="how-to-obtain-kernels"><strong>How to Obtain Kernels?</strong></h5> <p>There are two primary ways to define kernels:</p> <ol> <li> <p><strong>Explicit Construction</strong>: Define the feature map \(\psi(\mathbf{x})\) and use it to compute the kernel: \(k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle.\) (e.g. monomials)</p> </li> <li> <p><strong>Direct Definition</strong>: Directly define the kernel \(k(\mathbf{x}, \mathbf{x}')\) as a similarity score and verify that it corresponds to an inner product for some \(\psi\). This verification is often guided by mathematical theorems.</p> </li> </ol> <p>To understand this better, let’s first equip ourselves with some essential linear algebra concepts.</p> <h5 id="positive-semidefinite-matrices-and-kernels"><strong>Positive Semidefinite Matrices and Kernels</strong></h5> <p>To verify if a kernel corresponds to a valid inner product, we rely on the concept of <strong>positive semidefinite (PSD) matrices</strong>. Here’s a quick refresher:</p> <ul> <li> <p>A matrix \(\mathbf{M} \in \mathbb{R}^{n \times n}\) is positive semidefinite if: \(\mathbf{x}^\top \mathbf{M} \mathbf{x} \geq 0, \quad \forall \mathbf{x} \in \mathbb{R}^n.\)</p> </li> <li> <p>Equivalent conditions, each necessary and sufficient for a symmetric matrixfor \(\mathbf{M}\) being <strong>PSD</strong>:</p> <ul> <li>\(\mathbf{M} = \mathbf{R}^\top \mathbf{R}\), for some matrix \(\mathbf{R}\).</li> <li>All eigenvalues of \(\mathbf{M}\) are non-negative or \(\geq 0\).</li> </ul> </li> </ul> <p>Next, we define a <strong>positive definite (PD) kernel</strong>:</p> <h5 id="positive-definite-kernel"><strong>Positive Definite Kernel</strong></h5> <p><strong>Definition:</strong></p> <p>A symmetric function \(k: X \times X \to \mathbb{R}\) is a <strong>PD</strong> kernel if, for any finite set \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\} \subset X\), the kernel matrix:</p> \[\mathbf{K} = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}\] <p>is positive semidefinite.</p> <ol> <li>Symmetry: \(k(\mathbf{x}, \mathbf{x}') = k(\mathbf{x}', \mathbf{x})\).</li> <li>The kernel matrix needs to be positive semidefinite for any finite set of points.</li> <li>Equivalently: \(\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j k(\mathbf{x}_i, \mathbf{x}_j) \geq 0\), for all \(\alpha_i \in \mathbb{R}\) \(\forall i\).</li> </ol> <p>[How, better way of stating it!]</p> <h5 id="mercers-theorem"><strong>Mercer’s Theorem</strong></h5> <p>Mercer’s Theorem provides a foundational result for kernels. It states:</p> <ul> <li> <p>A symmetric function \(k(\mathbf{x}, \mathbf{x}')\) can be expressed as an inner product</p> \[k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle\] <p>if and only if \(k(\mathbf{x}, \mathbf{x}')\) is positive definite.</p> </li> </ul> <p>While proving that a kernel is <strong>PD</strong> can be challenging, we can use known kernels to construct new ones.</p> <h5 id="constructing-new-kernels-from-existing-ones"><strong>Constructing New Kernels from Existing Ones</strong></h5> <p>Given valid PD kernels \(k_1\) and \(k_2\), we can create new kernels using the following operations:</p> <ol> <li><strong>Non-Negative Scaling</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = \alpha k(\mathbf{x}, \mathbf{x}')\), where \(\alpha \geq 0\).</li> <li><strong>Addition</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') + k_2(\mathbf{x}, \mathbf{x}')\).</li> <li><strong>Multiplication</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')\).</li> <li><strong>Recursion</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k(\psi(\mathbf{x}), \psi(\mathbf{x}'))\), for any function \(\psi(\cdot)\).</li> <li><strong>Feature Mapping</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = f(\mathbf{x}) f(\mathbf{x}')\), for any function \(f(\cdot)\).</li> </ol> <p>And, Lots more theorems to help you construct new kernels from old.</p> <p>[Add reference to mercer theorem]</p> <hr/> <h4 id="popular-kernel-functions"><strong>Popular Kernel Functions</strong></h4> <h5 id="the-linear-kernel"><strong>The Linear Kernel</strong></h5> <p>The linear kernel is the simplest and most intuitive kernel function. Imagine working with data in an input space represented as \(X = \mathbb{R}^d\). Here, the feature space, denoted as \(\mathcal{H}\), is the same as the input space \(\mathbb{R}^d\). The feature map for this kernel is straightforward: \(\psi(x) = x\).</p> <p>The kernel function itself is defined as:</p> \[k(x, x') = \langle x, x' \rangle = x^\top x',\] <p>where \(\langle x, x' \rangle\) represents the standard inner product. This simplicity makes the linear kernel computationally efficient and ideal for linear models.</p> <h5 id="the-quadratic-kernel"><strong>The Quadratic Kernel</strong></h5> <p>The quadratic kernel takes us a step further by mapping the input space \(X = \mathbb{R}^d\) into a higher-dimensional feature space \(\mathcal{H} = \mathbb{R}^D\), where \(D\) is approximately \(d + \binom d2 \approx \frac{d^2}{2}\). This expanded feature space enables the kernel to capture quadratic relationships in the data.</p> <p>The feature map for the quadratic kernel is given by:</p> \[\psi(x) = \left(x_1, \dots, x_d, x_1^2, \dots, x_d^2, \sqrt{2}x_1x_2, \dots, \sqrt{2}x_ix_j, \dots, \sqrt{2}x_{d-1}x_d\right)^\top.\] <p>To compute the kernel function, we use the inner product of the feature maps:</p> \[k(x, x') = \langle \psi(x), \psi(x') \rangle.\] <p>Expanding this yields:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2.\] <p><strong>Derivation of the Quadratic Kernel form:</strong></p> <p>The quadratic kernel is defined as the inner product in a higher-dimensional feature space. The feature map \(\psi(x)\) includes:</p> <ol> <li>Original features: \(x_1, x_2, \dots, x_d\)</li> <li>Squared features: \(x_1^2, x_2^2, \dots, x_d^2\)</li> <li>Cross-product terms: \(\sqrt{2}x_i x_j\) for \(i \neq j\)</li> </ol> <p>Thus:</p> \[\psi(x) = \left(x_1, x_2, \dots, x_d, x_1^2, x_2^2, \dots, x_d^2, \sqrt{2}x_1x_2, \sqrt{2}x_1x_3, \dots, \sqrt{2}x_{d-1}x_d\right)^\top\] <p>The kernel is computed as:</p> \[k(x, x') = \langle \psi(x), \psi(x') \rangle\] <p>Expanding this, we have:</p> <ol> <li> <p><strong>Linear terms</strong>:<br/> \(\langle x, x' \rangle = \sum_{i} x_i x_i'\)</p> </li> <li> <p><strong>Squared terms</strong>:<br/> \(\sum_{i} x_i^2 x_i'^2\)</p> </li> <li> <p><strong>Cross-product terms</strong>: \(2 \sum_{i \neq j} x_i x_j x_i' x_j'\)</p> </li> </ol> <p>Combining these, the kernel becomes:</p> \[k(x, x') = \langle x, x' \rangle + \sum_{i} x_i^2 x_i'^2 + 2 \sum_{i \neq j} x_i x_j x_i' x_j'\] <p>Recognizing that:</p> \[\langle x, x' \rangle^2 = \left( \sum_{i} x_i x_i' \right)^2 = \sum_{i} x_i^2 x_i'^2 + 2 \sum_{i \neq j} x_i x_j x_i' x_j'\] <p>The kernel simplifies to:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2\] <p>One of the key advantages of kernel methods is computational efficiency. While the explicit computation of the inner product in the feature space requires \(O(d^2)\) operations, the implicit kernel calculation only requires \(O(d)\) operations.</p> <p>A good example will make it much clearer.</p> <p>Let \(x = [1, 2]\) and \(x' = [3, 4]\).</p> <p>The quadratic kernel is defined as:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2\] <p><strong>Step 1</strong>: Compute \(\langle x, x' \rangle\) \(\langle x, x' \rangle = (1)(3) + (2)(4) = 3 + 8 = 11\)</p> <p><strong>Step 2</strong>: Compute \(\langle x, x' \rangle^2\) \(\langle x, x' \rangle^2 = 11^2 = 121\)</p> <p><strong>Step 3</strong>: Compute \(k(x, x')\) \(k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2 = 11 + 121 = 132\)</p> <p><strong>Step 4</strong>: Verify with the Feature Map</p> <p>The feature map for the quadratic kernel is:</p> \[\psi(x) = [x_1, x_2, x_1^2, x_2^2, \sqrt{2}x_1x_2]\] <p>For \(x = [1, 2]\): \(\psi(x) = [1, 2, 1^2, 2^2, \sqrt{2}(1)(2)] = [1, 2, 1, 4, 2\sqrt{2}]\)</p> <p>For \(x' = [3, 4]\): \(\psi(x') = [3, 4, 3^2, 4^2, \sqrt{2}(3)(4)] = [3, 4, 9, 16, 12\sqrt{2}]\)</p> <p>Compute the inner product:</p> <p>\(\langle \psi(x), \psi(x') \rangle = (1)(3) + (2)(4) + (1)(9) + (4)(16) + (2\sqrt{2})(12\sqrt{2})\) \(= 3 + 8 + 9 + 64 + 48 = 132\)</p> <p>Thus, the quadratic kernel gives: \(k(x, x') = 132\)</p> <h5 id="the-polynomial-kernel"><strong>The Polynomial Kernel</strong></h5> <p>Building on the quadratic kernel, the polynomial kernel generalizes the concept by introducing a degree parameter \(M\). The kernel function is defined as:</p> \[k(x, x') = (1 + \langle x, x' \rangle)^M.\] <p>This kernel corresponds to a feature space that includes all monomials of the input features up to degree \(M\). Notably, the computational cost of evaluating the kernel function remains constant, regardless of \(M\). However, explicitly computing the inner product in the feature space grows rapidly as \(M\) increases.</p> <h5 id="the-radial-basis-function-rbf-or-gaussian-kernel"><strong>The Radial Basis Function (RBF) or Gaussian Kernel</strong></h5> <p>The RBF kernel, also known as the Gaussian kernel, is one of the most widely used kernels for nonlinear problems. The input space remains \(X = \mathbb{R}^d\), but the feature space is infinite-dimensional, making it capable of capturing complex relationships in the data.</p> <p>The kernel function is expressed as:</p> \[k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right),\] <p>where \(\sigma^2\) is a parameter known as the bandwidth, controlling the smoothness of the kernel.</p> <p>One might wonder if this kernel still adheres to the principle of inner products in a feature space. The answer is both yes and no. While it acts like a similarity score, it corresponds to the inner product of feature vectors in an infinite-dimensional space.</p> <p>[Explain it intuitively, and how to make sense of it]</p> <p>[some visualization needed]</p> <hr/> <h4 id="kernelization-the-recipe"><strong>Kernelization: The Recipe</strong></h4> <p>To effectively leverage kernel methods, follow this general recipe:</p> <ol> <li>Recognize problems that can benefit from kernelization. These are cases where the feature map \(\psi(x)\) only appears in inner products \(\langle \psi(x), \psi(x') \rangle\).</li> <li>Select an appropriate kernel function(‘similarity score’) that suits the data and the task at hand.</li> <li>Compute the kernel matrix, a symmetric matrix of size \(n \times n\) for a dataset with \(n\) data points.</li> <li>Use the kernel matrix to optimize the model and make predictions.</li> </ol> <p>This approach allows us to solve problems in high-dimensional feature spaces without the computational burden of explicit mappings.</p> <h5 id="whats-next"><strong>What’s Next?</strong></h5> <p>We explored the theoretical foundations of kernel functions, how to construct valid kernels, and the properties of popular kernels. But, a key question remains: under what conditions can we apply kernelization effectively? Understanding this involves diving deeper into the properties of kernel functions and their applicability to different problem domains. In the next post, we will explore these conditions in detail and discuss their implications for solving SVM problems.</p> <p>Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Add some visualization for kernels intuition</li> <li></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.]]></summary></entry><entry><title type="html">Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps</title><link href="https://monishver11.github.io/blog/2025/feature-maps/" rel="alternate" type="text/html" title="Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps"/><published>2025-01-13T21:26:00+00:00</published><updated>2025-01-13T21:26:00+00:00</updated><id>https://monishver11.github.io/blog/2025/feature-maps</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/feature-maps/"><![CDATA[<h4 id="understanding-the-input-space-mathcalx"><strong>Understanding the Input Space \(\mathcal{X}\)</strong></h4> <p>In machine learning, the input data we work with often originates from domains far removed from the mathematical structures we typically rely on. Text documents, image files, sound recordings, and even DNA sequences all serve as examples of such diverse input spaces. While these data types can be represented numerically, their raw form often lacks the structure necessary for effective analysis.</p> <p>Consider text data: each sequence might consist of words or characters, but the numerical representation of one text sequence may not align with another. The \(i\)-th entry of one sequence might hold a different meaning compared to the same position in another. Moreover, sequences like these often vary in length, which makes it even more challenging to align them into a consistent numerical format.</p> <p>This lack of structure highlights a fundamental challenge: we need a way to standardize and represent inputs in a meaningful way. The solution lies in a process called <strong>feature extraction</strong>.</p> <h5 id="feature-extraction-bridging-the-gap-between-input-and-model"><strong>Feature Extraction: Bridging the Gap Between Input and Model</strong></h5> <p>Feature extraction, also known as <strong>featurization</strong>, is the process of transforming inputs from their raw forms into structured numerical vectors that can be processed by machine learning models. Think of it as translating the data into a language that models can understand and interpret.</p> <p>Mathematically, we define feature extraction as a mapping: \(\phi: \mathcal{X} \to \mathbb{R}^d\) Here, \(\phi(x)\) takes an input \(x\) from the space \(\mathcal{X}\) and maps it into a \(d\)-dimensional vector in \(\mathbb{R}^d\). This transformation is crucial because it creates a consistent numerical structure that machine learning algorithms require.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Feature_Maps_1-480.webp 480w,/assets/img/Feature_Maps_1-800.webp 800w,/assets/img/Feature_Maps_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Feature_Maps_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Feature_Maps_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h4 id="linear-models-and-explicit-feature-maps"><strong>Linear Models and Explicit Feature Maps</strong></h4> <p>Let’s delve into how feature extraction integrates with linear models. In this setup, we make no assumptions about the input space \(\mathcal{X}\). Instead, we introduce a <strong>feature map</strong>: \(\phi: \mathcal{X} \to \mathbb{R}^d\)</p> <p>This feature map transforms inputs into a feature space \(\mathbb{R}^d\), enabling the use of standard linear model frameworks. Once in the feature space, the hypothesis space of affine functions is defined as:</p> \[F = \left\{ x \mapsto w^T \phi(x) + b \mid w \in \mathbb{R}^d, \, b \in \mathbb{R} \right\}\] <p>In this formulation:</p> <ul> <li>\(w\) represents the weights assigned to each feature.</li> <li>\(b\) is the bias term.</li> <li>\(\phi(x)\) is the feature-transformed representation of the input.</li> </ul> <p>This approach allows linear models to leverage the structured feature space effectively.</p> <h5 id="geometric-intuition-solving-nonlinear-problems-with-featurization"><strong>Geometric Intuition: Solving Nonlinear Problems with Featurization</strong></h5> <p>Imagine a two-class classification problem where the decision boundary is nonlinear. Using the <strong>identity feature map</strong> \(\phi(x) = (x_1, x_2)\), a linear model fails because the data points cannot be separated by a straight line in the original input space.</p> <p>However, by applying an appropriate feature map, such as: \(\phi(x) = (x_1, x_2, x_1^2 + x_2^2)\) we can transform the data into a higher-dimensional space. In this transformed space, the previously nonlinear boundary becomes linearly separable.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Feature_Maps_2-480.webp 480w,/assets/img/Feature_Maps_2-800.webp 800w,/assets/img/Feature_Maps_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Feature_Maps_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Feature_Maps_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This geometric perspective is a powerful way to understand how feature maps enhance the capability of machine learning models. If you’d like to visualize this concept, consider watching <a href="https://www.youtube.com/watch?v=3liCbRZPrZA">this illustrative video</a>.</p> <h5 id="expanding-the-hypothesis-space-the-role-of-features"><strong>Expanding the Hypothesis Space: The Role of Features</strong></h5> <p>The expressivity of a linear model—its ability to capture complex relationships—depends directly on the features it has access to. To increase the hypothesis space’s expressivity, we must introduce additional features.</p> <p>For instance, moving from basic linear features to polynomial or interaction features enables the model to capture more intricate patterns in the data. This expanded hypothesis space is often described as more <strong>expressive</strong> because it can fit a broader range of input-output relationships.</p> <p>However, with great expressivity comes the challenge of ensuring these features are meaningful and contribute to the task at hand. The art of feature design lies in striking the right balance: creating features that enhance the model’s capacity without overfitting or adding noise.</p> <hr/> <h4 id="handling-nonlinearity-with-linear-methods"><strong>Handling Nonlinearity with Linear Methods</strong></h4> <p>In machine learning, linear models are often preferred for their simplicity, interpretability, and efficiency. However, real-world problems rarely exhibit purely linear relationships, and this is where the challenge arises. How can we handle nonlinearity while retaining the advantages of linear methods?</p> <p>Let’s take an example task: predicting an individual’s health score. At first glance, this might seem straightforward—after all, we can list plenty of features relevant to medical diagnosis, such as:</p> <ul> <li>Height</li> <li>Weight</li> <li>Body temperature</li> <li>Blood pressure</li> </ul> <p>While these features are clearly useful, their relationships with the health score may not be linear. Furthermore, complex dependencies among these features can make predictions challenging. To address this, we must carefully consider the nature of nonlinearity and how it affects linear predictors.</p> <p>Nonlinearities in data can broadly be categorized into three types:</p> <ol> <li><strong>Non-monotonicity</strong>: When the relationship between a feature and the label does not follow a single increasing or decreasing trend.</li> <li><strong>Saturation</strong>: When the effect of a feature diminishes beyond a certain point, despite continuing to grow.</li> <li><strong>Interactions</strong>: When the effect of one feature depends on the value of another.</li> </ol> <p>Each of these presents unique challenges for linear models. Let’s explore them in detail.</p> <h5 id="non-monotonicity-when-extremes-behave-differently"><strong>Non-monotonicity: When Extremes Behave Differently</strong></h5> <p>Imagine we want to predict a health score \(y\) (where higher is better) based on body temperature \(t\). A simple feature map \(\phi(x) = [1, t(x)]\) assumes an affine relationship between temperature and health, meaning it can only model cases where:</p> <ul> <li>Higher temperatures are better, or</li> <li>Lower temperatures are better.</li> </ul> <p>But in reality, both extremes of temperature are harmful, and health is best around a “normal” temperature (e.g., 37°C). This non-monotonic relationship poses a problem.</p> <h6 id="solution-1-domain-knowledge"><strong>Solution 1: Domain Knowledge</strong></h6> <p>One approach is to manually transform the input to account for the non-monotonicity. For instance: \(\phi(x) = \left[1, \left(t(x) - 37\right)^2 \right]\) Here, we explicitly encode the deviation from normal temperature. While effective, this solution relies heavily on domain knowledge and manual feature engineering.</p> <h6 id="solution-2-let-the-model-decide"><strong>Solution 2: Let the Model Decide</strong></h6> <p>An alternative approach is to include additional features, such as: \(\phi(x) = \left[1, t(x), t(x)^2 \right]\) This makes the model more expressive, allowing it to learn the non-monotonic relationship directly from the data. As a general rule, features should be simple, modular building blocks that can adapt to various patterns.</p> <h5 id="saturation-when-effects-diminish"><strong>Saturation: When Effects Diminish</strong></h5> <p>Consider a recommendation system that scores products based on their relevance to a user query. One feature might be \(N(x)\), the number of people who purchased the product \(x\). Intuitively, relevance increases with \(N(x)\), but the relationship is not linear—beyond a certain point, each additional purchase contributes progressively less to relevance, reflecting diminishing returns.</p> <h6 id="the-solution"><strong>The Solution:</strong></h6> <p>To address saturation, we can apply nonlinear transformations to the feature. Two common methods are:</p> <ol> <li> <p><strong>Smooth nonlinear transformation</strong>:</p> \[\phi(x) = [1, \log(1 + N(x))]\] <p>The logarithm is particularly effective for features with large dynamic ranges, as it captures diminishing returns naturally.</p> </li> <li> <p><strong>Discretization</strong>:</p> \[\phi(x) = [1[0 \leq N(x) &lt; 10], 1[10 \leq N(x) &lt; 100], \ldots]\] <p>By bucketing \(N(x)\) into intervals, this method provides flexibility while maintaining interpretability.</p> </li> </ol> <h5 id="interactions-when-features-work-together"><strong>Interactions: When Features Work Together</strong></h5> <p>Suppose we want to predict a health score based on a patient’s height \(h\) and weight \(w\). Using a feature map \(\phi(x) = [h(x), w(x)]\) assumes these features independently influence the outcome. However, it’s the relationship between height and weight (e.g., body mass index) that matters most.</p> <h6 id="approach-1-domain-specific-features"><strong>Approach 1: Domain-Specific Features</strong></h6> <p>One way to capture this interaction is to use domain knowledge, such as Robinson’s ideal weight formula: \(\text{weight(kg)} = 52 + 1.9 \cdot (\text{height(in)} - 60)\)</p> <p>We can then score the deviation between actual weight \(w\) and ideal weight: \(f(x) = \left(52 + 1.9 \cdot [h(x) - 60] - w(x)\right)^2\)</p> <p>While precise, this approach depends heavily on external knowledge and is less adaptable to other problems.</p> <h6 id="approach-2-general-interaction-terms"><strong>Approach 2: General Interaction Terms</strong></h6> <p>A more flexible solution is to include all second-order features: \(\phi(x) = [1, h(x), w(x), h(x)^2, w(x)^2, h(x)w(x)]\) This approach eliminates the need for predefined formulas, letting the model discover relationships on its own. By using interaction terms like \(h(x)w(x)\), we can model complex dependencies in the data.</p> <hr/> <h4 id="monomial-interaction-terms-a-building-block-for-nonlinearity"><strong>Monomial Interaction Terms: A Building Block for Nonlinearity</strong></h4> <p>Interaction terms are fundamental for modeling nonlinearities. Starting with an input \(x = [1, x_1, \ldots, x_d]\), we can add monomials of degree \(M\), such as:</p> \[x_1^{p_1} \cdot x_2^{p_2} \cdots x_d^{p_d}, \quad \text{where } p_1 + \cdots + p_d = M\] <p>For example, in a 2D space with \(M = 2\), the interaction terms would include \(x_1^2, x_2^2, \text{and } x_1x_2\).</p> <h6 id="big-feature-spaces-challenges-and-solutions"><strong>Big Feature Spaces: Challenges and Solutions</strong></h6> <p>Adding interaction terms and monomials rapidly increases the size of the feature space. For \(d = 40\) and \(M = 8\), the number of features grows to an astronomical \(314,457,495\). Such large feature spaces bring two major challenges:</p> <ol> <li><strong>Overfitting</strong>: Addressed through regularization techniques like \(L1/L2\) penalties.</li> <li><strong>Memory and Computational Costs</strong>: Kernel methods can help handle high (or infinite) dimensional spaces efficiently by computing feature interactions implicitly.</li> </ol> <p>By leveraging feature maps and understanding the nuances of nonlinearities, we can significantly enhance the performance of linear models. In the next part, we’ll explore kernel methods and their role in handling complex feature spaces efficiently. Stay tuned, Bye 👋!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.]]></summary></entry><entry><title type="html">Demystifying SVMs - Understanding Complementary Slackness and Support Vectors</title><link href="https://monishver11.github.io/blog/2025/svm-dual-problem/" rel="alternate" type="text/html" title="Demystifying SVMs - Understanding Complementary Slackness and Support Vectors"/><published>2025-01-10T21:02:00+00:00</published><updated>2025-01-10T21:02:00+00:00</updated><id>https://monishver11.github.io/blog/2025/svm-dual-problem</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/svm-dual-problem/"><![CDATA[<p>At the heart of SVMs lies a fascinating optimization framework that balances maximizing the margin between classes and minimizing classification errors. This post dives into the dual formulation of the SVM optimization problem, exploring its mathematical underpinnings, derivation, and insights.</p> <hr/> <h4 id="the-svm-primal-problem"><strong>The SVM Primal Problem</strong></h4> <p>To understand the dual problem, we first start with the <strong>primal optimization problem</strong> of SVMs. It aims to find the optimal hyperplane that separates two classes while allowing for some misclassification through slack variables. The primal problem is expressed as:</p> \[\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + \frac{c}{n} \sum_{i=1}^n \xi_i\] <p>subject to the constraints:</p> \[-\xi_i \leq 0 \quad \text{for } i = 1, \dots, n\] \[1 - y_i (w^T x_i + b) - \xi_i \leq 0 \quad \text{for } i = 1, \dots, n\] <p>Here:</p> <ul> <li>\(w\) is the weight vector defining the hyperplane,</li> <li>\(b\) is the bias term,</li> <li>\(\xi_i\) are slack variables that allow some points to violate the margin, and</li> <li>\(C\) is the regularization parameter controlling the trade-off between maximizing the margin and minimizing errors.</li> </ul> <h4 id="lagrangian-formulation"><strong>Lagrangian Formulation</strong></h4> <p>To solve this constrained optimization problem, we use the method of <strong>Lagrange multipliers</strong>. Introducing \(\alpha_i\) and \(\lambda_i\) as multipliers for the inequality constraints, the <strong>Lagrangian</strong> becomes:</p> \[L(w, b, \xi, \alpha, \lambda) = \frac{1}{2} \|w\|^2 + \frac{c}{n} \sum_{i=1}^n \xi_i + \sum_{i=1}^n \alpha_i \left( 1 - y_i (w^T x_i + b) - \xi_i \right) + \sum_{i=1}^n \lambda_i (-\xi_i)\] <p>Here, the terms involving \(\alpha_i\) and \(\lambda_i\) enforce the constraints, while the first term captures the objective of maximizing the margin.</p> <table> <thead> <tr> <th>Lagrange Multiplier</th> <th>Constraint</th> </tr> </thead> <tbody> <tr> <td>\(\lambda_i\)</td> <td>\(-\xi_i \leq 0\)</td> </tr> <tr> <td>\(\alpha_i\)</td> <td>\((1 - y_i[w^T x_i + b]) - \xi_i \leq 0\)</td> </tr> </tbody> </table> <hr/> <h4 id="strong-duality-and-slaters-condition"><strong>Strong Duality and Slater’s Condition</strong></h4> <p>The next step is to leverage <strong>strong duality</strong>, which states that for certain optimization problems, the dual problem provides the same optimal value as the primal. For SVMs, strong duality holds due to <strong>Slater’s constraint qualification</strong>, which requires the problem to:</p> <ul> <li>Have a convex objective function,</li> <li>Include affine constraints, and</li> <li>Possess feasible points.</li> </ul> <p>In the context of <strong>Slater’s constraint qualification</strong> and <strong>strong duality</strong> for SVMs, <strong>feasible points</strong> refer to points in the feasible region that satisfy all the constraints of the primal optimization problem. Specifically, for SVMs, these points are:</p> <ol> <li> <p><strong>Convex Objective Function</strong>: The objective of the SVM (maximizing the margin, which is a quadratic optimization problem) is convex, meaning it has a global minimum.</p> </li> <li> <p><strong>Affine Constraints</strong>: These constraints are linear equations (or inequalities) that define the feasible region, such as ensuring that all data points are correctly classified. In mathematical form, for each data point \(y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1\).</p> </li> <li> <p><strong>Existence of Feasible Points</strong>: There must be at least one point in the domain that satisfies all of these constraints. In SVMs, this is satisfied when the data is linearly separable, meaning there exists a hyperplane that can perfectly separate the positive and negative classes. Slater’s condition requires that there be strictly feasible points, where the constraints are strictly satisfied (i.e., not just touching the boundary of the feasible region).</p> </li> </ol> <p>For SVMs, the feasible points are those that satisfy: \(y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 \quad \text{for all data points}\)</p> <p>These points are strictly inside the feasible region, meaning there is a margin between the hyperplane and the data points, ensuring a gap.</p> <p>In practical terms, <strong>Slater’s condition</strong> implies that there exists a hyperplane that not only separates the two classes but also satisfies the strict inequalities for the margin (i.e., it does not lie on the boundary). This strict feasibility is critical for the <strong>strong duality</strong> theorem to hold.</p> <h4 id="deriving-the-svm-dual-function"><strong>Deriving the SVM Dual Function</strong></h4> <p>The dual function is obtained by minimizing the Lagrangian over the primal variables \(w\), \(b\), and \(\xi\):</p> \[g(\alpha, \lambda) = \inf_{w, b, \xi} L(w, b, \xi, \alpha, \lambda)\] <p>This can be simplified to (after shuffling and grouping):</p> \[g(\alpha, \lambda) = \inf_{w, b, \xi} \left[ \frac{1}{2} w^T w + \sum_{i=1}^n \xi_i \left( \frac{c}{n} - \alpha_i - \lambda_i \right) + \sum_{i=1}^n \alpha_i \left( 1 - y_i \left[ w^T x_i + b \right] \right) \right]\] <p>This minimization leads to the following <strong>first-order optimality conditions</strong>:</p> <ol> <li> <p><strong>Gradient with respect to \(w\):</strong> Differentiating \(L\) with respect to \(w\), we get:</p> \[\frac{\partial L}{\partial w} = w - \sum_{i=1}^n \alpha_i y_i x_i = 0\] <p>Solving for \(w\), we find:</p> \[w = \sum_{i=1}^n \alpha_i y_i x_i\] </li> <li> <p><strong>Gradient with respect to \(b\):</strong> Differentiating \(L\) with respect to \(b\), we obtain:</p> \[\frac{\partial L}{\partial b} = -\sum_{i=1}^n \alpha_i y_i = 0\] <p>This implies the constraint:</p> \[\sum_{i=1}^n \alpha_i y_i = 0\] </li> <li> <p><strong>Gradient with respect to \(\xi_i\):</strong> Differentiating \(L\) with respect to \(\xi_i\), we have:</p> \[\frac{\partial L}{\partial \xi_i} = \frac{c}{n} - \alpha_i - \lambda_i = 0\] <p>This leads to the relationship:</p> \[\alpha_i + \lambda_i = \frac{c}{n}\] </li> </ol> <h4 id="the-svm-dual-problem"><strong>The SVM Dual Problem</strong></h4> <p>Substituting these conditions back into \(L\)(Lagrangian), the second term disappears.</p> <p>First and third terms become:</p> \[\frac{1}{2}w^T w = \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j x_j^T x_i\] \[\sum_{i=1}^n \alpha_i \left( 1 - y_i \left[ w^T x_i + b \right] \right) = \sum_i \alpha_i - \sum_{i,j} \alpha_i \alpha_j y_i y_j x_j^T x_i - b \sum_{i=1}^n \alpha_i y_i\] <p>Putting it together, the dual function is:</p> \[g(\alpha, \lambda) = \begin{cases} \sum_{i=1}^{n} \alpha_i - \frac{1}{2}\sum_{i,j=1}^{n} \alpha_i \alpha_j y_i y_j x_j^T x_i &amp; \text{if } \sum_{i=1}^{n} \alpha_i y_i = 0 \text{ and } \alpha_i + \lambda_i = \frac{c}{n}, \text{ all } i \\ -\infty &amp; \text{otherwise} \end{cases}\] <p><strong>Quick tip</strong>: Go ahead and write the derivation yourself to see what cancels out. It’s much easier to follow the flow this way, and you’ll better understand how the second term in the equation above is derived.</p> <p><strong>The dual problem is</strong></p> \[\sup_{\alpha, \lambda \geq 0} g(\alpha, \lambda)\] \[\text{s.t. } \begin{cases} \sum_{i=1}^{n} \alpha_i y_i = 0 \\ \alpha_i + \lambda_i = \frac{c}{n}, \text{ } \alpha_i, \lambda_i \geq 0, \text{ } i = 1, ..., n \end{cases}\] <p>Don’t stress over this complex equation; we’ll break down its meaning and significance as we continue. Keep reading!</p> <h5 id="insights-from-the-dual-problem"><strong>Insights from the Dual Problem</strong></h5> <p>The dual problem offers several key insights into the optimization process of SVMs:</p> <ol> <li> <p><strong>Duality and Optimality:</strong><br/> Strong duality ensures that the primal and dual problems yield the same optimal value, provided conditions like Slater’s are met.</p> </li> <li> <p><strong>Dual Variables:</strong><br/> The variables \(\alpha_i\) and \(\lambda_i\) are Lagrange multipliers, indicating how sensitive the objective function is to the constraints. Large \(\alpha_i\) values correspond to constraints that are most violated.</p> </li> <li> <p><strong>Constraint Interpretation:</strong><br/> The constraint \(\sum_{i=1}^{n} \alpha_i y_i = 0\) ensures the hyperplane passes through the origin, while \(\alpha_i + \lambda_i = \frac{c}{n}\) connects the dual variables with the regularization parameter \(c\).</p> </li> <li> <p><strong>Support Vectors:</strong><br/> Non-zero \(\alpha_i\) values indicate support vectors, which are the data points closest to the decision boundary and crucial for defining the margin.</p> </li> <li> <p><strong>Weight Vector Representation:</strong><br/> The weight vector \(w\) lies in the space spanned by the support vectors: \(w = \sum_{i=1}^n \alpha_i y_i x_i\)</p> </li> </ol> <p>In essence, the dual problem simplifies the primal by focusing on constraints and provides insights into how data points affect the model’s decision boundary.</p> <h4 id="kkt-conditions"><strong>KKT Conditions</strong></h4> <p>For convex problems, if Slater’s condition is satisfied, the <strong>Karush-Kuhn-Tucker (KKT) conditions</strong> provide necessary and sufficient conditions for the optimal solution. For the <strong>SVM dual problem</strong>, these conditions can be expressed as:</p> <ul> <li> <p><strong>Primal Feasibility:</strong> \(f_i(x) \leq 0 \quad \forall i\)<br/> This condition ensures that the constraints of the primal problem are satisfied. In the context of SVM, this means that all data points are correctly classified by the hyperplane, i.e., for each data point \(i\), the constraint \(y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1\) is satisfied.</p> </li> <li> <p><strong>Dual Feasibility:</strong> \(\lambda_i \geq 0\)<br/> This condition ensures that the dual variables (Lagrange multipliers) are non-negative. For SVMs, it means the Lagrange multipliers \(\alpha_i\) associated with the classification constraints must be non-negative, i.e., \(\alpha_i \geq 0\).</p> </li> <li> <p><strong>Complementary Slackness:</strong> \(\lambda_i f_i(x) = 0\)<br/> This condition means that either the constraint is <strong>active</strong> (i.e., the constraint is satisfied with equality) or the dual variable is zero. For SVMs, it implies that if a data point is a support vector (i.e., it lies on the margin), then the corresponding \(\alpha_i\) is positive. Otherwise, for non-support vectors, \(\alpha_i = 0\).</p> </li> <li> <p><strong>First-Order Condition:</strong> \(\frac{\partial}{\partial x} L(x, \lambda) = 0\)<br/> The first-order condition ensures that the Lagrangian \(L(x, \lambda)\) is minimized with respect to the optimization variables. In SVMs, this condition leads to the optimal weights \(\mathbf{w}\) and bias \(b\) that define the separating hyperplane.</p> </li> </ul> <p><strong>To summarize</strong>:</p> <ul> <li><strong>Slater’s Condition</strong> ensures strong duality.</li> <li><strong>KKT Conditions</strong> ensure the existence of the optimal solution and give the specific conditions under which the solution occurs.</li> </ul> <h4 id="the-svm-dual-solution"><strong>The SVM Dual Solution</strong></h4> <p>We can express the <strong>SVM dual problem</strong> as follows:</p> \[\sup_{\alpha} \sum_{i=1}^{n} \alpha_{i} - \frac{1}{2} \sum_{i,j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{j}^{T} x_{i}\] <p>subject to:</p> \[\sum_{i=1}^{n} \alpha_{i} y_{i} = 0\] \[\alpha_{i} \in [0, \frac{c}{n}], \quad i = 1, ..., n\] <p>In this formulation, \(\alpha_i\) are the Lagrange multipliers, which must satisfy the constraints. The dual problem maximizes the objective function involving these multipliers, while ensuring that the constraints are met.</p> <p>Once we have the optimal solution \(\alpha^*\) to the dual problem, the primal solution \(w^*\) can be derived as:</p> \[w^{*} = \sum_{i=1}^{n} \alpha_{i}^{*} y_{i} x_{i}\] <p>This shows that the optimal weight vector \(w^*\) is a linear combination of the input vectors \(x_i\), weighted by the corresponding \(\alpha_i^*\) and \(y_i\).</p> <p>It’s important to note that the solution is in the <strong>space spanned by the inputs</strong>. This means the decision boundary is influenced by the data points that lie closest to the hyperplane, i.e., the <strong>support vectors</strong>.</p> <p>The constraints \(\alpha_{i} \in [0, c/n]\) indicate that \(c\) controls the maximum weight assigned to each example. In other words, \(c\) acts as a regularization parameter, controlling the trade-off between achieving a large margin and minimizing classification errors. A larger \(c\) leads to less regularization, allowing the model to fit more closely to the training data, while a smaller \(c\) introduces more regularization, promoting a simpler model that may generalize better.</p> <p>Think of \(c\) as a <strong>“penalty meter”</strong> that controls how much you care about fitting the training data:</p> <ul> <li>A <strong>high \(c\)</strong> means you are <strong>less tolerant of mistakes</strong>. The model will try to fit the data perfectly, even if it leads to overfitting (less regularization).</li> <li>A <strong>low \(c\)</strong> means you’re more focused on <strong>simplicity and generalization</strong>. The model will allow some mistakes in the training data to avoid overfitting and create a smoother decision boundary (more regularization).</li> </ul> <p>Next, we will explore how the <strong>Complementary Slackness</strong> condition in the SVM dual formulation extends to <strong>kernel trick</strong>, enabling SVMs to handle non-linear decision boundaries effectively.</p> <hr/> <h4 id="understanding-complementary-slackness-in-svms"><strong>Understanding Complementary Slackness in SVMs</strong></h4> <p>In this section, we will focus on <strong>complementary slackness</strong>, a key property of optimization problems, and its implications for SVMs. Then we will explore how it connects with the margin, slack variables, and the role of support vectors.</p> <h5 id="revisiting-constraints-and-lagrange-multipliers"><strong>Revisiting Constraints and Lagrange Multipliers</strong></h5> <p>To understand complementary slackness, let’s start by recalling the constraints and Lagrange multipliers in the SVM problem:</p> <ol> <li> <p>The constraint on the slack variables:</p> \[-\xi_i \leq 0,\] <p>with Lagrange multiplier \(\lambda_i\).</p> </li> <li> <p>The margin constraint:</p> \[1 - y_i f(x_i) - \xi_i \leq 0,\] <p>with Lagrange multiplier \(\alpha_i\).</p> </li> </ol> <p>From the <strong>first-order condition</strong> with respect to \(\xi_i\), we derived the relationship:</p> \[\lambda_i^* = \frac{c}{n} - \alpha_i^*,\] <p>where \(c\) is the regularization parameter.</p> <p>By <strong>strong duality</strong>, the complementary slackness conditions must hold, which state:</p> \[\alpha_i^* \left( 1 - y_i f^*(x_i) - \xi_i^* \right) = 0\] <p>and,</p> \[\lambda_i^* \xi_i^* = \left( \frac{c}{n} - \alpha_i^* \right) \xi_i^* = 0\] <p>These conditions essentially enforce that either the constraints are satisfied exactly or their corresponding Lagrange multipliers vanish.</p> <h5 id="what-does-complementary-slackness-tell-us"><strong>What Does Complementary Slackness Tell Us?</strong></h5> <p>Complementary slackness provides crucial insights into the relationship between the dual variables \(\alpha_i^*\), the slack variables \(\xi_i^*\), and the margin \(1 - y_i f^*(x_i)\). Let’s break this down:</p> <ul> <li><strong>When \(y_i f^*(x_i) &gt; 1\):</strong> <ul> <li>The margin loss is zero (\(\xi_i^* = 0\)), meaning the data point is correctly classified and lies outside the margin.</li> <li>As a result, \(\alpha_i^* = 0\). Since the dual variable \(\alpha_i^*\) only applies to active constraints, a zero value indicates that this example has no effect on the decision boundary. These are non-support vectors that do not influence the margin or hyperplane.</li> </ul> </li> <li><strong>When \(y_i f^*(x_i) &lt; 1\):</strong> <ul> <li>The margin loss is positive (\(\xi_i^* &gt; 0\)), meaning the data point either lies inside the margin or is misclassified.</li> <li>In this case, \(\alpha_i^* = \frac{c}{n}\), assigning the maximum weight to these examples. These points are critical as they represent either boundary violations or significant misclassifications, making them influential in determining the hyperplane.</li> </ul> </li> <li><strong>When \(\alpha_i^* = 0\):</strong> <ul> <li>This implies \(\xi_i^* = 0\), meaning the margin loss is zero and the data point satisfies \(y_i f^*(x_i) \geq 1\).</li> <li>Such examples are correctly classified and lie well outside the margin, contributing nothing to the optimization. They remain irrelevant to the final decision boundary.</li> </ul> </li> <li><strong>When \(\alpha_i^* \in (0, \frac{c}{n})\):</strong> <ul> <li>This implies \(\xi_i^* = 0\), so the example lies exactly on the margin, where \(1 - y_i f^*(x_i) = 0\).</li> <li>These are the <strong>support vectors</strong>, the critical points that define the hyperplane. Their non-zero \(\alpha_i^*\) values indicate their contribution to maximizing the margin while satisfying the constraints.</li> </ul> </li> </ul> <p><strong>Why It Matters?</strong> Complementary slackness essentially acts as a filter, identifying which examples influence the decision boundary (support vectors) and which do not. It helps focus only on the most relevant points, reducing computational complexity and enhancing the interpretability of the model.</p> <p>We can summarize these relationships(between margin and example weights) as follows:</p> <ol> <li><strong>If \(\alpha_i^* = 0\):</strong> The example satisfies \(y_i f^*(x_i) \geq 1\), indicating no margin loss.</li> <li><strong>If \(\alpha_i^* \in (0, \frac{c}{n})\):</strong> The example lies exactly on the margin, with \(y_i f^*(x_i) = 1\).</li> <li><strong>If \(\alpha_i^* = \frac{c}{n}\):</strong> The example incurs a margin loss, with \(y_i f^*(x_i) \leq 1\).</li> </ol> <p>and the other way is:</p> \[y_if^*(x_i) &lt; 1 \Rightarrow α_i^* = \frac{c}{n}\] \[y_if^*(x_i) = 1 \Rightarrow α_i^* \in [0, \frac{c}{n}]\] \[y_if^*(x_i) &gt; 1 \Rightarrow α_i^* = 0\] <p>These relationships are foundational to understanding how SVMs allocate weights to examples and define the decision boundary.</p> <h5 id="analogy-tug-of-war-with-a-rope"><strong>Analogy: Tug of War with a Rope</strong></h5> <p>Imagine a tug-of-war game where each data point is trying to pull a rope (the decision boundary) towards itself. The strength of the pull (weight \(\alpha_i^*\)) depends on how far the point is from the ideal margin position:</p> <ol> <li><strong>If the point is far outside the margin (\(y_i f^*(x_i) &gt; 1\)):</strong> <ul> <li><strong>No pull (\(\alpha_i^* = 0\)):</strong><br/> The point is satisfied with its position and doesn’t exert any force on the rope. It’s correctly classified and irrelevant to defining the decision boundary.</li> </ul> </li> <li><strong>If the point is exactly on the margin (\(y_i f^*(x_i) = 1\)):</strong> <ul> <li><strong>Light pull (\(\alpha_i^* \in [0, \frac{c}{n}]\)):</strong><br/> The point contributes just enough force to keep the rope in its place. These are the <strong>support vectors</strong>, the critical points holding the boundary in position.</li> </ul> </li> <li><strong>If the point is inside the margin or misclassified (\(y_i f^*(x_i) &lt; 1\)):</strong> <ul> <li><strong>Maximum pull (\(\alpha_i^* = \frac{c}{n}\)):</strong><br/> The point exerts its full force, pulling the boundary to correct the violation. These points dominate the optimization problem because they need the most adjustment.</li> </ul> </li> </ol> <p>Looking at it from the perspective of \(y_i f^*(x_i)\):</p> <ul> <li><strong>\(y_i f^*(x_i) &gt; 1\):</strong> No pull (\(\alpha_i^* = 0\)) – the point is far and satisfied.</li> <li><strong>\(y_i f^*(x_i) = 1\):</strong> Light pull (\(\alpha_i^* \in [0, \frac{c}{n}]\)) – the point is holding the margin.</li> <li><strong>\(y_i f^*(x_i) &lt; 1\):</strong> Maximum pull (\(\alpha_i^* = \frac{c}{n}\)) – the point is violating the margin.</li> </ul> <p>This helps you remember which data points influence the decision boundary and how they do so.</p> <hr/> <h4 id="support-vectors-the-pillars-of-svms"><strong>Support Vectors: The Pillars of SVMs</strong></h4> <p>The dual formulation of SVMs reveals that the weight vector \(w^*\) can be expressed as:</p> \[w^* = \sum_{i=1}^n \alpha_i^* y_i x_i.\] <p>Here, the examples \(x_i\) with \(\alpha_i^* &gt; 0\) (Few margin errors or “on the margin”) are known as <strong>support vectors</strong>. These are the critical data points that determine the hyperplane. Examples with \(\alpha_i^* = 0\) do not influence the solution, leading to <strong>sparsity</strong> in the SVM model. This sparsity is one of the key reasons why SVMs are computationally efficient for large datasets.</p> <h5 id="the-role-of-inner-products-in-the-dual-problem"><strong>The Role of Inner Products in the Dual Problem</strong></h5> <p>An intriguing aspect of the dual problem is that it depends on the input data \(x_i\) and \(x_j\) only through their <strong>inner product</strong>:</p> \[\langle x_j, x_i \rangle = x_j^T x_i.\] <p>This dependence on inner products allows us to generalize SVMs using <strong>kernel methods</strong>, where the inner product \(x_j^T x_i\) is replaced with a kernel function \(K(x_j, x_i)\). Kernels enable SVMs to implicitly operate in high-dimensional feature spaces without explicitly transforming the data, making it possible to model complex, non-linear decision boundaries.</p> <p>The kernelized dual problem is written as:</p> \[\sup_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^{n} \alpha_i \alpha_j y_i y_j K(x_j, x_i),\] <p>subject to:</p> <ul> <li>\(\sum_{i=1}^n \alpha_i y_i = 0\),</li> <li>\(0 \leq \alpha_i \leq \frac{c}{n}\), for \(i = 1, \dots, n\).</li> </ul> <p>We’ll dive into kernels next and explore how this powerful trick enhances the usefulness of SVMs.</p> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>Complementary slackness conditions reveal much about the structure and workings of SVMs. They show how the margin, slack variables, and dual variables interact and highlight the pivotal role of support vectors. Moreover, the reliance on inner products paves the way for kernel methods, unlocking the power of SVMs for non-linear classification problems.</p> <p>In the next post, we’ll explore kernel functions in depth, including popular choices like Gaussian and polynomial kernels, and see how they influence SVM performance. See you!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Math parts verification</li> <li><a href="https://www.stat.cmu.edu/~ryantibs/convexopt-F16/scribes/kkt-scribed.pdf"> KKT conditions - KKT conditions</a></li> <li><a href="https://math.stackexchange.com/questions/2162932/big-picture-behind-how-to-use-kkt-conditions-for-constrained-optimization">Big picture behind how to use KKT conditions for constrained optimization</a></li> <li><a href="https://davidrosenberg.github.io/mlcourse/Archive/2019/Notes/SVM-main-points.pdf">SVM: Main Takeaways from Duality</a></li> <li><a href="https://davidrosenberg.github.io/mlcourse/Archive/2019/Notes/convex-optimization.pdf">Extreme Abridgment of Boyd and Vandenberghe’s Convex Optimization</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.]]></summary></entry><entry><title type="html">Subgradient and Subgradient Descent</title><link href="https://monishver11.github.io/blog/2025/subgradient/" rel="alternate" type="text/html" title="Subgradient and Subgradient Descent"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://monishver11.github.io/blog/2025/subgradient</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/subgradient/"><![CDATA[<p>Optimization is a cornerstone of machine learning, as it allows us to fine-tune models and minimize error. For smooth, differentiable functions, gradient descent is the go-to method. However, in the real world, many functions are not differentiable. This is where <strong>subgradients</strong> come into play. In this post, we’ll explore subgradients, understand their properties, and see how they are used in subgradient descent. Finally, we’ll dive into their application in support vector machines (SVMs).</p> <hr/> <h4 id="first-order-condition-for-convex-differentiable-functions"><strong>First-Order Condition for Convex, Differentiable Functions</strong></h4> <p>Let’s start with the basics of convex functions. For a function \(f : \mathbb{R}^d \to \mathbb{R}\) that is convex and differentiable, the <strong>first-order condition</strong> states that:</p> \[f(y) \geq f(x) + \nabla f(x)^\top (y - x), \quad \forall x, y \in \mathbb{R}^d\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Subgradient_1-480.webp 480w,/assets/img/Subgradient_1-800.webp 800w,/assets/img/Subgradient_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Subgradient_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Subgradient_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This equation tells us something profound: the linear approximation to \(f\) at \(x\) is a global underestimator of the function. In other words, the gradient provides a plane below the function, ensuring the convexity property holds. A direct implication is that if \(\nabla f(x) = 0\), then \(x\) is a global minimizer of \(f\). This serves as the foundation for gradient-based optimization.</p> <h4 id="subgradients-a-generalization-of-gradients"><strong>Subgradients: A Generalization of Gradients</strong></h4> <p>While gradients work well for differentiable functions, what happens when a function has kinks or sharp corners? This is where <strong>subgradients</strong> step in. A subgradient is a generalization of the gradient, defined as follows:</p> <p>A vector \(g \in \mathbb{R}^d\) is a subgradient of a convex function \(f : \mathbb{R}^d \to \mathbb{R}\) at \(x\) if:</p> \[f(z) \geq f(x) + g^\top (z - x), \quad \forall z \in \mathbb{R}^d\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Subgradient_2-480.webp 480w,/assets/img/Subgradient_2-800.webp 800w,/assets/img/Subgradient_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Subgradient_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Subgradient_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Subgradients essentially maintain the same idea as gradients: they provide a linear function that underestimates \(f\). However, while the gradient is unique, a subgradient can belong to a set of possible vectors called the <strong>subdifferential</strong>, denoted \(\partial f(x)\).</p> <p>For convex functions, the subdifferential is always non-empty. If \(f\) is differentiable at \(x\), the subdifferential collapses to a single point: \(\{ \nabla f(x) \}\). Importantly, for a convex function, a point \(x\) is a global minimizer if and only if \(0 \in \partial f(x)\). This property allows us to apply subgradients even when gradients don’t exist.</p> <hr/> <p><strong>To build an understanding for subgradients</strong>, think of them as a safety net for optimization when the function isn’t smooth. Imagine you’re skiing down a mountain, and instead of a smooth slope, you encounter a flat plateau or a jagged edge. A regular gradient would fail to guide you because there’s no single steepest slope at such points. Subgradients, however, provide a range of valid directions — all the paths you could safely take without “climbing uphill.”</p> <p>In mathematical terms, subgradients are generalizations of gradients. For a non-differentiable function, the subdifferential \(\partial f(x)\) at a point \(x\) is the set of all possible subgradients. Each of these subgradients \(g\) satisfies:</p> \[f(z) \geq f(x) + g^\top (z - x), \quad \forall z \in \mathbb{R}^d\] <p>This means that every subgradient defines a hyperplane that lies below the function \(f(x)\). It acts as a valid approximation of the function’s behavior, even at sharp corners or flat regions.</p> <p>Here’s a useful way to interpret subgradients: if you were to try and “push” the function \(f(x)\) upward using any of the subgradients at \(x\), you would never exceed the true value of \(f(z)\). Subgradients provide all the slopes that respect this property, even when the function isn’t smooth.</p> <p>Finally, the key insight: if the zero vector is in the subdifferential, \(0 \in \partial f(x)\), it means that there’s no descent direction left — you’ve reached the global minimum. Subgradients help us navigate these tricky, non-smooth terrains where gradients fail, making them a versatile tool in optimization.</p> <h5 id="subgradient-example-absolute-value-function"><strong>Subgradient Example: Absolute Value Function</strong></h5> <p>Let’s consider a classic example: \(f(x) = \lvert x \rvert\). This function is non-differentiable at \(x = 0\), making it an ideal candidate to illustrate subgradients. The subdifferential of \(f(x)\) is:</p> \[\partial f(x) = \begin{cases} \{-1\} &amp; \text{if } x &lt; 0, \\ \{1\} &amp; \text{if } x &gt; 0, \\ [-1, 1] &amp; \text{if } x = 0. \end{cases}\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Subgradient_3-480.webp 480w,/assets/img/Subgradient_3-800.webp 800w,/assets/img/Subgradient_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Subgradient_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Subgradient_3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The plot on the right shows: $$ \{(x, g) \mid x \in \mathbb{R}, g \in \partial f(x)\} $$ </div> <p>At \(x = 0\), the subgradient set contains all values in the interval \([-1, 1]\), which corresponds to all possible slopes of lines that underapproximate \(f(x)\) at that point.</p> <h4 id="subgradient-descent-the-optimization-method"><strong>Subgradient Descent: The Optimization Method</strong></h4> <p>Subgradient descent extends gradient descent to non-differentiable convex functions. The update rule is simple:</p> \[x_{t+1} = x_t - \eta_t g,\] <p>where \(g \in \partial f(x_t)\) is a subgradient, and \(\eta_t &gt; 0\) is the step size.</p> <p>Unlike gradients, subgradients do not always converge to zero as the algorithm progresses. <strong>Why?</strong> This is because subgradients are not as tightly coupled to the function’s local geometry as gradients are. In gradient descent, the gradient vanishes at critical points (e.g., minima, maxima, or saddle points), forcing the updates to slow down as the algorithm approaches a minimum. However, in subgradient descent, the subgradients remain non-zero even near a minimum due to the nature of subdifferentiability.</p> <p>For example, consider the absolute value function, \(f(x) = \lvert x \rvert\), which has a sharp corner at \(x = 0\). The subdifferential at \(x = 0\) is the interval \([-1, 1]\), meaning that even at the minimizer \(x^\ast = 0\), valid subgradients exist (e.g., \(g = 0.5\)). Consequently, the algorithm does not inherently “slow down” as it approaches the minimum, unlike gradient descent.</p> <p>This behavior requires us to carefully choose a decreasing step size \(\eta_t\) to make sure that the updates shrink over time, leading to convergence.</p> <p>For convex functions, subgradient descent ensures that the iterates move closer to the minimizer:</p> \[\|x_{t+1} - x^\ast\| &lt; \|x_t - x^\ast\|,\] <p>provided that the step size is small enough. However, subgradient methods tend to be slower than gradient descent because they rely on weaker information about the function’s structure.</p> <h4 id="subgradient-descent-for-svms-the-pegasos-algorithm"><strong>Subgradient Descent for SVMs: The Pegasos Algorithm</strong></h4> <p>Subgradients are particularly useful in optimizing the objective function of support vector machines (SVMs). The SVM objective is:</p> \[J(w) = \frac{1}{n} \sum_{i=1}^n \max(0, 1 - y_i w^\top x_i) + \frac{\lambda}{2} \|w\|^2,\] <p>where the first term represents the hinge loss, and the second term penalizes large weights to prevent overfitting. Optimizing this objective using gradient-based methods is tricky due to the non-differentiability of the hinge loss. Enter <strong>Pegasos</strong>, a stochastic subgradient descent algorithm.</p> <h5 id="the-pegasos-algorithm"><strong>The Pegasos Algorithm</strong></h5> <p>The Pegasos algorithm follows these steps:</p> <ol> <li>Initialize \(w_1 = 0\), \(t = 0\) and \(\lambda &gt; 0\)</li> <li>Randomly permute the data at the beginning of each epoch.</li> <li>While termination condition not met: <ul> <li>For each data point \((x_j, y_j)\), update the parameters: <ul> <li>Increment \(t\): \(t = t + 1\).</li> <li>Compute the step size: \(\eta_t = \frac{1}{t \lambda}\).</li> <li>If \(y_j w_t^\top x_j &lt; 1\), update: \(w_{t+1} = (1 - \eta_t \lambda) w_t + \eta_t y_j x_j.\)</li> <li>Otherwise, update: \(w_{t+1} = (1 - \eta_t \lambda) w_t.\)</li> </ul> </li> </ul> </li> </ol> <p>This algorithm cleverly combines subgradient updates with a decreasing step size to ensure convergence. The hinge loss ensures that the model maintains a margin of separation, while the regularization term prevents overfitting. By carefully adjusting the step size \(\eta_t = \frac{1}{t \lambda}\), Pegasos ensures convergence to the optimal \(w\) while handling the non-differentiable hinge loss.</p> <h6 id="derivation-for-both-cases"><strong>Derivation for Both Cases:</strong></h6> <ol> <li> <p><strong>Case 1: \(y_j w_t^\top x_j &lt; 1\) (misclassified or on the margin)</strong></p> <p>The subgradient of the hinge loss for this case is:</p> \[\nabla \text{hinge loss} = -y_j x_j.\] <p>Adding the subgradient of the regularization term:</p> \[\nabla J(w) = -y_j x_j + \lambda w_t.\] <p>Using the subgradient descent update rule:</p> \[w_{t+1} = w_t - \eta_t \nabla J(w),\] <p>we get:</p> \[w_{t+1} = w_t - \eta_t (-y_j x_j + \lambda w_t).\] <p>Simplifying:</p> \[w_{t+1} = (1 - \eta_t \lambda) w_t + \eta_t y_j x_j.\] </li> <li> <p><strong>Case 2: \(y_j w_t^\top x_j \geq 1\) (correctly classified)</strong></p> <p>In this case, the hinge loss is zero, so the subgradient is purely from the regularization term:</p> \[\nabla J(w) = \lambda w_t.\] <p>The subgradient descent update rule becomes:</p> \[w_{t+1} = w_t - \eta_t \nabla J(w).\] <p>Substituting \(\nabla J(w) = \lambda w_t\):</p> \[w_{t+1} = w_t - \eta_t \lambda w_t.\] <p>Simplifying:</p> \[w_{t+1} = (1 - \eta_t \lambda) w_t.\] </li> </ol> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>Subgradients are a powerful tool for dealing with non-differentiable convex functions, and subgradient descent provides a straightforward yet effective way to optimize such functions. While slower than gradient descent, subgradient descent shines in scenarios where gradients are unavailable.</p> <p>In the next part of this series, we’ll delve into the <strong>dual problem</strong> and uncover its connection to the primal SVM formulation. Stay tuned for more insights into the fascinating world of ML!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.]]></summary></entry><entry><title type="html">The Dual Problem of SVM</title><link href="https://monishver11.github.io/blog/2025/dual-problem/" rel="alternate" type="text/html" title="The Dual Problem of SVM"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://monishver11.github.io/blog/2025/dual-problem</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/dual-problem/"><![CDATA[<p>In machine learning, optimization problems often present themselves as key challenges. For example, when training a <strong>Support Vector Machine (SVM)</strong>, we are tasked with finding the optimal hyperplane that separates two classes in a high-dimensional feature space. While we can solve this directly using methods like subgradient descent, we can also leverage a more analytical approach through <strong>Quadratic Programming (QP)</strong> solvers.</p> <p>Moreover, for convex optimization problems, there is a powerful technique known as solving the <strong>dual problem</strong>. Understanding this duality is not only essential for theory, but it also offers computational advantages. In this blog, we’ll dive into the dual formulation of SVM and its implications.</p> <hr/> <h4 id="svm-as-a-quadratic-program"><strong>SVM as a Quadratic Program</strong></h4> <p>To understand the dual problem of SVM, let’s first revisit the primal optimization problem for SVMs. The goal of an SVM is to find the hyperplane that maximizes the margin between two classes. The optimization problem can be written as:</p> \[\begin{aligned} \min_{w, b, \xi} \quad &amp; \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\ \text{subject to} \quad &amp; -\xi_i \leq 0 \quad \text{for } i = 1, \dots, n, \\ &amp; 1 - y_i (w^T x_i + b) - \xi_i \leq 0 \quad \text{for } i = 1, \dots, n, \end{aligned}\] <p><strong>Primal</strong>—sounds technical, right? Here’s what it means: The <strong>primal problem</strong> refers to the original formulation of the optimization problem in terms of the decision variables (in this case, \(w\), \(b\), and \(\xi\)) and the objective function. It is called “primal” because it directly represents the problem without transformation. The primal form of SVM is concerned with finding the optimal hyperplane parameters that minimize the classification error while balancing the margin size.</p> <h5 id="breakdown-of-the-problem"><strong>Breakdown of the Problem</strong></h5> <ul> <li> <p><strong>Objective Function</strong>: The term \(\frac{1}{2} \|w\|^2\) is a regularization term that aims to minimize the complexity of the model, ensuring that the hyperplane is as “wide” as possible. The second term, \(C \sum_{i=1}^n \xi_i\), penalizes the violations of the margin (through slack variables \(\xi_i\)) and controls the trade-off between margin size and misclassification.</p> </li> <li> <p><strong>Constraints</strong>: The constraints consist of two parts:</p> <ul> <li>The first part ensures that slack variables are non-negative: \(-\xi_i \leq 0\), meaning that each slack variable must be at least zero. <strong>Why?</strong> The slack variables represent the margin violations, and they must be non-negative because they quantify how much a data point violates the margin, which cannot be negative.</li> <li>The second part enforces that the data points are correctly classified or their margin violations are captured by \(\xi_i\). For each data point \(i\), the condition \(1 - y_i (w^T x_i + b) - \xi_i \leq 0\) must hold. Here, \(y_i\) is the true label of the data point, and \(w^T x_i + b\) represents the signed distance of the point from the hyperplane. If the point is correctly classified and lies outside the margin (i.e., its signed distance from the hyperplane is greater than 1), the constraint holds true. If the point is misclassified or falls inside the margin, the slack variable \(\xi_i\) will account for this violation.</li> </ul> </li> </ul> <p>This problem has a <strong>differentiable objective function</strong>, <strong>affine constraints</strong>, and includes a number of <strong>unknowns</strong> that can be solved using Quadratic Programming (QP) solvers.</p> <p>So, <strong>Quadratic Programming (QP)</strong> is an optimization problem where the objective function is quadratic (i.e., includes squared terms like \(\|w\|^2\)), and the constraints are linear. In the context of SVM, QP is utilized because the objective function involves the squared norm of \(w\) (which is quadratic), and the constraints are linear inequalities.</p> <p>The QP formulation for SVM involves minimizing a quadratic objective function (with respect to \(w\), \(b\), and \(\xi\)) subject to linear constraints. Now, while QP solvers provide an efficient way to tackle this problem, let’s explore the <strong>dual problem(next)</strong> to gain further insights.</p> <p>But, why bother with the <strong>dual problem</strong>? Here’s why it’s worth the dive:</p> <ol> <li> <p><strong>Efficient Computation with Kernels</strong>: The dual formulation focuses on Lagrange multipliers and inner products between data points, rather than directly optimizing \(w\) and \(b\). This is particularly beneficial when using kernels, as it avoids explicit computation in high-dimensional spaces. For non-linear problems or datasets where relationships are better captured in transformed spaces, the dual approach enables efficient computation while leveraging the kernel trick.</p> </li> <li> <p><strong>Geometrical Insights</strong>: The dual formulation emphasizes the relationship between support vectors and the margin, offering a clearer geometrical interpretation. It shows that only the support vectors (the points closest to the decision boundary) determine the optimal hyperplane.</p> </li> <li> <p><strong>Convexity and Global Optimality</strong>: The dual problem is convex, ensuring that solving it leads to the global optimal solution. This is particularly beneficial when the primal problem has a large number of variables and constraints.</p> </li> </ol> <p>In short, while QP solvers can efficiently solve the primal problem, the dual problem formulation offers computational benefits, the potential for kernel methods, and a clearer understanding of the SVM model’s properties. This makes the dual approach a powerful tool in SVM optimization.</p> <p>No need to worry about the details above—we’ll cover them step by step. For now, keep reading!</p> <hr/> <h4 id="the-lagrangian"><strong>The Lagrangian</strong></h4> <p>To begin understanding the dual problem, we need to define the <strong>Lagrangian</strong> of the optimization problem. For general inequality-constrained optimization problems, the goal is:</p> \[\begin{aligned} \min_x \quad &amp; f_0(x) \\ \text{subject to} \quad &amp; f_i(x) \leq 0, \quad i = 1, \dots, m. \end{aligned}\] <p>The corresponding <strong>Lagrangian</strong> is defined as:</p> \[L(x, \lambda) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x),\] <p>where:</p> <ul> <li>\(\lambda_i\) are the <strong>Lagrange multipliers</strong> (also known as <strong>dual variables</strong>).</li> <li>The Lagrangian function combines the objective function \(f_0(x)\) with the constraints \(f_i(x)\), weighted by the Lagrange multipliers \(\lambda_i\). These multipliers represent how much the objective function will change if we relax or tighten the corresponding constraint.</li> </ul> <h5 id="why-do-we-use-the-lagrangian"><strong>Why Do We Use the Lagrangian?</strong></h5> <p>The Lagrangian serves as a bridge between constrained and unconstrained optimization. Here’s the intuition behind its design and necessity:</p> <ol> <li><strong>Softening Hard Constraints</strong>: <ul> <li>In constrained optimization, the solution must strictly satisfy all constraints, which can make direct optimization challenging.</li> <li>By introducing Lagrange multipliers, the constraints are “softened” into penalties. This means that instead of strictly enforcing constraints during every step of optimization, we penalize deviations from the constraints in the objective function.</li> </ul> </li> <li><strong>Unified Objective</strong>: <ul> <li>The Lagrangian integrates the objective function and constraints into a single function. This allows us to handle both aspects of the problem (maximizing or minimizing while respecting constraints) in one unified framework.</li> </ul> </li> <li><strong>Flexibility</strong>: <ul> <li>The Lagrange multipliers \(\lambda_i\) provide a mechanism to adjust the influence of each constraint. If a constraint is more critical, its corresponding multiplier will have a larger value, increasing its contribution to the Lagrangian.</li> </ul> </li> <li><strong>Theoretical Insights</strong>: <ul> <li>The Lagrangian formulation is foundational to deriving the <strong>dual problem</strong>, which can sometimes simplify the original (primal) problem. It also provides deeper insights into the sensitivity of the solution to changes in the constraints.</li> </ul> </li> </ol> <p><strong>Think of it this way:</strong></p> <p>Imagine you’re managing a budget to purchase items for a project. Your primary goal is to minimize costs (the objective function), but you have constraints like a maximum budget and a minimum quality requirement for each item.</p> <ul> <li>Without the Lagrangian: You’d need to find solutions that satisfy the budget and quality constraints explicitly, which could be cumbersome.</li> <li>With the Lagrangian: You assign a penalty (via Lagrange multipliers) to every dollar you overspend or every unit of quality you fail to meet. Now, your goal is to minimize the combined cost (original cost + penalties), which naturally leads you to solutions that respect the constraints.</li> </ul> <p>In optimization terms, the Lagrangian lets us trade off between violating constraints and optimizing the objective function. This trade-off is critical in complex problems where perfect feasibility might not always be achievable during intermediate steps.</p> <h5 id="the-role-of-lagrange-multipliers"><strong>The Role of Lagrange Multipliers</strong></h5> <p>The multipliers \(\lambda_i\) play a dual role:</p> <ol> <li> <p>They measure the <strong>sensitivity</strong> of the objective function to the corresponding constraint. For instance, if increasing the limit of a constraint by a small amount significantly improves the objective, its multiplier will have a large value.</p> </li> <li> <p>They ensure that the optimal solution respects the constraints. If a constraint is not active at the solution (i.e., it is satisfied without being tight), its multiplier will be zero. This is formalized in the concept of <strong>complementary slackness</strong>, which we’ll explore later.</p> </li> </ol> <p>In essence, the Lagrangian is not just a mathematical tool; it reflects the natural balance between objectives and constraints, making it indispensable in optimization theory.</p> <hr/> <h4 id="lagrange-dual-function"><strong>Lagrange Dual Function</strong></h4> <p>Next, we define the <strong>Lagrange dual function</strong>, which plays a crucial role in deriving the dual problem. The dual function is obtained by minimizing the Lagrangian with respect to the primal variables (denoted as \(x\)):</p> \[g(\lambda) = \inf_x L(x, \lambda) = \inf_x \left[ f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) \right].\] <h5 id="what-does-this-mean"><strong>What Does This Mean?</strong></h5> <p>The Lagrange dual function \(g(\lambda)\) gives us the smallest possible value of the Lagrangian \(L(x, \lambda)\) for a given set of Lagrange multipliers \(\lambda_i \geq 0\). It represents the “best” value of the Lagrangian when we optimize over the primal variables \(x\).</p> <p>In simpler terms:</p> <ul> <li>The primal variables \(x\) are those we’re directly trying to optimize in the original problem.</li> <li>By minimizing over \(x\) for fixed \(\lambda\), we explore how well the Lagrangian balances the objective function \(f_0(x)\) and the weighted constraints \(f_i(x)\).</li> </ul> <h5 id="why-do-we-minimize-over-x"><strong>Why Do We Minimize Over \(x\)?</strong></h5> <p>The reason for this step is that it helps us decouple the influence of the primal variables \(x\) and the dual variables \(\lambda\). By focusing only on \(x\), we shift our attention to understanding the properties of the dual variables, which simplifies the problem and provides valuable insights.</p> <p>Minimizing the Lagrangian with respect to \(x\) ensures that:</p> <ol> <li><strong>Feasibility of Constraints</strong>: <ul> <li>The minimization respects the constraints \(f_i(x) \leq 0\) through the penalization mechanism introduced by \(\lambda_i\).</li> </ul> </li> <li><strong>Dual Representation</strong>: <ul> <li>The dual function \(g(\lambda)\) captures how “good” a particular choice of \(\lambda\) is in approximating the original problem.</li> </ul> </li> <li><strong>Foundation for the Dual Problem</strong>: <ul> <li>The minimization step builds the foundation for solving the optimization problem via its <strong>dual formulation</strong>, which is often simpler than the primal.</li> </ul> </li> </ol> <h5 id="why-do-we-need-this"><strong>Why Do We Need This?</strong></h5> <p>The goal of introducing the dual function is to exploit the following properties, which help us solve the original optimization problem more efficiently:</p> <ul> <li> <p><strong>Lower Bound Property</strong>: The dual function \(g(\lambda)\) provides a lower bound on the optimal value of the primal problem. If \(p^*\) is the optimal value of the primal problem, then: \(g(\lambda) \leq p^*, \quad \forall \lambda \geq 0.\) This property is useful because even if we cannot solve the primal problem directly, we can approximate its solution by maximizing \(g(\lambda)\).</p> </li> <li> <p><strong>Convexity of \(g(\lambda)\)</strong>: The dual function is always concave, regardless of whether the primal problem is convex. This makes the dual problem easier to solve using convex optimization techniques.</p> </li> </ul> <h5 id="how-does-this-work-why-does-it-work"><strong>How Does This Work? Why Does It Work?</strong></h5> <ol> <li><strong>Lower Bound Property</strong>: <ul> <li>When we minimize the Lagrangian \(L(x, \lambda)\) with respect to the primal variables \(x\), we’re essentially finding the “best possible value” of the objective function \(f_0(x)\) for a fixed choice of \(\lambda\).</li> <li>Since the dual function \(g(\lambda)\) is derived from a relaxed version of the primal problem (allowing \(\lambda_i \geq 0\) to penalize constraint violations), it cannot exceed the true optimal value \(p^*\) of the primal problem. This creates the lower bound.</li> </ul> <p><strong>Intuition</strong>: Think of the dual function as a “proxy” for the primal problem. By maximizing \(g(\lambda)\), we try to approach the primal solution as closely as possible from below.</p> </li> <li><strong>Convexity of \(g(\lambda)\)</strong>: <ul> <li>The concavity of \(g(\lambda)\) follows from its definition as the infimum (or greatest lower bound) of a family of affine functions. In optimization, operations involving infima tend to preserve convexity (or result in concavity for maximization problems).</li> <li>This property ensures that maximizing \(g(\lambda)\) is computationally efficient, even if the original primal problem is non-convex.</li> </ul> <p><strong>Intuition</strong>: The concave structure of \(g(\lambda)\) creates an inverted “bowl-shaped” surface, making it easier to find the maximum using gradient-based optimization methods.</p> </li> </ol> <h5 id="what-is-inf-and-how-is-it-different-from-min"><strong>What is \(\inf\) and How is it Different from \(\min\)?</strong></h5> <p>The <strong>infimum</strong> (denoted as \(\inf\)) is a generalization of the minimum (\(\min\)) in optimization and analysis. Here’s the distinction:</p> <ol> <li><strong>Minimum (\(\min\))</strong>: <ul> <li>The minimum is the smallest value attained by a function within its domain. It must be achieved by some point \(x\) in the domain.</li> <li>Example: For \(f(x) = x^2\) on \([0, 2]\), the minimum is \(f(0) = 0\).</li> </ul> </li> <li><strong>Infimum (\(\inf\))</strong>: <ul> <li>The infimum is the greatest lower bound of a function, but it may not be attained by any point in the domain. It represents the “smallest possible value” the function can approach, even if it doesn’t reach it.</li> <li>Example: For \(f(x) = 1/x\) on \((0, 2]\), the infimum is \(\inf f(x) = 0\), but \(f(x)\) never actually equals \(0\) within the domain.</li> </ul> </li> </ol> <h5 id="why-use-inf-instead-of-min"><strong>Why Use \(\inf\) Instead of \(\min\)?</strong></h5> <p>In the context of the dual function:</p> <ul> <li>The infimum \(\inf_x L(x, \lambda)\) is used because the Lagrangian \(L(x, \lambda)\) might not achieve a true minimum for certain values of \(\lambda\) (e.g., the domain could be open or unbounded).</li> <li>Using \(\inf\) ensures that the dual function \(g(\lambda)\) is well-defined, even in cases where \(\min_x L(x, \lambda)\) doesn’t exist.</li> </ul> <h5 id="a-lighter-take"><strong>A Lighter Take</strong></h5> <p>Think of this as “playing with math language” to get to the result we want. Just as you might rephrase a sentence to make it clearer or more persuasive, in mathematics, we transform the problem into a new form (the dual) that’s easier to work with.</p> <p>For now, trust the process. This step might seem abstract, but it leads us to a form of the problem where powerful mathematical tools can come into play. Once we see the bigger picture, the reasoning behind these transformations will become clear.</p> <p>In essence, the Lagrange dual function \(g(\lambda)\) gives us a way to shift our perspective on the optimization problem, helping us solve it through duality principles. As we proceed, you’ll see how this approach simplifies the original problem and why it’s such a powerful concept.</p> <hr/> <h4 id="the-primal-and-the-dual"><strong>The Primal and the Dual</strong></h4> <p>For any general primal optimization problem:</p> \[\begin{aligned} \min_x \quad &amp; f_0(x) \\ \text{subject to} \quad &amp; f_i(x) \leq 0, \quad i = 1, \dots, m, \end{aligned}\] <p>we can formulate the corresponding <strong>dual problem</strong> as:</p> \[\begin{aligned} \max_\lambda \quad &amp; g(\lambda) \\ \text{subject to} \quad &amp; \lambda_i \geq 0, \quad i = 1, \dots, m. \end{aligned}\] <p>The dual problem has some remarkable properties:</p> <ul> <li><strong>Convexity</strong>: The dual problem is always a <strong>convex optimization problem</strong>, even if the primal problem is not convex.</li> <li><strong>Simplification</strong>: In some cases, solving the dual problem is easier than solving the primal problem directly. This is particularly true when the primal problem is difficult to solve or the number of constraints is large.</li> </ul> <h5 id="contract-negotiation-analogy"><strong>Contract Negotiation Analogy</strong></h5> <ul> <li> <p><strong>Primal Problem (Your Terms)</strong>: Think of the primal problem as negotiating a contract where you aim to minimize costs while respecting certain constraints (like budget or timelines). You’re focused on getting the best deal for yourself under these limitations.</p> </li> <li> <p><strong>Lagrangian (Penalties for Violations)</strong>: During the negotiation, you introduce penalties — if you can’t meet a certain term, you adjust other aspects of the contract. This is similar to using Lagrange multipliers in the Lagrangian function to penalize constraint violations.</p> </li> <li> <p><strong>Dual Problem (Value Assessment)</strong>: In the dual problem, you step into the other party’s shoes and assess how much value they assign to the contract’s terms, maximizing the value they place on the constraints (such as how much they’d pay for more time or resources).</p> </li> <li> <p><strong>Duality</strong>: The primal (minimizing costs) and dual (maximizing value) problems balance each other. Weak duality means the dual value is a lower bound to the primal, and strong duality means the best deal is the same for both sides when the optimal values align.</p> </li> </ul> <p>Below are the next two important results derived from the above formulation.</p> <h4 id="weak-and-strong-duality"><strong>Weak and Strong Duality</strong></h4> <ol> <li> <p><strong>Weak Duality</strong>: This property tells us that the optimal value of the primal problem is always greater than or equal to the optimal value of the dual problem:</p> \[p^* \geq d^*,\] <p>where \(p^*\) and \(d^*\) are the optimal values of the primal and dual problems, respectively. This is a fundamental result in optimization theory. <strong>Why so?</strong> Because the dual problem is designed to provide a lower bound on the primal objective through the Lagrangian.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Lagrangian_1-480.webp 480w,/assets/img/Lagrangian_1-800.webp 800w,/assets/img/Lagrangian_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Lagrangian_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lagrangian_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p><strong>Strong Duality</strong>: In some special cases (such as when the problem satisfies <strong>Slater’s condition</strong>), <strong>strong duality</strong> holds, meaning the optimal values of the primal and dual problems are equal:</p> \[p^* = d^*.\] <p>Strong duality is particularly useful because it allows us to solve the dual problem instead of the primal one, often simplifying the problem or reducing computational complexity.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Lagrangian_2-480.webp 480w,/assets/img/Lagrangian_2-800.webp 800w,/assets/img/Lagrangian_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Lagrangian_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lagrangian_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="slaters-conditions"><strong>Slater’s Conditions</strong></h5> <ul> <li>They are a set of conditions that <strong>guarantee strong duality</strong> in certain types of constrained optimization problems (particularly convex problems).</li> <li>They state that for strong duality to hold, there must exist a <strong>strictly feasible point</strong> (a point where all inequality constraints are strictly satisfied).</li> <li>In simpler terms, Slater’s conditions ensure that there is at least one point where all the constraints are strictly satisfied, which <strong>enables strong duality</strong> and ensures that the primal and dual solutions align.</li> <li>These conditions are crucial in convex optimization as they help guarantee the optimal solutions for both the primal and dual problems are the same.</li> </ul> <h4 id="complementary-slackness"><strong>Complementary Slackness</strong></h4> <p>When <strong>strong duality</strong> holds, we can derive the <strong>complementary slackness</strong> condition. This condition provides deeper insight into the relationship between the primal and dual solutions. Specifically, if \(x^*\) is the optimal primal solution and \(\lambda^*\) is the optimal dual solution, we have:</p> \[f_0(x^*) = g(\lambda^*) = \inf_x L(x, \lambda^*) \leq L(x^*, \lambda^*) = f_0(x^*) + \sum_{i=1}^m \lambda_i^* f_i(x^*).\] <p>For this equality to hold, the term \(\sum_{i=1}^m \lambda_i^* f_i(x^*)\) must be zero for each constraint. This leads to the following <strong>complementary slackness condition</strong>:</p> <ol> <li>If \(\lambda_i^* &gt; 0\), then \(f_i(x^*) = 0\), meaning that the corresponding constraint is <strong>active</strong> at the optimal point. Also, it means the constraint is tight and directly affects the optimal solution.</li> <li>If \(f_i(x^*) &lt; 0\), then \(\lambda_i^* = 0\), meaning that the corresponding constraint is <strong>inactive</strong>. This indicates that the constraint doesn’t influence the optimal solution at all.</li> </ol> <p>This condition tells us which constraints are binding (active) at the optimal solution and which are not, providing critical information about the structure of the optimal solution.</p> <h5 id="contract-negotiation-analogy-duality-and-complementary-slackness"><strong>Contract Negotiation Analogy: Duality and Complementary Slackness</strong></h5> <h6 id="strong-duality-and-weak-duality">Strong Duality and Weak Duality:</h6> <ul> <li> <p><strong>Weak Duality</strong>: In a contract negotiation, <strong>weak duality</strong> is like the <strong>minimum acceptable price</strong> for the other party. The price (dual value) they would accept for your offer will always be a <strong>lower bound</strong> to what you are willing to pay (primal cost). The other party cannot ask for more than what you’re offering, but they may accept less.</p> </li> <li> <p><strong>Strong Duality</strong>: <strong>Strong duality</strong> happens when both parties agree on the same <strong>optimal terms</strong> for the contract. Both your <strong>best offer</strong> (primal) and the <strong>value assigned</strong> to the terms (dual) align perfectly, resulting in the best possible contract for both sides.</p> </li> </ul> <h6 id="complementary-slackness-1">Complementary Slackness:</h6> <ul> <li><strong>Complementary Slackness</strong> tells us which terms of the contract are really influencing the negotiation outcome. <ul> <li>If a <strong>dual variable</strong> (e.g., the price or terms the other party values) is <strong>positive</strong>, then the corresponding <strong>primal constraint</strong> (e.g., a term you care about, like the delivery time or cost) is <strong>active</strong> — it <strong>must</strong> be part of the final agreement.</li> <li>If a <strong>primal constraint</strong> (e.g., a timeline or budget) is not strict enough (it’s not a deal-breaker), then the <strong>dual variable</strong> (e.g., the value the other party assigns to it) is <strong>zero</strong>, meaning it doesn’t impact the outcome.</li> </ul> </li> </ul> <p><strong>Example:</strong></p> <ul> <li>Suppose you’re negotiating a <strong>project deadline</strong>. If the other party <strong>values</strong> the deadline highly (dual value is positive), it <strong>must</strong> be part of the contract (the deadline is <strong>active</strong>). If they don’t care much about it, then <strong>it doesn’t matter</strong> to the final deal (dual value is zero), and you can relax that constraint.</li> </ul> <hr/> <p>Finally, there might be one question left—this is the one I had, so here’s the explanation:</p> <h5 id="if-the-lagrangian-dual-function-is-concave-how-is-the-lagrangian-dual-problem-always-a-convex-optimization-problem"><strong>If the Lagrangian dual function is concave, how is the Lagrangian dual problem always a convex optimization problem?</strong></h5> <p>To understand why the dual problem is always a convex optimization problem, let’s revisit its structure: \(\max_{\lambda \geq 0} \; g(\lambda).\)</p> <ol> <li> <p><strong>Maximizing a Concave Function:</strong><br/> The dual function \(g(\lambda)\) is <strong>concave</strong> by construction. In optimization, maximizing a concave function is equivalent to minimizing a convex function (negating the objective). Therefore, the objective of the dual problem aligns with the structure of a convex optimization problem.</p> </li> <li> <p><strong>Convex Feasible Region:</strong><br/> The constraints in the dual problem (\(\lambda \geq 0\)) define a <strong>convex set</strong>, as the non-negative orthant in \(\mathbb{R}^m\) is convex.</p> </li> </ol> <h6 id="definition-of-a-convex-optimization-problem"><strong>Definition of a Convex Optimization Problem:</strong></h6> <p>A problem is a convex optimization problem if:</p> <ul> <li>The objective function is <strong>convex</strong> (for minimization) or <strong>concave</strong> (for maximization).</li> <li>The feasible region is a <strong>convex set</strong>.</li> </ul> <p>The dual problem satisfies these conditions because:</p> <ul> <li>The objective function \(g(\lambda)\) is concave.</li> <li>The constraint \(\lambda \geq 0\) defines a convex feasible region.</li> </ul> <p>Thus, the dual problem is always a <strong>convex optimization problem</strong>, regardless of whether the primal problem is convex or not.</p> <h6 id="intuition-behind-convexity-of-the-dual-problem"><strong>Intuition Behind Convexity of the Dual Problem</strong></h6> <p>The dual problem’s convexity comes from the way it is constructed:</p> <ol> <li>The Lagrangian combines the primal objective and constraints into a single function that penalizes constraint violations.</li> <li>By minimizing the Lagrangian over the primal variables \(x\), the dual function \(g(\lambda)\) captures the <strong>tightest lower bound</strong> of the primal objective.</li> <li>The pointwise infimum of affine functions (as in \(g(\lambda)\)) is guaranteed to be concave.</li> <li>The dual problem maximizes this concave function over a convex set (\(\lambda \geq 0\)), making it a convex optimization problem.</li> </ol> <p>This ensures that solving the dual problem is computationally efficient and well-structured, even when the primal problem is non-convex.</p> <hr/> <h4 id="conclusion"><strong>Conclusion</strong></h4> <p>By exploring the <strong>dual problem</strong> of SVM, we gain both theoretical insights and practical benefits. The dual formulation provides a new perspective on the original optimization problem, and solving it can sometimes be more efficient or insightful. The duality between the primal and dual problems underpins many of the optimization techniques used in machine learning, particularly in the context of support vector machines.</p> <p>Pat yourselves on the back for making it to the end of this blog! Take a well-deserved break, and stay tuned for the next one, where we’ll apply everything we’ve learned so far to formulate the SVM dual problem.</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://www.youtube.com/watch?v=thuYiebq1cE&amp;t=136s">https://www.youtube.com/watch?v=thuYiebq1cE&amp;t=136s - David S. Rosenberg</a></li> <li><a href="https://www.youtube.com/watch?v=8mjcnxGMwFo">Lagrange Multipliers | Geometric Meaning &amp; Full Example - Dr. Trefor Bazett</a></li> <li><a href="https://www.youtube.com/watch?v=d0CF3d5aEGc&amp;t=216s">Convexity and The Principle of Duality - Visually Explained</a></li> <li><a href="https://math.stackexchange.com/questions/515812/pointwise-infimum-of-affine-functions-is-concave?noredirect=1&amp;lq=1">Pointwise infimum of affine functions is concave</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.]]></summary></entry><entry><title type="html">Support Vector Machines(SVM) - From Hinge Loss to Optimization</title><link href="https://monishver11.github.io/blog/2025/svm/" rel="alternate" type="text/html" title="Support Vector Machines(SVM) - From Hinge Loss to Optimization"/><published>2025-01-07T14:40:00+00:00</published><updated>2025-01-07T14:40:00+00:00</updated><id>https://monishver11.github.io/blog/2025/svm</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/svm/"><![CDATA[<h4 id="a-quick-recap-hinge-loss"><strong>A Quick Recap: Hinge Loss</strong></h4> <p>To understand Support Vector Machines (SVM) fully, we need to revisit <strong>Hinge Loss</strong>, which is fundamental to SVM’s working mechanism. Hinge loss is mathematically defined as:</p> \[\ell_{\text{Hinge}} = \max(1 - m, 0) = (1 - m)_+\] <p>Here, \(m = y f(x)\) represents the margin, and the notation \((x)_+\) refers to the “positive part” of \(x\). This simply means that \((x)_+\) equals \(x\) when \(x \geq 0\), and is zero otherwise.</p> <p>Why is this loss function important? Hinge loss provides a convex, upper bound approximation of the 0–1 loss, making it computationally efficient for optimization. However, it’s not without limitations—it is <strong>not differentiable</strong> at \(m = 1\), which we’ll address later. A “margin error” occurs whenever \(m &lt; 1\), and this forms the basis of SVM’s classification mechanism.</p> <p>With this understanding of Hinge Loss in place, we’re ready to explore how SVM is formulated as an optimization problem.</p> <hr/> <h4 id="svm-as-an-optimization-problem"><strong>SVM as an Optimization Problem</strong></h4> <p>At its core, SVM aims to find a hyperplane that maximizes the separation, or <strong>margin</strong>, between data points of different classes. This task is mathematically framed as the following optimization problem:</p> <p>We minimize the objective:</p> \[\frac{1}{2} \|w\|^2 + c \sum_{i=1}^n \xi_i\] <p>subject to the constraints:</p> \[\xi_i \geq 1 - y_i (w^T x_i + b), \quad \text{for } i = 1, \dots, n\] \[\xi_i \geq 0, \quad \text{for } i = 1, \dots, n\] <p>which is equivalent to</p> \[\text{minimize } \frac{1}{2} \|w\|^2 + \frac{c}{n} \sum_{i=1}^n \xi_i\] \[\text{subject to } \xi_i \geq \max \big(0, 1 - y_i (w^T x_i + b) \big), \quad \text{for } i = 1, \dots, n\] <p>In this formulation:</p> <ul> <li>The term \(\|w\|^2\) represents the <strong>L2 regularizer</strong>, which helps prevent overfitting by penalizing large weights.</li> <li>The variables \(\xi_i\) (slack variables) account for data points that violate the margin constraint, allowing some degree of misclassification.</li> <li>The parameter \(c\) controls the trade-off between maximizing the margin and minimizing classification errors.</li> </ul> <p>To simplify, we can integrate the constraints directly into the objective function. This gives us:</p> \[\min_{w \in \mathbb{R}^d, b \in \mathbb{R}} \frac{1}{2} \|w\|^2 + c \sum_{i=1}^n \max(0, 1 - y_i (w^T x_i + b))\] <p>This new formulation has two terms:</p> <ol> <li>The first term, \(\frac{1}{2} \|w\|^2\), is the <strong>L2 regularizer</strong>.</li> <li>The second term, \(\sum_{i=1}^n \max(0, 1 - y_i (w^T x_i + b))\), captures the <strong>Hinge Loss</strong> for all data points.</li> </ol> <p>This concise representation highlights the two fundamental objectives of SVM: maintaining a large margin and minimizing misclassifications.</p> <p>Depending on the nature of the data, SVM can be approached in two ways. If the data is perfectly separable, we use a <strong>hard-margin SVM</strong>. In this case, all data points must be correctly classified while maintaining the margin constraints.</p> <p>However, real-world data is often noisy and not perfectly separable. Here, we use a <strong>soft-margin SVM</strong>, which introduces the slack variables \(\xi_i\) to allow some margin violations. The degree of permissible violations is controlled by the parameter \(c\), striking a balance between classification accuracy and margin size.</p> <h5 id="the-svm-objective-function"><strong>The SVM Objective Function</strong></h5> <p>The final objective function for SVM, combining regularization and hinge loss, is given by:</p> \[J(w) = \frac{1}{n} \sum_{i=1}^n \max(0, 1 - y_i w^T x_i) + \lambda \|w\|^2\] <p>Here, \(\lambda\) is the regularization parameter, which is inversely related to \(c\). This function encapsulates both the classification objective (minimizing hinge loss) and the regularization goal (keeping weights small).</p> <p>The relationship is inverse because:</p> <ul> <li>When \(c\) is large, the model is more focused on minimizing misclassification, meaning less regularization (\(\lambda\)) is needed.</li> <li>When \(c\) is small, the model allows more misclassifications, which means it can afford to have more regularization (\(\lambda\)) to prevent overfitting.</li> </ul> <hr/> <h5 id="gradients-and-optimization"><strong>Gradients and Optimization</strong></h5> <p>At this point, you might wonder: how do we actually optimize the SVM objective? This is where gradients come into play. Let’s break it down.</p> <h6 id="derivative-of-hinge-loss"><strong>Derivative of Hinge Loss</strong></h6> <p>The derivative of Hinge Loss, \(\ell(m) = \max(0, 1 - m)\), is as follows:</p> \[\ell'(m) = \begin{cases} 0 &amp; \text{if } m &gt; 1 \\ -1 &amp; \text{if } m &lt; 1 \\ \text{undefined} &amp; \text{if } m = 1 \end{cases}\] <p>Using the chain rule, the gradient of \(\ell(y_i w^T x_i)\) with respect to \(w\) is:</p> \[\nabla_w \ell(y_i w^T x_i) = \begin{cases} 0 &amp; \text{if } y_i w^T x_i &gt; 1 \\ - y_i x_i &amp; \text{if } y_i w^T x_i &lt; 1 \\ \text{undefined} &amp; \text{if } y_i w^T x_i = 1 \end{cases}\] <h6 id="gradient-of-the-svm-objective"><strong>Gradient of the SVM Objective</strong></h6> <p>Combining the gradients for all data points, the gradient of the SVM objective is:</p> \[\nabla_w J(w) = \nabla_w \left( \frac{1}{n} \sum_{i=1}^{n} \ell(y_i w^T x_i) + \lambda \|w\|^2 \right)\] \[= \frac{1}{n} \sum_{i=1}^{n} \nabla_w \ell(y_i w^T x_i) + 2\lambda w\] \[= \frac{1}{n} \sum_{i: y_i w^T x_i &lt; 1} (-y_i x_i) + 2\lambda w\] \[y_i w^T x_i \neq 1 \text{ for all i}\] <ul> <li>For \(y_i w^T x_i \geq 1\), the gradient is undefined.</li> </ul> <p>A common concern with the SVM objective is that it is <strong>not differentiable</strong> at \(y_i w^T x_i = 1\). However, in practice:</p> <ol> <li>Starting with a random \(w\), the probability of hitting exactly \(y_i w^T x_i = 1\) is negligible.</li> <li>Even if this occurs, small perturbations in the step size can help bypass such points.</li> </ol> <p>Thus, gradient-based optimization methods, such as gradient descent, can be effectively applied to the SVM objective.</p> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>SVM is a robust and versatile algorithm, but understanding it fully requires breaking it into manageable pieces. In this post, we explored its formulation, loss functions, and gradients. This sets the stage for discussing <strong>subgradients</strong> and <strong>subgradient descent</strong> in the next post, which address the non-differentiability issues of hinge loss.</p> <p>SVM is undoubtedly a vast topic, but step by step, it all begins to make sense. Trust the process, and by the end of this series, you’ll appreciate the journey. Stay tuned!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.]]></summary></entry><entry><title type="html">Understanding the Maximum Margin Classifier</title><link href="https://monishver11.github.io/blog/2025/max-margin-classifier/" rel="alternate" type="text/html" title="Understanding the Maximum Margin Classifier"/><published>2025-01-06T05:57:00+00:00</published><updated>2025-01-06T05:57:00+00:00</updated><id>https://monishver11.github.io/blog/2025/max-margin-classifier</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/max-margin-classifier/"><![CDATA[<h4 id="linearly-separable-data"><strong>Linearly Separable Data</strong></h4> <p>Let’s start with the simplest case: linearly separable data. Imagine a dataset where we can draw a straight line (or more generally, a hyperplane in higher dimensions) to perfectly separate two classes of points. Formally, for a dataset \(D\) with points \((x_i, y_i)\), we seek a hyperplane that satisfies the following conditions:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_1-480.webp 480w,/assets/img/Max_Margin_Classifier_1-800.webp 800w,/assets/img/Max_Margin_Classifier_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>\(w^T x_i &gt; 0\) for all \(x_i\) where \(y_i = +1\),</li> <li>\(w^T x_i &lt; 0\) for all \(x_i\) where \(y_i = -1\).</li> </ul> <p>This hyperplane is defined by a weight vector \(w\) and a bias \(b\), and our goal is to find \(w\) and \(b\) such that all points are correctly classified.</p> <p>But how do we design a learning algorithm to find such a hyperplane? This brings us to the <strong>Perceptron Algorithm</strong>.</p> <h4 id="the-perceptron-algorithm"><strong>The Perceptron Algorithm</strong></h4> <p>The perceptron is one of the earliest learning algorithms developed to find a separating hyperplane. Here’s how it works: we start with an initial guess for \(w\) (usually a zero vector) and iteratively adjust it based on misclassified examples.</p> <p>Each time we encounter a point \((x_i, y_i)\) that is misclassified (i.e., \(y_i w^T x_i &lt; 0\)), we update the weight vector as follows:</p> \[w \gets w + y_i x_i.\] <p>This update rule ensures that the algorithm moves the hyperplane towards misclassified positive examples and away from misclassified negative examples.</p> <p>The perceptron algorithm has a remarkable property: if the data is linearly separable, it will converge to a solution with zero classification error in a finite number of steps.</p> <p>In terms of loss functions, the perceptron can be viewed as minimizing the <strong>hinge loss</strong>:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_2-480.webp 480w,/assets/img/Max_Margin_Classifier_2-800.webp 800w,/assets/img/Max_Margin_Classifier_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> \[\ell(x, y, w) = \max(0, -y w^T x).\] <p>However, while the perceptron guarantees a solution, it doesn’t always find the best one. This brings us to the concept of <strong>maximum-margin classifiers</strong>. But before exploring that, let’s take a deeper look at why the this update rule works.</p> <h5 id="understanding-why-the-perceptron-update-rule-works"><strong>Understanding why the Perceptron Update Rule works?</strong></h5> <p>The <strong>perceptron update rule</strong> shifts the hyperplane differently depending on whether the misclassified point belongs to the positive class (\(y_i = 1\)) or the negative class (\(y_i = -1\)). Let’s take the two cases:</p> <h6 id="positive-case-y_i--1"><strong>Positive Case (\(y_i = 1\))</strong></h6> <ul> <li> <p><strong>Condition for misclassification:</strong> \(w^T x_i &lt; 0.\)<br/> This means the point \(x_i\) is on the wrong side of the hyperplane or too far from the correct side.</p> </li> <li> <p><strong>Update rule:</strong></p> \[w \gets w + x_i.\] </li> <li> <p><strong>Effect of the update:</strong></p> <ul> <li>Adding \(x_i\) to \(w\) increases the dot product \(w^T x_i\) because \(w\) is now pointing more in the direction of \(x_i\).</li> <li>This adjustment shifts the hyperplane towards \(x_i\), ensuring \(x_i\) is more likely to be correctly classified in the next iteration.</li> </ul> </li> </ul> <h6 id="negative-case-y_i---1"><strong>Negative Case (\(y_i = -1\))</strong></h6> <ul> <li> <p><strong>Condition for misclassification:</strong> \(w^T x_i &gt; 0.\)<br/> This means the point \(x_i\) is either incorrectly classified as positive or too close to the positive side.</p> </li> <li> <p><strong>Update rule:</strong></p> \[w \gets w - x_i.\] </li> <li> <p><strong>Effect of the update:</strong></p> <ul> <li>Subtracting \(x_i\) from \(w\) decreases the dot product \(w^T x_i\) because \(w\) is now pointing less in the direction of \(x_i\).</li> <li>This adjustment shifts the hyperplane away from \(x_i\), making it more likely to correctly classify \(x_i\) as negative in subsequent iterations.</li> </ul> </li> </ul> <p><strong>Geometric Interpretation</strong>: The perceptron update ensures that the weight vector \(w\) aligns more closely with the correctly classified side.</p> <hr/> <h4 id="maximum-margin-separating-hyperplane"><strong>Maximum-Margin Separating Hyperplane</strong></h4> <p>When the data is linearly separable, there are infinitely many hyperplanes that can separate the classes. The perceptron algorithm, for instance, might return any one of these. But not all hyperplanes are equally desirable.</p> <p>We prefer a hyperplane that is farthest from both classes of points. This idea leads to the concept of the <strong>maximum-margin classifier</strong>, which finds the hyperplane that maximizes the smallest distance between the hyperplane and the data points.</p> <h5 id="geometric-margin"><strong>Geometric Margin</strong></h5> <p>The <strong>geometric margin</strong> of a hyperplane is defined as the smallest distance between the hyperplane and any data point. For a hyperplane defined by \(w\) and \(b\), this margin can be expressed as:</p> \[\gamma = \min_i \frac{y_i (w^T x_i + b)}{\|w\|_2}.\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_3-480.webp 480w,/assets/img/Max_Margin_Classifier_3-800.webp 800w,/assets/img/Max_Margin_Classifier_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Maximizing this geometric margin provides a hyperplane that is robust to small perturbations in the data, making it a desirable choice.</p> <h5 id="distance-between-a-point-and-a-hyperplane"><strong>Distance Between a Point and a Hyperplane</strong></h5> <p>To understand the geometric margin more concretely, let’s calculate the distance from a point \(x'\) to a hyperplane \(H: w^T v + b = 0\). This derivation involves the following steps:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_4-480.webp 480w,/assets/img/Max_Margin_Classifier_4-800.webp 800w,/assets/img/Max_Margin_Classifier_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h6 id="step-1-perpendicular-distance-from-a-point-to-a-hyperplane"><strong>Step 1: Perpendicular Distance from a Point to a Hyperplane</strong></h6> <p>The distance from a point \(x'\) to the hyperplane is defined as the shortest (perpendicular) distance between the point and the hyperplane. The equation of the hyperplane is:</p> \[w^T v + b = 0,\] <p>where:</p> <ul> <li>\(w\) is the normal vector to the hyperplane.</li> <li>\(b\) is the bias term.</li> <li>\(v\) represents any point on the hyperplane.</li> </ul> <h6 id="step-2-projecting-the-point-onto-the-normal-vector"><strong>Step 2: Projecting the Point onto the Normal Vector</strong></h6> <p>The perpendicular distance is proportional to the projection of the point \(x'\) onto the normal vector \(w\). Mathematically, the projection of \(x'\) onto \(w\), denoted \(\text{Proj}_{w}(x')\), is given by:</p> \[\text{Proj}_{w}(x') = \frac{x' \cdot w}{w \cdot w} w = \left( \frac{w^T x'}{\|w\|_2^2} \right) w\] <p>For the hyperplane \(H: w^T v + b = 0\), the bias term \(b\) shifts the hyperplane as the hyperplane is not always centered at the origin Incorporating this into the projection formula, the signed distance becomes:</p> \[d(x', H) = \frac{w^T x' + b}{\|w\|_2}.\] <h6 id="step-3-accounting-for-the-label-y"><strong>Step 3: Accounting for the Label \(y\)</strong></h6> <p>The label \(y\) of the point \(x'\) determines whether the point is on the positive or negative side of the hyperplane:</p> <ul> <li>For correctly classified points, \(y (w^T x' + b) &gt; 0\).</li> <li>For misclassified points, \(y (w^T x' + b) &lt; 0\).</li> </ul> <p>Including the label ensures that the signed distance is positive for correctly classified points and negative for misclassified points. Thus, the signed distance becomes:</p> \[d(x', H) = \frac{y (w^T x' + b)}{\|w\|_2}.\] <hr/> <h5 id="maximizing-the-margin"><strong>Maximizing the Margin</strong></h5> <p>To maximize the margin, we solve the following optimization problem:</p> \[\max \min_i \frac{y_i (w^T x_i + b)}{\|w\|_2}.\] <p>To simplify, let \(M = \min_i \frac{y_i (w^T x_i + b)}{\|w\|_2}\). The problem becomes:</p> \[\max M, \quad \text{subject to } \frac{y_i (w^T x_i + b)}{\|w\|_2} \geq M, \; \forall i.\] <p><strong>This means:</strong> We want to maximize \(M\), which corresponds to maximizing the smallest margin across all data points.</p> <p>The constraint: \(\frac{y_i (w^T x_i + b)}{\|w\|_2} \geq M\) ensures that for every data point \(x_i\), the margin is at least \(M\), i.e., the data point lies on the correct side of the margin boundary.</p> <p><strong>Another way to put this is:</strong> Since \(M\) is the smallest margin, the constraint \(\frac{y_i (w^T x_i + b)}{\|w\|_2} \geq M\) ensures that every data point has a margin at least as large as \(M\) and this condition is enforced for every data point \(i\).</p> <p>Next, by fixing \(\|w\|_2 = \frac{1}{M}\), we reformulate it as:</p> \[\min \frac{1}{2} \|w\|_2^2, \quad \text{subject to } y_i (w^T x_i + b) \geq 1, \; \forall i.\] <p>This is the optimization problem solved by a <strong>hard margin support vector machine (SVM)</strong>.</p> <p><strong>Note:</strong> Maximizing the margin \(M\) is equivalent to minimizing the inverse, \(\frac{1}{2} \|w\|_2^2\), since the margin is inversely proportional to the norm of \(w\).</p> <h5 id="what-if-the-data-is-not-linearly-separable"><strong>What If the Data Is Not Linearly Separable?</strong></h5> <p>In real-world scenarios, data is often not perfectly linearly separable. For any \(w\), there might be points with negative margins. To handle such cases, we introduce <strong>slack variables</strong> \(\xi_i\), which allow some margin violations.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_5-480.webp 480w,/assets/img/Max_Margin_Classifier_5-800.webp 800w,/assets/img/Max_Margin_Classifier_5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h4 id="soft-margin-svm"><strong>Soft Margin SVM</strong></h4> <p>The optimization problem for a soft margin SVM is:</p> \[\min \frac{1}{2} \|w\|_2^2 + C \sum_{i=1}^n \xi_i,\] <p>subject to:</p> \[y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \; \forall i.\] <h5 id="breaking-this-down"><strong>Breaking this down:</strong></h5> <ul> <li> <p><strong>Regularization Term</strong>:</p> \[\frac{1}{2} \|w\|_2^2\] <p>This term is the <strong>regularization</strong> component of the objective function. It penalizes large values of \(w\), which corresponds to smaller margins. By minimizing this term, we aim to <strong>maximize the margin</strong> between the two classes. A larger margin typically leads to better generalization and lower overfitting.</p> </li> <li> <p><strong>Penalty Term</strong>:</p> \[C \sum_{i=1}^n \xi_i\] <p>This term introduces <strong>penalties</strong> for margin violations. The \(\xi_i\) are the <strong>slack variables</strong> that measure how much each data point violates the margin. The parameter \(C\) controls the trade-off between <strong>maximizing the margin</strong> (by minimizing \(\|w\|_2^2\)) and <strong>minimizing the violations</strong> (the sum of the slack variables).</p> <ul> <li>A <strong>larger value of \(C\)</strong> places more emphasis on minimizing violations, which results in a stricter margin but could lead to overfitting if \(C\) is too large.</li> <li>A <strong>smaller value of \(C\)</strong> allows for more margin violations, potentially leading to a <strong>wider margin</strong> and better generalization.</li> </ul> </li> <li> <p><strong>Margin Constraint</strong>:</p> \[y_i (w^T x_i + b) \geq 1 - \xi_i\] <p>This constraint ensures that the data points are correctly classified with a margin of at least 1, unless there is a violation. If a data point violates the margin (i.e., it lies inside the margin or on the wrong side of the hyperplane), the slack variable \(\xi_i\) becomes positive. The value of \(\xi_i\) measures how much the margin is violated for the data point \(x_i\).</p> <ul> <li> <p>When \(\xi_i = 0\), the data point \(x_i\) satisfies the margin condition:</p> \[y_i (w^T x_i + b) \geq 1\] <p>This represents the ideal case where the point lies correctly outside or on the margin.</p> </li> <li> <p>When \(\xi_i &gt; 0\), the point <strong>violates the margin</strong>. The larger the value of \(\xi_i\), the greater the violation. For example, if \(\xi_i = 0.5\), the point lies inside the margin or is misclassified by 0.5 units.</p> </li> </ul> </li> <li> <p><strong>Non-Negativity of Slack Variables</strong>:</p> \[\xi_i \geq 0\] <p>This ensures that the slack variables \(\xi_i\) are always non-negative, as they represent the <strong>degree of violation</strong> of the margin. Since it’s not possible to have a negative violation, this constraint enforces that \(\xi_i\) cannot be less than zero.</p> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_6-480.webp 480w,/assets/img/Max_Margin_Classifier_6-800.webp 800w,/assets/img/Max_Margin_Classifier_6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h6 id="wrapping-up"><strong>Wrapping Up</strong></h6> <p>The maximum-margin classifier forms the foundation of modern support vector machines. For non-linearly separable data, the introduction of slack variables allows SVMs to adapt while maintaining their core principle of maximizing the margin.</p> <p>In the next post, we’ll dive deeper into the world of SVMs, explore how they work under the hood, and work through this optimization problem to solve it. Stay tuned!</p> <h6 id="references"><strong>References:</strong></h6> <ul> <li><a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html">Lecture 9: SVM</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.]]></summary></entry><entry><title type="html">L1 and L2 Regularization - Nuanced Details</title><link href="https://monishver11.github.io/blog/2025/l1-l2-reg-indepth/" rel="alternate" type="text/html" title="L1 and L2 Regularization - Nuanced Details"/><published>2025-01-05T17:50:00+00:00</published><updated>2025-01-05T17:50:00+00:00</updated><id>https://monishver11.github.io/blog/2025/l1-l2-reg-indepth</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/l1-l2-reg-indepth/"><![CDATA[<p>Regularization is a cornerstone in machine learning, providing a mechanism to prevent overfitting while controlling model complexity. Among the most popular techniques are <strong>L1</strong> and <strong>L2 regularization</strong>, which serve different purposes but share a common goal of improving model generalization. In this post, we will delve deep into the theory, mathematics, and practical implications of these regularization methods.</p> <p>Let’s set the stage with linear regression. For a dataset</p> \[D_n = \{(x_1, y_1), \dots, (x_n, y_n)\},\] <p>the objective in ordinary least squares is to minimize the mean squared error:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2.\] <p>While effective, this approach can overfit when the number of features \(d\) is large compared to the number of samples \(n\). For example, in natural language processing, it is common to have millions of features but only thousands of documents.</p> <h5 id="addressing-overfitting-with-regularization"><strong>Addressing Overfitting with Regularization</strong></h5> <p>To mitigate overfitting, <strong>\(L_2\) regularization</strong> (also known as <strong>ridge regression</strong>) adds a penalty term proportional to the \(L_2\) norm of the weights:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2 + \lambda \|w\|_2^2,\] <p>where:</p> \[\|w\|_2^2 = w_1^2 + w_2^2 + \dots + w_d^2.\] <p>This penalty term discourages large weight values, effectively shrinking them toward zero. When \(\lambda = 0\), the solution reduces to ordinary least squares. As \(\lambda\) increases, the penalty grows, favoring simpler models with smaller weights.</p> <h5 id="understanding-l_2-regularization"><strong>Understanding \(L_2\) Regularization</strong></h5> <p>L2 regularization is particularly effective at reducing sensitivity to fluctuations in the input data. To understand this, consider a simple linear function:</p> \[\hat{f}(x) = \hat{w}^\top x.\] <p>The function \(\hat{f}(x)\) is said to be <strong>Lipschitz continuous</strong>, with a Lipschitz constant defined as:</p> \[L = \|\hat{w}\|_2.\] <p>This implies that when the input changes from \(x\) to \(x + h\), the function’s output change is bounded by \(L\|h\|_2\). In simpler terms, \(L_2\) regularization controls the rate of change of \(\hat{f}(x)\), making the model less sensitive to variations in the input data.</p> <h6 id="mathematical-proof-of-lipschitz-continuity"><strong>Mathematical Proof of Lipschitz Continuity</strong></h6> <p>To formalize this property, let’s derive the Lipschitz bound:</p> \[|\hat{f}(x + h) - \hat{f}(x)| = |\hat{w}^\top (x + h) - \hat{w}^\top x| = |\hat{w}^\top h|.\] <p>Using the <strong>Cauchy-Schwarz inequality</strong>, this can be bounded as:</p> \[|\hat{w}^\top h| \leq \|\hat{w}\|_2 \|h\|_2.\] <p>Thus, the Lipschitz constant \(L = \|\hat{w}\|_2\) quantifies the maximum rate of change for the function \(\hat{f}(x)\).</p> <h5 id="generalization-to-other-norms"><strong>Generalization to Other Norms</strong></h5> <p>The generalization to other norms comes from the equivalence of norms in finite-dimensional vector spaces. Here’s the reasoning:</p> <p><strong>Norm Equivalence:</strong></p> <p>In finite-dimensional spaces (e.g., \(\mathbb{R}^d\)), all norms are equivalent. This means there exist constants \(C_1, C_2 &gt; 0\) such that for any vector \(\mathbf{w} \in \mathbb{R}^d\):</p> \[C_1 \| \mathbf{w} \|_p \leq \| \mathbf{w} \|_q \leq C_2 \| \mathbf{w} \|_p\] <p>For example, the \(L_1\), \(L_2\), and \(L_\infty\) norms can all bound one another with appropriate scaling constants.</p> <p><strong>Lipschitz Continuity:</strong></p> <p>The Lipschitz constant for \(\hat{f}(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}\) depends on the norm of \(\mathbf{w}\) because the bound for the rate of change involves the norm of \(\mathbf{w}\). When using a different norm \(\| \cdot \|_p\) to regularize, the Lipschitz constant adapts to that norm.</p> <p>Specifically, for the \(L_p\) norm:</p> \[| \hat{f}(\mathbf{x} + \mathbf{h}) - \hat{f}(\mathbf{x}) | \leq \| \mathbf{w} \|_p \| \mathbf{h} \|_q\] <p>where \(p\) and \(q\) satisfies:</p> \[\frac{1}{p} + \frac{1}{q} = 1\] <p><strong>Key Insight:</strong></p> <p>This shows that the idea of controlling the sensitivity of the model (through the Lipschitz constant) extends naturally to any norm. The choice of norm alters how the regularization penalizes weights but retains the fundamental property of bounding the function’s rate of change.</p> <h6 id="an-analogy-to-internalize-this"><strong>An analogy to internalize this:</strong></h6> <p>Think of \(L_2\) regularization as a bungee cord attached to a daring rock climber. The climber represents the model trying to navigate a complex landscape (data). Without the cord (regularization), they might venture too far and fall into overfitting. The cord adds just enough tension (penalty) to keep the climber balanced and safe, ensuring they explore the terrain without taking reckless leaps. Similarly, regularization helps the model stay grounded, generalizing well without succumbing to overfitting.</p> <p>Now, imagine different types of bungee cords for different norms. The \(L_2\) regularization bungee cord is like a standard elastic cord, providing a smooth and consistent tension, ensuring the climber doesn’t over-extend but can still make significant progress.</p> <p>For \(L_1\) regularization, the bungee cord is more rigid and less forgiving, preventing large movements in any direction. It forces the climber to stick to fewer, more significant paths, like sparsity in feature selection — only the most important features remain.</p> <p>In the case of \(L_\infty\) regularization, the bungee cord has a fixed maximum stretch. No matter how hard the climber tries to move, they cannot go beyond a certain point, ensuring the model remains under tight control, limiting the complexity of each individual parameter.</p> <p>In each case, the regularization (the cord) helps the climber (the model) stay within safe bounds, preventing them from falling into overfitting while ensuring they can still navigate the data effectively.</p> <hr/> <h4 id="linear-vs-ridge-regression"><strong>Linear vs. Ridge Regression</strong></h4> <p>The inclusion of L2 regularization modifies the optimization objective, as illustrated by the difference between <strong>linear regression</strong> and <strong>ridge regression</strong>.</p> <p>In <strong>linear regression</strong>, the goal is to minimize the sum of squared residuals, expressed as:</p> \[L(w) = \frac{1}{2} \|Xw - y\|_2^2\] <p>In contrast, <strong>ridge regression</strong> introduces an additional penalty term proportional to the L2 norm of the weights:</p> \[L(w) = \frac{1}{2} \|Xw - y\|_2^2 + \frac{\lambda}{2} \|w\|_2^2\] <p>This additional term penalizes large weights, helping to control model complexity and reduce overfitting.</p> <h6 id="gradients-of-the-objective"><strong>Gradients of the Objective:</strong></h6> <p>The inclusion of the regularization term affects the gradient of the loss function. For linear regression, the gradient is:</p> \[\nabla L(w) = X^T (Xw - y)\] <p>For ridge regression, the gradient becomes:</p> \[\nabla L(w) = X^T (Xw - y) + \lambda w\] <p>The regularization term \(\lambda w\) biases the solution toward smaller weights, thereby stabilizing the optimization. By adding this term, the model is less sensitive to small changes in the data, especially in cases where multicollinearity exists, i.e., when features are highly correlated.</p> <h6 id="closed-form-solutions"><strong>Closed-form Solutions:</strong></h6> <p>Both linear regression and ridge regression admit closed-form solutions. For linear regression, the weights are given by:</p> \[w = (X^T X)^{-1} X^T y\] <p>For ridge regression, the solution is slightly modified:</p> \[w = (X^T X + \lambda I)^{-1} X^T y\] <p>The addition of \(\lambda I\) ensures that \(X^T X + \lambda I\) is always invertible, addressing potential issues of singularity in the design matrix. In linear regression, if the matrix \(X^T X\) is singular or nearly singular (which can occur when features are linearly dependent or when there are more features than samples), the inverse may not exist or be unstable. By adding \(\lambda I\), where \(I\) is the identity matrix, we effectively shift the eigenvalues of \(X^T X\), making the matrix non-singular and ensuring a stable solution.</p> <hr/> <h4 id="a-constrained-optimization-perspective"><strong>A Constrained Optimization Perspective</strong></h4> <p>L2 regularization can also be understood through the lens of constrained optimization. In this perspective, the ridge regression objective is expressed in <strong>Tikhonov regularization</strong> form as:</p> \[w^* = \arg\min_w \left( \frac{1}{2} \|Xw - y\|_2^2 + \frac{\lambda}{2} \|w\|_2^2 \right)\] <p>The <strong>Ivanov form</strong> is another perspective where the objective is similarly constrained, but the constraint is typically applied in a more specific way, usually in the context of ill-posed problems or regularization approaches in functional analysis. It focuses on minimizing the error while controlling the solution’s smoothness or complexity. While this form is less commonly used directly in machine learning, it is foundational in understanding regularization in more theoretical settings. We mention this now because both forms will appear later in the discussion of other concepts, and it’s helpful to have a brief overview before we revisit them in more depth.</p> <p>Alternatively, using <strong>Lagrangian theory</strong>, we can reframe ridge regression as a constrained optimization problem. The objective is to minimize the residual sum of squares subject to a constraint on the L2 norm of the weights:</p> \[w^* = \arg\min_{w : \|w\|_2^2 \leq r} \frac{1}{2} \|Xw - y\|_2^2\] <p>Here, \(r\) represents the maximum allowed value for the squared norm of the weights, effectively placing a limit on their size. The Lagrange multiplier adjusts the importance of the constraint during optimization. This form emphasizes the constraint on model complexity, ensuring that the weights don’t grow too large.</p> <p>At the optimal solution, the gradients of the objective function and the constraint term balance each other, providing a geometric interpretation of how regularization controls the model complexity.</p> <p><strong>Note:</strong> The Lagrangian theory will be explored further when we discuss Support Vector Machines (SVMs), where this approach plays a central role in optimization.</p> <hr/> <h4 id="lasso-regression-and-l_1-regularization"><strong>Lasso Regression and \(L_1\) Regularization</strong></h4> <p>While L2 regularization minimizes the sum of squared weights, <strong>L1 regularization</strong> (used in Lasso regression) minimizes the sum of absolute weights. This is expressed as:</p> \[w^* = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n (\hat{w}^T x_i - y_i)^2 + \lambda \|w\|_1\] <p>Here, the L1 norm</p> \[\|w\|_1 = |w_1| + |w_2| + \dots + |w_d|\] <p>encourages sparsity in the weight vector, setting some coefficients exactly to zero. <strong>But what’s behind this, really?</strong> Keep reading!</p> <h5 id="ridge-vs-lasso-regression"><strong>Ridge vs. Lasso Regression</strong></h5> <p>The key difference between ridge and lasso regression lies in their impact on the weights. Ridge regression tends to shrink all coefficients toward zero but does not eliminate any of them. In contrast, lasso regression produces sparse solutions, where some coefficients are exactly zero. <strong>We’ll dive into this next.</strong></p> <p>This sparsity has significant practical advantages. By zeroing out irrelevant features, lasso regression simplifies the model, making it:</p> <ul> <li><strong>Faster</strong> to compute, as fewer features need to be processed.</li> <li><strong>Cheaper</strong> to store and deploy, especially on resource-constrained devices.</li> <li><strong>More interpretable</strong>, as it highlights the most important features.</li> <li><strong>Less prone to overfitting</strong>, since the reduced complexity often leads to better generalization.</li> </ul> <hr/> <h4 id="why-does-l_1-regularization-lead-to-sparsity"><strong>Why Does \(L_1\) Regularization Lead to Sparsity?</strong></h4> <p>A distinctive property of <strong>L1 regularization</strong> is its ability to produce sparse solutions, where some weights are exactly zero. This characteristic makes L1 regularization particularly useful for feature selection, as it effectively identifies the most important features by eliminating irrelevant ones. To understand this better, let’s explore the theoretical underpinnings and geometric intuition behind this phenomenon.</p> <h6 id="revisiting-lasso-regression"><strong>Revisiting Lasso Regression:</strong></h6> <p>Lasso regression penalizes the <strong>L1 norm</strong> of the weights. The objective function, also known as the <strong>Tikhonov form</strong>, is given by:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \big(w^T x_i - y_i\big)^2 + \lambda \|w\|_1\] <p>Here, the L1 norm is defined as:</p> \[\|w\|_1 = |w_1| + |w_2| + \dots + |w_d|\] <p>This formulation encourages sparsity by applying a uniform penalty across all weights, effectively “pushing” some weights to zero when they contribute minimally to the prediction.</p> <h6 id="regularization-as-constrained-empirical-risk-minimization-erm"><strong>Regularization as Constrained Empirical Risk Minimization (ERM)</strong></h6> <p>Regularization can also be viewed through the lens of <strong>constrained ERM</strong>. For a given complexity measure \(\Omega\) and a fixed threshold \(r \geq 0\), the optimization problem is expressed as:</p> \[\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i) \quad \text{s.t.} \quad \Omega(f) \leq r\] <p>In the case of Lasso regression, this is equivalent to the <strong>Ivanov form</strong>:</p> \[\hat{w} = \arg\min_{\|w\|_1 \leq r} \frac{1}{n} \sum_{i=1}^n \big(w^T x_i - y_i\big)^2\] <p>Here, \(r\) plays the same role as the regularization parameter \(\lambda\) in the penalized ERM (Tikhonov) form. The choice between these forms depends on whether the complexity is penalized directly or constrained explicitly.</p> <h5 id="the-ℓ1-and-ℓ2-norm-constraints"><strong>The ℓ1 and ℓ2 Norm Constraints</strong></h5> <p>To understand why L1 regularization promotes sparsity, consider a simple hypothesis space \(\mathcal{F} = \{f(x) = w_1x_1 + w_2x_2\}\). Each function can be represented as a point \((w_1, w_2)\) in \(\mathbb{R}^2\). The regularization constraints can be visualized as follows:</p> <ul> <li><strong>L2 norm constraint:</strong> \(w_1^2 + w_2^2 \leq r\), which is a <strong>circle</strong> in \(\mathbb{R}^2\).</li> <li><strong>L1 norm constraint:</strong> \(|w_1| + |w_2| \leq r\), which forms a <strong>diamond</strong> in \(\mathbb{R}^2\).</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_1-480.webp 480w,/assets/img/L1_Reg_1-800.webp 800w,/assets/img/L1_Reg_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><code class="language-plaintext highlighter-rouge">Note</code>: The sparse solutions correspond to the vertices of the diamond, where at least one weight is zero.</p> <p><strong>To build intuition</strong>, let’s analyze the geometry of the optimization:</p> <ol> <li>The <strong>blue region</strong> represents the feasible space defined by the regularization constraint (e.g., \(w_1^2 + w_2^2 \leq r\) for L2, or \(|w_1| + |w_2| \leq r\) for L1).</li> <li> <p>The <strong>red contours</strong> represent the level sets of the empirical risk function:</p> \[\hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \big(w^T x_i - y_i\big)^2\] </li> </ol> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_2_1-480.webp 480w,/assets/img/L1_Reg_2_1-800.webp 800w,/assets/img/L1_Reg_2_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_2_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_2_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_2_2-480.webp 480w,/assets/img/L1_Reg_2_2-800.webp 800w,/assets/img/L1_Reg_2_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_2_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_2_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The optimal solution is found where the smallest contour intersects the feasible region. For L1 regularization, this intersection tends to occur at the corners of the diamond, where one or more weights are exactly zero.</p> <p>Suppose the loss contours grow as perfect circles (or spheres in higher dimensions). When these contours intersect the diamond-shaped feasible region of L1 regularization, the corners of the diamond are more likely to be touched. These corners correspond to solutions where at least one weight is zero.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_3_2-480.webp 480w,/assets/img/L1_Reg_3_2-800.webp 800w,/assets/img/L1_Reg_3_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_3_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_3_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_3_1-480.webp 480w,/assets/img/L1_Reg_3_1-800.webp 800w,/assets/img/L1_Reg_3_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_3_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_3_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In contrast, for L2 regularization, the feasible region is a circle (or sphere), and the intersection is equally likely to occur in any direction. This results in small, but non-zero, weights across all features, rather than sparse solutions.</p> <h6 id="optimization-perspective"><strong>Optimization Perspective:</strong></h6> <p>From an optimization viewpoint, the difference between L1 and L2 regularization lies in how the penalty affects the gradient:</p> <ul> <li>For <strong>L2 regularization</strong>, as a weight \(w_i\) becomes smaller, the penalty \(\lambda w_i^2\) decreases more rapidly. However, the gradient of the penalty also diminishes, providing less incentive to shrink the weight to exactly zero.</li> <li>For <strong>L1 regularization</strong>, the penalty \(\lambda |w_i|\) decreases linearly, and its gradient remains constant regardless of the weight’s size. This consistent gradient drives small weights to zero, promoting sparsity.</li> </ul> <p><strong>Consider the following idea:</strong> Imagine you’re packing items into a small rectangular box, and you have two kinds of items: rigid boxes (representing \(L_1\) regularization) and pebbles (representing \(L_2\) regularization).</p> <p>The rigid boxes are shaped with sharp corners and don’t squish or deform. When you try to fit them into the small box, they naturally stack at the edges or corners of the space. This means some of the rigid boxes might not fit at all, so you leave them out—just like \(L_1\) regularization pushing weights to zero.</p> <p>The pebbles, on the other hand, are smooth and can be squished slightly. When you pack them into the box, they distribute evenly, filling in gaps without leaving any pebbles completely outside. This is like \(L_2\) regularization, where weights are reduced but not exactly zero.</p> <p>So, that’s why \(L_1\) regularization creates sparse solutions (only the most critical items get packed) while \(L_2\) regularization spreads the influence across all features (everything gets included, but smaller).</p> <h5 id="generalizing-to-ell_q-regularization"><strong>Generalizing to \(\ell_q\) Regularization</strong></h5> <p>\(\ell_1\) and \(\ell_2\) regularization are specific cases of the more general \(\ell_q\) regularization, defined as:</p> \[\|w\|_q^q = |w_1|^q + |w_2|^q + \dots + |w_d|^q\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_4-480.webp 480w,/assets/img/L1_Reg_4-800.webp 800w,/assets/img/L1_Reg_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here are some notable cases:</p> <ul> <li>For \(q \geq 1\), \(\|w\|_q\) is a valid norm.</li> <li>For \(0 &lt; q &lt; 1\), the constraint becomes non-convex, making optimization challenging. While \(\ell_q\) regularization with \(q &lt; 1\) can induce even sparser solutions than L1, it is often impractical in real-world scenarios. For instance when \(q=0.5\), the regularization takes the form of a square root function, which is non-convex.</li> <li>The \(\ell_0\) norm, defined as the number of non-zero weights, corresponds to <strong>subset selection</strong> but is computationally infeasible due to its combinatorial nature.</li> </ul> <p><strong>Note:</strong> \(L_n\)and \(\ell_n\) represent the same concept, so don’t let the difference in notation confuse you.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>\(L_1\) regularization’s sparsity-inducing property makes it an indispensable tool in feature selection and high-dimensional problems. Its optimization characteristics and ability to simplify models while retaining interpretability set it apart from \(L_2\) regularization.</p> <p>Next, we’ll talk about the <strong>maximum margin classifier &amp; SVM</strong>. Stay tuned, as moving on, it’s going to get a little intense, but don’t worry—we’ll get through it together!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models">why-l1-norm-for-sparse-models</a></li> <li><a href="https://blog.mlreview.com/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a">L1 Norm Regularization and Sparsity Explained for Dummies</a></li> <li><a href="https://math.stackexchange.com/questions/1904767/why-small-l1-norm-means-sparsity">why-small-l1-norm-means-sparsity</a></li> <li><a href="https://medium.com/analytics-vidhya/regularization-path-using-lasso-regression-c450eea9321e">Regularization path using Lasso regression</a></li> <li>Image Credits: Mairal et al.’s Sparse Modeling for Image and Vision Processing Fig 1.6, KPM Fig. 13</li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.]]></summary></entry><entry><title type="html">Regularization - Balancing Model Complexity and Overfitting</title><link href="https://monishver11.github.io/blog/2025/regularization/" rel="alternate" type="text/html" title="Regularization - Balancing Model Complexity and Overfitting"/><published>2025-01-03T16:39:00+00:00</published><updated>2025-01-03T16:39:00+00:00</updated><id>https://monishver11.github.io/blog/2025/regularization</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/regularization/"><![CDATA[<p>When building machine learning models, one of the core challenges is finding the right balance between <strong>approximation error</strong> and <strong>estimation error</strong>. The trade-off can be understood in terms of the size and complexity of the hypothesis space, denoted by \(F\).</p> <p>On the one hand, a <strong>larger hypothesis space</strong> allows the model to better approximate the true underlying function. However, this flexibility comes at a cost: the risk of overfitting the training data, especially if the dataset is small. On the other hand, a <strong>smaller hypothesis space</strong> is less prone to overfitting, but it may lack the expressiveness needed to capture the true relationship between inputs and outputs, leading to higher approximation error.</p> <p>To control this trade-off, we need a way to quantify and limit the complexity of \(F\). This can be done in various ways, such as limiting the number of variables or restricting the degree of polynomials in a model.</p> <hr/> <h4 id="how-to-control-model-complexity"><strong>How to Control Model Complexity</strong></h4> <p>A common strategy to manage complexity involves learning a sequence of models with increasing levels of sophistication. Mathematically, this sequence can be represented as:</p> \[F_1 \subset F_2 \subset \dots \subset F_n \subset F\] <p>where each subsequent space, \(F_i\), is a superset of the previous one, representing models of greater complexity.</p> <p>For example, consider polynomial regression. The full hypothesis space, \(F\), includes all polynomial functions, while \(F_d\) is restricted to polynomials of degree \(\leq d\). By increasing \(d\), we explore more complex models within the same overarching hypothesis space.</p> <p>Once this sequence of models is defined, we evaluate them using a scoring metric, such as validation error, to identify the one that best balances complexity and accuracy. This approach ensures a systematic way to control overfitting while retaining sufficient expressive power.</p> <hr/> <h4 id="feature-selection-in-linear-regression"><strong>Feature Selection in Linear Regression</strong></h4> <p>In linear regression, the concept of nested hypothesis spaces is closely tied to <strong>feature selection</strong>. The idea is to construct a series of models using progressively fewer features:</p> \[F_1 \subset F_2 \subset \dots \subset F_n \subset F\] <p>where \(F\) represents models that use all available features, and \(F_d\) contains models using fewer than \(d\) features.</p> <p>For example, if we have two features, \(\{X_1, X_2\}\), we can train models using the subsets \(\{\}\), \(\{X_1\}\), \(\{X_2\}\), and \(\{X_1, X_2\}\). Each subset corresponds to a different hypothesis space, and the goal is to select the one that performs best according to a validation score.</p> <p>However, this approach quickly becomes computationally infeasible as the number of features grows. Exhaustively searching through all subsets of features leads to a combinatorial explosion, making it impractical for datasets with many features.</p> <h5 id="greedy-feature-selection-methods"><strong>Greedy Feature Selection Methods</strong></h5> <p>To overcome the inefficiency of exhaustive search, greedy algorithms such as forward selection and backward selection are commonly used.</p> <h6 id="forward-selection"><strong>Forward Selection</strong></h6> <p>Forward selection begins with an empty set of features and incrementally adds the most promising feature at each step. Initially, the model contains no features, represented as \(S = \{\}\). At each iteration:</p> <ol> <li> <p>For every feature not in the current set \(S\), a model is trained using the combined set \(S \cup \{i\}\).</p> </li> <li> <p>The performance of the model is evaluated, and a score, \(\alpha_i\), is assigned to each feature.</p> </li> <li> <p>The feature \(j\) with the highest score is added to the set, provided it improves the model’s performance.</p> </li> <li> <p>This process repeats until adding more features no longer improves the score.</p> </li> </ol> <h6 id="backward-selection"><strong>Backward Selection</strong></h6> <p>Backward selection starts at the opposite end of the spectrum. Instead of beginning with an empty set, it starts with all available features, \(S = \{X_1, X_2, \dots, X_p\}\). At each step, the feature that contributes the least to the model’s performance is removed. This process continues until no further removals improve the model’s score.</p> <h6 id="reflections-on-feature-selection"><strong>Reflections on Feature Selection</strong></h6> <p>Feature selection provides a natural way to control the complexity of a linear prediction function by limiting the number of features. The overarching goal is to strike a balance between minimizing training error and controlling model complexity, often through a scoring metric that incorporates both factors.</p> <p>While forward and backward selection methods are intuitive and computationally efficient, they have their limitations. For one, they do not guarantee finding the optimal subset of features. Additionally, the subsets selected by the two methods may differ, as the process is sensitive to the order in which features are evaluated.</p> <p>This brings us to an important question:</p> <blockquote> <p>Can feature selection be framed as a consistent optimization problem, leading to more robust and reliable solutions?</p> </blockquote> <p>In the next section, we explore how <strong>regularization</strong> offers a principled way to tackle this problem, providing a unified framework to balance model complexity and performance.</p> <hr/> <h4 id="l_1-and-l_2-regularization"><strong>\(L_1\) and \(L_2\) Regularization</strong></h4> <p>In the previous section, we discussed feature selection as a means to control model complexity. While effective, these methods are often computationally expensive and can lack consistency. Regularization offers a more systematic approach by introducing a <strong>complexity penalty</strong> directly into the objective function. This allows us to balance prediction performance with model simplicity in a principled manner.</p> <h5 id="complexity-penalty-balancing-simplicity-and-accuracy"><strong>Complexity Penalty: Balancing Simplicity and Accuracy</strong></h5> <p>The idea behind regularization is to augment the loss function with a penalty term that discourages overly complex models. For example, a scoring function for feature selection can be expressed as:</p> \[\text{score}(S) = \text{training_loss}(S) + \lambda |S|\] <p>where \(|S|\) is the number of selected features, and \(\lambda\) is a hyperparameter that controls the trade-off between training loss and complexity.</p> <p>A larger \(\lambda\) imposes a heavier penalty on complexity, meaning that adding an extra feature is only justified if it significantly improves the training loss—by at least \(\lambda\). This approach discourages the inclusion of unnecessary features, effectively shrinking the hypothesis space \(F\).</p> <p>However, directly using the number of features as a complexity measure is non-differentiable, making it hard to optimize. This limitation motivates the use of alternative measures, such as norms on the model weights, which provide a differentiable and computationally efficient framework.</p> <p><strong>Consider it like this:</strong>: Think of choosing ingredients for a dish. The training loss is like the flavor of the dish, and the penalty term is like the cost of adding ingredients. If you add too many ingredients (features), the cost goes up, and the dish may become overcomplicated or unbalanced. By introducing a penalty (regularization), you’re essentially saying, “Only add more ingredients if they significantly improve the flavor.” The larger the penalty (larger \(\lambda\)), the more careful you have to be about adding new ingredients, encouraging simplicity and preventing the dish from becoming too cluttered. This approach keeps the recipe (model) balanced and prevents unnecessary complexity.</p> <h6 id="soft-selection-through-weight-shrinkage"><strong>Soft Selection Through Weight Shrinkage</strong></h6> <p>Instead of hard feature selection, regularization encourages <strong>soft selection</strong> by penalizing the magnitude of the model weights. Consider a linear regression model:</p> \[f(x) = w^\top x\] <p>where \(w_i\) represents the weight for the \(i\)-th feature. If \(w_i\) is zero or close to zero, it effectively excludes the corresponding feature from the model.</p> <h6 id="why-shrink-weights"><strong>Why Shrink Weights?</strong></h6> <p>Intuitively, smaller weights make the model more stable. A regression line with a smaller slope produces smaller changes in the output for a given change in the input. This stability has two key benefits:</p> <ol> <li> <p><strong>Reduced Sensitivity to Noise:</strong> Smaller weights make the model less prone to overfitting, as predictions are less sensitive to fluctuations in the training data.</p> </li> <li> <p><strong>Better Generalization:</strong> By pushing weights toward zero, the model becomes less sensitive to variations in new datasets, improving its robustness.</p> </li> </ol> <h6 id="weight-shrinkage-in-polynomial-regression"><strong>Weight Shrinkage in Polynomial Regression</strong></h6> <p>In polynomial regression, where the \(n\)-th feature corresponds to the \(n\)-th power of \(x\), weight shrinkage plays a crucial role in preventing overfitting. For instance, consider two polynomial models:</p> \[\hat{y} = 0.001x^7 + 0.003x^3 + 1, \quad \text{and} \quad \hat{y} = 1000x^7 + 500x^3 + 1\] <p>The second model has large coefficients, making the curve “wiggle” excessively to fit the training data, a hallmark of overfitting. In contrast, the first model—with smaller weights—is smoother and less prone to overfitting.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Polynomial_Regression_Plot-480.webp 480w,/assets/img/Polynomial_Regression_Plot-800.webp 800w,/assets/img/Polynomial_Regression_Plot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Polynomial_Regression_Plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Polynomial_Regression_Plot" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Function Plots in Desmos </div> <p><strong>Think of it this way</strong>: Imagine you’re driving a car down a winding road. A car with a sensitive steering wheel (large weights) will make sharp turns with every slight variation in the road, making the ride bumpy and unpredictable. In contrast, a car with a more stable, less sensitive steering wheel (smaller weights) will handle the same road with smoother, more controlled movements, reducing the impact of small bumps and ensuring a more stable journey. Similarly, in regression, smaller weights lead to smoother, more stable models that are less prone to overfitting and better at handling new data.</p> <h5 id="linear-regression-with-l_2-regularization"><strong>Linear Regression with \(L_2\) Regularization</strong></h5> <p>Let’s formalize this idea using linear regression. For a dataset \(D_n = \{(x_1, y_1), \dots, (x_n, y_n)\}\), the objective in ordinary least squares is to minimize the mean squared error:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2.\] <p>While effective, this approach can overfit when the number of features \(d\) is large compared to the number of samples \(n\). For example, in natural language processing, it’s common to have millions of features but only thousands of documents.</p> <p>To address this, <strong>\(L_2\) regularization</strong> (also known as <strong>ridge regression</strong>) adds a penalty on the \(L_2\) norm of the weights:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2 + \lambda \|w\|_2^2,\] <p>where:</p> \[\|w\|_2^2 = w_1^2 + w_2^2 + \dots + w_d^2\] <p>This additional term penalizes large weights, shrinking them toward zero. When \(\lambda = 0\), the solution reduces to ordinary least squares. As \(\lambda\) increases, the penalty grows, favoring simpler models with smaller weights.</p> <p><strong>Intuition</strong>: Think of fitting a suit to someone. In ordinary least squares, you would tailor the suit to fit perfectly according to every measurement. However, if the person has an unusual body shape or you have limited data, the suit might end up being too tight in some areas, causing discomfort. With \(L_2\) regularization, it’s like adding some flexibility to the design, allowing for slight adjustments to ensure the suit is comfortable and fits well, even if the measurements aren’t perfect. This prevents overfitting and makes the model more robust, much like a well-tailored suit that remains comfortable under different conditions.</p> <h6 id="generalization-to-other-models"><strong>Generalization to Other Models</strong></h6> <p>Although we’ve illustrated \(L_2\) regularization with linear regression, the concept extends naturally to other models, including neural networks. By penalizing the magnitude of weights, \(L_2\) regularization helps improve generalization across a wide range of machine learning tasks.</p> <hr/> <h6 id="closing-thoughts"><strong>Closing Thoughts</strong></h6> <p>Regularization, whether through weight shrinkage or complexity penalties, provides a robust mechanism to balance model expressiveness and generalization. In the next section, we’ll explore <strong>\(L_1\) regularization</strong>, its sparsity-inducing properties, and how it differs from \(L_2\) regularization.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.]]></summary></entry><entry><title type="html">Loss Functions - Regression and Classification</title><link href="https://monishver11.github.io/blog/2025/loss-functions/" rel="alternate" type="text/html" title="Loss Functions - Regression and Classification"/><published>2025-01-02T21:28:00+00:00</published><updated>2025-01-02T21:28:00+00:00</updated><id>https://monishver11.github.io/blog/2025/loss-functions</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/loss-functions/"><![CDATA[<p>Loss functions are of critical importance to machine learning, guiding models to minimize errors and improve predictions. They quantify how far off a model’s predictions are from the actual outcomes and serve as the basis for optimization. In this post, we’ll explore loss functions for <strong>regression</strong> and <strong>classification</strong> problems, breaking down their mathematical foundations and building intuitive understanding along the way. We will then transition our focus to logistic regression, examining its relationship with loss functions in classification tasks.</p> <hr/> <h4 id="loss-functions-for-regression"><strong>Loss Functions for Regression</strong></h4> <p>Regression tasks focus on predicting continuous values. Think about forecasting stock prices, estimating medical costs based on patient details, or predicting someone’s age from their photograph. These problems share a common requirement: accurately measuring how close the predicted values are to the true values.</p> <h6 id="setting-the-stage-notation"><strong>Setting the Stage: Notation</strong></h6> <p>Before diving in, let’s clarify the notation:</p> <ul> <li>\(\hat{y}\) represents the predicted value (the model’s output).</li> <li>\(y\) denotes the actual observed value (the ground truth).</li> </ul> <p>A <strong>loss function</strong> for regression maps the predicted and actual values to a real number: \(\ell(\hat{y}, y) \in \mathbb{R}.\) Most regression losses are based on the <strong>residual</strong>, defined as:</p> \[r = y - \hat{y}\] <p>The residual captures the difference between the true value and the prediction.</p> <h6 id="what-makes-a-loss-function-distance-based"><strong>What Makes a Loss Function Distance-Based?</strong></h6> <p>A loss function is <strong>distance-based</strong> if it meets two criteria:</p> <ol> <li>It depends solely on the residual: \(\ell(\hat{y}, y) = \psi(y - \hat{y}),\) where \(\psi: \mathbb{R} \to \mathbb{R}.\)</li> <li>It equals zero when the residual is zero: \(\psi(0) = 0.\)</li> </ol> <p>Such loss functions are <strong>translation-invariant</strong>, meaning they remain unaffected if both the prediction and the actual value are shifted by the same amount: \(\ell(\hat{y} + b, y + b) = \ell(\hat{y}, y), \quad \forall b \in \mathbb{R}.\)</p> <p>However, in some scenarios, translation invariance may not be desirable. For example, using the <strong>relative error</strong>: \(\text{Relative error} = \frac{\hat{y} - y}{y},\) provides a loss function better suited to cases where proportional differences matter.</p> <p>For instance:</p> <ul> <li>If the actual stock price is $100, and your model predicts $110, the absolute error is $10, but the relative error is 10%.</li> <li>But, if the actual stock price is $10, and your model predicts $11, the absolute error is still $1, but the relative error is 10%.</li> </ul> <h5 id="exploring-common-loss-functions-for-regression"><strong>Exploring Common Loss Functions for Regression</strong></h5> <h6 id="1-squared-loss-l2-loss"><strong>1. Squared Loss (L2 Loss)</strong></h6> <p>Squared loss is one of the most widely used loss functions:</p> \[\ell(r) = r^2 = (y - \hat{y})^2\] <p>This loss penalizes large residuals more heavily, making it sensitive to outliers. Its simplicity and differentiability make it popular in linear regression and similar models.</p> <h6 id="2-absolute-loss-l1-loss"><strong>2. Absolute Loss (L1 Loss)</strong></h6> <p>Absolute loss measures the magnitude of the residual:</p> \[\ell(r) = |r| = |y - \hat{y}|\] <p>Unlike squared loss, absolute loss is robust to outliers but lacks smooth differentiability.</p> <p><strong>Think of it this way</strong>: Imagine predicting house prices based on size. If one house in the dataset has an extremely high price (an outlier), using absolute loss will make the model focus more on the typical pricing pattern of most houses and ignore the outlier. In contrast, least squares regression would try to minimize the error caused by that outlier, potentially distorting the model.</p> <h6 id="3-huber-loss"><strong>3. Huber Loss</strong></h6> <p>The Huber loss combines the best of both worlds:</p> \[\ell(r) = \begin{cases} \frac{1}{2}r^2 &amp; \text{if } |r| \leq \delta, \\ \delta |r| - \frac{1}{2}\delta^2 &amp; \text{if } |r| &gt; \delta. \end{cases}\] <p>For small residuals, it behaves like squared loss, while for large residuals, it switches to absolute loss, providing robustness without sacrificing differentiability. <strong>Note</strong>: Equal values and slopes at \((r = \delta)\).</p> <p><strong>Understanding Robustness</strong>: It describes a loss function’s resistance to the influence of outliers.</p> <ul> <li><strong>Squared loss</strong> is highly sensitive to outliers.</li> <li><strong>Absolute loss</strong> is much more robust.</li> <li><strong>Huber loss</strong> strikes a balance between sensitivity and robustness. Meaning, it is sensitive enough to provide a useful gradient for smaller errors (via L2), but becomes more robust to large residuals, preventing them from disproportionately influencing the model (via L1).</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Regression_Losses-480.webp 480w,/assets/img/Regression_Losses-800.webp 800w,/assets/img/Regression_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Regression_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Regression_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Regression Loss Functions </div> <hr/> <h4 id="loss-functions-for-classification"><strong>Loss Functions for Classification</strong></h4> <p>Classification tasks involve predicting discrete labels. For instance, we might want to decide whether an email is spam or if an image contains a cat. The challenge lies in guiding the model to make accurate predictions while quantifying the degree of correctness.</p> <h6 id="the-role-of-the-score-function"><strong>The Role of the Score Function</strong></h6> <p>In binary classification, the model predicts a score, \(f(x)\), for each input \(x\):</p> <ul> <li>If \(f(x) &gt; 0\), the model predicts the label \(1\).</li> <li>If \(f(x) &lt; 0\), the model predicts the label \(-1\).</li> </ul> <p>This score represents the model’s confidence, and its magnitude indicates how certain the prediction is.</p> <h6 id="what-is-the-margin"><strong>What is the Margin?</strong></h6> <p>The <strong>margin</strong> captures the relationship between the predicted score and the true label:</p> \[m = y\hat{y}\] <p>or equivalently:</p> \[m = yf(x)\] <p>The margin measures correctness:</p> <ul> <li><strong>Positive margin</strong>: The prediction is correct.</li> <li><strong>Negative margin</strong>: The prediction is incorrect.</li> </ul> <p>The goal of many classification tasks is to maximize this margin, ensuring confident and accurate predictions.</p> <h5 id="common-loss-functions-for-classification"><strong>Common Loss Functions for Classification</strong></h5> <h6 id="1-0-1-loss"><strong>1. 0-1 Loss</strong></h6> <p>The 0-1 loss is a simple yet impractical loss function:</p> \[\ell(y, \hat{y}) = \begin{cases} 0 &amp; \text{if } y = \hat{y} \\ 1 &amp; \text{if } y \neq \hat{y} \end{cases}\] <p>Alternatively,</p> \[\ell_{0-1}(f(x), y) = \mathbf{1}[yf(x) \leq 0]\] <p>Here, \(\mathbf{1}\) is the indicator function, which equals 1 if the condition is true and 0 otherwise.</p> <p>Although intuitive, the 0-1 loss is:</p> <ul> <li><strong>Non-convex</strong>, making optimization difficult, because its value is either 0 or 1, which creates a step-like behavior.</li> <li><strong>Non-differentiable</strong>, rendering gradient-based methods inapplicable. For instance, if \(\hat{y} = 0.5\), the loss could change abruptly from 0 to 1 depending on whether the true label \(y\) is 0 or 1, leading to no gradient at this boundary.</li> </ul> <h6 id="2-hinge-loss"><strong>2. Hinge Loss</strong></h6> <p>Hinge loss, commonly used in Support Vector Machines (SVMs), addresses the limitations of 0-1 loss:</p> \[\ell_{\text{Hinge}}(m) = \max(1 - m, 0)\] <p>It is a convex, upper bound on 0-1 loss and encourages a positive margin. However, it is not differentiable at \(m = 1\).</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Classification_Losses-480.webp 480w,/assets/img/Classification_Losses-800.webp 800w,/assets/img/Classification_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Classification_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classification_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Classification Loss Functions </div> <hr/> <h4 id="diving-deeper-logistic-regression"><strong>Diving Deeper: Logistic Regression</strong></h4> <p>In our exploration above, we’ve covered the basics of regression and classification losses. Now, let’s shift our focus to <strong>logistic regression</strong> and its corresponding loss functions, which are pivotal in classification problems. We’ll also touch on why square loss isn’t typically used for classification.</p> <p>Despite its name, <strong>logistic regression</strong> is not actually a regression algorithm—it’s a <strong>linear classification</strong> method. Logistic regression predicts probabilities, making it well-suited for binary classification problems.</p> <p>The predictions are modeled using the <strong>sigmoid function</strong>, denoted by \(\sigma(z)\), where:</p> \[\sigma(z) = \frac{1}{1 + \exp(-z)}\] <p>and \(z = f(x) = w^\top x\) is the score computed from the input features and weights.</p> <h5 id="logistic-regression-with-labels-as-0-or-1"><strong>Logistic Regression with Labels as 0 or 1</strong></h5> <p>When the labels are in \(\{0, 1\}\):</p> <ul> <li> <p>The predicted probability is: \(\hat{y} = \sigma(z)\)</p> </li> <li> <p>The loss function for logistic regression in this case is the <strong>binary cross-entropy loss</strong>:</p> \[\ell_{\text{Logistic}} = -y \log(\hat{y}) - (1 - y) \log(1 - \hat{y})\] </li> </ul> <p>Here’s how it works based on different predicted values of \(\hat{y}\):</p> <ul> <li><strong>If \(y = 1\) (True label is 1)</strong>: <ul> <li> <p>The loss is:</p> \[\ell_{\text{Logistic}} = -\log(\hat{y})\] <p>This means if the predicted probability \(\hat{y}\) is close to 1 (i.e., the model is confident that the class is 1), the loss will be very small (approaching 0). On the other hand, if \(\hat{y}\) is close to 0, the loss becomes large, penalizing the model for being very wrong.</p> </li> </ul> </li> <li><strong>If \(y = 0\) (True label is 0)</strong>: <ul> <li> <p>The loss is:</p> \[\ell_{\text{Logistic}} = -\log(1 - \hat{y})\] <p>In this case, if the predicted probability \(\hat{y}\) is close to 0 (i.e., the model correctly predicts the class as 0), the loss will be very small (approaching 0). However, if \(\hat{y}\) is close to 1, the loss becomes large, penalizing the model for incorrectly predicting class 1.</p> </li> </ul> </li> </ul> <h6 id="example-of-different-predicted-values">Example of Different Predicted Values:</h6> <ol> <li><strong>For a true label \(y = 1\):</strong> <ul> <li> <p>If \(\hat{y} = 0.9\): \(\ell_{\text{Logistic}} = -\log(0.9) \approx 0.105\) This is a small loss, since the model predicted a high probability for class 1, which is correct.</p> </li> <li> <p>If \(\hat{y} = 0.1\): \(\ell_{\text{Logistic}} = -\log(0.1) \approx 2.302\) This is a large loss, since the model predicted a low probability for class 1, which is incorrect.</p> </li> </ul> </li> <li><strong>For a true label \(y = 0\):</strong> <ul> <li> <p>If \(\hat{y} = 0.1\): \(\ell_{\text{Logistic}} = -\log(1 - 0.1) \approx 0.105\) This is a small loss, since the model predicted a low probability for class 1, which is correct.</p> </li> <li> <p>If \(\hat{y} = 0.9\): \(\ell_{\text{Logistic}} = -\log(1 - 0.9) \approx 2.302\) This is a large loss, since the model predicted a high probability for class 1, which is incorrect.</p> </li> </ul> </li> </ol> <h6 id="key-points">Key Points:</h6> <ul> <li>The <strong>negative sign</strong> in the loss function ensures that when the model predicts correctly (i.e., \(\hat{y}\) is close to the true label), the loss is minimized (approaching 0).</li> <li>The loss grows as the predicted probability \(\hat{y}\) moves away from the true label \(y\), and it grows more rapidly as the predicted probability becomes more confident but incorrect.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Binary_Cross_Entropy_Loss-480.webp 480w,/assets/img/Binary_Cross_Entropy_Loss-800.webp 800w,/assets/img/Binary_Cross_Entropy_Loss-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Binary_Cross_Entropy_Loss.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Binary_Cross_Entropy_Loss" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Binary Cross Entropy Loss Function(https://www.desmos.com/calculator/ygciza1leg) </div> <h5 id="logistic-regression-with-labels-as--1-or-1"><strong>Logistic Regression with Labels as -1 or 1</strong></h5> <p>When the labels are in \(\{-1, 1\}\), the sigmoid function simplifies using the property: \(1 - \sigma(z) = \sigma(-z).\)</p> <p>This allows us to express the loss equivalently as:</p> \[\ell_{\text{Logistic}} = \begin{cases} -\log(\sigma(z)) &amp; \text{if } y = 1, \\ -\log(\sigma(-z)) &amp; \text{if } y = -1. \end{cases}\] <p>Simplifying further:</p> \[\ell_{\text{Logistic}} = -\log(\sigma(yz)) = -\log\left(\frac{1}{1 + e^{-yz}}\right) = \log(1 + e^{-m})\] <p>where \(m = yz\) is the margin.</p> <h6 id="key-insights-of-logistic-loss"><strong>Key Insights of Logistic loss</strong>:</h6> <ul> <li>Is differentiable, enabling gradient-based optimization.</li> <li>Always rewards larger margins, encouraging more confident predictions.</li> <li>Never becomes zero, ensuring continuous optimization pressure.</li> </ul> <h6 id="what-about-square-loss-for-classification"><strong>What About Square Loss for Classification?</strong></h6> <p>Square loss, while effective for regression, is rarely used for classification. Let’s break it down:</p> \[\ell(f(x), y) = (f(x) - y)^2\] <p>For binary classification where \(y \in \{-1, 1\}\), we can rewrite this in terms of the margin:</p> \[\ell(f(x), y) = (f(x) - y)^2 = f^2(x) - 2f(x)y + y^2.\] <p>Using the fact that \(y^2 = 1\):</p> \[\ell(f(x), y) = f^2(x) - 2f(x)y + 1 = (1 - f(x)y)^2 = (1 - m)^2.\] <h6 id="why-not-use-square-loss"><strong>Why Not Use Square Loss?</strong></h6> <p>Square loss heavily penalizes outliers, such as mislabeled examples, making it unsuitable for classification tasks where robust performance on noisy data is crucial.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Logistic_Regression_Losses-480.webp 480w,/assets/img/Logistic_Regression_Losses-800.webp 800w,/assets/img/Logistic_Regression_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Logistic_Regression_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Logistic_Regression_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Logistic Regression Loss Functions </div> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>Loss functions form the backbone of machine learning, providing a mathematical framework for optimization. A quick recap:</p> <ul> <li><strong>Regression Losses</strong>: <ul> <li>Squared (L2) loss: Sensitive to outliers.</li> <li>Absolute (L1) loss: Robust but non-differentiable.</li> <li>Huber loss: Balances robustness and smoothness.</li> </ul> </li> <li><strong>Classification Losses</strong>: <ul> <li>Hinge loss: Encourages a large positive margin (used in SVMs).</li> <li>Logistic loss: Differentiable and rewards confidence.</li> </ul> </li> </ul> <p>These concepts tie back to critical components of machine learning workflows, such as <strong>gradient descent</strong>, which relies on the properties of loss functions to update model parameters effectively.</p> <p>Up next, we’ll dive into <strong>Regularization</strong>, focusing on how it combats overfitting and improves model performance. Stay tuned!</p> ]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.]]></summary></entry></feed>