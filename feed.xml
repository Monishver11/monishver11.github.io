<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-11T02:38:01+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Demystifying SVMs - Understanding Complementary Slackness and Support Vectors</title><link href="https://monishver11.github.io/blog/2025/svm-dual-problem/" rel="alternate" type="text/html" title="Demystifying SVMs - Understanding Complementary Slackness and Support Vectors"/><published>2025-01-10T21:02:00+00:00</published><updated>2025-01-10T21:02:00+00:00</updated><id>https://monishver11.github.io/blog/2025/svm-dual-problem</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/svm-dual-problem/"><![CDATA[<p>At the heart of SVMs lies a fascinating optimization framework that balances maximizing the margin between classes and minimizing classification errors. This post dives into the dual formulation of the SVM optimization problem, exploring its mathematical underpinnings, derivation, and insights.</p> <hr/> <h4 id="the-svm-primal-problem"><strong>The SVM Primal Problem</strong></h4> <p>To understand the dual problem, we first start with the <strong>primal optimization problem</strong> of SVMs. It aims to find the optimal hyperplane that separates two classes while allowing for some misclassification through slack variables. The primal problem is expressed as:</p> \[\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + \frac{c}{n} \sum_{i=1}^n \xi_i\] <p>subject to the constraints:</p> \[-\xi_i \leq 0 \quad \text{for } i = 1, \dots, n\] \[1 - y_i (w^T x_i + b) - \xi_i \leq 0 \quad \text{for } i = 1, \dots, n\] <p>Here:</p> <ul> <li>\(w\) is the weight vector defining the hyperplane,</li> <li>\(b\) is the bias term,</li> <li>\(\xi_i\) are slack variables that allow some points to violate the margin, and</li> <li>\(C\) is the regularization parameter controlling the trade-off between maximizing the margin and minimizing errors.</li> </ul> <h4 id="lagrangian-formulation"><strong>Lagrangian Formulation</strong></h4> <p>To solve this constrained optimization problem, we use the method of <strong>Lagrange multipliers</strong>. Introducing \(\alpha_i\) and \(\lambda_i\) as multipliers for the inequality constraints, the <strong>Lagrangian</strong> becomes:</p> \[L(w, b, \xi, \alpha, \lambda) = \frac{1}{2} \|w\|^2 + \frac{c}{n} \sum_{i=1}^n \xi_i + \sum_{i=1}^n \alpha_i \left( 1 - y_i (w^T x_i + b) - \xi_i \right) + \sum_{i=1}^n \lambda_i (-\xi_i)\] <p>Here, the terms involving \(\alpha_i\) and \(\lambda_i\) enforce the constraints, while the first term captures the objective of maximizing the margin.</p> <table> <thead> <tr> <th>Lagrange Multiplier</th> <th>Constraint</th> </tr> </thead> <tbody> <tr> <td>\(\lambda_i\)</td> <td>\(-\xi_i \leq 0\)</td> </tr> <tr> <td>\(\alpha_i\)</td> <td>\((1 - y_i[w^T x_i + b]) - \xi_i \leq 0\)</td> </tr> </tbody> </table> <hr/> <h4 id="strong-duality-and-slaters-condition"><strong>Strong Duality and Slater’s Condition</strong></h4> <p>The next step is to leverage <strong>strong duality</strong>, which states that for certain optimization problems, the dual problem provides the same optimal value as the primal. For SVMs, strong duality holds due to <strong>Slater’s constraint qualification</strong>, which requires the problem to:</p> <ul> <li>Have a convex objective function,</li> <li>Include affine constraints, and</li> <li>Possess feasible points. [How and what are those points?]</li> </ul> <p>In the context of <strong>Slater’s constraint qualification</strong> and <strong>strong duality</strong> for SVMs, <strong>feasible points</strong> refer to points in the feasible region that satisfy all the constraints of the primal optimization problem. Specifically, for SVMs, these points are:</p> <ol> <li> <p><strong>Convex Objective Function</strong>: The objective of the SVM (maximizing the margin, which is a quadratic optimization problem) is convex, meaning it has a global minimum.</p> </li> <li> <p><strong>Affine Constraints</strong>: These constraints are linear equations (or inequalities) that define the feasible region, such as ensuring that all data points are correctly classified. In mathematical form, for each data point \(y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1\).</p> </li> <li> <p><strong>Existence of Feasible Points</strong>: There must be at least one point in the domain that satisfies all of these constraints. In SVMs, this is satisfied when the data is linearly separable, meaning there exists a hyperplane that can perfectly separate the positive and negative classes. Slater’s condition requires that there be strictly feasible points, where the constraints are strictly satisfied (i.e., not just touching the boundary of the feasible region).</p> </li> </ol> <p>For SVMs, the feasible points are those that satisfy: \(y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 \quad \text{for all data points}\)</p> <p>These points are strictly inside the feasible region, meaning there is a margin between the hyperplane and the data points, ensuring a gap.</p> <p>In practical terms, <strong>Slater’s condition</strong> implies that there exists a hyperplane that not only separates the two classes but also satisfies the strict inequalities for the margin (i.e., it does not lie on the boundary). This strict feasibility is critical for the <strong>strong duality</strong> theorem to hold.</p> <h4 id="deriving-the-svm-dual-function"><strong>Deriving the SVM Dual Function</strong></h4> <p>The dual function is obtained by minimizing the Lagrangian over the primal variables \(w\), \(b\), and \(\xi\):</p> \[g(\alpha, \lambda) = \inf_{w, b, \xi} L(w, b, \xi, \alpha, \lambda)\] <p>This can be simplified to (after shuffling and grouping):</p> \[g(\alpha, \lambda) = \inf_{w, b, \xi} \left[ \frac{1}{2} w^T w + \sum_{i=1}^n \xi_i \left( \frac{c}{n} - \alpha_i - \lambda_i \right) + \sum_{i=1}^n \alpha_i \left( 1 - y_i \left[ w^T x_i + b \right] \right) \right]\] <p>This minimization leads to the following <strong>first-order optimality conditions</strong>:</p> <ol> <li> <p><strong>Gradient with respect to \(w\):</strong> Differentiating \(L\) with respect to \(w\), we get:</p> \[\frac{\partial L}{\partial w} = w - \sum_{i=1}^n \alpha_i y_i x_i = 0\] <p>Solving for \(w\), we find:</p> \[w = \sum_{i=1}^n \alpha_i y_i x_i\] </li> <li> <p><strong>Gradient with respect to \(b\):</strong> Differentiating \(L\) with respect to \(b\), we obtain:</p> \[\frac{\partial L}{\partial b} = -\sum_{i=1}^n \alpha_i y_i = 0\] <p>This implies the constraint:</p> \[\sum_{i=1}^n \alpha_i y_i = 0\] </li> <li> <p><strong>Gradient with respect to \(\xi_i\):</strong> Differentiating \(L\) with respect to \(\xi_i\), we have:</p> \[\frac{\partial L}{\partial \xi_i} = \frac{c}{n} - \alpha_i - \lambda_i = 0\] <p>This leads to the relationship:</p> \[\alpha_i + \lambda_i = \frac{c}{n}\] </li> </ol> <h4 id="the-svm-dual-problem"><strong>The SVM Dual Problem</strong></h4> <p>Substituting these conditions back into \(L\)(Lagrangian), the second term disappears.</p> <p>First and third terms become:</p> \[\frac{1}{2}w^T w = \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j x_j^T x_i\] \[\sum_{i=1}^n \alpha_i \left( 1 - y_i \left[ w^T x_i + b \right] \right) = \sum_i \alpha_i - \sum_{i,j} \alpha_i \alpha_j y_i y_j x_j^T x_i - b \sum_{i=1}^n \alpha_i y_i\] <p>Putting it together, the dual function is:</p> \[g(\alpha, \lambda) = \begin{cases} \sum_{i=1}^{n} \alpha_i - \frac{1}{2}\sum_{i,j=1}^{n} \alpha_i \alpha_j y_i y_j x_j^T x_i &amp; \text{if } \sum_{i=1}^{n} \alpha_i y_i = 0 \text{ and } \alpha_i + \lambda_i = \frac{c}{n}, \text{ all } i \\ -\infty &amp; \text{otherwise} \end{cases}\] <p><strong>Quick tip</strong>: Go ahead and write the derivation yourself to see what cancels out. It’s much easier to follow the flow this way, and you’ll better understand how the second term in the equation above is derived.</p> <p><strong>The dual problem is</strong></p> \[\sup_{\alpha, \lambda \geq 0} g(\alpha, \lambda)\] \[\text{s.t. } \begin{cases} \sum_{i=1}^{n} \alpha_i y_i = 0 \\ \alpha_i + \lambda_i = \frac{c}{n}, \text{ } \alpha_i, \lambda_i \geq 0, \text{ } i = 1, ..., n \end{cases}\] <p>Don’t stress over this complex equation; we’ll break down its meaning and significance as we continue. Keep reading!</p> <h5 id="insights-from-the-dual-problem"><strong>Insights from the Dual Problem</strong></h5> <p>The dual problem offers several key insights into the optimization process of SVMs:</p> <ol> <li> <p><strong>Duality and Optimality:</strong><br/> Strong duality ensures that the primal and dual problems yield the same optimal value, provided conditions like Slater’s are met.</p> </li> <li> <p><strong>Dual Variables:</strong><br/> The variables \(\alpha_i\) and \(\lambda_i\) are Lagrange multipliers, indicating how sensitive the objective function is to the constraints. Large \(\alpha_i\) values correspond to constraints that are most violated.</p> </li> <li> <p><strong>Constraint Interpretation:</strong><br/> The constraint \(\sum_{i=1}^{n} \alpha_i y_i = 0\) ensures the hyperplane passes through the origin, while \(\alpha_i + \lambda_i = \frac{c}{n}\) connects the dual variables with the regularization parameter \(c\).</p> </li> <li> <p><strong>Support Vectors:</strong><br/> Non-zero \(\alpha_i\) values indicate support vectors, which are the data points closest to the decision boundary and crucial for defining the margin.</p> </li> <li> <p><strong>Weight Vector Representation:</strong><br/> The weight vector \(w\) lies in the space spanned by the support vectors: \(w = \sum_{i=1}^n \alpha_i y_i x_i\)</p> </li> </ol> <p>In essence, the dual problem simplifies the primal by focusing on constraints and provides insights into how data points affect the model’s decision boundary.</p> <p>[So, instead of this a better explanation is required for the above dual problem interpretation. check if an intuition is needed]</p> <h4 id="kkt-conditions"><strong>KKT Conditions</strong></h4> <p>For convex problems, if Slater’s condition is satisfied, the <strong>Karush-Kuhn-Tucker (KKT) conditions</strong> provide necessary and sufficient conditions for the optimal solution. For the <strong>SVM dual problem</strong>, these conditions can be expressed as:</p> <ul> <li> <p><strong>Primal Feasibility:</strong> \(f_i(x) \leq 0 \quad \forall i\)<br/> This condition ensures that the constraints of the primal problem are satisfied. In the context of SVM, this means that all data points are correctly classified by the hyperplane, i.e., for each data point \(i\), the constraint \(y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1\) is satisfied.</p> </li> <li> <p><strong>Dual Feasibility:</strong> \(\lambda_i \geq 0\)<br/> This condition ensures that the dual variables (Lagrange multipliers) are non-negative. For SVMs, it means the Lagrange multipliers \(\alpha_i\) associated with the classification constraints must be non-negative, i.e., \(\alpha_i \geq 0\).</p> </li> <li> <p><strong>Complementary Slackness:</strong> \(\lambda_i f_i(x) = 0\)<br/> This condition means that either the constraint is <strong>active</strong> (i.e., the constraint is satisfied with equality) or the dual variable is zero. For SVMs, it implies that if a data point is a support vector (i.e., it lies on the margin), then the corresponding \(\alpha_i\) is positive. Otherwise, for non-support vectors, \(\alpha_i = 0\).</p> </li> <li> <p><strong>First-Order Condition:</strong> \(\frac{\partial}{\partial x} L(x, \lambda) = 0\)<br/> The first-order condition ensures that the Lagrangian \(L(x, \lambda)\) is minimized with respect to the optimization variables. In SVMs, this condition leads to the optimal weights \(\mathbf{w}\) and bias \(b\) that define the separating hyperplane.</p> </li> </ul> <p><strong>To summarize</strong>:</p> <ul> <li><strong>Slater’s Condition</strong> ensures strong duality.</li> <li><strong>KKT Conditions</strong> ensure the existence of the optimal solution and give the specific conditions under which the solution occurs.</li> </ul> <p>[Add reference and explain the above part well, specifically in this case]</p> <h4 id="the-svm-dual-solution"><strong>The SVM Dual Solution</strong></h4> <p>We can express the <strong>SVM dual problem</strong> as follows:</p> \[\sup_{\alpha} \sum_{i=1}^{n} \alpha_{i} - \frac{1}{2} \sum_{i,j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{j}^{T} x_{i}\] <p>subject to:</p> \[\sum_{i=1}^{n} \alpha_{i} y_{i} = 0\] \[\alpha_{i} \in [0, \frac{c}{n}], \quad i = 1, ..., n\] <p>In this formulation, \(\alpha_i\) are the Lagrange multipliers, which must satisfy the constraints. The dual problem maximizes the objective function involving these multipliers, while ensuring that the constraints are met.</p> <p>Once we have the optimal solution \(\alpha^*\) to the dual problem, the primal solution \(w^*\) can be derived as:</p> \[w^{*} = \sum_{i=1}^{n} \alpha_{i}^{*} y_{i} x_{i}\] <p>This shows that the optimal weight vector \(w^*\) is a linear combination of the input vectors \(x_i\), weighted by the corresponding \(\alpha_i^*\) and \(y_i\).</p> <p>It’s important to note that the solution is in the <strong>space spanned by the inputs</strong>. This means the decision boundary is influenced by the data points that lie closest to the hyperplane, i.e., the <strong>support vectors</strong>.</p> <p>The constraints \(\alpha_{i} \in [0, c/n]\) indicate that \(c\) controls the maximum weight assigned to each example. In other words, \(c\) acts as a regularization parameter, controlling the trade-off between achieving a large margin and minimizing classification errors. A larger \(c\) leads to less regularization, allowing the model to fit more closely to the training data, while a smaller \(c\) introduces more regularization, promoting a simpler model that may generalize better.</p> <p>Think of \(c\) as a <strong>“penalty meter”</strong> that controls how much you care about fitting the training data:</p> <ul> <li>A <strong>high \(c\)</strong> means you are <strong>less tolerant of mistakes</strong>. The model will try to fit the data perfectly, even if it leads to overfitting (less regularization).</li> <li>A <strong>low \(c\)</strong> means you’re more focused on <strong>simplicity and generalization</strong>. The model will allow some mistakes in the training data to avoid overfitting and create a smoother decision boundary (more regularization).</li> </ul> <p>Next, we will explore how the <strong>Complementary Slackness</strong> condition in the SVM dual formulation extends to <strong>kernel trick</strong>, enabling SVMs to handle non-linear decision boundaries effectively.</p> <hr/> <h4 id="understanding-complementary-slackness-in-svms"><strong>Understanding Complementary Slackness in SVMs</strong></h4> <p>In this section, we will focus on <strong>complementary slackness</strong>, a key property of optimization problems, and its implications for SVMs. We will also discuss how it connects with the margin, slack variables, and the role of support vectors.</p> <h5 id="revisiting-constraints-and-lagrange-multipliers"><strong>Revisiting Constraints and Lagrange Multipliers</strong></h5> <p>To understand complementary slackness, let’s start by recalling the constraints and Lagrange multipliers in the SVM problem:</p> <ol> <li> <p>The constraint on the slack variables:</p> \[-\xi_i \leq 0,\] <p>with Lagrange multiplier \(\lambda_i\).</p> </li> <li> <p>The margin constraint:</p> \[1 - y_i f(x_i) - \xi_i \leq 0,\] <p>with Lagrange multiplier \(\alpha_i\).</p> </li> </ol> <p>From the <strong>first-order condition</strong> with respect to \(\xi_i\), we derived the relationship:</p> \[\lambda_i^* = \frac{c}{n} - \alpha_i^*,\] <p>where \(c\) is the regularization parameter.</p> <p>By <strong>strong duality</strong>, the complementary slackness conditions must hold, which state:</p> \[\alpha_i^* \left( 1 - y_i f^*(x_i) - \xi_i^* \right) = 0\] <p>and,</p> \[\lambda_i^* \xi_i^* = \left( \frac{c}{n} - \alpha_i^* \right) \xi_i^* = 0\] <p>These conditions essentially enforce that either the constraints are satisfied exactly or their corresponding Lagrange multipliers vanish.</p> <h5 id="what-does-complementary-slackness-tell-us"><strong>What Does Complementary Slackness Tell Us?</strong></h5> <p>Complementary slackness provides crucial insights into the relationship between the dual variables \(\alpha_i^*\), the slack variables \(\xi_i^*\), and the margin \(1 - y_i f^*(x_i)\):</p> <ul> <li><strong>When \(y_i f^*(x_i) &gt; 1\):</strong> <ul> <li>The margin loss is zero (\(\xi_i^* = 0\)).</li> <li>As a result, \(\alpha_i^* = 0\), meaning these examples do not influence the decision boundary.</li> </ul> </li> <li><strong>When \(y_i f^*(x_i) &lt; 1\):</strong> <ul> <li>The margin loss is positive (\(\xi_i^* &gt; 0\)).</li> <li>In this case, \(\alpha_i^* = \frac{c}{n}\), assigning the maximum weight to these examples.</li> </ul> </li> <li><strong>When \(\alpha_i^* = 0\):</strong> <ul> <li>This implies \(\xi_i^* = 0\), so \(y_i f^*(x_i) \geq 1\), meaning the example is correctly classified with no margin loss.</li> </ul> </li> <li><strong>When \(\alpha_i^* \in (0, \frac{c}{n})\):</strong> <ul> <li>This implies \(\xi_i^* = 0\), and the example lies exactly on the margin (\(1 - y_i f^*(x_i) = 0\)).</li> </ul> </li> </ul> <p>We can summarize these relationships(between margin and example weights) as follows:</p> <ol> <li><strong>If \(\alpha_i^* = 0\):</strong> The example satisfies \(y_i f^*(x_i) \geq 1\), indicating no margin loss.</li> <li><strong>If \(\alpha_i^* \in (0, \frac{c}{n})\):</strong> The example lies exactly on the margin, with \(y_i f^*(x_i) = 1\).</li> <li><strong>If \(\alpha_i^* = \frac{c}{n}\):</strong> The example incurs a margin loss, with \(y_i f^*(x_i) \leq 1\).</li> </ol> <p>and the other way is:</p> \[y_if^*(x_i) &lt; 1 \Rightarrow α_i^* = \frac{c}{n}\] \[y_if^*(x_i) = 1 \Rightarrow α_i^* \in [0, \frac{c}{n}]\] \[y_if^*(x_i) &gt; 1 \Rightarrow α_i^* = 0\] <p>These relationships are foundational to understanding how SVMs allocate weights to examples and define the decision boundary.</p> <p>[A easy/good way to remember this or interalize this relationships]</p> <hr/> <h4 id="support-vectors-the-pillars-of-svms"><strong>Support Vectors: The Pillars of SVMs</strong></h4> <p>The dual formulation of SVMs reveals that the weight vector \(w^*\) can be expressed as:</p> \[w^* = \sum_{i=1}^n \alpha_i^* y_i x_i.\] <p>Here, the examples \(x_i\) with \(\alpha_i^* &gt; 0\) are known as <strong>support vectors</strong>. These are the critical data points that determine the hyperplane. Examples with \(\alpha_i^* = 0\) do not influence the solution, leading to <strong>sparsity</strong> in the SVM model. This sparsity is one of the key reasons why SVMs are computationally efficient for large datasets.</p> <h5 id="the-role-of-inner-products-in-the-dual-problem"><strong>The Role of Inner Products in the Dual Problem</strong></h5> <p>An intriguing aspect of the dual problem is that it depends on the input data \(x_i\) and \(x_j\) only through their <strong>inner product</strong>:</p> \[\langle x_i, x_j \rangle = x_i^T x_j.\] <p>This dependence on inner products allows us to generalize SVMs using <strong>kernel methods</strong>, where the inner product \(x_i^T x_j\) is replaced with a kernel function \(K(x_i, x_j)\). Kernels enable SVMs to implicitly operate in high-dimensional feature spaces without explicitly transforming the data, making it possible to model complex, non-linear decision boundaries.</p> <p>The kernelized dual problem is written as:</p> \[\max_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j),\] <p>subject to:</p> <ul> <li>\(\sum_{i=1}^n \alpha_i y_i = 0\),</li> <li>\(0 \leq \alpha_i \leq C\), for \(i = 1, \dots, n\).</li> </ul> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>Complementary slackness conditions reveal much about the structure and workings of SVMs. They show how the margin, slack variables, and dual variables interact and highlight the pivotal role of support vectors. Moreover, the reliance on inner products paves the way for kernel methods, unlocking the power of SVMs for non-linear classification problems.</p> <p>In the next post, we’ll explore kernel functions in depth, including popular choices like Gaussian and polynomial kernels, and see how they influence SVM performance. Stay tuned!</p> <hr/> <h5 id="references">References</h5> <ul> <li>Math part verification</li> <li>KKT</li> <li></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.]]></summary></entry><entry><title type="html">Subgradient and Subgradient Descent</title><link href="https://monishver11.github.io/blog/2025/subgradient/" rel="alternate" type="text/html" title="Subgradient and Subgradient Descent"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://monishver11.github.io/blog/2025/subgradient</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/subgradient/"><![CDATA[<p>Optimization is a cornerstone of machine learning, as it allows us to fine-tune models and minimize error. For smooth, differentiable functions, gradient descent is the go-to method. However, in the real world, many functions are not differentiable. This is where <strong>subgradients</strong> come into play. In this post, we’ll explore subgradients, understand their properties, and see how they are used in subgradient descent. Finally, we’ll dive into their application in support vector machines (SVMs).</p> <hr/> <h4 id="first-order-condition-for-convex-differentiable-functions"><strong>First-Order Condition for Convex, Differentiable Functions</strong></h4> <p>Let’s start with the basics of convex functions. For a function \(f : \mathbb{R}^d \to \mathbb{R}\) that is convex and differentiable, the <strong>first-order condition</strong> states that:</p> \[f(y) \geq f(x) + \nabla f(x)^\top (y - x), \quad \forall x, y \in \mathbb{R}^d\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Subgradient_1-480.webp 480w,/assets/img/Subgradient_1-800.webp 800w,/assets/img/Subgradient_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Subgradient_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Subgradient_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This equation tells us something profound: the linear approximation to \(f\) at \(x\) is a global underestimator of the function. In other words, the gradient provides a plane below the function, ensuring the convexity property holds. A direct implication is that if \(\nabla f(x) = 0\), then \(x\) is a global minimizer of \(f\). This serves as the foundation for gradient-based optimization.</p> <h4 id="subgradients-a-generalization-of-gradients"><strong>Subgradients: A Generalization of Gradients</strong></h4> <p>While gradients work well for differentiable functions, what happens when a function has kinks or sharp corners? This is where <strong>subgradients</strong> step in. A subgradient is a generalization of the gradient, defined as follows:</p> <p>A vector \(g \in \mathbb{R}^d\) is a subgradient of a convex function \(f : \mathbb{R}^d \to \mathbb{R}\) at \(x\) if:</p> \[f(z) \geq f(x) + g^\top (z - x), \quad \forall z \in \mathbb{R}^d\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Subgradient_2-480.webp 480w,/assets/img/Subgradient_2-800.webp 800w,/assets/img/Subgradient_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Subgradient_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Subgradient_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Subgradients essentially maintain the same idea as gradients: they provide a linear function that underestimates \(f\). However, while the gradient is unique, a subgradient can belong to a set of possible vectors called the <strong>subdifferential</strong>, denoted \(\partial f(x)\).</p> <p>For convex functions, the subdifferential is always non-empty. If \(f\) is differentiable at \(x\), the subdifferential collapses to a single point: \(\{ \nabla f(x) \}\). Importantly, for a convex function, a point \(x\) is a global minimizer if and only if \(0 \in \partial f(x)\). This property allows us to apply subgradients even when gradients don’t exist.</p> <hr/> <p><strong>To build an understanding for subgradients</strong>, think of them as a safety net for optimization when the function isn’t smooth. Imagine you’re skiing down a mountain, and instead of a smooth slope, you encounter a flat plateau or a jagged edge. A regular gradient would fail to guide you because there’s no single steepest slope at such points. Subgradients, however, provide a range of valid directions — all the paths you could safely take without “climbing uphill.”</p> <p>In mathematical terms, subgradients are generalizations of gradients. For a non-differentiable function, the subdifferential \(\partial f(x)\) at a point \(x\) is the set of all possible subgradients. Each of these subgradients \(g\) satisfies:</p> \[f(z) \geq f(x) + g^\top (z - x), \quad \forall z \in \mathbb{R}^d\] <p>This means that every subgradient defines a hyperplane that lies below the function \(f(x)\). It acts as a valid approximation of the function’s behavior, even at sharp corners or flat regions.</p> <p>Here’s a useful way to interpret subgradients: if you were to try and “push” the function \(f(x)\) upward using any of the subgradients at \(x\), you would never exceed the true value of \(f(z)\). Subgradients provide all the slopes that respect this property, even when the function isn’t smooth.</p> <p>Finally, the key insight: if the zero vector is in the subdifferential, \(0 \in \partial f(x)\), it means that there’s no descent direction left — you’ve reached the global minimum. Subgradients help us navigate these tricky, non-smooth terrains where gradients fail, making them a versatile tool in optimization.</p> <h5 id="subgradient-example-absolute-value-function"><strong>Subgradient Example: Absolute Value Function</strong></h5> <p>Let’s consider a classic example: \(f(x) = \lvert x \rvert\). This function is non-differentiable at \(x = 0\), making it an ideal candidate to illustrate subgradients. The subdifferential of \(f(x)\) is:</p> \[\partial f(x) = \begin{cases} \{-1\} &amp; \text{if } x &lt; 0, \\ \{1\} &amp; \text{if } x &gt; 0, \\ [-1, 1] &amp; \text{if } x = 0. \end{cases}\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Subgradient_3-480.webp 480w,/assets/img/Subgradient_3-800.webp 800w,/assets/img/Subgradient_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Subgradient_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Subgradient_3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The plot on the right shows: $$ \{(x, g) \mid x \in \mathbb{R}, g \in \partial f(x)\} $$ </div> <p>At \(x = 0\), the subgradient set contains all values in the interval \([-1, 1]\), which corresponds to all possible slopes of lines that underapproximate \(f(x)\) at that point.</p> <h4 id="subgradient-descent-the-optimization-method"><strong>Subgradient Descent: The Optimization Method</strong></h4> <p>Subgradient descent extends gradient descent to non-differentiable convex functions. The update rule is simple:</p> \[x_{t+1} = x_t - \eta_t g,\] <p>where \(g \in \partial f(x_t)\) is a subgradient, and \(\eta_t &gt; 0\) is the step size.</p> <p>Unlike gradients, subgradients do not always converge to zero as the algorithm progresses. <strong>Why?</strong> This is because subgradients are not as tightly coupled to the function’s local geometry as gradients are. In gradient descent, the gradient vanishes at critical points (e.g., minima, maxima, or saddle points), forcing the updates to slow down as the algorithm approaches a minimum. However, in subgradient descent, the subgradients remain non-zero even near a minimum due to the nature of subdifferentiability.</p> <p>For example, consider the absolute value function, \(f(x) = \lvert x \rvert\), which has a sharp corner at \(x = 0\). The subdifferential at \(x = 0\) is the interval \([-1, 1]\), meaning that even at the minimizer \(x^\ast = 0\), valid subgradients exist (e.g., \(g = 0.5\)). Consequently, the algorithm does not inherently “slow down” as it approaches the minimum, unlike gradient descent.</p> <p>This behavior requires us to carefully choose a decreasing step size \(\eta_t\) to make sure that the updates shrink over time, leading to convergence.</p> <p>For convex functions, subgradient descent ensures that the iterates move closer to the minimizer:</p> \[\|x_{t+1} - x^\ast\| &lt; \|x_t - x^\ast\|,\] <p>provided that the step size is small enough. However, subgradient methods tend to be slower than gradient descent because they rely on weaker information about the function’s structure.</p> <h4 id="subgradient-descent-for-svms-the-pegasos-algorithm"><strong>Subgradient Descent for SVMs: The Pegasos Algorithm</strong></h4> <p>Subgradients are particularly useful in optimizing the objective function of support vector machines (SVMs). The SVM objective is:</p> \[J(w) = \frac{1}{n} \sum_{i=1}^n \max(0, 1 - y_i w^\top x_i) + \frac{\lambda}{2} \|w\|^2,\] <p>where the first term represents the hinge loss, and the second term penalizes large weights to prevent overfitting. Optimizing this objective using gradient-based methods is tricky due to the non-differentiability of the hinge loss. Enter <strong>Pegasos</strong>, a stochastic subgradient descent algorithm.</p> <h5 id="the-pegasos-algorithm"><strong>The Pegasos Algorithm</strong></h5> <p>The Pegasos algorithm follows these steps:</p> <ol> <li>Initialize \(w_1 = 0\), \(t = 0\) and \(\lambda &gt; 0\)</li> <li>Randomly permute the data at the beginning of each epoch.</li> <li>While termination condition not met: <ul> <li>For each data point \((x_j, y_j)\), update the parameters: <ul> <li>Increment \(t\): \(t = t + 1\).</li> <li>Compute the step size: \(\eta_t = \frac{1}{t \lambda}\).</li> <li>If \(y_j w_t^\top x_j &lt; 1\), update: \(w_{t+1} = (1 - \eta_t \lambda) w_t + \eta_t y_j x_j.\)</li> <li>Otherwise, update: \(w_{t+1} = (1 - \eta_t \lambda) w_t.\)</li> </ul> </li> </ul> </li> </ol> <p>This algorithm cleverly combines subgradient updates with a decreasing step size to ensure convergence. The hinge loss ensures that the model maintains a margin of separation, while the regularization term prevents overfitting. By carefully adjusting the step size \(\eta_t = \frac{1}{t \lambda}\), Pegasos ensures convergence to the optimal \(w\) while handling the non-differentiable hinge loss.</p> <h6 id="derivation-for-both-cases"><strong>Derivation for Both Cases:</strong></h6> <ol> <li> <p><strong>Case 1: \(y_j w_t^\top x_j &lt; 1\) (misclassified or on the margin)</strong></p> <p>The subgradient of the hinge loss for this case is:</p> \[\nabla \text{hinge loss} = -y_j x_j.\] <p>Adding the subgradient of the regularization term:</p> \[\nabla J(w) = -y_j x_j + \lambda w_t.\] <p>Using the subgradient descent update rule:</p> \[w_{t+1} = w_t - \eta_t \nabla J(w),\] <p>we get:</p> \[w_{t+1} = w_t - \eta_t (-y_j x_j + \lambda w_t).\] <p>Simplifying:</p> \[w_{t+1} = (1 - \eta_t \lambda) w_t + \eta_t y_j x_j.\] </li> <li> <p><strong>Case 2: \(y_j w_t^\top x_j \geq 1\) (correctly classified)</strong></p> <p>In this case, the hinge loss is zero, so the subgradient is purely from the regularization term:</p> \[\nabla J(w) = \lambda w_t.\] <p>The subgradient descent update rule becomes:</p> \[w_{t+1} = w_t - \eta_t \nabla J(w).\] <p>Substituting \(\nabla J(w) = \lambda w_t\):</p> \[w_{t+1} = w_t - \eta_t \lambda w_t.\] <p>Simplifying:</p> \[w_{t+1} = (1 - \eta_t \lambda) w_t.\] </li> </ol> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>Subgradients are a powerful tool for dealing with non-differentiable convex functions, and subgradient descent provides a straightforward yet effective way to optimize such functions. While slower than gradient descent, subgradient descent shines in scenarios where gradients are unavailable.</p> <p>In the next part of this series, we’ll delve into the <strong>dual problem</strong> and uncover its connection to the primal SVM formulation. Stay tuned for more insights into the fascinating world of ML!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.]]></summary></entry><entry><title type="html">The Dual Problem of SVM</title><link href="https://monishver11.github.io/blog/2025/dual-problem/" rel="alternate" type="text/html" title="The Dual Problem of SVM"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://monishver11.github.io/blog/2025/dual-problem</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/dual-problem/"><![CDATA[<p>In machine learning, optimization problems often present themselves as key challenges. For example, when training a <strong>Support Vector Machine (SVM)</strong>, we are tasked with finding the optimal hyperplane that separates two classes in a high-dimensional feature space. While we can solve this directly using methods like subgradient descent, we can also leverage a more analytical approach through <strong>Quadratic Programming (QP)</strong> solvers.</p> <p>Moreover, for convex optimization problems, there is a powerful technique known as solving the <strong>dual problem</strong>. Understanding this duality is not only essential for theory, but it also offers computational advantages. In this blog, we’ll dive into the dual formulation of SVM and its implications.</p> <hr/> <h4 id="svm-as-a-quadratic-program"><strong>SVM as a Quadratic Program</strong></h4> <p>To understand the dual problem of SVM, let’s first revisit the primal optimization problem for SVMs. The goal of an SVM is to find the hyperplane that maximizes the margin between two classes. The optimization problem can be written as:</p> \[\begin{aligned} \min_{w, b, \xi} \quad &amp; \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\ \text{subject to} \quad &amp; -\xi_i \leq 0 \quad \text{for } i = 1, \dots, n, \\ &amp; 1 - y_i (w^T x_i + b) - \xi_i \leq 0 \quad \text{for } i = 1, \dots, n, \end{aligned}\] <p><strong>Primal</strong>—sounds technical, right? Here’s what it means: The <strong>primal problem</strong> refers to the original formulation of the optimization problem in terms of the decision variables (in this case, \(w\), \(b\), and \(\xi\)) and the objective function. It is called “primal” because it directly represents the problem without transformation. The primal form of SVM is concerned with finding the optimal hyperplane parameters that minimize the classification error while balancing the margin size.</p> <h5 id="breakdown-of-the-problem"><strong>Breakdown of the Problem</strong></h5> <ul> <li> <p><strong>Objective Function</strong>: The term \(\frac{1}{2} \|w\|^2\) is a regularization term that aims to minimize the complexity of the model, ensuring that the hyperplane is as “wide” as possible. The second term, \(C \sum_{i=1}^n \xi_i\), penalizes the violations of the margin (through slack variables \(\xi_i\)) and controls the trade-off between margin size and misclassification.</p> </li> <li> <p><strong>Constraints</strong>: The constraints consist of two parts:</p> <ul> <li>The first part ensures that slack variables are non-negative: \(-\xi_i \leq 0\), meaning that each slack variable must be at least zero. <strong>Why?</strong> The slack variables represent the margin violations, and they must be non-negative because they quantify how much a data point violates the margin, which cannot be negative.</li> <li>The second part enforces that the data points are correctly classified or their margin violations are captured by \(\xi_i\). For each data point \(i\), the condition \(1 - y_i (w^T x_i + b) - \xi_i \leq 0\) must hold. Here, \(y_i\) is the true label of the data point, and \(w^T x_i + b\) represents the signed distance of the point from the hyperplane. If the point is correctly classified and lies outside the margin (i.e., its signed distance from the hyperplane is greater than 1), the constraint holds true. If the point is misclassified or falls inside the margin, the slack variable \(\xi_i\) will account for this violation.</li> </ul> </li> </ul> <p>This problem has a <strong>differentiable objective function</strong>, <strong>affine constraints</strong>, and includes a number of <strong>unknowns</strong> that can be solved using Quadratic Programming (QP) solvers.</p> <p>So, <strong>Quadratic Programming (QP)</strong> is an optimization problem where the objective function is quadratic (i.e., includes squared terms like \(\|w\|^2\)), and the constraints are linear. In the context of SVM, QP is utilized because the objective function involves the squared norm of \(w\) (which is quadratic), and the constraints are linear inequalities.</p> <p>The QP formulation for SVM involves minimizing a quadratic objective function (with respect to \(w\), \(b\), and \(\xi\)) subject to linear constraints. Now, while QP solvers provide an efficient way to tackle this problem, let’s explore the <strong>dual problem(next)</strong> to gain further insights.</p> <p>But, why bother with the <strong>dual problem</strong>? Here’s why it’s worth the dive:</p> <ol> <li> <p><strong>Efficient Computation with Kernels</strong>: The dual formulation focuses on Lagrange multipliers and inner products between data points, rather than directly optimizing \(w\) and \(b\). This is particularly beneficial when using kernels, as it avoids explicit computation in high-dimensional spaces. For non-linear problems or datasets where relationships are better captured in transformed spaces, the dual approach enables efficient computation while leveraging the kernel trick.</p> </li> <li> <p><strong>Geometrical Insights</strong>: The dual formulation emphasizes the relationship between support vectors and the margin, offering a clearer geometrical interpretation. It shows that only the support vectors (the points closest to the decision boundary) determine the optimal hyperplane.</p> </li> <li> <p><strong>Convexity and Global Optimality</strong>: The dual problem is convex, ensuring that solving it leads to the global optimal solution. This is particularly beneficial when the primal problem has a large number of variables and constraints.</p> </li> </ol> <p>In short, while QP solvers can efficiently solve the primal problem, the dual problem formulation offers computational benefits, the potential for kernel methods, and a clearer understanding of the SVM model’s properties. This makes the dual approach a powerful tool in SVM optimization.</p> <p>No need to worry about the details above—we’ll cover them step by step. For now, keep reading!</p> <hr/> <h4 id="the-lagrangian"><strong>The Lagrangian</strong></h4> <p>To begin understanding the dual problem, we need to define the <strong>Lagrangian</strong> of the optimization problem. For general inequality-constrained optimization problems, the goal is:</p> \[\begin{aligned} \min_x \quad &amp; f_0(x) \\ \text{subject to} \quad &amp; f_i(x) \leq 0, \quad i = 1, \dots, m. \end{aligned}\] <p>The corresponding <strong>Lagrangian</strong> is defined as:</p> \[L(x, \lambda) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x),\] <p>where:</p> <ul> <li>\(\lambda_i\) are the <strong>Lagrange multipliers</strong> (also known as <strong>dual variables</strong>).</li> <li>The Lagrangian function combines the objective function \(f_0(x)\) with the constraints \(f_i(x)\), weighted by the Lagrange multipliers \(\lambda_i\). These multipliers represent how much the objective function will change if we relax or tighten the corresponding constraint.</li> </ul> <h5 id="why-do-we-use-the-lagrangian"><strong>Why Do We Use the Lagrangian?</strong></h5> <p>The Lagrangian serves as a bridge between constrained and unconstrained optimization. Here’s the intuition behind its design and necessity:</p> <ol> <li><strong>Softening Hard Constraints</strong>: <ul> <li>In constrained optimization, the solution must strictly satisfy all constraints, which can make direct optimization challenging.</li> <li>By introducing Lagrange multipliers, the constraints are “softened” into penalties. This means that instead of strictly enforcing constraints during every step of optimization, we penalize deviations from the constraints in the objective function.</li> </ul> </li> <li><strong>Unified Objective</strong>: <ul> <li>The Lagrangian integrates the objective function and constraints into a single function. This allows us to handle both aspects of the problem (maximizing or minimizing while respecting constraints) in one unified framework.</li> </ul> </li> <li><strong>Flexibility</strong>: <ul> <li>The Lagrange multipliers \(\lambda_i\) provide a mechanism to adjust the influence of each constraint. If a constraint is more critical, its corresponding multiplier will have a larger value, increasing its contribution to the Lagrangian.</li> </ul> </li> <li><strong>Theoretical Insights</strong>: <ul> <li>The Lagrangian formulation is foundational to deriving the <strong>dual problem</strong>, which can sometimes simplify the original (primal) problem. It also provides deeper insights into the sensitivity of the solution to changes in the constraints.</li> </ul> </li> </ol> <p><strong>Think of it this way:</strong></p> <p>Imagine you’re managing a budget to purchase items for a project. Your primary goal is to minimize costs (the objective function), but you have constraints like a maximum budget and a minimum quality requirement for each item.</p> <ul> <li>Without the Lagrangian: You’d need to find solutions that satisfy the budget and quality constraints explicitly, which could be cumbersome.</li> <li>With the Lagrangian: You assign a penalty (via Lagrange multipliers) to every dollar you overspend or every unit of quality you fail to meet. Now, your goal is to minimize the combined cost (original cost + penalties), which naturally leads you to solutions that respect the constraints.</li> </ul> <p>In optimization terms, the Lagrangian lets us trade off between violating constraints and optimizing the objective function. This trade-off is critical in complex problems where perfect feasibility might not always be achievable during intermediate steps.</p> <h5 id="the-role-of-lagrange-multipliers"><strong>The Role of Lagrange Multipliers</strong></h5> <p>The multipliers \(\lambda_i\) play a dual role:</p> <ol> <li> <p>They measure the <strong>sensitivity</strong> of the objective function to the corresponding constraint. For instance, if increasing the limit of a constraint by a small amount significantly improves the objective, its multiplier will have a large value.</p> </li> <li> <p>They ensure that the optimal solution respects the constraints. If a constraint is not active at the solution (i.e., it is satisfied without being tight), its multiplier will be zero. This is formalized in the concept of <strong>complementary slackness</strong>, which we’ll explore later.</p> </li> </ol> <p>In essence, the Lagrangian is not just a mathematical tool; it reflects the natural balance between objectives and constraints, making it indispensable in optimization theory.</p> <hr/> <h4 id="lagrange-dual-function"><strong>Lagrange Dual Function</strong></h4> <p>Next, we define the <strong>Lagrange dual function</strong>, which plays a crucial role in deriving the dual problem. The dual function is obtained by minimizing the Lagrangian with respect to the primal variables (denoted as \(x\)):</p> \[g(\lambda) = \inf_x L(x, \lambda) = \inf_x \left[ f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) \right].\] <h5 id="what-does-this-mean"><strong>What Does This Mean?</strong></h5> <p>The Lagrange dual function \(g(\lambda)\) gives us the smallest possible value of the Lagrangian \(L(x, \lambda)\) for a given set of Lagrange multipliers \(\lambda_i \geq 0\). It represents the “best” value of the Lagrangian when we optimize over the primal variables \(x\).</p> <p>In simpler terms:</p> <ul> <li>The primal variables \(x\) are those we’re directly trying to optimize in the original problem.</li> <li>By minimizing over \(x\) for fixed \(\lambda\), we explore how well the Lagrangian balances the objective function \(f_0(x)\) and the weighted constraints \(f_i(x)\).</li> </ul> <h5 id="why-do-we-minimize-over-x"><strong>Why Do We Minimize Over \(x\)?</strong></h5> <p>The reason for this step is that it helps us decouple the influence of the primal variables \(x\) and the dual variables \(\lambda\). By focusing only on \(x\), we shift our attention to understanding the properties of the dual variables, which simplifies the problem and provides valuable insights.</p> <p>Minimizing the Lagrangian with respect to \(x\) ensures that:</p> <ol> <li><strong>Feasibility of Constraints</strong>: <ul> <li>The minimization respects the constraints \(f_i(x) \leq 0\) through the penalization mechanism introduced by \(\lambda_i\).</li> </ul> </li> <li><strong>Dual Representation</strong>: <ul> <li>The dual function \(g(\lambda)\) captures how “good” a particular choice of \(\lambda\) is in approximating the original problem.</li> </ul> </li> <li><strong>Foundation for the Dual Problem</strong>: <ul> <li>The minimization step builds the foundation for solving the optimization problem via its <strong>dual formulation</strong>, which is often simpler than the primal.</li> </ul> </li> </ol> <h5 id="why-do-we-need-this"><strong>Why Do We Need This?</strong></h5> <p>The goal of introducing the dual function is to exploit the following properties, which help us solve the original optimization problem more efficiently:</p> <ul> <li> <p><strong>Lower Bound Property</strong>: The dual function \(g(\lambda)\) provides a lower bound on the optimal value of the primal problem. If \(p^*\) is the optimal value of the primal problem, then: \(g(\lambda) \leq p^*, \quad \forall \lambda \geq 0.\) This property is useful because even if we cannot solve the primal problem directly, we can approximate its solution by maximizing \(g(\lambda)\).</p> </li> <li> <p><strong>Convexity of \(g(\lambda)\)</strong>: The dual function is always concave, regardless of whether the primal problem is convex. This makes the dual problem easier to solve using convex optimization techniques.</p> </li> </ul> <h5 id="how-does-this-work-why-does-it-work"><strong>How Does This Work? Why Does It Work?</strong></h5> <ol> <li><strong>Lower Bound Property</strong>: <ul> <li>When we minimize the Lagrangian \(L(x, \lambda)\) with respect to the primal variables \(x\), we’re essentially finding the “best possible value” of the objective function \(f_0(x)\) for a fixed choice of \(\lambda\).</li> <li>Since the dual function \(g(\lambda)\) is derived from a relaxed version of the primal problem (allowing \(\lambda_i \geq 0\) to penalize constraint violations), it cannot exceed the true optimal value \(p^*\) of the primal problem. This creates the lower bound.</li> </ul> <p><strong>Intuition</strong>: Think of the dual function as a “proxy” for the primal problem. By maximizing \(g(\lambda)\), we try to approach the primal solution as closely as possible from below.</p> </li> <li><strong>Convexity of \(g(\lambda)\)</strong>: <ul> <li>The concavity of \(g(\lambda)\) follows from its definition as the infimum (or greatest lower bound) of a family of affine functions. In optimization, operations involving infima tend to preserve convexity (or result in concavity for maximization problems).</li> <li>This property ensures that maximizing \(g(\lambda)\) is computationally efficient, even if the original primal problem is non-convex.</li> </ul> <p><strong>Intuition</strong>: The concave structure of \(g(\lambda)\) creates an inverted “bowl-shaped” surface, making it easier to find the maximum using gradient-based optimization methods.</p> </li> </ol> <h5 id="what-is-inf-and-how-is-it-different-from-min"><strong>What is \(\inf\) and How is it Different from \(\min\)?</strong></h5> <p>The <strong>infimum</strong> (denoted as \(\inf\)) is a generalization of the minimum (\(\min\)) in optimization and analysis. Here’s the distinction:</p> <ol> <li><strong>Minimum (\(\min\))</strong>: <ul> <li>The minimum is the smallest value attained by a function within its domain. It must be achieved by some point \(x\) in the domain.</li> <li>Example: For \(f(x) = x^2\) on \([0, 2]\), the minimum is \(f(0) = 0\).</li> </ul> </li> <li><strong>Infimum (\(\inf\))</strong>: <ul> <li>The infimum is the greatest lower bound of a function, but it may not be attained by any point in the domain. It represents the “smallest possible value” the function can approach, even if it doesn’t reach it.</li> <li>Example: For \(f(x) = 1/x\) on \((0, 2]\), the infimum is \(\inf f(x) = 0\), but \(f(x)\) never actually equals \(0\) within the domain.</li> </ul> </li> </ol> <h5 id="why-use-inf-instead-of-min"><strong>Why Use \(\inf\) Instead of \(\min\)?</strong></h5> <p>In the context of the dual function:</p> <ul> <li>The infimum \(\inf_x L(x, \lambda)\) is used because the Lagrangian \(L(x, \lambda)\) might not achieve a true minimum for certain values of \(\lambda\) (e.g., the domain could be open or unbounded).</li> <li>Using \(\inf\) ensures that the dual function \(g(\lambda)\) is well-defined, even in cases where \(\min_x L(x, \lambda)\) doesn’t exist.</li> </ul> <h5 id="a-lighter-take"><strong>A Lighter Take</strong></h5> <p>Think of this as “playing with math language” to get to the result we want. Just as you might rephrase a sentence to make it clearer or more persuasive, in mathematics, we transform the problem into a new form (the dual) that’s easier to work with.</p> <p>For now, trust the process. This step might seem abstract, but it leads us to a form of the problem where powerful mathematical tools can come into play. Once we see the bigger picture, the reasoning behind these transformations will become clear.</p> <p>In essence, the Lagrange dual function \(g(\lambda)\) gives us a way to shift our perspective on the optimization problem, helping us solve it through duality principles. As we proceed, you’ll see how this approach simplifies the original problem and why it’s such a powerful concept.</p> <hr/> <h4 id="the-primal-and-the-dual"><strong>The Primal and the Dual</strong></h4> <p>For any general primal optimization problem:</p> \[\begin{aligned} \min_x \quad &amp; f_0(x) \\ \text{subject to} \quad &amp; f_i(x) \leq 0, \quad i = 1, \dots, m, \end{aligned}\] <p>we can formulate the corresponding <strong>dual problem</strong> as:</p> \[\begin{aligned} \max_\lambda \quad &amp; g(\lambda) \\ \text{subject to} \quad &amp; \lambda_i \geq 0, \quad i = 1, \dots, m. \end{aligned}\] <p>The dual problem has some remarkable properties:</p> <ul> <li><strong>Convexity</strong>: The dual problem is always a <strong>convex optimization problem</strong>, even if the primal problem is not convex.</li> <li><strong>Simplification</strong>: In some cases, solving the dual problem is easier than solving the primal problem directly. This is particularly true when the primal problem is difficult to solve or the number of constraints is large.</li> </ul> <h5 id="contract-negotiation-analogy"><strong>Contract Negotiation Analogy</strong></h5> <ul> <li> <p><strong>Primal Problem (Your Terms)</strong>: Think of the primal problem as negotiating a contract where you aim to minimize costs while respecting certain constraints (like budget or timelines). You’re focused on getting the best deal for yourself under these limitations.</p> </li> <li> <p><strong>Lagrangian (Penalties for Violations)</strong>: During the negotiation, you introduce penalties — if you can’t meet a certain term, you adjust other aspects of the contract. This is similar to using Lagrange multipliers in the Lagrangian function to penalize constraint violations.</p> </li> <li> <p><strong>Dual Problem (Value Assessment)</strong>: In the dual problem, you step into the other party’s shoes and assess how much value they assign to the contract’s terms, maximizing the value they place on the constraints (such as how much they’d pay for more time or resources).</p> </li> <li> <p><strong>Duality</strong>: The primal (minimizing costs) and dual (maximizing value) problems balance each other. Weak duality means the dual value is a lower bound to the primal, and strong duality means the best deal is the same for both sides when the optimal values align.</p> </li> </ul> <p>Below are the next two important results derived from the above formulation.</p> <h4 id="weak-and-strong-duality"><strong>Weak and Strong Duality</strong></h4> <ol> <li> <p><strong>Weak Duality</strong>: This property tells us that the optimal value of the primal problem is always greater than or equal to the optimal value of the dual problem:</p> \[p^* \geq d^*,\] <p>where \(p^*\) and \(d^*\) are the optimal values of the primal and dual problems, respectively. This is a fundamental result in optimization theory. <strong>Why so?</strong> Because the dual problem is designed to provide a lower bound on the primal objective through the Lagrangian.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Lagrangian_1-480.webp 480w,/assets/img/Lagrangian_1-800.webp 800w,/assets/img/Lagrangian_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Lagrangian_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lagrangian_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p><strong>Strong Duality</strong>: In some special cases (such as when the problem satisfies <strong>Slater’s condition</strong>), <strong>strong duality</strong> holds, meaning the optimal values of the primal and dual problems are equal:</p> \[p^* = d^*.\] <p>Strong duality is particularly useful because it allows us to solve the dual problem instead of the primal one, often simplifying the problem or reducing computational complexity.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Lagrangian_2-480.webp 480w,/assets/img/Lagrangian_2-800.webp 800w,/assets/img/Lagrangian_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Lagrangian_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lagrangian_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="slaters-conditions"><strong>Slater’s Conditions</strong></h5> <ul> <li>They are a set of conditions that <strong>guarantee strong duality</strong> in certain types of constrained optimization problems (particularly convex problems).</li> <li>They state that for strong duality to hold, there must exist a <strong>strictly feasible point</strong> (a point where all inequality constraints are strictly satisfied).</li> <li>In simpler terms, Slater’s conditions ensure that there is at least one point where all the constraints are strictly satisfied, which <strong>enables strong duality</strong> and ensures that the primal and dual solutions align.</li> <li>These conditions are crucial in convex optimization as they help guarantee the optimal solutions for both the primal and dual problems are the same.</li> </ul> <h4 id="complementary-slackness"><strong>Complementary Slackness</strong></h4> <p>When <strong>strong duality</strong> holds, we can derive the <strong>complementary slackness</strong> condition. This condition provides deeper insight into the relationship between the primal and dual solutions. Specifically, if \(x^*\) is the optimal primal solution and \(\lambda^*\) is the optimal dual solution, we have:</p> \[f_0(x^*) = g(\lambda^*) = \inf_x L(x, \lambda^*) \leq L(x^*, \lambda^*) = f_0(x^*) + \sum_{i=1}^m \lambda_i^* f_i(x^*).\] <p>For this equality to hold, the term \(\sum_{i=1}^m \lambda_i^* f_i(x^*)\) must be zero for each constraint. This leads to the following <strong>complementary slackness condition</strong>:</p> <ol> <li>If \(\lambda_i^* &gt; 0\), then \(f_i(x^*) = 0\), meaning that the corresponding constraint is <strong>active</strong> at the optimal point. Also, it means the constraint is tight and directly affects the optimal solution.</li> <li>If \(f_i(x^*) &lt; 0\), then \(\lambda_i^* = 0\), meaning that the corresponding constraint is <strong>inactive</strong>. This indicates that the constraint doesn’t influence the optimal solution at all.</li> </ol> <p>This condition tells us which constraints are binding (active) at the optimal solution and which are not, providing critical information about the structure of the optimal solution.</p> <h5 id="contract-negotiation-analogy-duality-and-complementary-slackness"><strong>Contract Negotiation Analogy: Duality and Complementary Slackness</strong></h5> <h6 id="strong-duality-and-weak-duality">Strong Duality and Weak Duality:</h6> <ul> <li> <p><strong>Weak Duality</strong>: In a contract negotiation, <strong>weak duality</strong> is like the <strong>minimum acceptable price</strong> for the other party. The price (dual value) they would accept for your offer will always be a <strong>lower bound</strong> to what you are willing to pay (primal cost). The other party cannot ask for more than what you’re offering, but they may accept less.</p> </li> <li> <p><strong>Strong Duality</strong>: <strong>Strong duality</strong> happens when both parties agree on the same <strong>optimal terms</strong> for the contract. Both your <strong>best offer</strong> (primal) and the <strong>value assigned</strong> to the terms (dual) align perfectly, resulting in the best possible contract for both sides.</p> </li> </ul> <h6 id="complementary-slackness-1">Complementary Slackness:</h6> <ul> <li><strong>Complementary Slackness</strong> tells us which terms of the contract are really influencing the negotiation outcome. <ul> <li>If a <strong>dual variable</strong> (e.g., the price or terms the other party values) is <strong>positive</strong>, then the corresponding <strong>primal constraint</strong> (e.g., a term you care about, like the delivery time or cost) is <strong>active</strong> — it <strong>must</strong> be part of the final agreement.</li> <li>If a <strong>primal constraint</strong> (e.g., a timeline or budget) is not strict enough (it’s not a deal-breaker), then the <strong>dual variable</strong> (e.g., the value the other party assigns to it) is <strong>zero</strong>, meaning it doesn’t impact the outcome.</li> </ul> </li> </ul> <p><strong>Example:</strong></p> <ul> <li>Suppose you’re negotiating a <strong>project deadline</strong>. If the other party <strong>values</strong> the deadline highly (dual value is positive), it <strong>must</strong> be part of the contract (the deadline is <strong>active</strong>). If they don’t care much about it, then <strong>it doesn’t matter</strong> to the final deal (dual value is zero), and you can relax that constraint.</li> </ul> <hr/> <p>Finally, there might be one question left—this is the one I had, so here’s the explanation:</p> <h5 id="if-the-lagrangian-dual-function-is-concave-how-is-the-lagrangian-dual-problem-always-a-convex-optimization-problem"><strong>If the Lagrangian dual function is concave, how is the Lagrangian dual problem always a convex optimization problem?</strong></h5> <p>To understand why the dual problem is always a convex optimization problem, let’s revisit its structure: \(\max_{\lambda \geq 0} \; g(\lambda).\)</p> <ol> <li> <p><strong>Maximizing a Concave Function:</strong><br/> The dual function \(g(\lambda)\) is <strong>concave</strong> by construction. In optimization, maximizing a concave function is equivalent to minimizing a convex function (negating the objective). Therefore, the objective of the dual problem aligns with the structure of a convex optimization problem.</p> </li> <li> <p><strong>Convex Feasible Region:</strong><br/> The constraints in the dual problem (\(\lambda \geq 0\)) define a <strong>convex set</strong>, as the non-negative orthant in \(\mathbb{R}^m\) is convex.</p> </li> </ol> <h6 id="definition-of-a-convex-optimization-problem"><strong>Definition of a Convex Optimization Problem:</strong></h6> <p>A problem is a convex optimization problem if:</p> <ul> <li>The objective function is <strong>convex</strong> (for minimization) or <strong>concave</strong> (for maximization).</li> <li>The feasible region is a <strong>convex set</strong>.</li> </ul> <p>The dual problem satisfies these conditions because:</p> <ul> <li>The objective function \(g(\lambda)\) is concave.</li> <li>The constraint \(\lambda \geq 0\) defines a convex feasible region.</li> </ul> <p>Thus, the dual problem is always a <strong>convex optimization problem</strong>, regardless of whether the primal problem is convex or not.</p> <h6 id="intuition-behind-convexity-of-the-dual-problem"><strong>Intuition Behind Convexity of the Dual Problem</strong></h6> <p>The dual problem’s convexity comes from the way it is constructed:</p> <ol> <li>The Lagrangian combines the primal objective and constraints into a single function that penalizes constraint violations.</li> <li>By minimizing the Lagrangian over the primal variables \(x\), the dual function \(g(\lambda)\) captures the <strong>tightest lower bound</strong> of the primal objective.</li> <li>The pointwise infimum of affine functions (as in \(g(\lambda)\)) is guaranteed to be concave.</li> <li>The dual problem maximizes this concave function over a convex set (\(\lambda \geq 0\)), making it a convex optimization problem.</li> </ol> <p>This ensures that solving the dual problem is computationally efficient and well-structured, even when the primal problem is non-convex.</p> <hr/> <h4 id="conclusion"><strong>Conclusion</strong></h4> <p>By exploring the <strong>dual problem</strong> of SVM, we gain both theoretical insights and practical benefits. The dual formulation provides a new perspective on the original optimization problem, and solving it can sometimes be more efficient or insightful. The duality between the primal and dual problems underpins many of the optimization techniques used in machine learning, particularly in the context of support vector machines.</p> <p>Pat yourselves on the back for making it to the end of this blog! Take a well-deserved break, and stay tuned for the next one, where we’ll apply everything we’ve learned so far to formulate the SVM dual problem.</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://www.youtube.com/watch?v=thuYiebq1cE&amp;t=136s">https://www.youtube.com/watch?v=thuYiebq1cE&amp;t=136s - David S. Rosenberg</a></li> <li><a href="https://www.youtube.com/watch?v=8mjcnxGMwFo">Lagrange Multipliers | Geometric Meaning &amp; Full Example - Dr. Trefor Bazett</a></li> <li><a href="https://www.youtube.com/watch?v=d0CF3d5aEGc&amp;t=216s">Convexity and The Principle of Duality - Visually Explained</a></li> <li><a href="https://math.stackexchange.com/questions/515812/pointwise-infimum-of-affine-functions-is-concave?noredirect=1&amp;lq=1">Pointwise infimum of affine functions is concave</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.]]></summary></entry><entry><title type="html">Support Vector Machines(SVM) - From Hinge Loss to Optimization</title><link href="https://monishver11.github.io/blog/2025/svm/" rel="alternate" type="text/html" title="Support Vector Machines(SVM) - From Hinge Loss to Optimization"/><published>2025-01-07T14:40:00+00:00</published><updated>2025-01-07T14:40:00+00:00</updated><id>https://monishver11.github.io/blog/2025/svm</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/svm/"><![CDATA[<h4 id="a-quick-recap-hinge-loss"><strong>A Quick Recap: Hinge Loss</strong></h4> <p>To understand Support Vector Machines (SVM) fully, we need to revisit <strong>Hinge Loss</strong>, which is fundamental to SVM’s working mechanism. Hinge loss is mathematically defined as:</p> \[\ell_{\text{Hinge}} = \max(1 - m, 0) = (1 - m)_+\] <p>Here, \(m = y f(x)\) represents the margin, and the notation \((x)_+\) refers to the “positive part” of \(x\). This simply means that \((x)_+\) equals \(x\) when \(x \geq 0\), and is zero otherwise.</p> <p>Why is this loss function important? Hinge loss provides a convex, upper bound approximation of the 0–1 loss, making it computationally efficient for optimization. However, it’s not without limitations—it is <strong>not differentiable</strong> at \(m = 1\), which we’ll address later. A “margin error” occurs whenever \(m &lt; 1\), and this forms the basis of SVM’s classification mechanism.</p> <p>With this understanding of Hinge Loss in place, we’re ready to explore how SVM is formulated as an optimization problem.</p> <hr/> <h4 id="svm-as-an-optimization-problem"><strong>SVM as an Optimization Problem</strong></h4> <p>At its core, SVM aims to find a hyperplane that maximizes the separation, or <strong>margin</strong>, between data points of different classes. This task is mathematically framed as the following optimization problem:</p> <p>We minimize the objective:</p> \[\frac{1}{2} \|w\|^2 + c \sum_{i=1}^n \xi_i\] <p>subject to the constraints:</p> \[\xi_i \geq 1 - y_i (w^T x_i + b), \quad \text{for } i = 1, \dots, n\] \[\xi_i \geq 0, \quad \text{for } i = 1, \dots, n\] <p>which is equivalent to</p> \[\text{minimize } \frac{1}{2} \|w\|^2 + \frac{c}{n} \sum_{i=1}^n \xi_i\] \[\text{subject to } \xi_i \geq \max \big(0, 1 - y_i (w^T x_i + b) \big), \quad \text{for } i = 1, \dots, n\] <p>In this formulation:</p> <ul> <li>The term \(\|w\|^2\) represents the <strong>L2 regularizer</strong>, which helps prevent overfitting by penalizing large weights.</li> <li>The variables \(\xi_i\) (slack variables) account for data points that violate the margin constraint, allowing some degree of misclassification.</li> <li>The parameter \(c\) controls the trade-off between maximizing the margin and minimizing classification errors.</li> </ul> <p>To simplify, we can integrate the constraints directly into the objective function. This gives us:</p> \[\min_{w \in \mathbb{R}^d, b \in \mathbb{R}} \frac{1}{2} \|w\|^2 + c \sum_{i=1}^n \max(0, 1 - y_i (w^T x_i + b))\] <p>This new formulation has two terms:</p> <ol> <li>The first term, \(\frac{1}{2} \|w\|^2\), is the <strong>L2 regularizer</strong>.</li> <li>The second term, \(\sum_{i=1}^n \max(0, 1 - y_i (w^T x_i + b))\), captures the <strong>Hinge Loss</strong> for all data points.</li> </ol> <p>This concise representation highlights the two fundamental objectives of SVM: maintaining a large margin and minimizing misclassifications.</p> <p>Depending on the nature of the data, SVM can be approached in two ways. If the data is perfectly separable, we use a <strong>hard-margin SVM</strong>. In this case, all data points must be correctly classified while maintaining the margin constraints.</p> <p>However, real-world data is often noisy and not perfectly separable. Here, we use a <strong>soft-margin SVM</strong>, which introduces the slack variables \(\xi_i\) to allow some margin violations. The degree of permissible violations is controlled by the parameter \(c\), striking a balance between classification accuracy and margin size.</p> <h5 id="the-svm-objective-function"><strong>The SVM Objective Function</strong></h5> <p>The final objective function for SVM, combining regularization and hinge loss, is given by:</p> \[J(w) = \frac{1}{n} \sum_{i=1}^n \max(0, 1 - y_i w^T x_i) + \lambda \|w\|^2\] <p>Here, \(\lambda\) is the regularization parameter, which is inversely related to \(c\). This function encapsulates both the classification objective (minimizing hinge loss) and the regularization goal (keeping weights small).</p> <p>The relationship is inverse because:</p> <ul> <li>When \(c\) is large, the model is more focused on minimizing misclassification, meaning less regularization (\(\lambda\)) is needed.</li> <li>When \(c\) is small, the model allows more misclassifications, which means it can afford to have more regularization (\(\lambda\)) to prevent overfitting.</li> </ul> <hr/> <h5 id="gradients-and-optimization"><strong>Gradients and Optimization</strong></h5> <p>At this point, you might wonder: how do we actually optimize the SVM objective? This is where gradients come into play. Let’s break it down.</p> <h6 id="derivative-of-hinge-loss"><strong>Derivative of Hinge Loss</strong></h6> <p>The derivative of Hinge Loss, \(\ell(m) = \max(0, 1 - m)\), is as follows:</p> \[\ell'(m) = \begin{cases} 0 &amp; \text{if } m &gt; 1 \\ -1 &amp; \text{if } m &lt; 1 \\ \text{undefined} &amp; \text{if } m = 1 \end{cases}\] <p>Using the chain rule, the gradient of \(\ell(y_i w^T x_i)\) with respect to \(w\) is:</p> \[\nabla_w \ell(y_i w^T x_i) = \begin{cases} 0 &amp; \text{if } y_i w^T x_i &gt; 1 \\ - y_i x_i &amp; \text{if } y_i w^T x_i &lt; 1 \\ \text{undefined} &amp; \text{if } y_i w^T x_i = 1 \end{cases}\] <h6 id="gradient-of-the-svm-objective"><strong>Gradient of the SVM Objective</strong></h6> <p>Combining the gradients for all data points, the gradient of the SVM objective is:</p> \[\nabla_w J(w) = \nabla_w \left( \frac{1}{n} \sum_{i=1}^{n} \ell(y_i w^T x_i) + \lambda \|w\|^2 \right)\] \[= \frac{1}{n} \sum_{i=1}^{n} \nabla_w \ell(y_i w^T x_i) + 2\lambda w\] \[= \frac{1}{n} \sum_{i: y_i w^T x_i &lt; 1} (-y_i x_i) + 2\lambda w\] \[y_i w^T x_i \neq 1 \text{ for all i}\] <ul> <li>For \(y_i w^T x_i \geq 1\), the gradient is undefined.</li> </ul> <p>A common concern with the SVM objective is that it is <strong>not differentiable</strong> at \(y_i w^T x_i = 1\). However, in practice:</p> <ol> <li>Starting with a random \(w\), the probability of hitting exactly \(y_i w^T x_i = 1\) is negligible.</li> <li>Even if this occurs, small perturbations in the step size can help bypass such points.</li> </ol> <p>Thus, gradient-based optimization methods, such as gradient descent, can be effectively applied to the SVM objective.</p> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>SVM is a robust and versatile algorithm, but understanding it fully requires breaking it into manageable pieces. In this post, we explored its formulation, loss functions, and gradients. This sets the stage for discussing <strong>subgradients</strong> and <strong>subgradient descent</strong> in the next post, which address the non-differentiability issues of hinge loss.</p> <p>SVM is undoubtedly a vast topic, but step by step, it all begins to make sense. Trust the process, and by the end of this series, you’ll appreciate the journey. Stay tuned!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.]]></summary></entry><entry><title type="html">Understanding the Maximum Margin Classifier</title><link href="https://monishver11.github.io/blog/2025/max-margin-classifier/" rel="alternate" type="text/html" title="Understanding the Maximum Margin Classifier"/><published>2025-01-06T05:57:00+00:00</published><updated>2025-01-06T05:57:00+00:00</updated><id>https://monishver11.github.io/blog/2025/max-margin-classifier</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/max-margin-classifier/"><![CDATA[<h4 id="linearly-separable-data"><strong>Linearly Separable Data</strong></h4> <p>Let’s start with the simplest case: linearly separable data. Imagine a dataset where we can draw a straight line (or more generally, a hyperplane in higher dimensions) to perfectly separate two classes of points. Formally, for a dataset \(D\) with points \((x_i, y_i)\), we seek a hyperplane that satisfies the following conditions:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_1-480.webp 480w,/assets/img/Max_Margin_Classifier_1-800.webp 800w,/assets/img/Max_Margin_Classifier_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>\(w^T x_i &gt; 0\) for all \(x_i\) where \(y_i = +1\),</li> <li>\(w^T x_i &lt; 0\) for all \(x_i\) where \(y_i = -1\).</li> </ul> <p>This hyperplane is defined by a weight vector \(w\) and a bias \(b\), and our goal is to find \(w\) and \(b\) such that all points are correctly classified.</p> <p>But how do we design a learning algorithm to find such a hyperplane? This brings us to the <strong>Perceptron Algorithm</strong>.</p> <h4 id="the-perceptron-algorithm"><strong>The Perceptron Algorithm</strong></h4> <p>The perceptron is one of the earliest learning algorithms developed to find a separating hyperplane. Here’s how it works: we start with an initial guess for \(w\) (usually a zero vector) and iteratively adjust it based on misclassified examples.</p> <p>Each time we encounter a point \((x_i, y_i)\) that is misclassified (i.e., \(y_i w^T x_i &lt; 0\)), we update the weight vector as follows:</p> \[w \gets w + y_i x_i.\] <p>This update rule ensures that the algorithm moves the hyperplane towards misclassified positive examples and away from misclassified negative examples.</p> <p>The perceptron algorithm has a remarkable property: if the data is linearly separable, it will converge to a solution with zero classification error in a finite number of steps.</p> <p>In terms of loss functions, the perceptron can be viewed as minimizing the <strong>hinge loss</strong>:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_2-480.webp 480w,/assets/img/Max_Margin_Classifier_2-800.webp 800w,/assets/img/Max_Margin_Classifier_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> \[\ell(x, y, w) = \max(0, -y w^T x).\] <p>However, while the perceptron guarantees a solution, it doesn’t always find the best one. This brings us to the concept of <strong>maximum-margin classifiers</strong>. But before exploring that, let’s take a deeper look at why the this update rule works.</p> <h5 id="understanding-why-the-perceptron-update-rule-works"><strong>Understanding why the Perceptron Update Rule works?</strong></h5> <p>The <strong>perceptron update rule</strong> shifts the hyperplane differently depending on whether the misclassified point belongs to the positive class (\(y_i = 1\)) or the negative class (\(y_i = -1\)). Let’s take the two cases:</p> <h6 id="positive-case-y_i--1"><strong>Positive Case (\(y_i = 1\))</strong></h6> <ul> <li> <p><strong>Condition for misclassification:</strong> \(w^T x_i &lt; 0.\)<br/> This means the point \(x_i\) is on the wrong side of the hyperplane or too far from the correct side.</p> </li> <li> <p><strong>Update rule:</strong></p> \[w \gets w + x_i.\] </li> <li> <p><strong>Effect of the update:</strong></p> <ul> <li>Adding \(x_i\) to \(w\) increases the dot product \(w^T x_i\) because \(w\) is now pointing more in the direction of \(x_i\).</li> <li>This adjustment shifts the hyperplane towards \(x_i\), ensuring \(x_i\) is more likely to be correctly classified in the next iteration.</li> </ul> </li> </ul> <h6 id="negative-case-y_i---1"><strong>Negative Case (\(y_i = -1\))</strong></h6> <ul> <li> <p><strong>Condition for misclassification:</strong> \(w^T x_i &gt; 0.\)<br/> This means the point \(x_i\) is either incorrectly classified as positive or too close to the positive side.</p> </li> <li> <p><strong>Update rule:</strong></p> \[w \gets w - x_i.\] </li> <li> <p><strong>Effect of the update:</strong></p> <ul> <li>Subtracting \(x_i\) from \(w\) decreases the dot product \(w^T x_i\) because \(w\) is now pointing less in the direction of \(x_i\).</li> <li>This adjustment shifts the hyperplane away from \(x_i\), making it more likely to correctly classify \(x_i\) as negative in subsequent iterations.</li> </ul> </li> </ul> <p><strong>Geometric Interpretation</strong>: The perceptron update ensures that the weight vector \(w\) aligns more closely with the correctly classified side.</p> <hr/> <h4 id="maximum-margin-separating-hyperplane"><strong>Maximum-Margin Separating Hyperplane</strong></h4> <p>When the data is linearly separable, there are infinitely many hyperplanes that can separate the classes. The perceptron algorithm, for instance, might return any one of these. But not all hyperplanes are equally desirable.</p> <p>We prefer a hyperplane that is farthest from both classes of points. This idea leads to the concept of the <strong>maximum-margin classifier</strong>, which finds the hyperplane that maximizes the smallest distance between the hyperplane and the data points.</p> <h5 id="geometric-margin"><strong>Geometric Margin</strong></h5> <p>The <strong>geometric margin</strong> of a hyperplane is defined as the smallest distance between the hyperplane and any data point. For a hyperplane defined by \(w\) and \(b\), this margin can be expressed as:</p> \[\gamma = \min_i \frac{y_i (w^T x_i + b)}{\|w\|_2}.\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_3-480.webp 480w,/assets/img/Max_Margin_Classifier_3-800.webp 800w,/assets/img/Max_Margin_Classifier_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Maximizing this geometric margin provides a hyperplane that is robust to small perturbations in the data, making it a desirable choice.</p> <h5 id="distance-between-a-point-and-a-hyperplane"><strong>Distance Between a Point and a Hyperplane</strong></h5> <p>To understand the geometric margin more concretely, let’s calculate the distance from a point \(x'\) to a hyperplane \(H: w^T v + b = 0\). This derivation involves the following steps:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_4-480.webp 480w,/assets/img/Max_Margin_Classifier_4-800.webp 800w,/assets/img/Max_Margin_Classifier_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h6 id="step-1-perpendicular-distance-from-a-point-to-a-hyperplane"><strong>Step 1: Perpendicular Distance from a Point to a Hyperplane</strong></h6> <p>The distance from a point \(x'\) to the hyperplane is defined as the shortest (perpendicular) distance between the point and the hyperplane. The equation of the hyperplane is:</p> \[w^T v + b = 0,\] <p>where:</p> <ul> <li>\(w\) is the normal vector to the hyperplane.</li> <li>\(b\) is the bias term.</li> <li>\(v\) represents any point on the hyperplane.</li> </ul> <h6 id="step-2-projecting-the-point-onto-the-normal-vector"><strong>Step 2: Projecting the Point onto the Normal Vector</strong></h6> <p>The perpendicular distance is proportional to the projection of the point \(x'\) onto the normal vector \(w\). Mathematically, the projection of \(x'\) onto \(w\), denoted \(\text{Proj}_{w}(x')\), is given by:</p> \[\text{Proj}_{w}(x') = \frac{x' \cdot w}{w \cdot w} w = \left( \frac{w^T x'}{\|w\|_2^2} \right) w\] <p>For the hyperplane \(H: w^T v + b = 0\), the bias term \(b\) shifts the hyperplane as the hyperplane is not always centered at the origin Incorporating this into the projection formula, the signed distance becomes:</p> \[d(x', H) = \frac{w^T x' + b}{\|w\|_2}.\] <h6 id="step-3-accounting-for-the-label-y"><strong>Step 3: Accounting for the Label \(y\)</strong></h6> <p>The label \(y\) of the point \(x'\) determines whether the point is on the positive or negative side of the hyperplane:</p> <ul> <li>For correctly classified points, \(y (w^T x' + b) &gt; 0\).</li> <li>For misclassified points, \(y (w^T x' + b) &lt; 0\).</li> </ul> <p>Including the label ensures that the signed distance is positive for correctly classified points and negative for misclassified points. Thus, the signed distance becomes:</p> \[d(x', H) = \frac{y (w^T x' + b)}{\|w\|_2}.\] <hr/> <h5 id="maximizing-the-margin"><strong>Maximizing the Margin</strong></h5> <p>To maximize the margin, we solve the following optimization problem:</p> \[\max \min_i \frac{y_i (w^T x_i + b)}{\|w\|_2}.\] <p>To simplify, let \(M = \min_i \frac{y_i (w^T x_i + b)}{\|w\|_2}\). The problem becomes:</p> \[\max M, \quad \text{subject to } \frac{y_i (w^T x_i + b)}{\|w\|_2} \geq M, \; \forall i.\] <p><strong>This means:</strong> We want to maximize \(M\), which corresponds to maximizing the smallest margin across all data points.</p> <p>The constraint: \(\frac{y_i (w^T x_i + b)}{\|w\|_2} \geq M\) ensures that for every data point \(x_i\), the margin is at least \(M\), i.e., the data point lies on the correct side of the margin boundary.</p> <p><strong>Another way to put this is:</strong> Since \(M\) is the smallest margin, the constraint \(\frac{y_i (w^T x_i + b)}{\|w\|_2} \geq M\) ensures that every data point has a margin at least as large as \(M\) and this condition is enforced for every data point \(i\).</p> <p>Next, by fixing \(\|w\|_2 = \frac{1}{M}\), we reformulate it as:</p> \[\min \frac{1}{2} \|w\|_2^2, \quad \text{subject to } y_i (w^T x_i + b) \geq 1, \; \forall i.\] <p>This is the optimization problem solved by a <strong>hard margin support vector machine (SVM)</strong>.</p> <p><strong>Note:</strong> Maximizing the margin \(M\) is equivalent to minimizing the inverse, \(\frac{1}{2} \|w\|_2^2\), since the margin is inversely proportional to the norm of \(w\).</p> <h5 id="what-if-the-data-is-not-linearly-separable"><strong>What If the Data Is Not Linearly Separable?</strong></h5> <p>In real-world scenarios, data is often not perfectly linearly separable. For any \(w\), there might be points with negative margins. To handle such cases, we introduce <strong>slack variables</strong> \(\xi_i\), which allow some margin violations.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_5-480.webp 480w,/assets/img/Max_Margin_Classifier_5-800.webp 800w,/assets/img/Max_Margin_Classifier_5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h4 id="soft-margin-svm"><strong>Soft Margin SVM</strong></h4> <p>The optimization problem for a soft margin SVM is:</p> \[\min \frac{1}{2} \|w\|_2^2 + C \sum_{i=1}^n \xi_i,\] <p>subject to:</p> \[y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \; \forall i.\] <h5 id="breaking-this-down"><strong>Breaking this down:</strong></h5> <ul> <li> <p><strong>Regularization Term</strong>:</p> \[\frac{1}{2} \|w\|_2^2\] <p>This term is the <strong>regularization</strong> component of the objective function. It penalizes large values of \(w\), which corresponds to smaller margins. By minimizing this term, we aim to <strong>maximize the margin</strong> between the two classes. A larger margin typically leads to better generalization and lower overfitting.</p> </li> <li> <p><strong>Penalty Term</strong>:</p> \[C \sum_{i=1}^n \xi_i\] <p>This term introduces <strong>penalties</strong> for margin violations. The \(\xi_i\) are the <strong>slack variables</strong> that measure how much each data point violates the margin. The parameter \(C\) controls the trade-off between <strong>maximizing the margin</strong> (by minimizing \(\|w\|_2^2\)) and <strong>minimizing the violations</strong> (the sum of the slack variables).</p> <ul> <li>A <strong>larger value of \(C\)</strong> places more emphasis on minimizing violations, which results in a stricter margin but could lead to overfitting if \(C\) is too large.</li> <li>A <strong>smaller value of \(C\)</strong> allows for more margin violations, potentially leading to a <strong>wider margin</strong> and better generalization.</li> </ul> </li> <li> <p><strong>Margin Constraint</strong>:</p> \[y_i (w^T x_i + b) \geq 1 - \xi_i\] <p>This constraint ensures that the data points are correctly classified with a margin of at least 1, unless there is a violation. If a data point violates the margin (i.e., it lies inside the margin or on the wrong side of the hyperplane), the slack variable \(\xi_i\) becomes positive. The value of \(\xi_i\) measures how much the margin is violated for the data point \(x_i\).</p> <ul> <li> <p>When \(\xi_i = 0\), the data point \(x_i\) satisfies the margin condition:</p> \[y_i (w^T x_i + b) \geq 1\] <p>This represents the ideal case where the point lies correctly outside or on the margin.</p> </li> <li> <p>When \(\xi_i &gt; 0\), the point <strong>violates the margin</strong>. The larger the value of \(\xi_i\), the greater the violation. For example, if \(\xi_i = 0.5\), the point lies inside the margin or is misclassified by 0.5 units.</p> </li> </ul> </li> <li> <p><strong>Non-Negativity of Slack Variables</strong>:</p> \[\xi_i \geq 0\] <p>This ensures that the slack variables \(\xi_i\) are always non-negative, as they represent the <strong>degree of violation</strong> of the margin. Since it’s not possible to have a negative violation, this constraint enforces that \(\xi_i\) cannot be less than zero.</p> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_6-480.webp 480w,/assets/img/Max_Margin_Classifier_6-800.webp 800w,/assets/img/Max_Margin_Classifier_6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h6 id="wrapping-up"><strong>Wrapping Up</strong></h6> <p>The maximum-margin classifier forms the foundation of modern support vector machines. For non-linearly separable data, the introduction of slack variables allows SVMs to adapt while maintaining their core principle of maximizing the margin.</p> <p>In the next post, we’ll dive deeper into the world of SVMs, explore how they work under the hood, and work through this optimization problem to solve it. Stay tuned!</p> <h6 id="references"><strong>References:</strong></h6> <ul> <li><a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html">Lecture 9: SVM</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.]]></summary></entry><entry><title type="html">L1 and L2 Regularization - Nuanced Details</title><link href="https://monishver11.github.io/blog/2025/l1-l2-reg-indepth/" rel="alternate" type="text/html" title="L1 and L2 Regularization - Nuanced Details"/><published>2025-01-05T17:50:00+00:00</published><updated>2025-01-05T17:50:00+00:00</updated><id>https://monishver11.github.io/blog/2025/l1-l2-reg-indepth</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/l1-l2-reg-indepth/"><![CDATA[<p>Regularization is a cornerstone in machine learning, providing a mechanism to prevent overfitting while controlling model complexity. Among the most popular techniques are <strong>L1</strong> and <strong>L2 regularization</strong>, which serve different purposes but share a common goal of improving model generalization. In this post, we will delve deep into the theory, mathematics, and practical implications of these regularization methods.</p> <p>Let’s set the stage with linear regression. For a dataset</p> \[D_n = \{(x_1, y_1), \dots, (x_n, y_n)\},\] <p>the objective in ordinary least squares is to minimize the mean squared error:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2.\] <p>While effective, this approach can overfit when the number of features \(d\) is large compared to the number of samples \(n\). For example, in natural language processing, it is common to have millions of features but only thousands of documents.</p> <h5 id="addressing-overfitting-with-regularization"><strong>Addressing Overfitting with Regularization</strong></h5> <p>To mitigate overfitting, <strong>\(L_2\) regularization</strong> (also known as <strong>ridge regression</strong>) adds a penalty term proportional to the \(L_2\) norm of the weights:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2 + \lambda \|w\|_2^2,\] <p>where:</p> \[\|w\|_2^2 = w_1^2 + w_2^2 + \dots + w_d^2.\] <p>This penalty term discourages large weight values, effectively shrinking them toward zero. When \(\lambda = 0\), the solution reduces to ordinary least squares. As \(\lambda\) increases, the penalty grows, favoring simpler models with smaller weights.</p> <h5 id="understanding-l_2-regularization"><strong>Understanding \(L_2\) Regularization</strong></h5> <p>L2 regularization is particularly effective at reducing sensitivity to fluctuations in the input data. To understand this, consider a simple linear function:</p> \[\hat{f}(x) = \hat{w}^\top x.\] <p>The function \(\hat{f}(x)\) is said to be <strong>Lipschitz continuous</strong>, with a Lipschitz constant defined as:</p> \[L = \|\hat{w}\|_2.\] <p>This implies that when the input changes from \(x\) to \(x + h\), the function’s output change is bounded by \(L\|h\|_2\). In simpler terms, \(L_2\) regularization controls the rate of change of \(\hat{f}(x)\), making the model less sensitive to variations in the input data.</p> <h6 id="mathematical-proof-of-lipschitz-continuity"><strong>Mathematical Proof of Lipschitz Continuity</strong></h6> <p>To formalize this property, let’s derive the Lipschitz bound:</p> \[|\hat{f}(x + h) - \hat{f}(x)| = |\hat{w}^\top (x + h) - \hat{w}^\top x| = |\hat{w}^\top h|.\] <p>Using the <strong>Cauchy-Schwarz inequality</strong>, this can be bounded as:</p> \[|\hat{w}^\top h| \leq \|\hat{w}\|_2 \|h\|_2.\] <p>Thus, the Lipschitz constant \(L = \|\hat{w}\|_2\) quantifies the maximum rate of change for the function \(\hat{f}(x)\).</p> <h5 id="generalization-to-other-norms"><strong>Generalization to Other Norms</strong></h5> <p>The generalization to other norms comes from the equivalence of norms in finite-dimensional vector spaces. Here’s the reasoning:</p> <p><strong>Norm Equivalence:</strong></p> <p>In finite-dimensional spaces (e.g., \(\mathbb{R}^d\)), all norms are equivalent. This means there exist constants \(C_1, C_2 &gt; 0\) such that for any vector \(\mathbf{w} \in \mathbb{R}^d\):</p> \[C_1 \| \mathbf{w} \|_p \leq \| \mathbf{w} \|_q \leq C_2 \| \mathbf{w} \|_p\] <p>For example, the \(L_1\), \(L_2\), and \(L_\infty\) norms can all bound one another with appropriate scaling constants.</p> <p><strong>Lipschitz Continuity:</strong></p> <p>The Lipschitz constant for \(\hat{f}(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}\) depends on the norm of \(\mathbf{w}\) because the bound for the rate of change involves the norm of \(\mathbf{w}\). When using a different norm \(\| \cdot \|_p\) to regularize, the Lipschitz constant adapts to that norm.</p> <p>Specifically, for the \(L_p\) norm:</p> \[| \hat{f}(\mathbf{x} + \mathbf{h}) - \hat{f}(\mathbf{x}) | \leq \| \mathbf{w} \|_p \| \mathbf{h} \|_q\] <p>where \(p\) and \(q\) satisfies:</p> \[\frac{1}{p} + \frac{1}{q} = 1\] <p><strong>Key Insight:</strong></p> <p>This shows that the idea of controlling the sensitivity of the model (through the Lipschitz constant) extends naturally to any norm. The choice of norm alters how the regularization penalizes weights but retains the fundamental property of bounding the function’s rate of change.</p> <h6 id="an-analogy-to-internalize-this"><strong>An analogy to internalize this:</strong></h6> <p>Think of \(L_2\) regularization as a bungee cord attached to a daring rock climber. The climber represents the model trying to navigate a complex landscape (data). Without the cord (regularization), they might venture too far and fall into overfitting. The cord adds just enough tension (penalty) to keep the climber balanced and safe, ensuring they explore the terrain without taking reckless leaps. Similarly, regularization helps the model stay grounded, generalizing well without succumbing to overfitting.</p> <p>Now, imagine different types of bungee cords for different norms. The \(L_2\) regularization bungee cord is like a standard elastic cord, providing a smooth and consistent tension, ensuring the climber doesn’t over-extend but can still make significant progress.</p> <p>For \(L_1\) regularization, the bungee cord is more rigid and less forgiving, preventing large movements in any direction. It forces the climber to stick to fewer, more significant paths, like sparsity in feature selection — only the most important features remain.</p> <p>In the case of \(L_\infty\) regularization, the bungee cord has a fixed maximum stretch. No matter how hard the climber tries to move, they cannot go beyond a certain point, ensuring the model remains under tight control, limiting the complexity of each individual parameter.</p> <p>In each case, the regularization (the cord) helps the climber (the model) stay within safe bounds, preventing them from falling into overfitting while ensuring they can still navigate the data effectively.</p> <hr/> <h4 id="linear-vs-ridge-regression"><strong>Linear vs. Ridge Regression</strong></h4> <p>The inclusion of L2 regularization modifies the optimization objective, as illustrated by the difference between <strong>linear regression</strong> and <strong>ridge regression</strong>.</p> <p>In <strong>linear regression</strong>, the goal is to minimize the sum of squared residuals, expressed as:</p> \[L(w) = \frac{1}{2} \|Xw - y\|_2^2\] <p>In contrast, <strong>ridge regression</strong> introduces an additional penalty term proportional to the L2 norm of the weights:</p> \[L(w) = \frac{1}{2} \|Xw - y\|_2^2 + \frac{\lambda}{2} \|w\|_2^2\] <p>This additional term penalizes large weights, helping to control model complexity and reduce overfitting.</p> <h6 id="gradients-of-the-objective"><strong>Gradients of the Objective:</strong></h6> <p>The inclusion of the regularization term affects the gradient of the loss function. For linear regression, the gradient is:</p> \[\nabla L(w) = X^T (Xw - y)\] <p>For ridge regression, the gradient becomes:</p> \[\nabla L(w) = X^T (Xw - y) + \lambda w\] <p>The regularization term \(\lambda w\) biases the solution toward smaller weights, thereby stabilizing the optimization. By adding this term, the model is less sensitive to small changes in the data, especially in cases where multicollinearity exists, i.e., when features are highly correlated.</p> <h6 id="closed-form-solutions"><strong>Closed-form Solutions:</strong></h6> <p>Both linear regression and ridge regression admit closed-form solutions. For linear regression, the weights are given by:</p> \[w = (X^T X)^{-1} X^T y\] <p>For ridge regression, the solution is slightly modified:</p> \[w = (X^T X + \lambda I)^{-1} X^T y\] <p>The addition of \(\lambda I\) ensures that \(X^T X + \lambda I\) is always invertible, addressing potential issues of singularity in the design matrix. In linear regression, if the matrix \(X^T X\) is singular or nearly singular (which can occur when features are linearly dependent or when there are more features than samples), the inverse may not exist or be unstable. By adding \(\lambda I\), where \(I\) is the identity matrix, we effectively shift the eigenvalues of \(X^T X\), making the matrix non-singular and ensuring a stable solution.</p> <hr/> <h4 id="a-constrained-optimization-perspective"><strong>A Constrained Optimization Perspective</strong></h4> <p>L2 regularization can also be understood through the lens of constrained optimization. In this perspective, the ridge regression objective is expressed in <strong>Tikhonov regularization</strong> form as:</p> \[w^* = \arg\min_w \left( \frac{1}{2} \|Xw - y\|_2^2 + \frac{\lambda}{2} \|w\|_2^2 \right)\] <p>The <strong>Ivanov form</strong> is another perspective where the objective is similarly constrained, but the constraint is typically applied in a more specific way, usually in the context of ill-posed problems or regularization approaches in functional analysis. It focuses on minimizing the error while controlling the solution’s smoothness or complexity. While this form is less commonly used directly in machine learning, it is foundational in understanding regularization in more theoretical settings. We mention this now because both forms will appear later in the discussion of other concepts, and it’s helpful to have a brief overview before we revisit them in more depth.</p> <p>Alternatively, using <strong>Lagrangian theory</strong>, we can reframe ridge regression as a constrained optimization problem. The objective is to minimize the residual sum of squares subject to a constraint on the L2 norm of the weights:</p> \[w^* = \arg\min_{w : \|w\|_2^2 \leq r} \frac{1}{2} \|Xw - y\|_2^2\] <p>Here, \(r\) represents the maximum allowed value for the squared norm of the weights, effectively placing a limit on their size. The Lagrange multiplier adjusts the importance of the constraint during optimization. This form emphasizes the constraint on model complexity, ensuring that the weights don’t grow too large.</p> <p>At the optimal solution, the gradients of the objective function and the constraint term balance each other, providing a geometric interpretation of how regularization controls the model complexity.</p> <p><strong>Note:</strong> The Lagrangian theory will be explored further when we discuss Support Vector Machines (SVMs), where this approach plays a central role in optimization.</p> <hr/> <h4 id="lasso-regression-and-l_1-regularization"><strong>Lasso Regression and \(L_1\) Regularization</strong></h4> <p>While L2 regularization minimizes the sum of squared weights, <strong>L1 regularization</strong> (used in Lasso regression) minimizes the sum of absolute weights. This is expressed as:</p> \[w^* = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n (\hat{w}^T x_i - y_i)^2 + \lambda \|w\|_1\] <p>Here, the L1 norm</p> \[\|w\|_1 = |w_1| + |w_2| + \dots + |w_d|\] <p>encourages sparsity in the weight vector, setting some coefficients exactly to zero. <strong>But what’s behind this, really?</strong> Keep reading!</p> <h5 id="ridge-vs-lasso-regression"><strong>Ridge vs. Lasso Regression</strong></h5> <p>The key difference between ridge and lasso regression lies in their impact on the weights. Ridge regression tends to shrink all coefficients toward zero but does not eliminate any of them. In contrast, lasso regression produces sparse solutions, where some coefficients are exactly zero. <strong>We’ll dive into this next.</strong></p> <p>This sparsity has significant practical advantages. By zeroing out irrelevant features, lasso regression simplifies the model, making it:</p> <ul> <li><strong>Faster</strong> to compute, as fewer features need to be processed.</li> <li><strong>Cheaper</strong> to store and deploy, especially on resource-constrained devices.</li> <li><strong>More interpretable</strong>, as it highlights the most important features.</li> <li><strong>Less prone to overfitting</strong>, since the reduced complexity often leads to better generalization.</li> </ul> <hr/> <h4 id="why-does-l_1-regularization-lead-to-sparsity"><strong>Why Does \(L_1\) Regularization Lead to Sparsity?</strong></h4> <p>A distinctive property of <strong>L1 regularization</strong> is its ability to produce sparse solutions, where some weights are exactly zero. This characteristic makes L1 regularization particularly useful for feature selection, as it effectively identifies the most important features by eliminating irrelevant ones. To understand this better, let’s explore the theoretical underpinnings and geometric intuition behind this phenomenon.</p> <h6 id="revisiting-lasso-regression"><strong>Revisiting Lasso Regression:</strong></h6> <p>Lasso regression penalizes the <strong>L1 norm</strong> of the weights. The objective function, also known as the <strong>Tikhonov form</strong>, is given by:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \big(w^T x_i - y_i\big)^2 + \lambda \|w\|_1\] <p>Here, the L1 norm is defined as:</p> \[\|w\|_1 = |w_1| + |w_2| + \dots + |w_d|\] <p>This formulation encourages sparsity by applying a uniform penalty across all weights, effectively “pushing” some weights to zero when they contribute minimally to the prediction.</p> <h6 id="regularization-as-constrained-empirical-risk-minimization-erm"><strong>Regularization as Constrained Empirical Risk Minimization (ERM)</strong></h6> <p>Regularization can also be viewed through the lens of <strong>constrained ERM</strong>. For a given complexity measure \(\Omega\) and a fixed threshold \(r \geq 0\), the optimization problem is expressed as:</p> \[\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i) \quad \text{s.t.} \quad \Omega(f) \leq r\] <p>In the case of Lasso regression, this is equivalent to the <strong>Ivanov form</strong>:</p> \[\hat{w} = \arg\min_{\|w\|_1 \leq r} \frac{1}{n} \sum_{i=1}^n \big(w^T x_i - y_i\big)^2\] <p>Here, \(r\) plays the same role as the regularization parameter \(\lambda\) in the penalized ERM (Tikhonov) form. The choice between these forms depends on whether the complexity is penalized directly or constrained explicitly.</p> <h5 id="the-ℓ1-and-ℓ2-norm-constraints"><strong>The ℓ1 and ℓ2 Norm Constraints</strong></h5> <p>To understand why L1 regularization promotes sparsity, consider a simple hypothesis space \(\mathcal{F} = \{f(x) = w_1x_1 + w_2x_2\}\). Each function can be represented as a point \((w_1, w_2)\) in \(\mathbb{R}^2\). The regularization constraints can be visualized as follows:</p> <ul> <li><strong>L2 norm constraint:</strong> \(w_1^2 + w_2^2 \leq r\), which is a <strong>circle</strong> in \(\mathbb{R}^2\).</li> <li><strong>L1 norm constraint:</strong> \(|w_1| + |w_2| \leq r\), which forms a <strong>diamond</strong> in \(\mathbb{R}^2\).</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_1-480.webp 480w,/assets/img/L1_Reg_1-800.webp 800w,/assets/img/L1_Reg_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><code class="language-plaintext highlighter-rouge">Note</code>: The sparse solutions correspond to the vertices of the diamond, where at least one weight is zero.</p> <p><strong>To build intuition</strong>, let’s analyze the geometry of the optimization:</p> <ol> <li>The <strong>blue region</strong> represents the feasible space defined by the regularization constraint (e.g., \(w_1^2 + w_2^2 \leq r\) for L2, or \(|w_1| + |w_2| \leq r\) for L1).</li> <li> <p>The <strong>red contours</strong> represent the level sets of the empirical risk function:</p> \[\hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \big(w^T x_i - y_i\big)^2\] </li> </ol> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_2_1-480.webp 480w,/assets/img/L1_Reg_2_1-800.webp 800w,/assets/img/L1_Reg_2_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_2_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_2_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_2_2-480.webp 480w,/assets/img/L1_Reg_2_2-800.webp 800w,/assets/img/L1_Reg_2_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_2_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_2_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The optimal solution is found where the smallest contour intersects the feasible region. For L1 regularization, this intersection tends to occur at the corners of the diamond, where one or more weights are exactly zero.</p> <p>Suppose the loss contours grow as perfect circles (or spheres in higher dimensions). When these contours intersect the diamond-shaped feasible region of L1 regularization, the corners of the diamond are more likely to be touched. These corners correspond to solutions where at least one weight is zero.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_3_2-480.webp 480w,/assets/img/L1_Reg_3_2-800.webp 800w,/assets/img/L1_Reg_3_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_3_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_3_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_3_1-480.webp 480w,/assets/img/L1_Reg_3_1-800.webp 800w,/assets/img/L1_Reg_3_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_3_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_3_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In contrast, for L2 regularization, the feasible region is a circle (or sphere), and the intersection is equally likely to occur in any direction. This results in small, but non-zero, weights across all features, rather than sparse solutions.</p> <h6 id="optimization-perspective"><strong>Optimization Perspective:</strong></h6> <p>From an optimization viewpoint, the difference between L1 and L2 regularization lies in how the penalty affects the gradient:</p> <ul> <li>For <strong>L2 regularization</strong>, as a weight \(w_i\) becomes smaller, the penalty \(\lambda w_i^2\) decreases more rapidly. However, the gradient of the penalty also diminishes, providing less incentive to shrink the weight to exactly zero.</li> <li>For <strong>L1 regularization</strong>, the penalty \(\lambda |w_i|\) decreases linearly, and its gradient remains constant regardless of the weight’s size. This consistent gradient drives small weights to zero, promoting sparsity.</li> </ul> <p><strong>Consider the following idea:</strong> Imagine you’re packing items into a small rectangular box, and you have two kinds of items: rigid boxes (representing \(L_1\) regularization) and pebbles (representing \(L_2\) regularization).</p> <p>The rigid boxes are shaped with sharp corners and don’t squish or deform. When you try to fit them into the small box, they naturally stack at the edges or corners of the space. This means some of the rigid boxes might not fit at all, so you leave them out—just like \(L_1\) regularization pushing weights to zero.</p> <p>The pebbles, on the other hand, are smooth and can be squished slightly. When you pack them into the box, they distribute evenly, filling in gaps without leaving any pebbles completely outside. This is like \(L_2\) regularization, where weights are reduced but not exactly zero.</p> <p>So, that’s why \(L_1\) regularization creates sparse solutions (only the most critical items get packed) while \(L_2\) regularization spreads the influence across all features (everything gets included, but smaller).</p> <h5 id="generalizing-to-ell_q-regularization"><strong>Generalizing to \(\ell_q\) Regularization</strong></h5> <p>\(\ell_1\) and \(\ell_2\) regularization are specific cases of the more general \(\ell_q\) regularization, defined as:</p> \[\|w\|_q^q = |w_1|^q + |w_2|^q + \dots + |w_d|^q\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_4-480.webp 480w,/assets/img/L1_Reg_4-800.webp 800w,/assets/img/L1_Reg_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here are some notable cases:</p> <ul> <li>For \(q \geq 1\), \(\|w\|_q\) is a valid norm.</li> <li>For \(0 &lt; q &lt; 1\), the constraint becomes non-convex, making optimization challenging. While \(\ell_q\) regularization with \(q &lt; 1\) can induce even sparser solutions than L1, it is often impractical in real-world scenarios. For instance when \(q=0.5\), the regularization takes the form of a square root function, which is non-convex.</li> <li>The \(\ell_0\) norm, defined as the number of non-zero weights, corresponds to <strong>subset selection</strong> but is computationally infeasible due to its combinatorial nature.</li> </ul> <p><strong>Note:</strong> \(L_n\)and \(\ell_n\) represent the same concept, so don’t let the difference in notation confuse you.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>\(L_1\) regularization’s sparsity-inducing property makes it an indispensable tool in feature selection and high-dimensional problems. Its optimization characteristics and ability to simplify models while retaining interpretability set it apart from \(L_2\) regularization.</p> <p>Next, we’ll talk about the <strong>maximum margin classifier &amp; SVM</strong>. Stay tuned, as moving on, it’s going to get a little intense, but don’t worry—we’ll get through it together!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models">why-l1-norm-for-sparse-models</a></li> <li><a href="https://blog.mlreview.com/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a">L1 Norm Regularization and Sparsity Explained for Dummies</a></li> <li><a href="https://math.stackexchange.com/questions/1904767/why-small-l1-norm-means-sparsity">why-small-l1-norm-means-sparsity</a></li> <li><a href="https://medium.com/analytics-vidhya/regularization-path-using-lasso-regression-c450eea9321e">Regularization path using Lasso regression</a></li> <li>Image Credits: Mairal et al.’s Sparse Modeling for Image and Vision Processing Fig 1.6, KPM Fig. 13</li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.]]></summary></entry><entry><title type="html">Regularization - Balancing Model Complexity and Overfitting</title><link href="https://monishver11.github.io/blog/2025/regularization/" rel="alternate" type="text/html" title="Regularization - Balancing Model Complexity and Overfitting"/><published>2025-01-03T16:39:00+00:00</published><updated>2025-01-03T16:39:00+00:00</updated><id>https://monishver11.github.io/blog/2025/regularization</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/regularization/"><![CDATA[<p>When building machine learning models, one of the core challenges is finding the right balance between <strong>approximation error</strong> and <strong>estimation error</strong>. The trade-off can be understood in terms of the size and complexity of the hypothesis space, denoted by \(F\).</p> <p>On the one hand, a <strong>larger hypothesis space</strong> allows the model to better approximate the true underlying function. However, this flexibility comes at a cost: the risk of overfitting the training data, especially if the dataset is small. On the other hand, a <strong>smaller hypothesis space</strong> is less prone to overfitting, but it may lack the expressiveness needed to capture the true relationship between inputs and outputs, leading to higher approximation error.</p> <p>To control this trade-off, we need a way to quantify and limit the complexity of \(F\). This can be done in various ways, such as limiting the number of variables or restricting the degree of polynomials in a model.</p> <hr/> <h4 id="how-to-control-model-complexity"><strong>How to Control Model Complexity</strong></h4> <p>A common strategy to manage complexity involves learning a sequence of models with increasing levels of sophistication. Mathematically, this sequence can be represented as:</p> \[F_1 \subset F_2 \subset \dots \subset F_n \subset F\] <p>where each subsequent space, \(F_i\), is a superset of the previous one, representing models of greater complexity.</p> <p>For example, consider polynomial regression. The full hypothesis space, \(F\), includes all polynomial functions, while \(F_d\) is restricted to polynomials of degree \(\leq d\). By increasing \(d\), we explore more complex models within the same overarching hypothesis space.</p> <p>Once this sequence of models is defined, we evaluate them using a scoring metric, such as validation error, to identify the one that best balances complexity and accuracy. This approach ensures a systematic way to control overfitting while retaining sufficient expressive power.</p> <hr/> <h4 id="feature-selection-in-linear-regression"><strong>Feature Selection in Linear Regression</strong></h4> <p>In linear regression, the concept of nested hypothesis spaces is closely tied to <strong>feature selection</strong>. The idea is to construct a series of models using progressively fewer features:</p> \[F_1 \subset F_2 \subset \dots \subset F_n \subset F\] <p>where \(F\) represents models that use all available features, and \(F_d\) contains models using fewer than \(d\) features.</p> <p>For example, if we have two features, \(\{X_1, X_2\}\), we can train models using the subsets \(\{\}\), \(\{X_1\}\), \(\{X_2\}\), and \(\{X_1, X_2\}\). Each subset corresponds to a different hypothesis space, and the goal is to select the one that performs best according to a validation score.</p> <p>However, this approach quickly becomes computationally infeasible as the number of features grows. Exhaustively searching through all subsets of features leads to a combinatorial explosion, making it impractical for datasets with many features.</p> <h5 id="greedy-feature-selection-methods"><strong>Greedy Feature Selection Methods</strong></h5> <p>To overcome the inefficiency of exhaustive search, greedy algorithms such as forward selection and backward selection are commonly used.</p> <h6 id="forward-selection"><strong>Forward Selection</strong></h6> <p>Forward selection begins with an empty set of features and incrementally adds the most promising feature at each step. Initially, the model contains no features, represented as \(S = \{\}\). At each iteration:</p> <ol> <li> <p>For every feature not in the current set \(S\), a model is trained using the combined set \(S \cup \{i\}\).</p> </li> <li> <p>The performance of the model is evaluated, and a score, \(\alpha_i\), is assigned to each feature.</p> </li> <li> <p>The feature \(j\) with the highest score is added to the set, provided it improves the model’s performance.</p> </li> <li> <p>This process repeats until adding more features no longer improves the score.</p> </li> </ol> <h6 id="backward-selection"><strong>Backward Selection</strong></h6> <p>Backward selection starts at the opposite end of the spectrum. Instead of beginning with an empty set, it starts with all available features, \(S = \{X_1, X_2, \dots, X_p\}\). At each step, the feature that contributes the least to the model’s performance is removed. This process continues until no further removals improve the model’s score.</p> <h6 id="reflections-on-feature-selection"><strong>Reflections on Feature Selection</strong></h6> <p>Feature selection provides a natural way to control the complexity of a linear prediction function by limiting the number of features. The overarching goal is to strike a balance between minimizing training error and controlling model complexity, often through a scoring metric that incorporates both factors.</p> <p>While forward and backward selection methods are intuitive and computationally efficient, they have their limitations. For one, they do not guarantee finding the optimal subset of features. Additionally, the subsets selected by the two methods may differ, as the process is sensitive to the order in which features are evaluated.</p> <p>This brings us to an important question:</p> <blockquote> <p>Can feature selection be framed as a consistent optimization problem, leading to more robust and reliable solutions?</p> </blockquote> <p>In the next section, we explore how <strong>regularization</strong> offers a principled way to tackle this problem, providing a unified framework to balance model complexity and performance.</p> <hr/> <h4 id="l_1-and-l_2-regularization"><strong>\(L_1\) and \(L_2\) Regularization</strong></h4> <p>In the previous section, we discussed feature selection as a means to control model complexity. While effective, these methods are often computationally expensive and can lack consistency. Regularization offers a more systematic approach by introducing a <strong>complexity penalty</strong> directly into the objective function. This allows us to balance prediction performance with model simplicity in a principled manner.</p> <h5 id="complexity-penalty-balancing-simplicity-and-accuracy"><strong>Complexity Penalty: Balancing Simplicity and Accuracy</strong></h5> <p>The idea behind regularization is to augment the loss function with a penalty term that discourages overly complex models. For example, a scoring function for feature selection can be expressed as:</p> \[\text{score}(S) = \text{training_loss}(S) + \lambda |S|\] <p>where \(|S|\) is the number of selected features, and \(\lambda\) is a hyperparameter that controls the trade-off between training loss and complexity.</p> <p>A larger \(\lambda\) imposes a heavier penalty on complexity, meaning that adding an extra feature is only justified if it significantly improves the training loss—by at least \(\lambda\). This approach discourages the inclusion of unnecessary features, effectively shrinking the hypothesis space \(F\).</p> <p>However, directly using the number of features as a complexity measure is non-differentiable, making it hard to optimize. This limitation motivates the use of alternative measures, such as norms on the model weights, which provide a differentiable and computationally efficient framework.</p> <p><strong>Consider it like this:</strong>: Think of choosing ingredients for a dish. The training loss is like the flavor of the dish, and the penalty term is like the cost of adding ingredients. If you add too many ingredients (features), the cost goes up, and the dish may become overcomplicated or unbalanced. By introducing a penalty (regularization), you’re essentially saying, “Only add more ingredients if they significantly improve the flavor.” The larger the penalty (larger \(\lambda\)), the more careful you have to be about adding new ingredients, encouraging simplicity and preventing the dish from becoming too cluttered. This approach keeps the recipe (model) balanced and prevents unnecessary complexity.</p> <h6 id="soft-selection-through-weight-shrinkage"><strong>Soft Selection Through Weight Shrinkage</strong></h6> <p>Instead of hard feature selection, regularization encourages <strong>soft selection</strong> by penalizing the magnitude of the model weights. Consider a linear regression model:</p> \[f(x) = w^\top x\] <p>where \(w_i\) represents the weight for the \(i\)-th feature. If \(w_i\) is zero or close to zero, it effectively excludes the corresponding feature from the model.</p> <h6 id="why-shrink-weights"><strong>Why Shrink Weights?</strong></h6> <p>Intuitively, smaller weights make the model more stable. A regression line with a smaller slope produces smaller changes in the output for a given change in the input. This stability has two key benefits:</p> <ol> <li> <p><strong>Reduced Sensitivity to Noise:</strong> Smaller weights make the model less prone to overfitting, as predictions are less sensitive to fluctuations in the training data.</p> </li> <li> <p><strong>Better Generalization:</strong> By pushing weights toward zero, the model becomes less sensitive to variations in new datasets, improving its robustness.</p> </li> </ol> <h6 id="weight-shrinkage-in-polynomial-regression"><strong>Weight Shrinkage in Polynomial Regression</strong></h6> <p>In polynomial regression, where the \(n\)-th feature corresponds to the \(n\)-th power of \(x\), weight shrinkage plays a crucial role in preventing overfitting. For instance, consider two polynomial models:</p> \[\hat{y} = 0.001x^7 + 0.003x^3 + 1, \quad \text{and} \quad \hat{y} = 1000x^7 + 500x^3 + 1\] <p>The second model has large coefficients, making the curve “wiggle” excessively to fit the training data, a hallmark of overfitting. In contrast, the first model—with smaller weights—is smoother and less prone to overfitting.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Polynomial_Regression_Plot-480.webp 480w,/assets/img/Polynomial_Regression_Plot-800.webp 800w,/assets/img/Polynomial_Regression_Plot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Polynomial_Regression_Plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Polynomial_Regression_Plot" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Function Plots in Desmos </div> <p><strong>Think of it this way</strong>: Imagine you’re driving a car down a winding road. A car with a sensitive steering wheel (large weights) will make sharp turns with every slight variation in the road, making the ride bumpy and unpredictable. In contrast, a car with a more stable, less sensitive steering wheel (smaller weights) will handle the same road with smoother, more controlled movements, reducing the impact of small bumps and ensuring a more stable journey. Similarly, in regression, smaller weights lead to smoother, more stable models that are less prone to overfitting and better at handling new data.</p> <h5 id="linear-regression-with-l_2-regularization"><strong>Linear Regression with \(L_2\) Regularization</strong></h5> <p>Let’s formalize this idea using linear regression. For a dataset \(D_n = \{(x_1, y_1), \dots, (x_n, y_n)\}\), the objective in ordinary least squares is to minimize the mean squared error:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2.\] <p>While effective, this approach can overfit when the number of features \(d\) is large compared to the number of samples \(n\). For example, in natural language processing, it’s common to have millions of features but only thousands of documents.</p> <p>To address this, <strong>\(L_2\) regularization</strong> (also known as <strong>ridge regression</strong>) adds a penalty on the \(L_2\) norm of the weights:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2 + \lambda \|w\|_2^2,\] <p>where:</p> \[\|w\|_2^2 = w_1^2 + w_2^2 + \dots + w_d^2\] <p>This additional term penalizes large weights, shrinking them toward zero. When \(\lambda = 0\), the solution reduces to ordinary least squares. As \(\lambda\) increases, the penalty grows, favoring simpler models with smaller weights.</p> <p><strong>Intuition</strong>: Think of fitting a suit to someone. In ordinary least squares, you would tailor the suit to fit perfectly according to every measurement. However, if the person has an unusual body shape or you have limited data, the suit might end up being too tight in some areas, causing discomfort. With \(L_2\) regularization, it’s like adding some flexibility to the design, allowing for slight adjustments to ensure the suit is comfortable and fits well, even if the measurements aren’t perfect. This prevents overfitting and makes the model more robust, much like a well-tailored suit that remains comfortable under different conditions.</p> <h6 id="generalization-to-other-models"><strong>Generalization to Other Models</strong></h6> <p>Although we’ve illustrated \(L_2\) regularization with linear regression, the concept extends naturally to other models, including neural networks. By penalizing the magnitude of weights, \(L_2\) regularization helps improve generalization across a wide range of machine learning tasks.</p> <hr/> <h6 id="closing-thoughts"><strong>Closing Thoughts</strong></h6> <p>Regularization, whether through weight shrinkage or complexity penalties, provides a robust mechanism to balance model expressiveness and generalization. In the next section, we’ll explore <strong>\(L_1\) regularization</strong>, its sparsity-inducing properties, and how it differs from \(L_2\) regularization.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.]]></summary></entry><entry><title type="html">Loss Functions - Regression and Classification</title><link href="https://monishver11.github.io/blog/2025/loss-functions/" rel="alternate" type="text/html" title="Loss Functions - Regression and Classification"/><published>2025-01-02T21:28:00+00:00</published><updated>2025-01-02T21:28:00+00:00</updated><id>https://monishver11.github.io/blog/2025/loss-functions</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/loss-functions/"><![CDATA[<p>Loss functions are of critical importance to machine learning, guiding models to minimize errors and improve predictions. They quantify how far off a model’s predictions are from the actual outcomes and serve as the basis for optimization. In this post, we’ll explore loss functions for <strong>regression</strong> and <strong>classification</strong> problems, breaking down their mathematical foundations and building intuitive understanding along the way. We will then transition our focus to logistic regression, examining its relationship with loss functions in classification tasks.</p> <hr/> <h4 id="loss-functions-for-regression"><strong>Loss Functions for Regression</strong></h4> <p>Regression tasks focus on predicting continuous values. Think about forecasting stock prices, estimating medical costs based on patient details, or predicting someone’s age from their photograph. These problems share a common requirement: accurately measuring how close the predicted values are to the true values.</p> <h6 id="setting-the-stage-notation"><strong>Setting the Stage: Notation</strong></h6> <p>Before diving in, let’s clarify the notation:</p> <ul> <li>\(\hat{y}\) represents the predicted value (the model’s output).</li> <li>\(y\) denotes the actual observed value (the ground truth).</li> </ul> <p>A <strong>loss function</strong> for regression maps the predicted and actual values to a real number: \(\ell(\hat{y}, y) \in \mathbb{R}.\) Most regression losses are based on the <strong>residual</strong>, defined as:</p> \[r = y - \hat{y}\] <p>The residual captures the difference between the true value and the prediction.</p> <h6 id="what-makes-a-loss-function-distance-based"><strong>What Makes a Loss Function Distance-Based?</strong></h6> <p>A loss function is <strong>distance-based</strong> if it meets two criteria:</p> <ol> <li>It depends solely on the residual: \(\ell(\hat{y}, y) = \psi(y - \hat{y}),\) where \(\psi: \mathbb{R} \to \mathbb{R}.\)</li> <li>It equals zero when the residual is zero: \(\psi(0) = 0.\)</li> </ol> <p>Such loss functions are <strong>translation-invariant</strong>, meaning they remain unaffected if both the prediction and the actual value are shifted by the same amount: \(\ell(\hat{y} + b, y + b) = \ell(\hat{y}, y), \quad \forall b \in \mathbb{R}.\)</p> <p>However, in some scenarios, translation invariance may not be desirable. For example, using the <strong>relative error</strong>: \(\text{Relative error} = \frac{\hat{y} - y}{y},\) provides a loss function better suited to cases where proportional differences matter.</p> <p>For instance:</p> <ul> <li>If the actual stock price is $100, and your model predicts $110, the absolute error is $10, but the relative error is 10%.</li> <li>But, if the actual stock price is $10, and your model predicts $11, the absolute error is still $1, but the relative error is 10%.</li> </ul> <h5 id="exploring-common-loss-functions-for-regression"><strong>Exploring Common Loss Functions for Regression</strong></h5> <h6 id="1-squared-loss-l2-loss"><strong>1. Squared Loss (L2 Loss)</strong></h6> <p>Squared loss is one of the most widely used loss functions:</p> \[\ell(r) = r^2 = (y - \hat{y})^2\] <p>This loss penalizes large residuals more heavily, making it sensitive to outliers. Its simplicity and differentiability make it popular in linear regression and similar models.</p> <h6 id="2-absolute-loss-l1-loss"><strong>2. Absolute Loss (L1 Loss)</strong></h6> <p>Absolute loss measures the magnitude of the residual:</p> \[\ell(r) = |r| = |y - \hat{y}|\] <p>Unlike squared loss, absolute loss is robust to outliers but lacks smooth differentiability.</p> <p><strong>Think of it this way</strong>: Imagine predicting house prices based on size. If one house in the dataset has an extremely high price (an outlier), using absolute loss will make the model focus more on the typical pricing pattern of most houses and ignore the outlier. In contrast, least squares regression would try to minimize the error caused by that outlier, potentially distorting the model.</p> <h6 id="3-huber-loss"><strong>3. Huber Loss</strong></h6> <p>The Huber loss combines the best of both worlds:</p> \[\ell(r) = \begin{cases} \frac{1}{2}r^2 &amp; \text{if } |r| \leq \delta, \\ \delta |r| - \frac{1}{2}\delta^2 &amp; \text{if } |r| &gt; \delta. \end{cases}\] <p>For small residuals, it behaves like squared loss, while for large residuals, it switches to absolute loss, providing robustness without sacrificing differentiability. <strong>Note</strong>: Equal values and slopes at \((r = \delta)\).</p> <p><strong>Understanding Robustness</strong>: It describes a loss function’s resistance to the influence of outliers.</p> <ul> <li><strong>Squared loss</strong> is highly sensitive to outliers.</li> <li><strong>Absolute loss</strong> is much more robust.</li> <li><strong>Huber loss</strong> strikes a balance between sensitivity and robustness. Meaning, it is sensitive enough to provide a useful gradient for smaller errors (via L2), but becomes more robust to large residuals, preventing them from disproportionately influencing the model (via L1).</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Regression_Losses-480.webp 480w,/assets/img/Regression_Losses-800.webp 800w,/assets/img/Regression_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Regression_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Regression_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Regression Loss Functions </div> <hr/> <h4 id="loss-functions-for-classification"><strong>Loss Functions for Classification</strong></h4> <p>Classification tasks involve predicting discrete labels. For instance, we might want to decide whether an email is spam or if an image contains a cat. The challenge lies in guiding the model to make accurate predictions while quantifying the degree of correctness.</p> <h6 id="the-role-of-the-score-function"><strong>The Role of the Score Function</strong></h6> <p>In binary classification, the model predicts a score, \(f(x)\), for each input \(x\):</p> <ul> <li>If \(f(x) &gt; 0\), the model predicts the label \(1\).</li> <li>If \(f(x) &lt; 0\), the model predicts the label \(-1\).</li> </ul> <p>This score represents the model’s confidence, and its magnitude indicates how certain the prediction is.</p> <h6 id="what-is-the-margin"><strong>What is the Margin?</strong></h6> <p>The <strong>margin</strong> captures the relationship between the predicted score and the true label:</p> \[m = y\hat{y}\] <p>or equivalently:</p> \[m = yf(x)\] <p>The margin measures correctness:</p> <ul> <li><strong>Positive margin</strong>: The prediction is correct.</li> <li><strong>Negative margin</strong>: The prediction is incorrect.</li> </ul> <p>The goal of many classification tasks is to maximize this margin, ensuring confident and accurate predictions.</p> <h5 id="common-loss-functions-for-classification"><strong>Common Loss Functions for Classification</strong></h5> <h6 id="1-0-1-loss"><strong>1. 0-1 Loss</strong></h6> <p>The 0-1 loss is a simple yet impractical loss function:</p> \[\ell(y, \hat{y}) = \begin{cases} 0 &amp; \text{if } y = \hat{y} \\ 1 &amp; \text{if } y \neq \hat{y} \end{cases}\] <p>Alternatively,</p> \[\ell_{0-1}(f(x), y) = \mathbf{1}[yf(x) \leq 0]\] <p>Here, \(\mathbf{1}\) is the indicator function, which equals 1 if the condition is true and 0 otherwise.</p> <p>Although intuitive, the 0-1 loss is:</p> <ul> <li><strong>Non-convex</strong>, making optimization difficult, because its value is either 0 or 1, which creates a step-like behavior.</li> <li><strong>Non-differentiable</strong>, rendering gradient-based methods inapplicable. For instance, if \(\hat{y} = 0.5\), the loss could change abruptly from 0 to 1 depending on whether the true label \(y\) is 0 or 1, leading to no gradient at this boundary.</li> </ul> <h6 id="2-hinge-loss"><strong>2. Hinge Loss</strong></h6> <p>Hinge loss, commonly used in Support Vector Machines (SVMs), addresses the limitations of 0-1 loss:</p> \[\ell_{\text{Hinge}}(m) = \max(1 - m, 0)\] <p>It is a convex, upper bound on 0-1 loss and encourages a positive margin. However, it is not differentiable at \(m = 1\).</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Classification_Losses-480.webp 480w,/assets/img/Classification_Losses-800.webp 800w,/assets/img/Classification_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Classification_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classification_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Classification Loss Functions </div> <hr/> <h4 id="diving-deeper-logistic-regression"><strong>Diving Deeper: Logistic Regression</strong></h4> <p>In our exploration above, we’ve covered the basics of regression and classification losses. Now, let’s shift our focus to <strong>logistic regression</strong> and its corresponding loss functions, which are pivotal in classification problems. We’ll also touch on why square loss isn’t typically used for classification.</p> <p>Despite its name, <strong>logistic regression</strong> is not actually a regression algorithm—it’s a <strong>linear classification</strong> method. Logistic regression predicts probabilities, making it well-suited for binary classification problems.</p> <p>The predictions are modeled using the <strong>sigmoid function</strong>, denoted by \(\sigma(z)\), where:</p> \[\sigma(z) = \frac{1}{1 + \exp(-z)}\] <p>and \(z = f(x) = w^\top x\) is the score computed from the input features and weights.</p> <h5 id="logistic-regression-with-labels-as-0-or-1"><strong>Logistic Regression with Labels as 0 or 1</strong></h5> <p>When the labels are in \(\{0, 1\}\):</p> <ul> <li> <p>The predicted probability is: \(\hat{y} = \sigma(z)\)</p> </li> <li> <p>The loss function for logistic regression in this case is the <strong>binary cross-entropy loss</strong>:</p> \[\ell_{\text{Logistic}} = -y \log(\hat{y}) - (1 - y) \log(1 - \hat{y})\] </li> </ul> <p>Here’s how it works based on different predicted values of \(\hat{y}\):</p> <ul> <li><strong>If \(y = 1\) (True label is 1)</strong>: <ul> <li> <p>The loss is:</p> \[\ell_{\text{Logistic}} = -\log(\hat{y})\] <p>This means if the predicted probability \(\hat{y}\) is close to 1 (i.e., the model is confident that the class is 1), the loss will be very small (approaching 0). On the other hand, if \(\hat{y}\) is close to 0, the loss becomes large, penalizing the model for being very wrong.</p> </li> </ul> </li> <li><strong>If \(y = 0\) (True label is 0)</strong>: <ul> <li> <p>The loss is:</p> \[\ell_{\text{Logistic}} = -\log(1 - \hat{y})\] <p>In this case, if the predicted probability \(\hat{y}\) is close to 0 (i.e., the model correctly predicts the class as 0), the loss will be very small (approaching 0). However, if \(\hat{y}\) is close to 1, the loss becomes large, penalizing the model for incorrectly predicting class 1.</p> </li> </ul> </li> </ul> <h6 id="example-of-different-predicted-values">Example of Different Predicted Values:</h6> <ol> <li><strong>For a true label \(y = 1\):</strong> <ul> <li> <p>If \(\hat{y} = 0.9\): \(\ell_{\text{Logistic}} = -\log(0.9) \approx 0.105\) This is a small loss, since the model predicted a high probability for class 1, which is correct.</p> </li> <li> <p>If \(\hat{y} = 0.1\): \(\ell_{\text{Logistic}} = -\log(0.1) \approx 2.302\) This is a large loss, since the model predicted a low probability for class 1, which is incorrect.</p> </li> </ul> </li> <li><strong>For a true label \(y = 0\):</strong> <ul> <li> <p>If \(\hat{y} = 0.1\): \(\ell_{\text{Logistic}} = -\log(1 - 0.1) \approx 0.105\) This is a small loss, since the model predicted a low probability for class 1, which is correct.</p> </li> <li> <p>If \(\hat{y} = 0.9\): \(\ell_{\text{Logistic}} = -\log(1 - 0.9) \approx 2.302\) This is a large loss, since the model predicted a high probability for class 1, which is incorrect.</p> </li> </ul> </li> </ol> <h6 id="key-points">Key Points:</h6> <ul> <li>The <strong>negative sign</strong> in the loss function ensures that when the model predicts correctly (i.e., \(\hat{y}\) is close to the true label), the loss is minimized (approaching 0).</li> <li>The loss grows as the predicted probability \(\hat{y}\) moves away from the true label \(y\), and it grows more rapidly as the predicted probability becomes more confident but incorrect.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Binary_Cross_Entropy_Loss-480.webp 480w,/assets/img/Binary_Cross_Entropy_Loss-800.webp 800w,/assets/img/Binary_Cross_Entropy_Loss-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Binary_Cross_Entropy_Loss.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Binary_Cross_Entropy_Loss" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Binary Cross Entropy Loss Function(https://www.desmos.com/calculator/ygciza1leg) </div> <h5 id="logistic-regression-with-labels-as--1-or-1"><strong>Logistic Regression with Labels as -1 or 1</strong></h5> <p>When the labels are in \(\{-1, 1\}\), the sigmoid function simplifies using the property: \(1 - \sigma(z) = \sigma(-z).\)</p> <p>This allows us to express the loss equivalently as:</p> \[\ell_{\text{Logistic}} = \begin{cases} -\log(\sigma(z)) &amp; \text{if } y = 1, \\ -\log(\sigma(-z)) &amp; \text{if } y = -1. \end{cases}\] <p>Simplifying further:</p> \[\ell_{\text{Logistic}} = -\log(\sigma(yz)) = -\log\left(\frac{1}{1 + e^{-yz}}\right) = \log(1 + e^{-m})\] <p>where \(m = yz\) is the margin.</p> <h6 id="key-insights-of-logistic-loss"><strong>Key Insights of Logistic loss</strong>:</h6> <ul> <li>Is differentiable, enabling gradient-based optimization.</li> <li>Always rewards larger margins, encouraging more confident predictions.</li> <li>Never becomes zero, ensuring continuous optimization pressure.</li> </ul> <h6 id="what-about-square-loss-for-classification"><strong>What About Square Loss for Classification?</strong></h6> <p>Square loss, while effective for regression, is rarely used for classification. Let’s break it down:</p> \[\ell(f(x), y) = (f(x) - y)^2\] <p>For binary classification where \(y \in \{-1, 1\}\), we can rewrite this in terms of the margin:</p> \[\ell(f(x), y) = (f(x) - y)^2 = f^2(x) - 2f(x)y + y^2.\] <p>Using the fact that \(y^2 = 1\):</p> \[\ell(f(x), y) = f^2(x) - 2f(x)y + 1 = (1 - f(x)y)^2 = (1 - m)^2.\] <h6 id="why-not-use-square-loss"><strong>Why Not Use Square Loss?</strong></h6> <p>Square loss heavily penalizes outliers, such as mislabeled examples, making it unsuitable for classification tasks where robust performance on noisy data is crucial.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Logistic_Regression_Losses-480.webp 480w,/assets/img/Logistic_Regression_Losses-800.webp 800w,/assets/img/Logistic_Regression_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Logistic_Regression_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Logistic_Regression_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Logistic Regression Loss Functions </div> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>Loss functions form the backbone of machine learning, providing a mathematical framework for optimization. A quick recap:</p> <ul> <li><strong>Regression Losses</strong>: <ul> <li>Squared (L2) loss: Sensitive to outliers.</li> <li>Absolute (L1) loss: Robust but non-differentiable.</li> <li>Huber loss: Balances robustness and smoothness.</li> </ul> </li> <li><strong>Classification Losses</strong>: <ul> <li>Hinge loss: Encourages a large positive margin (used in SVMs).</li> <li>Logistic loss: Differentiable and rewards confidence.</li> </ul> </li> </ul> <p>These concepts tie back to critical components of machine learning workflows, such as <strong>gradient descent</strong>, which relies on the properties of loss functions to update model parameters effectively.</p> <p>Up next, we’ll dive into <strong>Regularization</strong>, focusing on how it combats overfitting and improves model performance. Stay tuned!</p> ]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.]]></summary></entry><entry><title type="html">Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training</title><link href="https://monishver11.github.io/blog/2025/sgd-tips/" rel="alternate" type="text/html" title="Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training"/><published>2025-01-01T19:18:00+00:00</published><updated>2025-01-01T19:18:00+00:00</updated><id>https://monishver11.github.io/blog/2025/sgd-tips</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/sgd-tips/"><![CDATA[<p>This is a continuation of the previous blog, and the content presented here consists of notes extracted from <a href="https://leon.bottou.org/">L´eon Bottou’s</a> <a href="https://leon.bottou.org/publications/pdf/tricks-2012.pdf">Stochastic Gradient Descent Tricks</a>. If you haven’t read my previous blog, I recommend taking a look, as the notations used here are introduced there. Alternatively, if you’re comfortable with the notations, you can jump straight into the content. So, let’s get started!</p> <h5 id="stochastic-gradient-descent-sgd"><strong>Stochastic Gradient Descent (SGD)</strong></h5> <p>Stochastic Gradient Descent (SGD) is a simplified version of the standard gradient descent algorithm. Rather than computing the exact gradient of the entire cost function \(E_n(f_w)\), each iteration of SGD estimates this gradient based on a <strong>single randomly chosen example</strong> \(z_t\). The update rule for the weights \(w\) at each iteration is:</p> \[w_{t+1} = w_t - \gamma_t \nabla_w Q(z_t, w_t)\] <p>where \(\gamma_t\) is the learning rate, and \(Q(z_t, w_t)\) represents the cost function evaluated at the current weights \(w_t\) for the randomly selected example \(z_t\).</p> <h6 id="key-features-of-sgd"><strong>Key Features of SGD:</strong></h6> <ul> <li> <p><strong>Randomness</strong>: The algorithm’s stochastic nature means that the updates depend on the examples randomly picked at each iteration. This randomness introduces some noise into the optimization process, but it is hoped that the algorithm behaves like its expectation despite this noise.</p> </li> <li> <p><strong>On-the-Fly Computation</strong>: Since SGD does not need to store information about the examples visited in previous iterations, it can process examples one by one. This makes it suitable for online learning or deployed systems, where data can arrive sequentially, and the model is updated in real-time.</p> </li> <li> <p><strong>Expected Risk Optimization</strong>: In a deployed system, where examples are drawn randomly from the ground truth distribution, SGD directly optimizes the expected risk, which is the expected value of the loss function over all possible examples. So, to put in simply - Each loss computed during SGD updates serves as an approximation of the expected loss over the true data distribution, and with more examples, it gradually optimizes the expected risk.</p> </li> </ul> <h6 id="convergence-of-sgd"><strong>Convergence of SGD:</strong></h6> <p>The convergence of SGD has been studied extensively in the stochastic approximation literature. Convergence typically requires that the learning rates satisfy the conditions:</p> \[\sum_{t=1}^{\infty} \gamma_t^2 &lt; \infty \quad\] \[\text{(It means the total sum of learning rates must go to infinity over time, ensuring enough updates for convergence)}\] \[\text{and} \quad \sum_{t=1}^{\infty} \gamma_t = \infty\] \[\text{(This mean the sum of squared learning rates must remain finite, ensuring the updates become smaller and smaller as the algorithm proceeds)}\] <p>These conditions help strike the balance between making large enough updates early on to explore the parameter space, but small enough updates later on to fine-tune the model and avoid overshooting the optimum.</p> <p>The <strong>Robbins-Siegmund theorem</strong> offers a formal proof that, under the right conditions—such as appropriate decreasing learning rates—<strong>SGD will converge almost surely</strong>, even when the loss function is non-smooth. This includes cases where the loss function has discontinuities or sharp gradients, making SGD a robust optimization method.</p> <h6 id="convergence-speed"><strong>Convergence Speed:</strong></h6> <p>The speed of convergence in SGD is ultimately limited by the noisy gradient approximations. Several factors impact the rate at which the algorithm converges:</p> <ul> <li><strong>Learning Rate Decay</strong>: <ul> <li>If the learning rates decrease too slowly, the variance of the parameter estimates \(w_t\) decreases at a similarly slow rate. <strong>Why?</strong> If the learning rate decreases too slowly, updates remain large for too long, causing high variance in parameter estimates and preventing the algorithm from stabilizing near the optimum.</li> <li>If the learning rates decay too quickly, the parameter estimates \(w_t\) take a long time to approach the optimum. <strong>Why?</strong> If the learning rate decreases too quickly, updates become too small early on, leading to insufficient exploration of the parameter space and slow convergence to the optimum.</li> </ul> </li> <li><strong>Optimal Convergence Speed</strong>: <ul> <li>When the <strong>Hessian matrix</strong> of the cost function at the optimum is <strong>strictly positive definite</strong>, the best convergence rate is achieved using learning rates of the form \(\gamma_t \sim t^{-1}\). In this case, the expectation of the residual error \(\rho\) decreases at the same rate, i.e., \(E(\rho) \sim t^{-1}\). This rate is commonly observed in practice.</li> </ul> </li> <li><strong>Relaxed Assumptions</strong>: <ul> <li>When these regularity assumptions(like a positive definite hessian, smoothness and strong convexity) are relaxed, the convergence rate slows down. The theoretical convergence rate in such cases is typically \(E(\rho) \sim t^{-1/2}\). However, in practice, this slower convergence tends to only manifest during the final stages of the optimization process. Often, optimization is stopped before this stage is reached, making the slower convergence less significant.</li> </ul> </li> </ul> <p>In summary, while the convergence of SGD can be slow due to its noisy nature, proper management of the learning rate and understanding of the problem’s characteristics can ensure good performance in practice.</p> <hr/> <h5 id="second-order-stochastic-gradient-descent-2sgd"><strong>Second-Order Stochastic Gradient Descent (2SGD)</strong></h5> <p>Second-Order Stochastic Gradient Descent (2SGD) extends stochastic gradient descent by incorporating curvature information through a positive definite matrix \(\Gamma_t\), which approximates the inverse of the Hessian matrix. The update rule for 2SGD is:</p> \[w_{t+1} = w_t - \gamma_t \Gamma_t \nabla_w Q(z_t, w_t),\] <p>where:</p> <ul> <li>\(w_t\): Current weights at iteration \(t\).</li> <li>\(\gamma_t\): Learning rate (step size), which may vary over iterations.</li> <li>\(\Gamma_t\): A positive definite matrix that approximates the inverse of the Hessian.</li> <li>\(\nabla_w Q(z_t, w_t)\): Gradient of the loss function \(Q\) with respect to \(w_t\) for the stochastic sample \(z_t\).</li> </ul> <h6 id="key-advantages-of-2sgd"><strong>Key Advantages of 2SGD</strong></h6> <ol> <li><strong>Curvature Awareness</strong>: <ul> <li>The inclusion of \(\Gamma_t\) enables the algorithm to account for the curvature of the loss surface.</li> <li>This adaptation improves convergence by rescaling updates to balance faster progress in flat directions and slower progress in steep directions.</li> </ul> </li> <li><strong>Improved Constants</strong>: <ul> <li>The scaling introduced by \(\Gamma_t\) can reduce the condition number of the problem. <strong>What?</strong> The condition number is the ratio of the largest to smallest eigenvalue of the Hessian, reflecting the curvature’s uniformity. A high condition number implies uneven curvature, slowing convergence.</li> <li>2SGD addresses this by scaling the parameter space to reduce the condition number, making the optimization landscape more uniform and this leads to faster convergence in terms of iteration efficiency when compared to standard SGD.</li> </ul> </li> </ol> <h6 id="challenges-in-2sgd"><strong>Challenges in 2SGD</strong></h6> <p>Despite the advantages, 2SGD has significant limitations:</p> <ol> <li><strong>Stochastic Noise</strong>: <ul> <li>The introduction of \(\Gamma_t\) does not address the stochastic noise inherent in gradient estimates.</li> <li>As a result, the variance in the weights \(w_t\) remains high, which limits its convergence benefits.</li> </ul> </li> <li><strong>Asymptotic Behavior</strong>: <ul> <li>The expected residual error decreases at a rate of \(\mathbb{E}[\rho] \sim t^{-1}\) at best.</li> <li>While constants(i.e., step size) are improved, the convergence rate remains fundamentally constrained by the stochastic nature of the gradients.</li> </ul> </li> </ol> <h6 id="comparison-with-batch-algorithms"><strong>Comparison with Batch Algorithms</strong></h6> <p><strong>Batch Algorithms</strong>:</p> <ul> <li>Batch methods utilize the full dataset to compute gradients at each iteration.</li> <li>They achieve better asymptotic performance with convergence rates that often scale as \(t^{-2}\) or better, depending on the algorithm.</li> </ul> <p><strong>2SGD</strong>:</p> <ul> <li>2SGD operates on a per-sample basis, which limits its ability to achieve higher convergence rates in expectation.</li> <li>The variance introduced by stochastic gradients limits its asymptotic efficiency compared to batch methods.</li> </ul> <h6 id="the-bigger-picture"><strong>The Bigger Picture</strong></h6> <p>Despite being asymptotically slower than batch algorithms, 2SGD remains highly relevant in modern machine learning for many reasons:</p> <ol> <li><strong>Efficiency in Large Datasets</strong>: <ul> <li>When datasets are too large to process as a batch, 2SGD provides an efficient alternative.</li> <li>It avoids the computational and memory overhead of storing and processing the entire dataset.</li> </ul> </li> <li><strong>Online Learning</strong>: <ul> <li>In online learning scenarios, where data arrives sequentially, 2SGD offers a practical approach to updating models in real time.</li> </ul> </li> </ol> <h6 id="summary-of-convergence-behavior"><strong>Summary of Convergence Behavior</strong></h6> <hr/> <table> <thead> <tr> <th><strong>Algorithm</strong></th> <th><strong>Error Decay</strong></th> <th><strong>Asymptotic Behavior</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gradient Descent (GD)</strong></td> <td>\(|w_t - w^*| \sim \rho^t\)</td> <td>Linear convergence: \(\mathcal{O}(t^{-1})\)</td> </tr> <tr> <td><strong>Stochastic Gradient Descent (SGD)</strong></td> <td>\(\mathbb{E}[|w_t - w^*|] \sim t^{-1}\)</td> <td>Asymptotic rate: \(t^{-1}\)</td> </tr> <tr> <td><strong>Second-Order Stochastic GD (2SGD)</strong></td> <td>\(\mathbb{E}[|w_t - w^*|] \sim t^{-1}\)</td> <td>Same as SGD, but with improved constants</td> </tr> </tbody> </table> <hr/> <p>Note:</p> <ul> <li> <p><strong>Linear Convergence</strong> (\(\mathcal{O}(t^{-1})\)): Implies an exponential decay of the error over time, with the error shrinking by a constant factor at each step.</p> </li> <li> <p><strong>Asymptotic Rate</strong> (\(t^{-1}\)): Describes the long-term error decay rate, indicating a polynomial decay (slower than exponential) where the error decreases inversely with time.</p> </li> </ul> <p>By incorporating second-order information through \(\Gamma_t\), 2SGD makes more informed updates. However, its performance is ultimately limited by the stochastic noise in gradient estimates. In practice, 2SGD is a compromise between computational efficiency and convergence speed, making it suitable for large-scale and online learning tasks.</p> <h4 id="when-to-use-stochastic-gradient-descent-sgd"><strong>When to Use Stochastic Gradient Descent (SGD)</strong></h4> <p>Stochastic Gradient Descent (SGD) is particularly well-suited when <strong>training time is the bottleneck</strong>. It is an effective choice in scenarios where computational efficiency and scalability are critical, such as in large-scale machine learning tasks.</p> <h6 id="key-insights-from-table"><strong>Key Insights from Table</strong></h6> <p>The table below summarizes the asymptotic behavior of four optimization algorithms:</p> <ul> <li><strong>Gradient Descent (GD)</strong>: Standard first-order method.</li> <li><strong>Second-Order Gradient Descent (2GD)</strong>: Incorporates curvature information.</li> <li><strong>Stochastic Gradient Descent (SGD)</strong>: A stochastic variant of GD.</li> <li><strong>Second-Order Stochastic Gradient Descent (2SGD)</strong>: Combines stochastic updates with curvature adaptation.</li> </ul> <hr/> <table> <thead> <tr> <th><strong>Algorithm</strong></th> <th><strong>Time per Iteration</strong></th> <th><strong>Iterations to Accuracy (\(\rho\))</strong></th> <th><strong>Time to Accuracy (\(\rho\))</strong></th> <th><strong>Time to Excess Error \(\epsilon\)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gradient Descent (GD)</strong></td> <td>\(n\)</td> <td>\(\log(1 / \rho)\)</td> <td>\(n \log(1 / \rho)\)</td> <td>\(\frac{1}{\epsilon^{1/\alpha}} \log(1 / \epsilon)\)</td> </tr> <tr> <td><strong>Second-Order Gradient Descent (2GD)</strong></td> <td>\(n\)</td> <td>\(\log \log(1 / \rho)\)</td> <td>\(n \log \log(1 / \rho)\)</td> <td>\(\frac{1}{\epsilon^{1/\alpha}} \log(1 / \epsilon) \log \log(1 / \epsilon)\)</td> </tr> <tr> <td><strong>Stochastic Gradient Descent (SGD)</strong></td> <td>\(1\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \epsilon\)</td> </tr> <tr> <td><strong>Second-Order Stochastic GD (2SGD)</strong></td> <td>\(1\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \epsilon\)</td> </tr> </tbody> </table> <hr/> <h6 id="discussion"><strong>Discussion</strong></h6> <ol> <li><strong>Per-Iteration Cost</strong>: <ul> <li><strong>GD and 2GD</strong>: Both require \(\mathcal{O}(n)\) time per iteration due to full-batch gradient computations.</li> <li><strong>SGD and 2SGD</strong>: Require \(\mathcal{O}(1)\) time per iteration, making them computationally inexpensive for large datasets.</li> </ul> </li> <li><strong>Convergence Speed</strong>: <ul> <li>GD and 2GD converge faster in terms of the number of iterations but incur higher computational costs because of full-batch updates.</li> <li>SGD and 2SGD require more iterations to converge but compensate with lower per-iteration costs.</li> </ul> </li> <li><strong>Asymptotic Performance</strong>: <ul> <li>While SGD and 2SGD have worse optimization noise, they require significantly less time to achieve a predefined expected risk \(\epsilon\) due to their reduced computational overhead.</li> <li>In large-scale settings where computation time is the limiting factor, <strong>stochastic learning algorithms are asymptotically better</strong>.</li> </ul> </li> </ol> <h6 id="key-takeaways"><strong>Key Takeaways</strong></h6> <ul> <li>Use <strong>SGD</strong> when: <ul> <li>Dataset size is large, and full-batch methods become computationally infeasible.</li> <li>Real-time or online learning scenarios require frequent updates with minimal latency.</li> <li>Memory efficiency is a concern, as SGD processes one sample at a time.</li> </ul> </li> <li>Despite higher variance in updates, <strong>SGD and 2SGD</strong> are preferred in large-scale setups due to their faster convergence to the expected risk with minimal computational resources.</li> </ul> <p>In conclusion, while SGD and 2SGD might appear less efficient in small-scale setups, their practical advantages in high-dimensional, data-intensive tasks make them highly favorable in modern machine learning applications.</p> <hr/> <h4 id="general-recommendations-for-stochastic-gradient-descent-sgd"><strong>General Recommendations for Stochastic Gradient Descent (SGD)</strong></h4> <p>The following is a series of recommendations for using stochastic gradient algorithms. Though seemingly trivial, the author’s experience highlights how easily they can be overlooked.</p> <h5 id="1-randomly-shuffle-the-training-examples"><strong>1. Randomly Shuffle the Training Examples</strong></h5> <p>Although the theory behind Stochastic Gradient Descent (SGD) calls for picking examples randomly, it is often tempting to process them sequentially through the training set. While sequentially passing through the examples may seem like an optimization, it can be problematic when the data is structured in a way that affects training performance.</p> <h6 id="key-points"><strong>Key Points:</strong></h6> <ul> <li><strong>Class Grouping and Order</strong>: If training examples are grouped by class or presented in a particular order, processing them in sequence can lead to biases in the gradient updates.</li> <li><strong>The Importance of Randomization</strong>: Randomizing the order helps break any inherent structure or patterns in the dataset that may skew the learning process. This ensures that each update is less dependent on the order of the examples, promoting better convergence.</li> </ul> <h6 id="analogy"><strong>Analogy:</strong></h6> <p>Think of SGD like a person learning to navigate a maze. If they always follow the same path (training examples in order), they may become “stuck” in a loop. However, if they randomly choose different routes (randomized examples), they are more likely to explore and discover the optimal path.</p> <h5 id="2-use-preconditioning-techniques"><strong>2. Use Preconditioning Techniques</strong></h5> <p>Stochastic Gradient Descent (SGD) is a first-order optimization algorithm, meaning it only uses the first derivatives (gradients) to guide the updates. However, this can lead to significant issues when the optimization process encounters areas where the <strong>Hessian</strong> (the matrix of second derivatives) is ill-conditioned. In such regions, the gradients may not provide efficient updates, slowing down convergence or leading to poor results.</p> <p>Fortunately, <strong>preconditioning techniques</strong> like Adagrad or Adam, adjust the learning rates based on past gradients, helping optimize in ill-conditioned regions for faster and more stable convergence.</p> <h6 id="key-points-1"><strong>Key Points:</strong></h6> <ul> <li><strong>Ill-conditioned regions</strong>: Areas where the curvature (second derivatives) of the cost function varies dramatically, making it hard for SGD to make efficient progress.</li> <li><strong>Improved convergence</strong>: Preconditioning techniques can rescale the gradients to make the learning process more stable and faster, improving convergence even in difficult regions.</li> </ul> <h6 id="analogy-1"><strong>Analogy:</strong></h6> <p>Imagine trying to push a boulder up a steep hill (representing optimization in ill-conditioned areas). Without a proper approach, the effort may be inefficient or lead you off-course. Preconditioning techniques act like a ramp, providing a smoother path and making it easier to move the boulder in the right direction.</p> <h5 id="3-monitor-both-the-training-cost-and-the-validation-error"><strong>3. Monitor Both the Training Cost and the Validation Error</strong></h5> <p>To effectively gauge the performance of your model during training, it is crucial to monitor both the <strong>training cost</strong> and the <strong>validation error</strong>. A simple yet effective approach involves repeating the following steps:</p> <h6 id="key-steps"><strong>Key Steps:</strong></h6> <ol> <li> <p><strong>Stochastic Gradient Descent (SGD) Update</strong>: Process once through the shuffled training set and perform the SGD updates. This helps adjust the model’s parameters based on the current data.</p> </li> <li> <p><strong>Compute Training Cost</strong>: After the updates, run another loop over the training set to compute the <strong>training cost</strong>. This cost represents the criterion (such as the loss function) the algorithm is optimizing. Monitoring the training cost provides insight into how well the model is minimizing the objective.</p> </li> <li> <p><strong>Compute Validation Error</strong>: With another loop, calculate the <strong>validation error</strong> using the validation set. This error is the performance measure of interest (such as classification error, accuracy, etc.). The validation error helps track how well the model generalizes to unseen data.</p> </li> </ol> <p>Although these steps require additional computational effort, including extra passes over both the training and validation datasets, they provide critical feedback and prevent training in isolation, avoiding the risk of overfitting or diverging from the optimal solution.</p> <h6 id="analogy-2"><strong>Analogy:</strong></h6> <p>Think of training a model like tuning a musical instrument. The training cost is like checking the sound of the instrument while you play (adjusting and fine-tuning as you go), while the validation error is like getting feedback from a concert audience (seeing how the performance holds up in a real-world scenario). Without both, you might end up with a well-tuned instrument that doesn’t sound good in a performance.</p> <h5 id="4-check-the-gradients-using-finite-differences"><strong>4. Check the Gradients Using Finite Differences</strong></h5> <p>When the computation of gradients is slightly incorrect, Stochastic Gradient Descent (SGD) tends to behave slowly and erratically. This often leads to the misconception that such behavior is the normal operation of the algorithm.</p> <p>Over the years, many practitioners have sought advice on how to set the learning rates \(\gamma_t\) for a rebellious SGD program. However, the best advice is often to <strong>forget about the learning rates</strong> and ensure that the gradients are being computed correctly. Once gradients are correctly computed, setting small enough learning rates becomes easy. Those who struggle with tuning learning rates often have faulty gradients in their computations.</p> <h6 id="how-to-check-gradients-using-finite-differences"><strong>How to Check Gradients Using Finite Differences:</strong></h6> <p>Rather than manually checking each line of the gradient computation code, use finite differences to verify the accuracy of the gradients.</p> <h6 id="steps"><strong>Steps:</strong></h6> <ol> <li> <p><strong>Pick an Example</strong>: Choose a training example \(z\) from the dataset.</p> </li> <li> <p><strong>Compute the Loss</strong>: Calculate the loss function \(Q(z, w)\) for the current weights \(w\).</p> </li> <li> <p><strong>Compute the Gradient</strong>: Calculate the gradient of the loss with respect to the weights: \(g = \nabla_w Q(z, w)\)</p> </li> <li><strong>Apply a Perturbation</strong>: Slightly perturb the weights by changing them. This can be done by either: <ul> <li>Changing a single weight by a small increment: \(w' = w + \delta\)</li> <li>Perturbing the weights using the gradient: \(w' = w - \gamma g\), where \(\gamma\) is small enough.</li> </ul> </li> <li> <p><strong>Compute the New Loss</strong>: After applying the perturbation, compute the new loss \(Q(z, w')\).</p> </li> <li><strong>Verify the Approximation</strong>: Ensure that the new loss approximates the original loss plus the perturbation multiplied by the gradient: \(Q(z, w') \approx Q(z, w) + \delta g\)</li> </ol> <p><strong>Example</strong>, consider the MSE loss function:</p> \[Q(z, w) = (w - z)^2\] <ol> <li><strong>Pick an Example</strong>: Let ( z = 5 ), ( w = 4 ).</li> <li><strong>Compute the Loss</strong>: \(Q(z, w) = (4 - 5)^2 = 1\)</li> <li><strong>Compute the Gradient</strong>: \(g = \nabla_w Q(z, w) = 2(w - z) = -2\)</li> <li><strong>Apply Perturbation</strong>: \(w' = w + 0.01 = 4.01\)</li> <li><strong>Compute the New Loss</strong>: \(Q(z, w') = (4.01 - 5)^2 = 0.9801\)</li> <li><strong>Verify</strong>: \(Q(z, w') \approx Q(z, w) + 0.01 \cdot (-2) = 0.98\)</li> </ol> <h6 id="automating-the-process"><strong>Automating the Process:</strong></h6> <p>This process can be automated and should be repeated for many examples \(z\), many perturbations \(\delta\), and many initial weights \(w\). Often, flaws in the gradient computation only appear under peculiar conditions, and it’s not uncommon to discover such bugs in SGD code that has been used for years without issue.</p> <h6 id="analogy-3"><strong>Analogy:</strong></h6> <p>Think of gradient checking like testing the brakes of a car. If the brakes (gradients) are faulty, the car (SGD) might not stop properly, leading to erratic behavior. Instead of repeatedly adjusting the speed (learning rate), you test the brakes by applying a small perturbation to the system (finite differences). If the brakes are working well, the car will stop smoothly at the right place (convergence).</p> <h5 id="5-experiment-with-the-learning-rates-gamma_t-using-a-small-sample-of-the-training-set"><strong>5. Experiment with the Learning Rates \(\gamma_t\) Using a Small Sample of the Training Set</strong></h5> <p>The mathematics behind Stochastic Gradient Descent (SGD) are surprisingly independent of the training set size. Specifically, the asymptotic convergence rates of SGD are not influenced by the sample size. This means that once you’ve ensured the gradients are correct, the most effective way to determine appropriate learning rates is to experiment with a <strong>small, but representative</strong> sample of the training set.</p> <h6 id="key-steps-1"><strong>Key Steps:</strong></h6> <ol> <li> <p><strong>Use a Small Sample</strong>: Select a small subset of the training data that still reflects the diversity of the full dataset. The small size allows you to test different learning rates quickly without incurring the computational cost of working with the entire dataset.</p> </li> <li> <p><strong>Traditional Optimization Methods</strong>: Since the sample is small, you can apply traditional optimization techniques (e.g., gradient descent or other optimization algorithms) to find a reference point and set the training cost target. This provides a useful benchmark for SGD.</p> </li> <li> <p><strong>Refining Learning Rates</strong>: Experiment with various learning rates on this small dataset to find a value that minimizes the training cost efficiently. Once you identify a good learning rate, it’s likely to work well on the full dataset.</p> </li> <li> <p><strong>Scale to Full Dataset</strong>: Once the learning rates are set based on the small sample, use the same rates on the full training set. Keep in mind that the performance on the validation set is expected to plateau after a number of epochs. The number of epochs required to reach this plateau should be roughly the same as what was needed on the small dataset.</p> </li> </ol> <h6 id="analogy-4"><strong>Analogy:</strong></h6> <p>Think of this like testing the settings of a new recipe. Instead of preparing a full meal, you start with a small portion of ingredients (a small sample). Once you find the perfect amount of seasoning (learning rates), you can apply it to the full dish (the entire training set). While the small sample may not capture every nuance of the full dish, it gives you a good starting point without wasting resources.</p> <hr/> <p>This concludes the key points related to Stochastic Gradient Descent (SGD). After iterating through Gradient Descent (GD) and SGD multiple times, I hope the concepts are now firmly imprinted, even if briefly. In the upcoming blog posts, we will delve into loss functions and regression, so stay tuned!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.]]></summary></entry><entry><title type="html">Gradient Descent Convergence - Prerequisites and Detailed Derivation</title><link href="https://monishver11.github.io/blog/2024/gd-convergence/" rel="alternate" type="text/html" title="Gradient Descent Convergence - Prerequisites and Detailed Derivation"/><published>2024-12-29T01:44:00+00:00</published><updated>2024-12-29T01:44:00+00:00</updated><id>https://monishver11.github.io/blog/2024/gd-convergence</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/gd-convergence/"><![CDATA[<p>To understand the <strong>Convergence Theorem for Fixed Step Size</strong>, it is essential to grasp a few foundational concepts like <strong>Lipschitz continuity</strong> and <strong>convexity</strong>. This section introduces these concepts and establishes the necessary prerequisites.</p> <p><strong>Quick note:</strong> If you find yourself struggling with any part or step, don’t worry—just copy and paste it into ChatGPT or Perplexity for an explanation. In most cases, you’ll be able to grasp the concept and move forward. If you’re still stuck, feel free to ask for help. The key is not to let small obstacles slow you down—keep going and seek assistance when needed!</p> <hr/> <h4 id="lipschitz-continuity"><strong>Lipschitz Continuity?</strong></h4> <p>At its core, Lipschitz continuity imposes a <strong>limit on how fast a function can change</strong>. Mathematically, a function \(g : \mathbb{R}^d \to \mathbb{R}\) is said to be <strong>Lipschitz continuous</strong> if there exists a constant \(L &gt; 0\) such that:</p> \[\|g(x) - g(x')\| \leq L \|x - x'\|, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This means the function’s rate of change is bounded by \(L\). For differentiable functions, Lipschitz continuity is often applied to the gradient. If \(\nabla f(x)\) is Lipschitz continuous with constant \(L &gt; 0\), then:</p> \[\|\nabla f(x) - \nabla f(x')\| \leq L \|x - x'\|, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This ensures the gradient does not change too rapidly, which is crucial for the convergence of optimization algorithms like gradient descent.</p> <h6 id="intuition-behind-lipschitz-continuity"><strong>Intuition Behind Lipschitz Continuity</strong></h6> <ol> <li><strong>Bounding the Slope</strong>: Lipschitz continuity ensures that the slope of the function (or the steepness of the graph) is bounded by \(L\). You can think of it as saying, “No part of the function can change too steeply.”</li> <li><strong>Gradient Smoothness</strong>: For \(\nabla f(x)\), Lipschitz continuity means the gradient varies smoothly between nearby points. This avoids abrupt jumps or erratic behavior in the optimization landscape.</li> </ol> <h6 id="visual-way-to-think-about-it"><strong>Visual Way to Think About It</strong></h6> <p>Imagine walking along a path represented by the graph of \(f(x)\). Lipschitz continuity guarantees:</p> <ul> <li>No sudden steep hills or cliffs.</li> <li>A smooth path where the steepness (gradient) is capped.</li> </ul> <p>Alternatively, picture a <strong>rubber band stretched smoothly over some pegs</strong>. The tension in the rubber band ensures there are no sharp kinks, making the graph smooth and predictable.</p> <h6 id="examples-of-lipschitz-continuous-functions"><strong>Examples of Lipschitz Continuous Functions</strong></h6> <ol> <li><strong>Linear Function</strong>: \(f(x) = mx + b\) is Lipschitz continuous because the slope \(m\) is constant, and \(|f'(x)| = |m|\) is bounded.</li> <li><strong>Quadratic Function</strong>: \(f(x) = x^2\) is \(L\)-smooth with \(L = 2\). Its gradient \(f'(x) = 2x\) satisfies:</li> </ol> \[|f'(x) - f'(x')| = |2x - 2x'| = 2|x - x'|.\] <ol> <li><strong>Non-Lipschitz Example</strong>: \(f(x) = \sqrt{x}\) (for \(x &gt; 0\)) is <strong>not Lipschitz continuous</strong> at \(x = 0\) because the slope becomes infinitely steep as \(x \to 0\). (If you’re not getting this, just plot \(\sqrt{x}\) function in <a href="https://www.desmos.com/">Desmos</a> and you’ll get it.)</li> </ol> <h6 id="why-does-lipschitz-continuity-matter"><strong>Why Does Lipschitz Continuity Matter?</strong></h6> <ol> <li><strong>Predictability</strong>: Lipschitz continuity ensures that a function behaves predictably, without sudden spikes or erratic changes.</li> <li><strong>Gradient Descent</strong>: If \(\nabla f(x)\) is Lipschitz continuous, we can choose a step size \(\eta \leq \frac{1}{L}\) to ensure gradient descent converges smoothly without overshooting the minimum.</li> </ol> <p>But Why? We’ll see that in the Convergence Theorem down below. For now, lets equip ourselves with the next important concept needed.</p> <hr/> <h4 id="2-convex-functions-and-convexity-condition"><strong>2. Convex Functions and Convexity Condition</strong></h4> <p>A function \(f : \mathbb{R}^d \to \mathbb{R}\) is <strong>convex</strong> if for any \(x, x' \in \mathbb{R}^d\) and \(\alpha \in [0, 1]\):</p> \[f(\alpha x + (1 - \alpha)x') \leq \alpha f(x) + (1 - \alpha)f(x').\] <p>Intuitively, the line segment between any two points on the graph of \(f\) lies above the graph itself.</p> <h6 id="convexity-condition-using-gradients"><strong>Convexity Condition Using Gradients</strong></h6> <p>If \(f\) is differentiable, convexity is equivalent to the following condition:</p> \[f(x') \geq f(x) + \langle \nabla f(x), x' - x \rangle, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This means that the function lies above its tangent plane at any point.</p> <hr/> <h4 id="3-l-smoothness"><strong>3. \(L\)-Smoothness</strong></h4> <p>A function \(f\) is said to be \(L\)-smooth if its gradient is Lipschitz continuous. This implies the following inequality:</p> \[f(x') \leq f(x) + \langle \nabla f(x), x' - x \rangle + \frac{L}{2} \|x' - x\|^2.\] <p>This property bounds the change in the function value using the gradient and the distance between \(x\) and \(x'\).</p> <hr/> <h4 id="4-optimality-conditions-for-convex-functions"><strong>4. Optimality Conditions for Convex Functions</strong></h4> <p>For convex functions, the following is true:</p> <ul> <li>If \(x^*\) is a minimizer of \(f\), then:</li> </ul> \[\nabla f(x^*) = 0.\] <ul> <li>For any \(x\), the difference between \(f(x)\) and \(f(x^*)\) can be bounded using the gradient:</li> </ul> \[f(x) - f(x^*) \leq \langle \nabla f(x), x - x^* \rangle.\] <p>These conditions help in deriving the convergence results for gradient descent.</p> <hr/> <p><strong>To quickly summarize, before we proceed further:</strong></p> <ol> <li><strong>Lipschitz continuity</strong> ensures the gradient does not change too rapidly.</li> <li><strong>Convexity</strong> guarantees that the function behaves well, with no local minima other than the global minimum.</li> <li><strong>\(L\)-smoothness</strong> combines convexity and Lipschitz continuity to bound the function’s behavior using gradients.</li> </ol> <hr/> <p>With these concepts in place, we can now proceed to derive the <strong>Convergence Theorem for Fixed Step Size</strong>.</p> <h4 id="convergence-of-gradient-descent-with-fixed-step-size"><strong>Convergence of Gradient Descent with Fixed Step Size</strong></h4> <h5 id="theorem"><strong>Theorem:</strong></h5> <p>Suppose the function \(f : \mathbb{R}^n \to \mathbb{R}\) is convex and differentiable, and its gradient is Lipschitz continuous with constant \(L &gt; 0\), i.e.,</p> \[\|\nabla f(x) - \nabla f(y)\|_2 \leq L \|x - y\|_2 \quad \text{for any} \quad x, y.\] <p>Then, if we run gradient descent for \(k\) iterations with a fixed step size \(t \leq \frac{1}{L}\), the solution \(x^{(k)}\) satisfies:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t k},\] <p>where \(f(x^*)\) is the optimal value.</p> <h5 id="proof"><strong>Proof:</strong></h5> <h6 id="step-1-lipschitz-continuity-and-smoothness"><strong>Step 1: Lipschitz Continuity and Smoothness</strong></h6> <p>From the Lipschitz continuity of \(\nabla f\), the function \(f\) satisfies the following inequality for any \(x, y \in \mathbb{R}^n\):</p> \[f(y) \leq f(x) + \nabla f(x)^T (y - x) + \frac{L}{2} \|y - x\|_2^2.\] <p>This inequality allows us to bound how the function \(f\) changes as we move from \(x\) to \(y\), given the Lipschitz constant \(L\).</p> <h6 id="step-2-gradient-descent-update"><strong>Step 2: Gradient Descent Update</strong></h6> <p>The gradient descent update step is defined as:</p> \[x^{+} = x - t \nabla f(x),\] <p>where \(t\) is the step size. Letting \(y = x^+\) in the smoothness inequality gives:</p> \[f(x^+) \leq f(x) + \nabla f(x)^T (x^+ - x) + \frac{L}{2} \|x^+ - x\|_2^2.\] <h6 id="step-3-substituting-the-update-rule"><strong>Step 3: Substituting the Update Rule</strong></h6> <p>Substituting \(x^+ - x = -t \nabla f(x)\), we get:</p> \[f(x^+) \leq f(x) + \nabla f(x)^T (-t \nabla f(x)) + \frac{L}{2} \| -t \nabla f(x)\|_2^2.\] <p>Simplifying each term:</p> <ul> <li>The second term simplifies to:</li> </ul> \[\nabla f(x)^T (-t \nabla f(x)) = -t \|\nabla f(x)\|_2^2.\] <ul> <li>The third term simplifies to:</li> </ul> \[\frac{L}{2} \| -t \nabla f(x)\|_2^2 = \frac{L t^2}{2} \|\nabla f(x)\|_2^2.\] <p>Combining these, we have:</p> \[f(x^+) \leq f(x) - t \|\nabla f(x)\|_2^2 + \frac{L t^2}{2} \|\nabla f(x)\|_2^2.\] <p>Factoring out \(\|\nabla f(x)\|_2^2\):</p> \[f(x^+) \leq f(x) - \left( t - \frac{L t^2}{2} \right) \|\nabla f(x)\|_2^2.\] <h6 id="step-4-ensuring-decrease-in-fx"><strong>Step 4: Ensuring Decrease in \(f(x)\)</strong></h6> <p>To ensure that the function value decreases at each iteration, the coefficient \(t - \frac{L t^2}{2}\) must be non-negative. This holds when \(t \leq \frac{1}{L}\). Substituting \(t = \frac{1}{L}\), we verify:</p> \[t - \frac{L t^2}{2} = \frac{1}{L} - \frac{L}{2} \cdot \frac{1}{L^2} = \frac{1}{L} - \frac{1}{2L} = \frac{1}{2L}.\] <p>Thus, with \(t \leq \frac{1}{L}\), the function value strictly decreases:</p> \[f(x^+) \leq f(x) - \frac{t}{2} \|\nabla f(x)\|_2^2.\] <h6 id="step-5-bounding-fx---fx"><strong>Step 5: Bounding \(f(x^+) - f(x^*)\)</strong></h6> <p>From the convexity of \(f\), we know:</p> \[f(x^*) \geq f(x) + \nabla f(x)^T (x^* - x).\] <p>Rearranging: \(f(x) \leq f(x^*) + \nabla f(x)^T (x - x^*).\)</p> <p>Substituting this into the inequality for \(f(x^+)\):</p> \[f(x^+) \leq f(x^*) + \nabla f(x)^T (x - x^*) - \frac{t}{2} \|\nabla f(x)\|_2^2.\] <p>Rearranging terms:</p> \[f(x^+) - f(x^*) \leq \frac{1}{2t} \left( \|x - x^*\|_2^2 - \|x^+ - x^*\|_2^2 \right).\] <p>This shows how the objective value at \(x^+\) is related to the distance between \(x\) and the optimal solution \(x^*\).</p> <h6 id="step-6-summing-over-k-iterations"><strong>Step 6: Summing Over \(k\) Iterations</strong></h6> <p>Let \(x^{(i)}\) denote the iterate after \(i\) steps. Applying the inequality iteratively, we have:</p> \[f(x^{(i)}) - f(x^*) \leq \frac{1}{2t} \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right).\] <p>Summing over \(i = 1, 2, \dots, k\):</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \sum_{i=1}^k \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right).\] <h6 id="step-7-telescoping-sum"><strong>Step 7: Telescoping Sum</strong></h6> <p>The terms on the right-hand side form a telescoping sum:</p> \[\sum_{i=1}^k \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right) = \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2.\] <p>Thus, we have:</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Since \(f(x^{(i)})\) is decreasing with each iteration, the largest term dominates the average:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right).\] <p>But, why is the above inequality right? Let’s find out:</p> <p>The inequality</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right)\] <p>is derived based on the property that \(f(x^{(i)})\) is <strong>monotonically decreasing</strong> during gradient descent. Let’s break it down step by step.</p> <ul> <li><strong>Key Property: Monotonic Decrease</strong>: In gradient descent, the function value decreases with each iteration due to the fixed step size \(t \leq \frac{1}{L}\). This means:</li> </ul> \[f(x^{(1)}) \geq f(x^{(2)}) \geq \cdots \geq f(x^{(k)}).\] <p>Thus, the latest value \(f(x^{(k)})\) is the smallest among all iterations.</p> <ul> <li><strong>Averaging the Function Values</strong>: The sum of the differences \(f(x^{(i)}) - f(x^*)\) over all \(k\) iterations can be written as:</li> </ul> \[\frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right),\] <p>which represents the average difference between the function values at each iteration and the optimal value \(f(x^*)\).</p> <ul> <li><strong>Bounding the Smallest Term by the Average</strong>: Since \(f(x^{(k)})\) is the smallest value (due to monotonic decrease), it cannot exceed the average value. In mathematical terms:</li> </ul> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right).\] <ul> <li> <p><strong>Intuition Behind the Inequality</strong>: This inequality reflects a simple fact: the smallest value in a decreasing sequence of numbers is less than or equal to their average. For example, if we have values \(10, 8, 7, 6\), the smallest value (6) will always be less than or equal to the average of these values.</p> </li> <li> <p><strong>Significance in Gradient Descent</strong>: This inequality is important because it allows us to bound the final iterate \(f(x^{(k)})\) using the sum of all previous iterations.</p> </li> </ul> <h6 id="step-8-final-substitution-to-derive-the-convergence-result"><strong>Step 8: Final Substitution to Derive the Convergence Result</strong></h6> <p>From the telescoping sum, we have:</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Using the inequality:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right),\] <p>we substitute the bound on the sum into this expression:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \cdot \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Since \(\|x^{(k)} - x^*\|_2^2 \geq 0\), we drop this term to get the worst-case bound:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2tk}.\] <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>We have derived the convergence guarantee for gradient descent with a fixed step size \(t \leq \frac{1}{L}\). The final result:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t k},\] <p>shows that the function value \(f(x^{(k)})\) decreases towards the optimal value \(f(x^*)\) at a rate proportional to \(O(1/k)\). This rate depends on the step size \(t\) and the initial distance \(\|x^{(0)} - x^*\|_2^2\).</p> <p>The result highlights that gradient descent converges reliably under the conditions of convexity, differentiability, and Lipschitz continuity of the gradient. As \(k \to \infty\), the function value approaches the optimal value, demonstrating the effectiveness of gradient descent for optimization problems with these properties.</p> <hr/> <p>Next,</p> <ul> <li>Convergence of gradient descent with adaptive step size</li> <li>Strongly convex - “linear convergence” rate</li> </ul> <h4 id="convergence-of-gradient-descent-with-adaptive-step-size"><strong>Convergence of gradient descent with adaptive step size</strong></h4> <p>In the above section, we derived the convergence rate for gradient descent with a <strong>fixed step size</strong>. In this part, we extend this analysis to the case where the step size is chosen adaptively using a <strong>backtracking line search</strong>. This method ensures that the step size decreases as necessary to guarantee sufficient decrease in the objective function at each iteration.</p> <h6 id="step-1-setup-and-assumptions"><strong>Step 1: Setup and Assumptions</strong></h6> <p>Consider a differentiable convex function \(f: \mathbb{R}^n \to \mathbb{R}\) with a <strong>Lipschitz continuous gradient</strong>. That is, for any two points \(x, y \in \mathbb{R}^n\),</p> \[\|\nabla f(x) - \nabla f(y)\|_2 \leq L \|x - y\|_2,\] <p>where \(L\) is the <strong>Lipschitz constant</strong> of the gradient.</p> <p>Let \(x^*\) be the minimizer of \(f\), and let \(x^{(i)}\) represent the iterates of gradient descent. The update rule for gradient descent with backtracking line search is:</p> \[x^{(i+1)} = x^{(i)} - t_i \nabla f(x^{(i)}),\] <p>where \(t_i\) is the step size at iteration \(i\), chosen adaptively using the backtracking procedure.</p> <h6 id="step-2-descent-lemma"><strong>Step 2: Descent Lemma</strong></h6> <p>In the case of gradient descent with a <strong>fixed step size</strong> \(t\), we know from the <strong>descent lemma</strong> (for smooth convex functions) that:</p> \[f(x^{(i+1)}) \leq f(x^{(i)}) - t \|\nabla f(x^{(i)})\|_2^2 + \frac{L}{2} t^2 \|\nabla f(x^{(i)})\|_2^2.\] <p>This inequality states that at each iteration, the function value decreases by a term proportional to the gradient’s squared norm, and this decrease depends on the step size \(t\).</p> <h6 id="step-3-backtracking-line-search"><strong>Step 3: Backtracking Line Search</strong></h6> <p>With <strong>backtracking line search</strong>, the step size \(t_i\) is chosen at each iteration to ensure sufficient decrease in the function value. Specifically, the step size is selected such that:</p> \[f(x^{(i+1)}) \leq f(x^{(i)}) + \alpha t_i \nabla f(x^{(i)})^T \nabla f(x^{(i)}),\] <p>where \(0 &lt; \alpha &lt; 1\) is a constant. The backtracking line search ensures that \(t_i\) satisfies the condition:</p> \[t_i \leq \frac{1}{L}.\] <p>Thus, the step size at each iteration is bounded by \(\frac{1}{L}\), which prevents the gradient from changing too rapidly and ensures that the update does not overshoot the optimal point.</p> <p><strong>Why “Adaptive”?</strong></p> <p>The step size is called <strong>adaptive</strong> because it changes at each iteration depending on the function’s behavior. If the function is steep or the gradient is large, the backtracking line search may choose a smaller step size to avoid overshooting. If the function is shallow or the gradient is small, it might allow a larger step size. This adaptive process uses a parameter \(\beta\) to control how the step size is reduced when the decrease condition is not met.</p> <h6 id="step-4-backtracking-process-and-beta"><strong>Step 4: Backtracking Process and \(\beta\)</strong></h6> <p>The process of backtracking works as follows:</p> <ul> <li> <p><strong>Initial Step Size</strong>: Start with an initial guess for the step size, typically \(t_0 = 1\).</p> </li> <li> <p><strong>Condition Check</strong>: Check whether the condition</p> </li> </ul> \[f(x^{(i+1)}) \leq f(x^{(i)}) + \alpha t_i \nabla f(x^{(i)})^T \nabla f(x^{(i)})\] <p>holds. If it does, accept \(t_i\); if not, reduce the step size.</p> <ul> <li><strong>Reduce Step Size</strong>: If the condition is not satisfied, reduce the step size \(t_i\) by a factor \(\beta\):</li> </ul> \[t_{i+1} = \beta t_i,\] <p>where \(\beta\) is a constant between 0 and 1 (usually around 0.5 or 0.8). This step size reduction continues until the condition is met.</p> <ul> <li><strong>Accept the Step Size</strong>: Once the condition is satisfied, the current \(t_i\) is accepted for the update.</li> </ul> <p>The use of \(\beta\) helps to ensure that the step size does not become too large, allowing the algorithm to converge smoothly without overshooting.</p> <h6 id="step-5-bounding-the-convergence"><strong>Step 5: Bounding the Convergence</strong></h6> <p>Now, let’s derive the convergence bound for gradient descent with backtracking line search. From the descent lemma, the change in the function value at each iteration can be bounded as:</p> \[f(x^{(i+1)}) - f(x^{(i)}) \leq - t_i \|\nabla f(x^{(i)})\|_2^2 \left( 1 - \frac{L}{2} t_i \right).\] <p>Because the backtracking line search ensures that \(t_i \leq t_{\text{min}} = \min\left( 1, \frac{\beta}{L} \right)\), we can bound the function value decrease as:</p> \[f(x^{(i+1)}) - f(x^{(i)}) \leq - t_{\text{min}} \|\nabla f(x^{(i)})\|_2^2 \left( 1 - \frac{L}{2} t_{\text{min}} \right).\] <p>This shows that the function value decreases at each iteration, with the step size \(t_{\text{min}}\) controlling the rate of decrease.</p> <p>Now, if you observe carefully, the equation above closely resembles the one we encountered in the fixed step size proof. The only minor difference is that \(t\) has been replaced with \(t_{\text{min}}\). Therefore, we can follow the same steps as in the fixed step size case and eventually arrive at the following result:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t_{\text{min}} k}.\] <p>This shows that by adaptively choosing the step size, we can achieve a convergence rate similar to that of the fixed step size approach, but without needing to manually set a fixed value for ( t ).</p> <p><strong>Quick Note:</strong> I’m still not completely satisfied with the proof for Adaptive Step Size. I’ll be working on refining the explanation further and will update you with any improvements.</p> <h5 id="and-finally"><strong>And finally…</strong></h5> <p>We’ve reached the end of this blog post! A huge kudos to you for making it all the way through and sticking with me. The reason we went through all of this is that understanding such proofs will lay the foundation for exploring the intricate details that drive machine learning and produce its remarkable results. To truly dive into ML research, we need to immerse ourselves in these depths and make it happen.</p> <p>So, take a well-deserved break, and in the next post, we’ll delve into the tips and tricks of SGD that are widely practiced in the industry. Until then, take care and see you soon!</p> <h5 id="references"><strong>References:</strong></h5> <ul> <li><a href="https://nyu-cs2565.github.io/mlcourse-public/2024-fall/lectures/lec02/gradient_descent_converge.pdf"> Gradient Descent: Convergence Analysis - Ryan Tibshirani</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.]]></summary></entry></feed>