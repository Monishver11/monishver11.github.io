<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-26T00:55:52+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A clear, theory-focused approach to machine learning, designed to take you beyond the basics. </subtitle><entry><title type="html">Introduction to Ensemble Methods</title><link href="https://monishver11.github.io/blog/2025/intro-to-ensemble-methods/" rel="alternate" type="text/html" title="Introduction to Ensemble Methods"/><published>2025-04-25T00:24:00+00:00</published><updated>2025-04-25T00:24:00+00:00</updated><id>https://monishver11.github.io/blog/2025/intro-to-ensemble-methods</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/intro-to-ensemble-methods/"><![CDATA[<p>Ensemble methods are a powerful set of techniques in machine learning that aim to improve prediction performance by combining the outputs of multiple models. Before diving into ensemble strategies, let’s revisit some foundational concepts that lead us to the rationale behind ensemble methods.</p> <hr/> <h5 id="review-decision-trees"><strong>Review: Decision Trees</strong></h5> <ul> <li><strong>Non-linear</strong>, <strong>non-metric</strong>, and <strong>non-parametric</strong> models</li> <li>Capable of <strong>regression</strong> or <strong>classification</strong></li> <li><strong>Interpretable</strong>, especially when shallow</li> <li>Constructed using a <strong>greedy algorithm</strong> that seeks to maximize the <strong>purity of nodes</strong></li> <li>Prone to <strong>overfitting</strong>, unless properly regularized</li> </ul> <p>These models serve as the building blocks for many ensemble techniques like Random Forests.</p> <h5 id="recap-statistics-and-point-estimators"><strong>Recap: Statistics and Point Estimators</strong></h5> <p>We begin with data:</p> \[D = (x_1, x_2, \dots, x_n)\] <p>sampled i.i.d. from a parametric distribution</p> \[p(\cdot \mid \theta)\] <p>A <strong>statistic</strong> is any function of the data, e.g.,</p> <ul> <li>Sample mean</li> <li>Sample variance</li> <li>Histogram</li> <li>Empirical distribution</li> </ul> <p>A <strong>point estimator</strong> is a statistic used to estimate a parameter:</p> \[\hat{\theta} = \hat{\theta}(D) \approx \theta\] <p><strong>Example:</strong> Suppose we’re estimating the average height \(\theta\) of a population. We collect a sample of \(n\) people and compute the sample mean:</p> \[\hat{\theta}(D) = \frac{1}{n} \sum_{i=1}^n x_i\] <p>This sample mean is a <strong>point estimator</strong> for the true average height \(\theta\) of the entire population.</p> <h5 id="recap-bias-and-variance-of-an-estimator"><strong>Recap: Bias and Variance of an Estimator</strong></h5> <p>Since statistics are derived from random samples, they themselves are <strong>random variables</strong>. The distribution of a statistic across different random samples is called the <strong>sampling distribution</strong>.</p> <p>Understanding the <strong>bias</strong> and <strong>variance</strong> of an estimator helps us evaluate how good the estimator is.</p> <ul> <li> <p><strong>Bias</strong> measures the systematic error — how far, on average, the estimator is from the true parameter:</p> \[\text{Bias}(\hat{\theta}) \overset{\text{def}}{=} \mathbb{E}[\hat{\theta}] - \theta\] </li> <li> <p><strong>Variance</strong> measures the variability of the estimator due to sampling randomness:</p> \[\text{Var}(\hat{\theta}) \overset{\text{def}}{=} \mathbb{E}[\hat{\theta}^2] - \left(\mathbb{E}[\hat{\theta}]\right)^2\] </li> </ul> <p>Intuitively:</p> <ul> <li><strong>Low bias</strong> means the estimator is <em>accurate</em> on average.</li> <li><strong>Low variance</strong> means the estimator is <em>stable</em> across different samples.</li> </ul> <p>Even an <strong>unbiased estimator</strong> can be <strong>unreliable</strong> if its variance is high. That is, it may give wildly different results on different samples, even if the average over many samples is correct.</p> <p><strong>Example:</strong> Suppose we are trying to estimate the <strong>true mean</strong> \(\theta\) of a population — for example, the average height of all adults in a city. We collect a sample of size \(n\):</p> \[D = (x_1, x_2, \dots, x_n)\] <p>where each \(x_i\) is drawn i.i.d. from a distribution with <strong>mean</strong> \(\theta\) and some unknown variance \(\sigma^2\).</p> <p>Consider two different estimators of the population mean:</p> <ol> <li>\(\hat{\theta}_1(D) = x_1\) — just the first point in the sample</li> <li>\(\hat{\theta}_2(D) = \frac{1}{n} \sum_{i=1}^n x_i\) — the sample mean</li> </ol> <p>We say an estimator \(\hat{\theta}\) is <strong>unbiased</strong> if, on average, it correctly estimates the true value of the parameter \(\theta\):</p> \[\mathbb{E}[\hat{\theta}] = \theta\] <p>In this case:</p> <ul> <li> <p>For \(\hat{\theta}_1\), since \(x_1\) is sampled from the distribution with mean \(\theta\), we have:</p> \[\mathbb{E}[x_1] = \theta \Rightarrow \mathbb{E}[\hat{\theta}_1] = \theta\] </li> <li> <p>For \(\hat{\theta}_2\), because of the linearity of expectation:</p> \[\mathbb{E}[\hat{\theta}_2] = \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n x_i\right] = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[x_i] = \frac{1}{n} \cdot n \cdot \theta = \theta\] </li> </ul> <p>Thus, <strong>both estimators are unbiased</strong> — their expected value equals the true mean \(\theta\).</p> <p>However, they differ in <strong>variance</strong>:</p> <ul> <li>\(\hat{\theta}_1\) uses only one data point, so its value can fluctuate greatly between different samples — it has <strong>high variance</strong>.</li> <li>\(\hat{\theta}_2\) averages over all \(n\) data points, which helps cancel out individual fluctuations — it has <strong>lower variance</strong>.</li> </ul> <p><strong>Key takeaway:</strong> Although both estimators are unbiased, the sample mean \(\hat{\theta}_2\) is <strong>more reliable</strong> due to its lower variance. This highlights the importance of considering both <strong>bias</strong> and <strong>variance</strong> when evaluating an estimator — a foundational idea for ensemble methods.</p> <h5 id="variance-of-a-mean"><strong>Variance of a Mean</strong></h5> <p>Suppose we have an unbiased estimator \(\hat{\theta}\) with variance \(\sigma^2\):</p> \[\mathbb{E}[\hat{\theta}] = \theta, \quad \text{Var}(\hat{\theta}) = \sigma^2\] <p>Now imagine we have \(n\) independent copies of this estimator — say, from different data samples or different random seeds — denoted by:</p> \[\hat{\theta}_1, \hat{\theta}_2, \dots, \hat{\theta}_n\] <p>We form a new estimator by averaging them:</p> \[\hat{\theta}_{\text{avg}} = \frac{1}{n} \sum_{i=1}^n \hat{\theta}_i\] <p>This average is still an unbiased estimator of \(\theta\), because the expectation of a sum is the sum of expectations:</p> \[\mathbb{E}[\hat{\theta}_{\text{avg}}] = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[\hat{\theta}_i] = \frac{1}{n} \cdot n \cdot \theta = \theta\] <p>But here’s the key insight: its <strong>variance is smaller</strong>. Since the \(\hat{\theta}_i\) are independent and each has variance \(\sigma^2\), we get:</p> \[\text{Var}(\hat{\theta}_{\text{avg}}) = \text{Var} \left( \frac{1}{n} \sum_{i=1}^n \hat{\theta}_i \right) = \frac{1}{n^2} \sum_{i=1}^n \text{Var}(\hat{\theta}_i) = \frac{n \cdot \sigma^2}{n^2} = \frac{\sigma^2}{n}\] <p>So, by averaging multiple estimators, we <strong>preserve unbiasedness</strong> while <strong>reducing variance</strong>.</p> <p>This simple statistical property is the backbone of many ensemble methods in machine learning — especially those like bagging and random forests, where we average multiple models to get a more stable and reliable prediction.</p> <hr/> <h5 id="averaging-independent-prediction-functions"><strong>Averaging Independent Prediction Functions</strong></h5> <p>Let’s now connect the earlier statistical insight to machine learning.</p> <p>Suppose we train \(B\) models <strong>independently</strong> on \(B\) different training sets, each drawn from the same underlying data distribution. This gives us a set of prediction functions:</p> \[\hat{f}_1(x),\ \hat{f}_2(x),\ \dots,\ \hat{f}_B(x)\] <p>We define their <strong>average prediction function</strong> as:</p> \[\hat{f}_{\text{avg}}(x) \overset{\text{def}}{=} \frac{1}{B} \sum_{b=1}^B \hat{f}_b(x)\] <p>At any specific input point \(x_0\):</p> <ul> <li>Each model’s prediction \(\hat{f}_b(x_0)\) is a random variable (since it depends on the randomly drawn training set), but they all share the same <strong>expected prediction</strong>.</li> <li> <p>The average prediction:</p> \[\hat{f}_{\text{avg}}(x_0) = \frac{1}{B} \sum_{b=1}^B \hat{f}_b(x_0)\] <p>has the <strong>same expected value</strong> as each individual \(\hat{f}_b(x_0)\), but with a <strong>reduced variance</strong>:</p> \[\text{Var}(\hat{f}_{\text{avg}}(x_0)) = \frac{1}{B} \cdot \text{Var}(\hat{f}_1(x_0))\] </li> </ul> <p>This means that by averaging multiple independent models, we can achieve a <strong>more stable and less noisy prediction</strong>, without increasing bias.</p> <p>However, here’s the challenge:</p> <blockquote> <p>In practice, we don’t have access to \(B\) truly independent training sets — we usually only have <strong>one dataset</strong>.</p> </blockquote> <h5 id="the-bootstrap-sample"><strong>The Bootstrap Sample</strong></h5> <p>So, how do we simulate multiple datasets when we only have <strong>one</strong>?</p> <p>The answer is: use <strong>bootstrap sampling</strong> — a clever statistical trick to mimic sampling variability.</p> <p>A <strong>bootstrap sample</strong> is formed by sampling <strong>with replacement</strong> from the original dataset:</p> \[D_n = (x_1, x_2, \dots, x_n)\] <p>We draw \(n\) points <em>with replacement</em> from \(D_n\), forming a new dataset (also of size \(n\)). Because sampling is done with replacement, some data points will appear multiple times, while others might not appear at all.</p> <p>What’s the chance a particular data point \(x_i\) is <strong>not</strong> selected in one draw?<br/> That’s \(1 - \frac{1}{n}\).</p> <p>The chance it’s not selected <strong>in any of the \(n\) draws</strong> is:</p> \[\left(1 - \frac{1}{n} \right)^n \approx \frac{1}{e} \approx 0.368\] <p>So, on average, <strong>about 36.8%</strong> of the data points are <strong>not</strong> included in a given bootstrap sample. This also means:</p> <blockquote> <p>Around <strong>63.2%</strong> of the original data points are expected to appear <strong>at least once</strong> in each bootstrap sample.</p> </blockquote> <h5 id="the-bootstrap-method"><strong>The Bootstrap Method</strong></h5> <p>Bootstrap gives us a way to <strong>simulate variability</strong> and generate multiple pseudo-datasets — without requiring any new data.</p> <p>Here’s how it works:</p> <ol> <li> <p>From the original dataset \(D_n\), generate \(B\) bootstrap samples:</p> \[D_n^1, D_n^2, \dots, D_n^B\] </li> <li> <p>Compute some function of interest — such as a statistic or a trained model — on each bootstrap sample:</p> \[\phi(D_n^1), \phi(D_n^2), \dots, \phi(D_n^B)\] </li> </ol> <p>These values behave almost like they were computed from <strong>\(B\) independent samples</strong> drawn from the original population distribution.</p> <p><strong>Why?</strong> Because each bootstrap sample is a randomized resampling of the original dataset — introducing enough variability to approximate the natural randomness we’d expect from drawing entirely new datasets from the true distribution.</p> <p>Although bootstrap samples are not truly independent, the statistical properties of estimators (like variance and confidence intervals) computed using bootstrapping tend to closely mirror those from actual independent samples.</p> <blockquote> <p>This makes bootstrap an incredibly useful tool for simulating sampling distributions, especially when acquiring more data is costly or impossible.</p> </blockquote> <h5 id="independent-samples-vs-bootstrap-samples"><strong>Independent Samples vs. Bootstrap Samples</strong></h5> <p>Let’s say we want to estimate a parameter \(\alpha\) using a point estimator:</p> \[\hat{\alpha} = \hat{\alpha}(D_{100})\] <p>Now, consider two scenarios:</p> <ul> <li><strong>Case 1:</strong> You collect <strong>1000 independent samples</strong>, each of size 100 (<strong>Left</strong>)</li> <li><strong>Case 2:</strong> You generate <strong>1000 bootstrap samples</strong> from a <strong>single dataset</strong> of size 100 (<strong>Right</strong>)</li> </ul> <p>If you compute \(\hat{\alpha}\) for each sample and plot the resulting histograms, you’ll notice something powerful:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-1-480.webp 480w,/assets/img/ensemble-1-800.webp 800w,/assets/img/ensemble-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>The distribution of estimates from bootstrap samples closely resembles that from truly independent samples.</p> </blockquote> <p>While not exact, the <strong>bootstrap approximation</strong> to the sampling distribution is often <strong>good enough</strong> for practical applications — especially when collecting new data is expensive or infeasible.</p> <hr/> <h5 id="ensemble-methods"><strong>Ensemble Methods</strong></h5> <p>This naturally leads us to the concept of <strong>ensemble learning</strong> — a powerful technique in modern machine learning.</p> <p><strong>Core idea:</strong> Combine multiple <strong>weak models</strong> into a <strong>strong, robust predictor</strong>.</p> <p>Why ensemble methods work:</p> <ul> <li>Averaging predictions from i.i.d. models <strong>reduces variance</strong> without increasing bias</li> <li>Bootstrap lets us <strong>simulate multiple training sets</strong>, even from just one dataset</li> </ul> <p>There are two primary flavors of ensemble methods:</p> <ul> <li> <p><strong>Parallel Ensembles</strong> (e.g., <strong>Bagging</strong>):<br/> Models are trained <strong>independently</strong> on different subsets of data</p> </li> <li> <p><strong>Sequential Ensembles</strong> (e.g., <strong>Boosting</strong>):<br/> Models are trained <strong>sequentially</strong>, with each new model <strong>focusing on the errors</strong> made by previous ones</p> </li> </ul> <p>By leveraging these strategies, ensemble methods often achieve <strong>greater accuracy</strong>, <strong>stability</strong>, and <strong>generalization</strong> than any single model could alone.</p> <hr/> <p>Stay tuned as we dive deeper into specific ensemble techniques — starting with <strong>Bagging</strong> and the incredibly popular <strong>Random Forests</strong> — and see how these ideas come to life in practice!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A beginner's guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.]]></summary></entry><entry><title type="html">Decision Trees for Classification</title><link href="https://monishver11.github.io/blog/2025/decision-trees-classification/" rel="alternate" type="text/html" title="Decision Trees for Classification"/><published>2025-04-20T16:31:00+00:00</published><updated>2025-04-20T16:31:00+00:00</updated><id>https://monishver11.github.io/blog/2025/decision-trees-classification</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/decision-trees-classification/"><![CDATA[<p>In the last post, we explored how decision trees can grow deep and complex—leading to overfitting—and how strategies like limiting tree depth or pruning can help us build simpler, more generalizable models.</p> <p>In this post, we turn our focus to <strong>classification trees</strong>, where our goal isn’t just fitting the data—it’s finding splits that <strong>create pure, confident predictions</strong> in each region.</p> <p>We’ll look at what makes a split “good,” explore different <strong>impurity measures</strong>, and understand how decision trees use these ideas to grow meaningfully.</p> <hr/> <h5 id="what-makes-a-good-split-for-classification"><strong>What Makes a Good Split for Classification?</strong></h5> <p>In classification tasks, a decision tree predicts the <strong>majority class</strong> in each region. So, a good split is one that increases the <strong>purity</strong> of the resulting nodes—that is, each node contains mostly (or only) examples from a single class.</p> <p>Let’s walk through a concrete example. Suppose we’re trying to classify data points into <strong>positive (+)</strong> and <strong>negative (−)</strong> classes. We consider two possible ways to split the data:</p> <p><strong>Split Option 1</strong></p> <ul> <li> \[R_1: 8+,\ 2−\] </li> <li> \[R_2: 2+,\ 8−\] </li> </ul> <p>Here, each region contains a clear majority: 8 out of 10 are of the same class. Not bad!</p> <p><strong>Split Option 2</strong></p> <ul> <li> \[R_1: 6+,\ 4−\] </li> <li> \[R_2: 4+,\ 6−\] </li> </ul> <p>This is more mixed—each region has a 60-40 class split, making it <strong>less pure</strong> than Split 1.</p> <p>Now, let’s consider a <strong>better version of Split 2</strong>:</p> <p><strong>Split Option 3 (Refined)</strong></p> <ul> <li> \[R_1: 6+,\ 4−\] </li> <li> \[R_2: 0+,\ 10−\] </li> </ul> <p>Now things look different! Region \(R_2\) contains <strong>only negatives</strong>, making it a <strong>perfectly pure node</strong>. Even though \(R_1\) isn’t completely pure, this split overall is more desirable than the others.</p> <p>This example shows that a good split isn’t just about balance—it’s about <strong>maximizing purity</strong>, ideally pushing each node toward containing only one class.</p> <h5 id="misclassification-error-in-a-node"><strong>Misclassification Error in a Node</strong></h5> <p>Once we split the data, how do we measure how well a node performs in <strong>classification</strong>?</p> <p>Suppose we’re working with <strong>multiclass classification</strong>, where the possible labels are:</p> \[Y = \{1, 2, \ldots, K\}\] <p>Let’s focus on a particular <strong>node</strong> \(m\) (i.e., a region \(R_m\) of the input space) that contains \(N_m\) data points.</p> <p>For each class \(k \in \{1, \ldots, K\}\), we compute the <strong>proportion</strong> of points in node \(m\) that belong to class \(k\):</p> \[\hat{p}_{mk} = \frac{1}{N_m} \sum_{i: x_i \in R_m} \mathbb{1}[y_i = k]\] <p>This gives us the <strong>empirical class distribution</strong> within the node.</p> <p>To make a prediction, we choose the <strong>majority class</strong> in that node:</p> \[k(m) = \arg\max_k \hat{p}_{mk}\] <p>This means the class with the highest proportion becomes the predicted label for <strong>all points</strong> in that region.</p> <hr/> <h5 id="node-impurity-measures"><strong>Node Impurity Measures</strong></h5> <p>In classification, our goal is to make each region (or <strong>node</strong>) as <strong>pure</strong> as possible—i.e., containing data points from mostly <strong>one class</strong>.</p> <p>To quantify how <strong>impure</strong> a node is, we use <strong>impurity measures</strong>. Let’s explore the three most common ones:</p> <p><strong>1. Misclassification Error</strong></p> <p>This measures the fraction of points <strong>not belonging to the majority class</strong> in a node:</p> \[\text{Misclassification Error} = 1 - \hat{p}_{mk(m)}\] <p>Here, \(\hat{p}_{mk(m)}\) is the proportion of the majority class in node \(m\).</p> <ul> <li>If a node has 90% of points from class A and 10% from class B, misclassification error = \(1 - 0.9 = 0.1\)</li> <li>It’s simple, but not very sensitive to class distribution beyond the majority vote.</li> </ul> <p><strong>2. Gini Index</strong></p> <p>The Gini index gives us a better sense of how <strong>mixed</strong> a node is:</p> \[\text{Gini}(m) = \sum_{k=1}^K \hat{p}_{mk}(1 - \hat{p}_{mk})\] <p>This value is <strong>0</strong> when the node is perfectly pure (i.e., all points belong to one class) and <strong>maximum</strong> when all classes are equally likely.</p> <ul> <li>For example, if a node has: <ul> <li>50% class A, 50% class B → Gini = \(0.5\)</li> <li>90% class A, 10% class B → Gini = \(0.18\)</li> <li>100% class A → Gini = \(0\) (pure)</li> </ul> </li> </ul> <p>The Gini index is widely used in practice because it’s differentiable and more sensitive to class proportions than misclassification error.</p> <p><strong>3. Entropy (a.k.a. Information Gain)</strong></p> <p>Entropy comes from information theory and captures the amount of <strong>uncertainty</strong> or <strong>disorder</strong> in the class distribution:</p> \[\text{Entropy}(m) = - \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}\] <p>Like Gini, it’s <strong>0</strong> for a pure node and <strong>higher</strong> for more mixed distributions.</p> <ul> <li>If a node has: <ul> <li>50% class A, 50% class B → Entropy = \(0.693\)</li> <li>90% class A, 10% class B → Entropy ≈ \(0.325\)</li> <li>100% class A → Entropy = \(0\)</li> </ul> </li> </ul> <p>Entropy grows slower than Gini but still encourages purity. It’s used in algorithms like <strong>ID3</strong> and <strong>C4.5</strong>.</p> <p><strong>Summary: When Are Nodes “Pure”?</strong></p> <p>All three measures hit <strong>zero</strong> when the node contains data from only one class. But Gini and Entropy give a smoother, more nuanced view of how mixed the classes are—helpful for greedy splitting.</p> <hr/> <table> <thead> <tr> <th>Class Distribution</th> <th>Misclassification</th> <th>Gini</th> <th>Entropy</th> </tr> </thead> <tbody> <tr> <td>100% A</td> <td>0</td> <td>0</td> <td>0</td> </tr> <tr> <td>90% A / 10% B</td> <td>0.1</td> <td>0.18</td> <td>0.325</td> </tr> <tr> <td>50% A / 50% B</td> <td>0.5</td> <td>0.5</td> <td>0.693</td> </tr> </tbody> </table> <hr/> <p>So, when deciding <strong>where to split</strong>, we prefer splits that lead to <strong>lower impurity</strong> in the resulting nodes.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DT-5-480.webp 480w,/assets/img/DT-5-800.webp 800w,/assets/img/DT-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/DT-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DT-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Both Gini and Entropy encourage the class proportions to be close to 0 or 1—i.e., pure nodes. </div> <p><strong>Analogy: Sorting Colored Balls into Boxes</strong></p> <p>Imagine you’re sorting colored balls into boxes. Each ball represents a data point, and the color represents its class label.</p> <ul> <li>A <strong>perfectly sorted box</strong> has balls of only one color—this is a <strong>pure</strong> node.</li> <li>A box with a mix of colors is <strong>impure</strong>—you’re less certain what color a randomly chosen ball will be.</li> </ul> <p>Now think of impurity measures as ways to <strong>score</strong> each box. To ground these interpretations with some math, let’s calculate each impurity measure using a simple example.</p> <p>Suppose we’re at a node where the class distribution is:</p> <ul> <li>8 red balls (Class A)</li> <li>2 blue balls (Class B)</li> </ul> <p>This gives us the class probabilities:</p> <ul> <li> \[\hat{p}_A = \frac{8}{10} = 0.8\] </li> <li> \[\hat{p}_B = \frac{2}{10} = 0.2\] </li> </ul> <p>Let’s compute each impurity measure and interpret what it tells us:</p> <p><strong>Misclassification Error:</strong> <em>“What’s the chance I assign the wrong class if I always predict the majority class?”</em></p> <ul> <li>The majority class is red (Class A), so we’ll predict red for all inputs.</li> <li>The only mistakes occur when the true class is blue.</li> <li>So, the misclassification error is:</li> </ul> \[1 - \hat{p}_{\text{majority}} = 1 - 0.8 = 0.2\] <p><strong>Gini Index:</strong> <em>“If I randomly pick two balls (with replacement), what’s the chance they belong to different classes?”</em></p> <ul> <li>Formula:</li> </ul> \[G = \sum_k \hat{p}_k (1 - \hat{p}_k)\] <ul> <li>For our example:</li> </ul> \[G = 0.8(1 - 0.8) + 0.2(1 - 0.2) = 0.8(0.2) + 0.2(0.8) = 0.16 + 0.16 = 0.32\] <ul> <li>Interpretation: There’s a 32% chance of getting different classes when picking two random samples. The more balanced the classes, the higher the Gini.</li> </ul> <p><strong>Entropy:</strong> <em>“How surprised am I when I pick a ball and see its class?”</em></p> <ul> <li>Formula:</li> </ul> \[H = -\sum_k \hat{p}_k \log_2 \hat{p}_k\] <ul> <li>For our example:</li> </ul> \[H = -0.8 \log_2(0.8) - 0.2 \log_2(0.2)\] <ul> <li>Approximating:</li> </ul> \[H \approx -0.8(-0.32) - 0.2(-2.32) = 0.256 + 0.464 = 0.72\] <ul> <li>Interpretation: Entropy is a measure of uncertainty. A pure node has zero entropy. Here, there’s moderate uncertainty because the node isn’t completely pure.</li> </ul> <p>This example shows how the impurity measures behave when the node is somewhat pure but not perfectly. Gini and entropy are more sensitive to changes in class proportions than misclassification error, which is why they’re often preferred during tree building.</p> <hr/> <h5 id="quantifying-the-impurity-of-a-split"><strong>Quantifying the Impurity of a Split</strong></h5> <p>Once we’ve chosen an impurity measure (like Gini, Entropy, or Misclassification Error), how do we decide if a split is good?</p> <p>When a split divides a node into two regions—left (\(R_L\)) and right (\(R_R\))—we compute the <strong>weighted average impurity</strong> of the child nodes:</p> \[\text{Impurity}_{\text{split}} = \frac{N_L \cdot Q(R_L) + N_R \cdot Q(R_R)}{N_L + N_R}\] <p>Where:</p> <ul> <li>\(N_L, N_R\) are the number of samples in the left and right nodes.</li> <li>\(Q(R)\) is the impurity of region \(R\), measured using Gini, Entropy, or Misclassification Error.</li> </ul> <p>We want to <strong>minimize</strong> this weighted impurity. A good split is one that sends the data into two groups that are as pure as possible.</p> <p><strong>Example:</strong></p> <p>Suppose we split a node of 10 samples into:</p> <ul> <li>Left node (\(R_L\)): 6 samples, Gini impurity = 0.1</li> <li>Right node (\(R_R\)): 4 samples, Gini impurity = 0.3</li> </ul> <p>Then the weighted impurity is:</p> \[\frac{6 \cdot 0.1 + 4 \cdot 0.3}{10} = \frac{0.6 + 1.2}{10} = 0.18\] <p>We’d compare this value with the weighted impurity of other candidate splits and choose the split with the <strong>lowest</strong> value.</p> <p>This process is repeated greedily at each step of the tree-building process.</p> <hr/> <h5 id="interpretability-of-decision-trees"><strong>Interpretability of Decision Trees</strong></h5> <div class="row justify-content-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DT-6-480.webp 480w,/assets/img/DT-6-800.webp 800w,/assets/img/DT-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/DT-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DT-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>One of the biggest advantages of decision trees is their <strong>interpretability</strong>.</p> <p>Each internal node represents a question about a feature, and each leaf node gives a prediction. You can <strong>follow a path</strong> from the root to a leaf to understand exactly how a prediction was made.</p> <p>This makes decision trees particularly useful when model transparency is important—such as in healthcare or finance.</p> <p>Even people without a technical background can often understand a <strong>small decision tree</strong> just by reading it. However, as trees grow deeper and wider, they can become <strong>harder to interpret</strong>, especially if overfit to the training data.</p> <h5 id="discussion-trees-vs-linear-models"><strong>Discussion: Trees vs. Linear Models</strong></h5> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DT-7-480.webp 480w,/assets/img/DT-7-800.webp 800w,/assets/img/DT-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/DT-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DT-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Unlike models like logistic regression or SVMs, <strong>decision trees don’t rely on geometric concepts</strong> such as distances, angles, or dot products. Instead, they work by recursively splitting the input space.</p> <p><strong>Decision trees are:</strong></p> <ul> <li><strong>Non-linear</strong>: They can carve out complex, axis-aligned regions in the input space.</li> <li><strong>Non-metric</strong>: No need to define or compute distances between points.</li> <li><strong>Non-parametric</strong>: They don’t assume a fixed form for the underlying function—tree complexity grows with data.</li> </ul> <p><strong>But there are tradeoffs:</strong></p> <ul> <li>Trees may <strong>struggle with problems that have linear decision boundaries</strong>, which linear models handle easily.</li> <li>They are <strong>high-variance models</strong>—small changes in the training data can lead to very different trees.</li> <li>Without constraints (like pruning or depth limits), they are prone to <strong>overfitting</strong>, especially on noisy data.</li> </ul> <hr/> <h5 id="recap--conclusion"><strong>Recap &amp; Conclusion</strong></h5> <ul> <li> <p>Trees partition the input space into regions, unlike linear models that rely on fixed decision boundaries.</p> </li> <li> <p>Split the data to minimize the <strong>sum of squared errors</strong> in each region.</p> </li> <li> <p>Building the best tree is computationally infeasible. We use a <strong>greedy algorithm</strong> to build the tree step-by-step.</p> </li> <li> <p>Fully grown trees overfit. We prevent this by limiting depth, size, or pruning based on validation performance.</p> </li> <li> <p>For classfication, a good split increases <strong>class purity</strong> in the nodes. We explored this with intuitive +/− examples.</p> </li> <li> <p><strong>Impurity Measures:</strong> Misclassification Error, Gini Index and Entropy</p> </li> <li> <p>We pick splits that <strong>reduce weighted impurity</strong> the most.</p> </li> <li> <p>Small trees are easy to understand; large ones can become complex.</p> </li> <li> <p>Trees are non-linear, non-parametric, and don’t need distance—but they may overfit or struggle with linear patterns.</p> </li> </ul> <p>This sets the stage for the next step: <strong>ensembles</strong> like Random Forests and Boosted Trees.</p> <p>Stay tuned!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.]]></summary></entry><entry><title type="html">Decision Trees - Our First Non-Linear Classifier</title><link href="https://monishver11.github.io/blog/2025/decision-trees/" rel="alternate" type="text/html" title="Decision Trees - Our First Non-Linear Classifier"/><published>2025-04-20T03:28:00+00:00</published><updated>2025-04-20T03:28:00+00:00</updated><id>https://monishver11.github.io/blog/2025/decision-trees</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/decision-trees/"><![CDATA[<p>So far, we’ve seen classifiers that try to separate data using straight lines or hyperplanes—like logistic regression and SVMs. But what happens when a linear boundary just doesn’t cut it?</p> <p>Welcome to <strong>decision trees</strong>, our first inherently non-linear classifier. These models can slice the input space into complex shapes, enabling them to learn highly flexible rules for classification and regression.</p> <hr/> <p>To understand the core idea, consider this question:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DT-1-480.webp 480w,/assets/img/DT-1-800.webp 800w,/assets/img/DT-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/DT-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DT-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>Can we classify these points using a linear classifier?</p> </blockquote> <p>Sometimes the answer is no. In such cases, we can instead <strong>partition the input space into axis-aligned regions</strong> recursively. This is exactly what a decision tree does—it recursively divides the space based on feature thresholds, creating regions where simple predictions (like mean or majority label) are made.</p> <hr/> <h5 id="decision-trees-setup"><strong>Decision Trees: Setup</strong></h5> <p>We’ll focus on <strong>binary decision trees</strong>, where each internal node splits the dataset into exactly two parts based on a single feature.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DT-2-480.webp 480w,/assets/img/DT-2-800.webp 800w,/assets/img/DT-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/DT-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DT-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Key structure of a binary decision tree:</strong></p> <ul> <li>Each <strong>node</strong> corresponds to a subset of the data.</li> <li>At every split, exactly <strong>one feature</strong> is used to divide the data.</li> <li>For <strong>continuous features</strong>, splits are typically of the form: \(x_i \leq t\) where \(t\) is a learned threshold.</li> <li>For <strong>categorical features</strong>, values can be partitioned into two disjoint groups (not covered in this post).</li> <li><strong>Predictions</strong> are made only at the <strong>terminal nodes</strong> (also called <strong>leaves</strong>), where no further splitting occurs.</li> </ul> <h5 id="constructing-the-tree"><strong>Constructing the Tree</strong></h5> <p>Before we dive into the mechanics of decision trees, let’s understand the core objective:</p> <p>We want to partition the input space into <strong>regions</strong> \(R_1, R_2, \ldots, R_J\) in a way that minimizes the prediction error in each region.</p> <p>Formally, our aim is to find the regions \(R_1, \ldots, R_J\) that minimize the <strong>sum of squared errors</strong> within each region:</p> \[\sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2\] <p>Here:</p> <ul> <li>\(y_i\) is the true target value,</li> <li>\(\hat{y}_{R_j}\) is the predicted value in region \(R_j\), typically the <strong>mean</strong> of \(y_i\)’s in that region.</li> </ul> <p><strong>Why this formulation?</strong></p> <p>This objective captures our desire to make <strong>accurate predictions</strong> within each region. By minimizing the <strong>sum of squared errors (SSE)</strong>, we’re encouraging the model to place similar target values together. In other words, a good split should group data points that can be well-represented by a single value—their <strong>mean</strong>.</p> <p>The mean minimizes the squared error within a region, making it the natural prediction for regression tasks. This also ensures that the tree focuses on reducing <strong>variance within each region</strong>, which is a core idea behind how decision trees generalize from data.</p> <p><strong>Intuition with a Simple Example:</strong></p> <p>Suppose you’re trying to predict someone’s salary based on their years of experience. If you create a region \(R_j\) that contains people with 2–4 years of experience and their salaries are: $45k, 48k, 46k, 47k$, then predicting the <strong>mean salary</strong> (≈ $46.5k) gives you the smallest total squared error.</p> <p>But if you instead included someone with 10 years of experience and a salary of $80k in the same region, your prediction would shift higher—and the squared error for the 2–4 year group would increase significantly. So, to <strong>minimize overall error</strong>, we should split the region before including such an outlier. This is exactly what the tree tries to do—split the data where it significantly improves prediction accuracy.</p> <p>In essence, the tree grows by trying to group together points that are similar in their target values—allowing for simple and accurate predictions within each region.</p> <hr/> <p><strong>Problem: Intractability</strong></p> <p>While our goal is clear—split the data into regions that minimize prediction error—<strong>actually finding the optimal binary tree</strong> is computationally infeasible.</p> <p><strong>Why?</strong> Because the number of possible binary trees grows <strong>exponentially</strong> with the number of features and data points. At each node, we can choose any feature and any possible split value, and each choice affects all future splits. This creates an enormous search space.</p> <p>Let’s go back to our salary prediction task. Suppose we have 100 employees with features like years of experience, education level, and job title. At the root node, we could split on:</p> <ul> <li><strong>Experience</strong>: maybe at 5 years?</li> <li><strong>Education</strong>: perhaps separate Bachelor’s from Master’s and PhDs?</li> <li><strong>Job title</strong>: group certain roles together?</li> </ul> <p>Each of these choices leads to different subsets of the data—then we repeat the process within each subset. Even for a small dataset, the number of ways to split and grow the tree quickly becomes astronomical.</p> <p>Evaluating <strong>every possible combination of splits and tree structures</strong> to find the one that gives the absolute minimum prediction error would require checking <strong>all trees</strong>, which is practically impossible.</p> <p><strong>Solution: Greedy Construction</strong></p> <p>Instead of searching the entire tree space, we use a <strong>greedy algorithm</strong>:</p> <ol> <li><strong>Start</strong> at the root node with the full dataset.</li> <li><strong>Evaluate</strong> all possible splits and choose the one that minimizes the error <strong>locally</strong>.</li> <li><strong>Split</strong> the data accordingly.</li> <li><strong>Repeat</strong> the process recursively until a stopping condition is met, such as: <ul> <li>Reaching a <strong>maximum tree depth</strong>,</li> <li>A <strong>minimum number of data points</strong> in a node,</li> <li>Or achieving a <strong>minimal decrease in error</strong>.</li> </ul> </li> </ol> <blockquote> <p>💡 We only split regions that have already been defined by earlier (non-terminal) splits. So the tree structure builds <strong>hierarchically</strong>, and each decision is <strong>context-dependent</strong>.</p> </blockquote> <p><strong>Prediction in Terminal Nodes</strong></p> <p>Once the tree is built, how do we make predictions?</p> <p>For <strong>regression</strong>, the predicted value at a terminal node \(R_m\) is just the <strong>average of the targets</strong> in that region:</p> \[\hat{y}_{R_m} = \text{mean}(y_i \mid x_i \in R_m)\] <div class="row justify-content-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DT-3-480.webp 480w,/assets/img/DT-3-800.webp 800w,/assets/img/DT-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/DT-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DT-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Prediction in a Regression Tree </div> <p><strong>Greedy Nature and Its Implications</strong></p> <p>This greedy approach makes <strong>locally optimal decisions</strong> at each step without considering their long-term impact. As a result:</p> <ul> <li>It is efficient and straightforward to implement.</li> <li>However, it may not lead to the <strong>globally optimal tree</strong>, since better overall structures might require different early splits.</li> </ul> <p><strong>Example:</strong></p> <p>Suppose we’re predicting salary and at the root node, we greedily choose to split on <strong>education level</strong>, since it gives the best reduction in squared error at that moment.</p> <p>But later, we realize that <strong>years of experience</strong> would have provided a much cleaner separation <strong>if it had been split first</strong>. Unfortunately, the tree can’t go back and revise that decision.</p> <p>That’s the key limitation of greedy methods:</p> <blockquote> <p>They make the best decision in the moment—but not necessarily the best decision overall.</p> </blockquote> <hr/> <h5 id="how-do-we-find-the-best-split-point"><strong>How do we find the best split point</strong>?</h5> <p>When building decision trees, we need to decide where to split the data at each node. Here’s how it’s typically done:</p> <ul> <li> <p><strong>Step 1: Iterate over all features.</strong><br/> For each feature, we try to find the best possible threshold that minimizes the loss (e.g., squared error).</p> </li> <li> <p><strong>Step 2: Handle continuous features efficiently.</strong><br/> While there are infinitely many possible split points for continuous features, we <strong>don’t need to try them all</strong>.</p> </li> </ul> <p><strong>Key Insight:</strong></p> <p>If we sort the values of a feature \(x_j\), say:</p> \[x_{j(1)}, x_{j(2)}, \ldots, x_{j(n)}\] <p>then we only need to consider splits <strong>between adjacent values</strong>. Why?<br/> Because any split point within the interval \((x_{j(r)}, x_{j(r+1)})\) will result in the <strong>same data partition</strong>, and hence the same loss.</p> <p><strong>Common Strategy:</strong></p> <p>We pick the <strong>midpoint</strong> between adjacent sorted values as the candidate split:</p> \[s_j \in \left\{ \frac{1}{2} \left(x_{j(r)} + x_{j(r+1)}\right) \,\middle|\, r = 1, \ldots, n - 1 \right\}\] <p>So instead of testing infinitely many thresholds, we reduce the search to just <strong>\(n - 1\) candidate splits</strong> for each feature—making the process tractable and efficient.</p> <h5 id="decision-trees-and-overfitting"><strong>Decision Trees and Overfitting</strong></h5> <p>What happens if we <strong>keep splitting</strong> the data?</p> <p>Eventually, the tree can grow so deep that <strong>every leaf contains just one data point</strong>. This means the model has memorized the training data—resulting in <strong>zero training error</strong>, but likely poor generalization to unseen data.</p> <p>In other words: <strong>overfitting</strong>.</p> <p>To prevent this, we need to <strong>regularize</strong> the tree by controlling its complexity. Common strategies include:</p> <ul> <li><strong>Limit the depth</strong> of the tree</li> <li><strong>Limit the total number of nodes</strong></li> <li><strong>Restrict the number of terminal (leaf) nodes</strong></li> <li><strong>Set a minimum number of samples required</strong> to split a node or to be in a leaf</li> </ul> <p>Another strategy is <strong>pruning</strong>, which is used in the famous CART algorithm (Breiman et al., 1984):</p> <ol> <li><strong>Grow a large tree</strong>: Allow the tree to grow deep, often until each leaf contains just a few data points (e.g., ≤ 5).</li> <li><strong>Prune it back</strong>: Starting from this large tree, <strong>recursively remove subtrees</strong> if doing so improves performance on a validation set.</li> </ol> <p>This two-phase approach ensures the model starts expressive but is ultimately <strong>simplified</strong> to avoid overfitting.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DT-4-480.webp 480w,/assets/img/DT-4-800.webp 800w,/assets/img/DT-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/DT-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DT-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pruning: An Example </div> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up:</strong></h5> <ul> <li> <p>Decision Trees are a powerful method that recursively splits the data to minimize prediction error, enabling flexible models for both regression and classification tasks.</p> </li> <li> <p>The Greedy Approach is used to make locally optimal decisions at each split, though it may not always lead to the globally best tree.</p> </li> <li> <p>Overfitting is a concern when the tree becomes too complex, which can be mitigated by using regularization techniques like limiting depth and pruning.</p> </li> <li> <p>Pruning simplifies the tree after it has been grown, helping to avoid overfitting and improve generalization.</p> </li> </ul> <p>Now that we’ve explored decision trees in a regression context, let’s shift gears and dive into how they work for classification. Specifically, we’ll focus on understanding what constitutes a good split in classification and how to define the misclassification error in a node. Let’s dive into these topics in the next post.</p> <p>See you!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.]]></summary></entry><entry><title type="html">Structured Perceptron &amp;amp; Structured SVM</title><link href="https://monishver11.github.io/blog/2025/structured-perceptron-svm/" rel="alternate" type="text/html" title="Structured Perceptron &amp;amp; Structured SVM"/><published>2025-04-16T19:07:00+00:00</published><updated>2025-04-16T19:07:00+00:00</updated><id>https://monishver11.github.io/blog/2025/structured-perceptron-svm</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/structured-perceptron-svm/"><![CDATA[<p>In the <a href="http://localhost:8080/blog/2025/structured-prediction/">previous post</a>, we explored how structured prediction works under the hood—we defined a compatibility function \(h(x, y) = \langle w, \Psi(x, y) \rangle\), designed rich feature representations using Unary and Markov features, and saw how the overall score of an output sequence decomposes into local parts.</p> <p>We also discussed how this score guides prediction by choosing the most compatible output structure:</p> \[f(x) = \arg\max_{y \in Y(x)} h(x, y)\] <p>But how do we actually <strong>learn</strong> the weight vector \(w\) that makes good predictions?</p> <p>In this post, we’ll walk through two popular algorithms for learning structured models:</p> <ul> <li>The <strong>Structured Perceptron</strong>, which extends the classic perceptron to structured outputs</li> <li>The <strong>Structured SVM</strong>, which builds in <strong>margins</strong> and <strong>regularization</strong> for better generalization</li> </ul> <p>Let’s dive in!</p> <h5 id="structured-perceptron"><strong>Structured Perceptron</strong></h5> <p>To learn the weight vector \(w\) that scores correct outputs higher than incorrect ones, we can use the <strong>structured perceptron algorithm</strong>.</p> <p>It works just like the multiclass perceptron, except the prediction is over a <strong>structured output space</strong>:</p> <hr/> <p><strong>Structured Perceptron Algorithm</strong></p> <ol> <li> <p><strong>Initialize</strong> the weights: \(w \leftarrow 0\)</p> </li> <li> <p><strong>For each training example</strong> \((x, y)\):</p> <ul> <li> <p>Predict the best structure under the current model: \(\hat{y} = \arg\max_{y' \in Y(x)} \langle w, \Psi(x, y') \rangle\)</p> </li> <li> <p>If the prediction is incorrect \((\hat{y} \ne y)\):</p> <ul> <li><strong>Update</strong> the weight vector: \(w \leftarrow w + \Psi(x, y) - \Psi(x, \hat{y})\)</li> </ul> </li> </ul> </li> </ol> <p>This update <strong>encourages the correct structure</strong> by increasing its score and <strong>penalizes the incorrect one</strong> by decreasing its score. This is identical to multiclass perceptron, except that the prediction \(\hat{y}\) comes from a structured space.</p> <hr/> <p>Up to this point, we’ve seen how to score structured outputs and how to train with the structured perceptron. But the perceptron only updates on mistakes and doesn’t consider <strong><em>“how wrong”</em></strong> a prediction is.</p> <p>So, what if we want a <strong>more principled way to penalize incorrect outputs</strong> based on how different they are from the correct one?</p> <p>This brings us to <strong>structured hinge loss</strong> and <strong>structured SVM</strong>.</p> <h5 id="structured-svm"><strong>Structured SVM</strong></h5> <p>In structured prediction, we want the correct output to <strong>not only score highest</strong>, but to <strong>beat all incorrect outputs by a margin</strong>.</p> <p>This leads to the generalized hinge loss:</p> \[\ell_{\text{hinge}}(x, y) = \max_{y' \in Y(x)} \left[ \Delta(y, y') + \langle w, \Psi(x, y') - \Psi(x, y) \rangle \right]\] <p>Let’s break it down:</p> <ul> <li>\(\Psi(x, y)\) is the feature vector for the true output.</li> <li>\(\Psi(x, y')\) is the feature vector for a wrong prediction.</li> <li>\(\Delta(y, y')\) is the <strong>loss function</strong> that tells us <em>how bad</em> the prediction \(y'\) is compared to the ground truth \(y\).</li> </ul> <p>A common choice for \(\Delta\) is the <strong>Hamming loss</strong> (i.e., how many labels are incorrect):</p> \[\Delta(y, y') = \frac{1}{L} \sum_{i=1}^L 1[y_i \ne y'_i]\] <p>This loss forces the model to <strong>separate the true output from the rest</strong>, with a margin proportional to how different they are.</p> <p><strong>Picture it this way:</strong></p> <p>Imagine you’re a teacher grading structured answers—say, full sentences submitted by students.</p> <p>You don’t just care whether a sentence is right or wrong—you also care <strong>how wrong</strong> a student’s answer is. If a student writes something that’s close to the correct answer, you might give partial credit. But if their answer is completely off, you’d deduct more points.</p> <p>This is exactly what structured hinge loss does.</p> <ul> <li>It ensures that the correct output <strong>not only wins</strong>, but wins <strong>by enough</strong>—with a <em>margin</em> that reflects how different the incorrect output is.</li> <li>If an incorrect output \(y'\) is very different from the ground truth \(y\) (i.e., high \(\Delta(y, y')\)), then the model is penalized more if it scores \(y'\) too closely to \(y\).</li> </ul> <p><strong>Example: POS Tagging with Margin-Based Loss</strong></p> <p>Suppose you have the input sentence: <strong>“He runs fast”</strong></p> <p>The correct POS tags are:</p> \[y = [\text{Pronoun}, \text{Verb}, \text{Adverb}]\] <p>Now, imagine two possible incorrect predictions:</p> <ul> <li>\(y'_1 = [\text{Pronoun}, \text{Noun}, \text{Adverb}]\) — only <strong>one</strong> mistake</li> <li>\(y'_2 = [\text{Noun}, \text{Noun}, \text{Noun}]\) — <strong>three</strong> mistakes</li> </ul> <p>The Hamming losses are:</p> <ul> <li> \[\Delta(y, y'_1) = \frac{1}{3}\] </li> <li> \[\Delta(y, y'_2) = 1\] </li> </ul> <p>According to the hinge loss, \(y'_2\) should be separated by a <strong>larger margin</strong> from the correct output than \(y'_1\). That is, the model must not only prefer the true output, but <strong>strongly penalize</strong> very wrong outputs.</p> <h5 id="structured-svm-objective"><strong>Structured SVM Objective</strong></h5> <p>We now define a learning objective that uses this hinge loss, along with regularization:</p> \[\min_{w} \frac{1}{2} \|w\|^2 + C \sum_{(x, y) \in D} \ell_{\text{hinge}}(x, y)\] <p>Where:</p> <ul> <li>The first term \(\frac{1}{2} \|w\|^2\) controls model complexity (regularization).</li> <li>The second term penalizes incorrect predictions.</li> <li>\(C\) is a hyperparameter that trades off margin size vs training error.</li> </ul> <h5 id="how-do-we-optimize-this"><strong>How Do We Optimize This?</strong></h5> <p>Optimizing the structured SVM objective might seem tricky because the hinge loss involves a <strong>max over all possible outputs</strong>:</p> \[\ell_{\text{hinge}}(x, y) = \max_{y' \in Y(x)} \left[ \Delta(y, y') + \langle w, \Psi(x, y') - \Psi(x, y) \rangle \right]\] <p>But here’s the clever trick:</p> <p>We don’t need to check <strong>all</strong> possible \(y'\)—we only need to find the <strong>worst violator</strong>; The structure \(y'\) that scores too close to or even higher than the true output \(y\)—<strong>after accounting for how wrong it is</strong>.</p> <p>This process is called <strong>loss-augmented inference</strong>.</p> <h5 id="understanding-loss-augmented-inference"><strong>Understanding Loss-Augmented Inference</strong></h5> <p>To optimize the structured SVM objective, we need to minimize the hinge loss:</p> \[\ell_{\text{hinge}}(x, y) = \max_{y' \in Y(x)} \left[ \Delta(y, y') + \langle w, \Psi(x, y') - \Psi(x, y) \rangle \right]\] <p>At a high level, this loss tries to ensure:</p> <blockquote> <p>“The score of the <strong>true output</strong> \(y\) is higher than that of <strong>every incorrect output</strong> \(y'\), by at least how wrong \(y'\) is.”</p> </blockquote> <p>This encourages <strong>margin-based separation</strong> between \(y\) and each \(y'\).</p> <p>But how do we compute or optimize this in practice?</p> <p><strong>Intuition Behind Loss-Augmented Inference</strong></p> <p>Think of the model as making predictions based on scores:</p> <ul> <li>\(\langle w, \Psi(x, y) \rangle\) — score for the <strong>true output</strong></li> <li>\(\langle w, \Psi(x, y') \rangle\) — score for a <strong>candidate output</strong></li> </ul> <p>The structured SVM doesn’t just care about which \(y'\) scores highest.</p> <p>It asks: <em>“Which wrong output \(y'\) is both very wrong <strong>and</strong> scores too well?”</em></p> <p>That’s where <strong>loss-augmented inference</strong> comes in:</p> \[y' = \arg\max_{y' \in Y(x)} \left[ \Delta(y, y') + \langle w, \Psi(x, y') \rangle \right]\] <ul> <li>The model score \(\langle w, \Psi(x, y') \rangle\) captures how likely the model thinks \(y'\) is.</li> <li>The task loss \(\Delta(y, y')\) captures how bad that \(y'\) is in the real world.</li> </ul> <p>By combining them, we search for the <strong>most offending structure</strong>—the one that violates the margin the most.</p> <h5 id="the-update-step"><strong>The Update Step</strong></h5> <p>Once we find this \(y'\), we update the weights:</p> \[w \leftarrow w + \eta \cdot \left( \Psi(x, y) - \Psi(x, y') \right)\] <ul> <li>This increases the score for the correct output \(y\)</li> <li>And decreases the score for the worst violator \(y'\)</li> </ul> <p>It’s almost like the perceptron, <strong>but more aware of the “danger level” of the mistake</strong>.</p> <p><strong>Analogy: Hiring Candidates</strong></p> <p>Imagine you’re hiring someone (the structured model) to assign tags to words in a sentence. You ask them to tag a sentence and also explain <strong>why</strong> they think the tags are right (that’s their score).</p> <p>Now, suppose they make a mistake. You don’t just say “wrong!”—you also ask:</p> <blockquote> <p>“How bad is this mistake? Did you call a verb a noun? Or a verb an adjective?”</p> </blockquote> <p>If the mistake is <strong>very wrong</strong> and the candidate seems <strong>very confident</strong> (high score), that’s a <strong>serious violation</strong>.</p> <p>So you correct them in a way that says:</p> <blockquote> <p>“This bad answer was <strong>too confident</strong>. Next time, lower your score for this kind of mistake.”</p> </blockquote> <p>That’s what loss-augmented inference does: it focuses on <strong>mistakes the model is confident about but shouldn’t be</strong>.</p> <h5 id="summary"><strong>Summary</strong></h5> <ul> <li>Structured prediction is essential when outputs are <strong>interdependent</strong>—such as sequences or trees.</li> <li>We define a <strong>compatibility score</strong> \(h(x, y)\) to evaluate how well an output \(y\) matches input \(x\).</li> <li>The score decomposes using <strong>local features</strong> like unary and Markov features, enabling efficient learning.</li> <li><strong>Structured Perceptron</strong> uses mistake-driven updates, like its multiclass counterpart.</li> <li><strong>Structured SVM</strong> introduces <strong>margins and hinge loss</strong>, providing a more robust and generalizable model.</li> </ul> <hr/> <p>That wraps up our exploration of structured prediction. Up next: <strong>Decision Trees</strong>, our first inherently non-linear classifier. Stay tuned, and see you!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.]]></summary></entry><entry><title type="html">Structured Prediction and Multiclass SVM</title><link href="https://monishver11.github.io/blog/2025/structured-prediction/" rel="alternate" type="text/html" title="Structured Prediction and Multiclass SVM"/><published>2025-04-13T18:05:00+00:00</published><updated>2025-04-13T18:05:00+00:00</updated><id>https://monishver11.github.io/blog/2025/structured-prediction</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/structured-prediction/"><![CDATA[<p>Structured prediction is a powerful framework used when our output space is complex and structured — such as sequences, trees, or graphs — rather than simple class labels. This post builds on multiclass SVMs to delve deeper into structured prediction, exploring how we define and learn over complex output spaces, as well as the notions of joint feature representations and local compatibility scores.</p> <hr/> <h5 id="what-is-structured-prediction"><strong>What is Structured Prediction?</strong></h5> <p>In standard classification, we predict a single label for each input—like identifying whether an image contains a cat or a dog.</p> <p>But what if our outputs aren’t that simple? What if the prediction itself has structure?</p> <p>That’s where <strong>structured prediction</strong> comes in. It refers to machine learning tasks where the output is not a single label but a <strong>structured object</strong>—like a sequence, a tree, or even a segmentation map. These outputs have dependencies and internal organization that we want to model directly.</p> <h5 id="example-1-part-of-speech-pos-tagging"><strong>Example 1: Part-of-Speech (POS) Tagging</strong></h5> <p>In POS tagging, we’re given a sentence and need to assign a grammatical label to each word—like “noun”, “verb”, or “pronoun”.</p> <p>Here’s an example:</p> \[\begin{aligned} x &amp;: [\text{START}],\ \text{He},\ \text{eats},\ \text{apples} \\ y &amp;: [\text{START}],\ \text{Pronoun},\ \text{Verb},\ \text{Noun} \end{aligned}\] <p>To formalize this:</p> <ul> <li> <p><strong>Vocabulary</strong><br/> Words we might encounter, including a special <code class="language-plaintext highlighter-rouge">[START]</code> symbol and punctuation:</p> \[V = \text{All English words} \cup \{ \text{[START]}, \text{.} \}\] </li> <li> <p><strong>Input space</strong><br/> A sequence of words of any length:</p> \[X = V^n, \quad n = 1, 2, 3, \dots\] </li> <li> <p><strong>Label set</strong><br/> The set of possible POS tags:</p> \[P = \{ \text{START, Pronoun, Verb, Noun, Adjective} \}\] </li> <li> <p><strong>Output space</strong><br/> A sequence of POS tags of the same length as the input:</p> \[Y = P^n, \quad n = 1, 2, 3, \dots\] </li> </ul> <p>This is a classic case of sequence labeling, where each position in the input has a corresponding label in the output.</p> <h5 id="example-2-action-grounding-in-long-form-videos"><strong>Example 2: Action Grounding in Long-Form Videos</strong></h5> <p>Structured prediction also shines in vision tasks like <strong>action grounding</strong>. Here, we’re given a long video and need to segment it and assign actions like “chopping” or “frying” to different time spans.</p> <ul> <li> <p><strong>Input</strong><br/> A video frame is represented as a feature vector:</p> \[V = \mathbb{R}^D\] </li> <li> <p><strong>Input sequence</strong><br/> A video is a sequence of these frame-level features:</p> \[X = V^n\] </li> <li> <p><strong>Label set</strong><br/> The set of possible actions:</p> \[P = \{ \text{Slicing, Chopping, Frying, Washing, ...} \}\] </li> <li> <p><strong>Output sequence</strong><br/> A sequence of actions corresponding to segments or frames:</p> \[Y = P^n\] </li> </ul> <p>This setup allows us to model real-world tasks where outputs have temporal structure—actions occur over time and are dependent on previous context. Structured prediction opens the door to powerful models that understand more than just isolated labels—they reason over entire sequences and structures.</p> <blockquote> <p><strong>But wait—doesn’t the model just predict POS tags for the given input? Where does context come in?</strong></p> </blockquote> <p>Great question! It might seem like we’re simply classifying each word. But in <strong>structured prediction</strong>, we <strong>don’t</strong> predict each tag independently. Instead, we predict the <strong>entire sequence jointly</strong>—which means the model <strong>does consider context</strong> while assigning tags.</p> <p><strong>How?</strong></p> <p>Structured prediction models use features that depend on both the current and <strong>previous tags</strong> (Markov dependencies). For example:</p> <ul> <li>If the previous tag is <code class="language-plaintext highlighter-rouge">Pronoun</code>, it’s likely the current tag is <code class="language-plaintext highlighter-rouge">Verb</code>.</li> <li>If the previous word is <code class="language-plaintext highlighter-rouge">He</code> and the current word is <code class="language-plaintext highlighter-rouge">runs</code>, the current tag is likely <code class="language-plaintext highlighter-rouge">Verb</code>.</li> </ul> <p>These dependencies are built into the model using <strong>joint feature vectors</strong> and <strong>structured scoring</strong>. Instead of a single-label classifier, we score the entire output sequence and pick the best-scoring one:</p> \[\hat{y} = \arg\max_{y \in Y(x)} h(x, y)\] <p>Now that we understand how structured models use context, let’s explore the hypothesis space that makes this possible.</p> <hr/> <h5 id="hypothesis-space-for-structured-outputs"><strong>Hypothesis Space for Structured Outputs</strong></h5> <p>In structured prediction, the output space \(Y(x)\) is <strong>large and structured</strong>—its size depends on the input \(x\).</p> <p>We define:</p> <ul> <li> <p><strong>Base hypothesis space</strong>:</p> \[H = \{ h : X \times Y \to \mathbb{R} \}\] </li> <li> <p><strong>Compatibility score</strong>:</p> \[h(x, y)\] <p>gives a real-valued score that measures how compatible an input \(x\) is with a candidate output \(y\).</p> </li> <li> <p><strong>Final prediction function</strong>:</p> \[f(x) = \arg\max_{y \in Y} h(x, y), \quad f \in F\] </li> </ul> <p>So, our model chooses the <strong>most compatible output structure</strong> based on the scoring function.</p> <h5 id="designing-the-compatibility-score"><strong>Designing the Compatibility Score</strong></h5> <p>We use a <strong>linear model</strong> to define the compatibility score:</p> \[h(x, y) = \langle w, \Psi(x, y) \rangle\] <p>Where:</p> <ul> <li>\(w\) is a parameter vector to be learned.</li> <li>\(\Psi(x, y)\) is a <strong>joint feature representation</strong> of the input-output pair.</li> </ul> <p>Let’s break down how to construct this feature vector.</p> <p>Structured prediction leverages <strong>decomposable features</strong> that split complex structures into simpler parts.</p> <p><strong>Unary Features</strong></p> <p>Unary features depend on the label at a single position \(i\):</p> <ul> <li> <p>Example features:</p> \[\phi_1(x, y_i) = 1[x_i = \text{runs}] \cdot 1[y_i = \text{Verb}]\] \[\phi_2(x, y_i) = 1[x_i = \text{runs}] \cdot 1[y_i = \text{Noun}]\] \[\phi_3(x, y_i) = 1[x_{i-1} = \text{He}] \cdot 1[x_i = \text{runs}] \cdot 1[y_i = \text{Verb}]\] </li> </ul> <p><strong>Markov Features</strong></p> <p>Markov features capture dependencies between <strong>adjacent labels</strong> (like in HMMs):</p> <ul> <li> <p>Example features:</p> \[\theta_1(x, y_{i-1}, y_i) = 1[y_{i-1} = \text{Pronoun}] \cdot 1[y_i = \text{Verb}]\] \[\theta_2(x, y_{i-1}, y_i) = 1[y_{i-1} = \text{Pronoun}] \cdot 1[y_i = \text{Noun}]\] </li> </ul> <p>These features are key to modeling the <strong>structure</strong> in structured prediction tasks. By combining them across all positions in a sequence, we construct the full joint feature vector \(\Psi(x, y)\).</p> <hr/> <p>Now that we’ve seen how structured prediction breaks down sequences into parts using Unary and Markov features, the next question is:</p> <p><strong>How do we combine these local components to score an entire sequence?</strong></p> <p>This leads us to the idea of <strong>local compatibility scores</strong>.</p> <h5 id="local-compatibility-score"><strong>Local Compatibility Score</strong></h5> <p>At each position \(i\) in the sequence, we compute a <strong>local feature vector</strong> that captures both the current label and the transition from the previous label.</p> <ul> <li> <p>Local feature vector: \(\Psi_i(x, y_{i-1}, y_i) = \big( \phi_1(x, y_i), \phi_2(x, y_i), \dots, \theta_1(x, y_{i-1}, y_i), \theta_2(x, y_{i-1}, y_i), \dots \big)\)</p> </li> <li> <p>Local compatibility score: \(\langle w, \Psi_i(x, y_{i-1}, y_i) \rangle\)</p> </li> </ul> <p>To get the <strong>total compatibility score</strong> for the input-output pair \((x, y)\), we <strong>sum these local scores</strong> over the sequence:</p> \[h(x, y) = \sum_i \langle w, \Psi_i(x, y_{i-1}, y_i) \rangle\] <p>This is equivalent to:</p> \[h(x, y) = \langle w, \Psi(x, y) \rangle\] <p>Where the <strong>global feature vector</strong> is the sum of all local feature vectors:</p> \[\Psi(x, y) = \sum_i \Psi_i(x, y_{i-1}, y_i)\] <p>This decomposition is what makes learning and inference tractable in structured models like CRFs, structured perceptrons, and structured SVMs.</p> <hr/> <h5 id="lets-walk-through-the-logic-with-an-example-part-of-speech-pos-tagging-for-the-sentence"><strong>Let’s walk through the logic with an example: Part-of-Speech (POS) Tagging for the sentence</strong></h5> <p>Input (x): [START] He runs fast</p> <p>Goal: Predict the most likely sequence of POS tags:</p> <p>Output (y): [START] Pronoun Verb Adverb</p> <p><strong>Step 1: What are we learning?</strong></p> <p>We want to <strong>learn a scoring function</strong>:</p> \[h(x, y) = \langle w, \Psi(x, y) \rangle\] <p>This function gives a <strong>score</strong> to a candidate output sequence \(y\) for a given input \(x\). The higher the score, the more compatible we believe \(x\) and \(y\) are.</p> <p><strong>Step 2: Why structured outputs are different</strong></p> <p>In structured prediction, the output \(y\) isn’t just a single label—it’s a whole <strong>sequence</strong> (or tree, or grid, etc.).</p> <p>For our sentence, that means predicting:</p> <p>[Pronoun, Verb, Adverb]</p> <p>instead of a single class like just “Verb”.</p> <p><strong>Step 3: Representing compatibility with features</strong></p> <p>We use <strong>feature functions</strong> to capture useful information from \((x, y)\):</p> <ul> <li><strong>Unary features</strong> look at the input and the label at a single position (e.g., “He” → “Pronoun”)</li> <li><strong>Markov features</strong> look at <strong>transitions between labels</strong> (e.g., “Pronoun” → “Verb”)</li> </ul> <p>These become the building blocks of our model.</p> <p><strong>Step 4: Breaking down the full sequence</strong></p> <p>For a sequence of length 3 (ignoring [START] token), we define <strong>local features</strong> at each position \(i\):</p> <ul> <li>At \(i = 1\): “He” tagged as Pronoun</li> <li>At \(i = 2\): “runs” tagged as Verb</li> <li>At \(i = 3\): “fast” tagged as Adverb</li> </ul> <p>At each step, we build a <strong>local feature vector</strong>:</p> \[\Psi_i(x, y_{i-1}, y_i)\] <p>This vector includes both:</p> <ul> <li>Unary features for \(x_i\) and \(y_i\)</li> <li>Markov features for \(y_{i-1}\) and \(y_i\)</li> </ul> <p><strong>Step 5: Computing local scores</strong></p> <p>We compute a <strong>local score</strong> at each position:</p> \[\langle w, \Psi_i(x, y_{i-1}, y_i) \rangle\] <p>This tells us how well the current word and label (and label transition) fit the model.</p> <p>Do this for all positions \(i\) in the sequence.</p> <p><strong>Let’s walk through this sequence step-by-step.</strong></p> <p>At \(i = 1\) (Word: <em>He</em>, Tag: <em>Pronoun</em>)</p> <p>Since this is the first word, we assume the previous tag is <code class="language-plaintext highlighter-rouge">START</code>:</p> \[y_0 = \text{START}\] <p>We define:</p> \[\Psi_1(x, y_0, y_1) = \begin{cases} \phi_1(x_1 = \text{He}, y_1 = \text{Pronoun}) = 1 \\ \theta_1(y_0 = \text{START}, y_1 = \text{Pronoun}) = 1 \end{cases}\] <p>All other components of \(\Psi_1\) are zero.</p> <p>At \(i = 2\) (Word: <em>runs</em>, Tag: <em>Verb</em>)</p> \[y_1 = \text{Pronoun}, \quad y_2 = \text{Verb}\] <p>We define:</p> \[\Psi_2(x, y_1, y_2) = \begin{cases} \phi_2(x_2 = \text{runs}, y_2 = \text{Verb}) = 1 \\ \theta_2(y_1 = \text{Pronoun}, y_2 = \text{Verb}) = 1 \end{cases}\] <p>Other entries in \(\Psi_2\) are zero.</p> <p>At \(i = 3\) (Word: <em>fast</em>, Tag: <em>Adverb</em>)</p> \[y_2 = \text{Verb}, \quad y_3 = \text{Adverb}\] <p>We define:</p> \[\Psi_3(x, y_2, y_3) = \begin{cases} \phi_3(x_3 = \text{fast}, y_3 = \text{Adverb}) = 1 \\ \theta_3(y_2 = \text{Verb}, y_3 = \text{Adverb}) = 1 \end{cases}\] <p><strong>Step 6: Summing up the local scores</strong></p> <p>To score the full sequence \((x, y)\), we <strong>sum all local scores</strong>:</p> \[h(x, y) = \sum_i \langle w, \Psi_i(x, y_{i-1}, y_i) \rangle\] <p>This total score tells us how compatible this <strong>entire sequence of labels</strong> is with the input.</p> <p>We also define the <strong>global feature vector</strong> as:</p> \[\Psi(x, y) = \sum_i \Psi_i(x, y_{i-1}, y_i)\] <p>So that the score becomes:</p> \[h(x, y) = \langle w, \Psi(x, y) \rangle\] <p>So, in our example, this will be:</p> \[\Psi(x, y) = \Psi_1(x, y_0, y_1) + \Psi_2(x, y_1, y_2) + \Psi_3(x, y_2, y_3)\] <p>Then, the <strong>total compatibility score</strong> is:</p> \[h(x, y) = \langle w, \Psi(x, y) \rangle = \sum_{i=1}^3 \langle w, \Psi_i(x, y_{i-1}, y_i) \rangle\] <p><strong>Step 7: Prediction</strong></p> <p>Finally, to predict the best output sequence for a new input \(x\), we find:</p> \[f(x) = \arg\max_{y \in Y} \langle w, \Psi(x, y) \rangle\] <p>This means: “Find the label sequence \(y\) that gives the highest compatibility score with \(x\).”</p> <hr/> <h5 id="summary-of-the-complete-flow"><strong>Summary of the Complete Flow</strong></h5> <ol> <li><strong>Input</strong>: Sentence \(x =\) [START] He runs fast</li> <li><strong>Output space</strong>: All possible tag sequences of same length</li> <li><strong>For each sequence \(y\)</strong>: <ul> <li>Break it into local pairs: \((y_{i-1}, y_i)\)</li> <li>Construct local features \(\Psi_i(x, y_{i-1}, y_i)\)</li> <li>Compute local scores and sum them</li> </ul> </li> <li><strong>Choose</strong> the sequence \(y\) with highest score \(h(x, y)\)</li> </ol> <p>We’ve now built a clear understanding of how compatibility scores work in structured prediction—by combining <strong>decomposable local features</strong> across a sequence. This formulation helps capture both local label associations and dependencies between adjacent labels.</p> <p>In the <strong>next section</strong>, we’ll dive into <strong>Structured Perceptron</strong> and <strong>Structured SVMs</strong>, where we learn how to train these models using mistake-driven updates and margin-based losses.</p> <p>Take care!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.]]></summary></entry><entry><title type="html">Multiclass Classification with SVM</title><link href="https://monishver11.github.io/blog/2025/multiclass-svm/" rel="alternate" type="text/html" title="Multiclass Classification with SVM"/><published>2025-04-13T02:31:00+00:00</published><updated>2025-04-13T02:31:00+00:00</updated><id>https://monishver11.github.io/blog/2025/multiclass-svm</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/multiclass-svm/"><![CDATA[<p>Support Vector Machines (SVMs) are widely used for binary classification, but how do we extend them to multiclass problems? This post dives into the <strong>generalization of SVMs to multiclass settings</strong>, focusing on deriving the loss function intuitively and mathematically.</p> <p><strong>Note:</strong> We’ve already covered SVMs in detail across multiple blog posts, so if any part of the SVM-related content here feels unclear, I highly recommend revisiting those earlier discussions. You can find the full list of all related blogs <a href="https://monishver11.github.io/blog/category/ml-nyu/">here</a>.</p> <hr/> <h5 id="from-binary-to-multiclass-revisiting-the-margin"><strong>From Binary to Multiclass: Revisiting the Margin</strong></h5> <p>For binary classification, the margin for a given training example \((\mathbf{x}^{(n)}, y^{(n)})\) is defined as:</p> \[y^{(n)} \mathbf{w}^\top \mathbf{x}^{(n)}\] <p>Here, we want this margin to be <strong>large and positive</strong>, meaning the classifier confidently assigns the correct label — i.e., the sign of \(\mathbf{w}^\top \mathbf{x}^{(n)}\) matches \(y^{(n)}\).</p> <p>In the <strong>multiclass setting</strong>, instead of a single weight vector \(\mathbf{w}\), we associate each class \(y \in \mathcal{Y}\) with a class-specific score function \(h(\mathbf{x}, y)\). The margin becomes a difference of scores:</p> \[h(\mathbf{x}^{(n)}, y^{(n)}) - h(\mathbf{x}^{(n)}, y)\] <p>This represents how much more confident the model is in the correct class over an incorrect one. Again, we want this margin to be large and positive for all \(y \neq y^{(n)}\).</p> <h5 id="multiclass-svm-the-separable-case"><strong>Multiclass SVM: The Separable Case</strong></h5> <p>Let’s build intuition by recalling the <strong>hard-margin binary SVM</strong>:</p> <p><strong>Binary Objective:</strong></p> \[\min_{\mathbf{w}} \ \frac{1}{2} \|\mathbf{w}\|^2\] <p>Subject to:</p> \[y^{(n)} \mathbf{w}^\top \mathbf{x}^{(n)} \geq 1 \quad \forall (\mathbf{x}^{(n)}, y^{(n)}) \in \mathcal{D}\] <p>Now, for the <strong>multiclass case</strong>, we again aim for a margin of at least 1, but now between the correct class and every other class:</p> <p>Define the <strong>margin</strong> for each incorrect class \(y\) as:</p> \[m_{n,y}(\mathbf{w}) \overset{\text{def}}{=} \langle \mathbf{w}, \Psi(\mathbf{x}^{(n)}, y^{(n)}) \rangle - \langle \mathbf{w}, \Psi(\mathbf{x}^{(n)}, y) \rangle\] <p>Then, the optimization becomes:</p> <p><strong>Multiclass Objective:</strong></p> \[\min_{\mathbf{w}} \ \frac{1}{2} \|\mathbf{w}\|^2\] <p>Subject to:</p> \[m_{n,y}(\mathbf{w}) \geq 1 \quad \forall (\mathbf{x}^{(n)}, y^{(n)}) \in \mathcal{D}, \quad y \neq y^{(n)}\] <p>This ensures the score of the correct class exceeds the score of every other class by at least 1.</p> <p><strong>Here’s a way to visualize it:</strong></p> <p>In contrast to the binary case, where the margin measures alignment between the weight vector and the input (modulated by the true label), the multiclass margin compares the model’s score for the correct label against its score for any incorrect label. This pairwise difference captures how much more the model “prefers” the correct class over a specific incorrect one. Intuitively, it’s no longer about pushing a single decision boundary away from the origin, but about ensuring the score of the true class is separated from all others by a margin — effectively enforcing a set of inequalities that distinguish the correct class from each competitor. This formulation naturally generalizes the binary margin while preserving the core idea: confident, separable predictions.</p> <blockquote> <p>Think of it like a race where the correct class isn’t just expected to win — it must outpace every other class by a clear stride. It’s not enough to cross the finish line first; it has to do so with a visible lead. The margin enforces this separation, ensuring the model’s predictions are not just accurate, but decisively confident.</p> </blockquote> <h5 id="from-hard-margins-to-hinge-loss"><strong>From Hard Margins to Hinge Loss</strong></h5> <p>In practice, perfect separation may not be possible, especially in noisy datasets or complex decision boundaries. To handle these scenarios, we shift to a <strong>soft-margin approach</strong> using <strong>hinge loss</strong>, which allows for some misclassification while still enforcing the desired separation between classes.</p> <p><strong>Binary Hinge Loss Recap:</strong></p> <p>Hinge loss provides a <strong>convex upper bound</strong> on the 0-1 loss, which is commonly used in classification tasks. The binary hinge loss is defined as:</p> \[\ell_{\text{hinge}}(y, \hat{y}) = \max(0, 1 - y h(\mathbf{x}))\] <p>Here, \(y\) is the true label, \(h(\mathbf{x})\) is the score or decision function (typically \(h(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}\) for a linear classifier), and \(\hat{y}\) is the predicted label. This loss function penalizes predictions that are either <strong>incorrect</strong> (when \(y \neq \hat{y}\)) or <strong>too close</strong> to the decision boundary (when \(\vert 1 - y h(\mathbf{x}) \vert\) is small).</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Classification_Losses-480.webp 480w,/assets/img/Classification_Losses-800.webp 800w,/assets/img/Classification_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Classification_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classification_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="generalized-hinge-loss-for-multiclass-classification"><strong>Generalized Hinge Loss for Multiclass Classification</strong></h5> <p>In the case of <strong>multiclass classification</strong>, the situation is more complex because we are dealing with more than two possible classes. In this setting, the goal is to ensure that the classifier correctly ranks the true class higher than the others, with a margin that is as large as possible.</p> <p>Let’s define the <strong>multiclass 0-1 loss</strong> as:</p> \[\Delta(y, y') = \mathbb{I}[y \neq y']\] <p>Where \(\mathbb{I}[\cdot]\) is the <strong>indicator function</strong>, which is 1 if \(y \neq y'\) and 0 if \(y = y'\). More generally, \(\Delta\) can encode <strong>different misclassification costs</strong>, allowing for a more flexible model.</p> <p>The model’s predicted class \(\hat{y}\) is the one that maximizes the score function \(h(\mathbf{x}, y)\) across all classes:</p> \[\hat{y} \overset{\text{def}}{=} \arg\max_{y' \in \mathcal{Y}} \langle \mathbf{w}, \Psi(\mathbf{x}, y') \rangle\] <p>Here, \(\Psi(\mathbf{x}, y')\) represents the feature transformation for class \(y'\) and input \(\mathbf{x}\), and \(\mathcal{Y}\) is the set of all possible classes.</p> <p>For a correct prediction, we want:</p> \[\langle \mathbf{w}, \Psi(\mathbf{x}, y) \rangle \geq \langle \mathbf{w}, \Psi(\mathbf{x}, \hat{y}) \rangle\] <p>Where \(y\) is the true class. However, if this condition is violated and the classifier incorrectly chooses an alternative class, we need to quantify the degree of misclassification using the hinge loss.</p> <p>To upper-bound the 0-1 loss, we use the following inequality:</p> \[\Delta(y, \hat{y}) \leq \Delta(y, \hat{y}) - \langle \mathbf{w}, \Psi(\mathbf{x}, y) - \Psi(\mathbf{x}, \hat{y}) \rangle\] <p>This may seem a bit abstract at first, but here’s the intuition: if the model predicts \(\hat{y} \neq y\), then the 0-1 loss is 1, and we want to <strong>penalize the model based on how poorly it ranked the true class</strong>. The inner product difference quantifies how much higher the model scores the incorrect class \(\hat{y}\) over the true class \(y\). By subtracting this difference from the misclassification cost \(\Delta(y, \hat{y})\), we effectively create a <strong>margin-based upper bound</strong> on the discrete loss.</p> <p>This turns the hard-to-optimize, non-differentiable 0-1 loss into a <strong>continuous, convex surrogate</strong> that encourages the correct class to score higher than incorrect ones — making it suitable for optimization using gradient-based methods.</p> <p><strong>Example to Internalize the Intuition:</strong></p> <p>Suppose you’re classifying handwritten digits, and the true label is \(y = 3\). The model incorrectly predicts \(\hat{y} = 8\) with the following scores:</p> <ul> <li>Score for class 3: 7.2</li> <li>Score for class 8: 7.8</li> </ul> <p>So the model prefers class 8 over the correct class 3. The inner product difference is:</p> \[\langle \mathbf{w}, \Psi(\mathbf{x}, 3) - \Psi(\mathbf{x}, 8) \rangle = 7.2 - 7.8 = -0.6\] <p>Assuming a standard 0-1 loss with \(\Delta(3, 8) = 1\), the hinge loss upper bound becomes:</p> \[1 - (-0.6) = 1.6\] <p>The model is penalized more than 1 because it was <strong>confidently wrong</strong>.</p> <p>Now consider a less confident mistake:</p> <ul> <li>Score for class 3: 7.2</li> <li>Score for class 8: 7.1</li> </ul> <p>Then,</p> \[\langle \mathbf{w}, \Psi(\mathbf{x}, 3) - \Psi(\mathbf{x}, 8) \rangle = 7.2 - 7.1 = 0.1\] <p>and the hinge loss becomes:</p> \[1 - 0.1 = 0.9\] <p>Still incorrect, but the model is <strong>barely wrong</strong>, so the penalty is smaller. This example illustrates how the hinge loss upper bound captures <em>how wrong</em> a prediction is — not just whether it’s wrong.</p> <p>Finally with this, we arrive at the <strong>generalized multiclass hinge loss</strong>:</p> \[\ell_{\text{hinge}}(y, \mathbf{x}, \mathbf{w}) \overset{\text{def}}{=} \max_{y' \in \mathcal{Y}} \left[ \Delta(y, y') - \langle \mathbf{w}, \Psi(\mathbf{x}, y) - \Psi(\mathbf{x}, y') \rangle \right]\] <p>This loss function is designed to <strong>penalize misclassifications</strong> while ensuring that the true class \(y\) is clearly separated from all other classes \(y'\). It is <strong>zero</strong> if the margin between the true class and all other classes exceeds the corresponding cost, meaning the prediction is both <strong>correct</strong> and <strong>confident</strong>. In cases where the true class is not sufficiently separated from an incorrect class, the hinge loss penalizes the model, pushing it to improve the separation.</p> <p>This approach, by enforcing a margin between classes, ensures that the classifier learns to correctly distinguish between classes in a way that is robust to small errors or noise in the training data. The key difference from the binary case is that we now have to consider <strong>pairwise comparisons</strong> between the true class and each possible alternative class, making the task of learning the optimal decision boundaries more complex, but also more flexible.</p> <h5 id="final-objective-multiclass-svm-with-hinge-loss"><strong>Final Objective: Multiclass SVM with Hinge Loss</strong></h5> <p>To put everything together, we incorporate the generalized multiclass hinge loss into a regularized optimization framework — just like in the binary SVM case. The goal is to not only minimize classification errors but also control the model complexity to avoid overfitting.</p> <p>We achieve this by combining the hinge loss with an \(L_2\) regularization term. The resulting objective is:</p> <p><strong>Multiclass SVM Objective:</strong></p> \[\min_{\mathbf{w} \in \mathbb{R}^d} \ \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{n=1}^{N} \max_{y' \in \mathcal{Y}} \left[ \Delta(y^{(n)}, y') - \langle \mathbf{w}, \Psi(\mathbf{x}^{(n)}, y^{(n)}) - \Psi(\mathbf{x}^{(n)}, y') \rangle \right]\] <p>Here:</p> <ul> <li>The <strong>first term</strong>, \(\frac{1}{2} \|\mathbf{w}\|^2\), is the regularization term. It encourages the model to find a weight vector with small norm, promoting simpler decision boundaries and better generalization.</li> <li>The <strong>second term</strong> accumulates the hinge loss over all training examples, penalizing violations of the desired margins based on the cost function \(\Delta(y^{(n)}, y')\).</li> </ul> <p>Each training example contributes to the loss <strong>only if</strong> the margin between the correct class \(y^{(n)}\) and some incorrect class \(y'\) is <strong>less than</strong> the cost \(\Delta(y^{(n)}, y')\). If all margins exceed their respective costs, the hinge loss evaluates to zero for that example — meaning it’s classified correctly and confidently, with no penalty incurred.</p> <p>This objective function forms the basis of many structured and multiclass prediction models, striking a balance between fitting the training data and maintaining a margin-based decision boundary that generalizes well.</p> <h5 id="what-comes-next-optimization"><strong>What Comes Next: Optimization</strong></h5> <p>Now that we have a continuous, convex surrogate loss, the next step is to <strong>optimize the objective</strong>. This typically involves minimizing the regularized hinge loss using gradient-based methods or specialized algorithms like <strong>stochastic subgradient descent</strong>. The form of the loss allows us to compute subgradients efficiently, even though the max operator introduces non-smoothness. By iteratively updating the weights to reduce the loss, we arrive at a classifier that balances <strong>margin maximization</strong> with <strong>error minimization</strong>, and generalizes well to unseen data.</p> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>Multiclass SVM loss builds on binary SVMs by comparing the score of the correct label with that of every other label, incorporating misclassification costs, and optimizing using hinge loss. The formulation retains interpretability and extends the margin-based principle elegantly into the multiclass realm.</p> <p>Stay tuned for a follow-up post on <strong>implementing multiclass SVMs using structured prediction techniques</strong>!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Lectures/09.multiclass.pdf">Professor David S. Rosenberg’s Mutliclass Lecture Slides</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.]]></summary></entry><entry><title type="html">Multiclass Logistic Regression &amp;amp; Multiclass Perceptron Algorithm</title><link href="https://monishver11.github.io/blog/2025/multiclass-loss/" rel="alternate" type="text/html" title="Multiclass Logistic Regression &amp;amp; Multiclass Perceptron Algorithm"/><published>2025-04-12T00:32:00+00:00</published><updated>2025-04-12T00:32:00+00:00</updated><id>https://monishver11.github.io/blog/2025/multiclass-loss</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/multiclass-loss/"><![CDATA[<p>In real-world machine learning problems, we often need to classify data into multiple categories, not just two. While binary classification is a fundamental building block, it’s crucial to understand how we can extend these ideas to handle multiple classes. This transition from binary to multiclass classification is what we’ll explore in this blog. We’ll start by revisiting <strong>binary logistic regression</strong>, then step into <strong>multiclass logistic regression</strong>, and finally discuss how we can generalize algorithms like the perceptron for multiclass classification.</p> <hr/> <h5 id="binary-logistic-regression-recap"><strong>Binary Logistic Regression Recap</strong></h5> <p>Let’s begin with the most basic form of classification: binary logistic regression.</p> <p>Given an input \(x\), our goal is to predict whether it belongs to class 1 or class 0. The function we use for binary classification is called the <strong>sigmoid function</strong>, which outputs a probability between 0 and 1:</p> \[f(x) = \sigma(z) = \frac{1}{1 + \exp(-z)} = \frac{1}{1 + \exp(-w^\top x - b)} \quad (1)\] <p>The output \(f(x)\) represents the probability of class 1. The probability of the other class (class 0) is simply:</p> \[1 - f(x) = \frac{\exp(-w^\top x - b)}{1 + \exp(-w^\top x - b)} = \frac{1}{1 + \exp(w^\top x + b)} = \sigma(-z) \quad (2)\] <p>Another way to think about this is that one class corresponds to the parameters \(w\) and \(b\), while the other class corresponds to the parameters \(-w\) and \(-b\). This helps set the foundation for extending this concept to multiple classes.</p> <hr/> <h5 id="extending-to-multiclass-logistic-regression"><strong>Extending to Multiclass Logistic Regression</strong></h5> <p>Now that we have a solid understanding of binary logistic regression, let’s consider the case where we have more than two classes. This is where <strong>multiclass logistic regression</strong> comes in. For each class \(c\), we assign a weight vector \(w_c\) and a bias \(b_c\). The probability of belonging to class \(c\) given an input \(x\) is computed using the <strong>softmax function</strong>:</p> \[f_c(x) = \frac{\exp(w_c^\top x + b_c)}{\sum_{c'} \exp(w_{c'}^\top x + b_{c'})} \quad (3)\] <p>This formulation, known as <strong>softmax regression</strong>, allows us to calculate the probability for each class and select the class with the highest probability.</p> <h5 id="the-loss-function"><strong>The Loss Function</strong></h5> <p>To train the model, we use a <strong>cross-entropy loss</strong> function, which measures how well the model’s predicted probabilities match the true labels. Given a dataset \(\{(x^{(i)}, y^{(i)})\}\), the loss is defined as:</p> \[L = \sum_i -\log f_{y^{(i)}}(x^{(i)})\] <p>This loss function encourages the model to assign higher probabilities to the correct class. The gradient of the loss with respect to the pre-activation (logits) is:</p> \[\frac{\partial L}{\partial z} = f - y\] <p><strong>Derivation:</strong></p> <p>Assume the true class is \(k = y^{(i)}\). The loss for this example is:</p> \[\ell^{(i)} = -\log f_k^{(i)}\] <p>Substituting in the softmax definition:</p> \[\ell^{(i)} = -\log\left( \frac{\exp(z_k^{(i)})}{\sum_{j=1}^C \exp(z_j^{(i)})} \right) = -z_k^{(i)} + \log \left( \sum_{j=1}^C \exp(z_j^{(i)}) \right)\] <p>We differentiate the loss \(\ell^{(i)}\) with respect to each logit \(z_c^{(i)}\). There are two cases:</p> <ul> <li><strong>Case 1: \(c = k\) (the correct class)</strong></li> </ul> \[\frac{\partial \ell^{(i)}}{\partial z_k^{(i)}} = -1 + \frac{\exp(z_k^{(i)})}{\sum_{j=1}^C \exp(z_j^{(i)})} = f_k^{(i)} - 1\] <ul> <li><strong>Case 2: \(c \ne k\)</strong></li> </ul> \[\frac{\partial \ell^{(i)}}{\partial z_c^{(i)}} = \frac{\exp(z_c^{(i)})}{\sum_{j=1}^C \exp(z_j^{(i)})} = f_c^{(i)}\] <p>We can express both cases together using the one-hot encoded label vector \(y^{(i)}\):</p> \[\frac{\partial \ell^{(i)}}{\partial z^{(i)}} = f^{(i)} - y^{(i)}\] <p>Now, let \(f\) and \(y\) now represent the matrices of predicted probabilities and one-hot labels over the entire dataset. Then the total loss is:</p> \[L = \sum_{i=1}^N \ell^{(i)} = -\sum_{i=1}^N \log f_{y^{(i)}}^{(i)}\] <p>By stacking all gradients, the overall gradient of the loss with respect to the logits becomes:</p> \[\frac{\partial L}{\partial z} = f - y\] <p>This fully vectorized form allows efficient implementation and is similar to the gradient descent update used in binary logistic regression but generalized to multiple classes.</p> <hr/> <h5 id="quick-comparison-to-one-vs-all-ova-approach"><strong>Quick Comparison to One-vs-All (OvA) Approach</strong></h5> <p>In many multiclass problems, instead of learning a separate model for each class, we can use the <strong>One-vs-All (OvA)</strong> strategy. In OvA, we train a binary classifier for each class, where the classifier tries to distinguish one class from all others. The base hypothesis space in this case is:</p> \[\mathcal{H} = \{ h: \mathcal{X} \to \mathbb{R} \} \quad \text{(score functions)}\] <p>For \(k\) classes, the <strong>multiclass hypothesis space</strong> is:</p> \[\mathcal{F} = \left\{ x \mapsto \arg\max_i h_i(x) \ \big| \ h_1, \ldots, h_k \in \mathcal{H} \right\}\] <p>Intuitively, each function \(h_i(x)\) scores how likely \(x\) belongs to class \(i\). During training, we want each classifier to output positive values for examples from its own class and negative values for examples from all other classes. At test time, the classifier that outputs the highest score determines the predicted class.</p> <hr/> <h5 id="multiclass-perceptron-generalizing-the-perceptron-algorithm"><strong>Multiclass Perceptron: Generalizing the Perceptron Algorithm</strong></h5> <p>The classic Perceptron algorithm is designed for binary classification, but it can be naturally extended to multiclass problems. In the multiclass setting, instead of a single weight vector, we maintain <strong>one weight vector per class</strong>.</p> <p>For each class \(i\), we define a <strong>linear scoring function</strong>:</p> \[h_i(x) = w_i^\top x, \quad w_i \in \mathbb{R}^d\] <p>Given an input \(x\), the model predicts the class with the highest score:</p> \[\hat{y} = \arg\max_{i} w_i^\top x\] <p>The algorithm proceeds iteratively, updating the weights when it makes a mistake:</p> <ol> <li><strong>Initialize</strong>: Set all weight vectors to zero, \(w_i = 0\) for all classes $i$.</li> <li>For \(T\) iterations over the training set: <ul> <li>For each training example \((x, y)\): <ul> <li>Predict the label: \(\hat{y} = \arg\max_{i} w_i^\top x\)</li> <li>If \(\hat{y} \neq y\) (i.e., the prediction is incorrect): <ul> <li><strong>Promote</strong> the correct class:<br/> \(w_y \leftarrow w_y + x\)</li> <li><strong>Demote</strong> the incorrect prediction: \(w_{\hat{y}} \leftarrow w_{\hat{y}} - x\)</li> </ul> </li> </ul> </li> </ul> </li> </ol> <p>This update increases the score for the true class and decreases the score for the incorrect one, helping the model learn to separate them better in future iterations.</p> <h5 id="rewrite-the-scoring-function"><strong>Rewrite the scoring function</strong></h5> <p>When the number of classes \(k\) is large, storing and updating \(k\) separate weight vectors can become computationally expensive. To address this, we can rewrite the scoring function in a more compact form using a <strong>shared weight vector</strong>.</p> <p>We define a <strong>joint feature map</strong> \(\psi(x, i)\) that combines both the input \(x\) and a class label \(i\). Then, the score for class \(i\) can be written as:</p> \[h_i(x) = w_i^\top x = w^\top \psi(x, i) \tag{4}\] <p>Now, instead of maintaining a separate \(w_i\) for each class, we use <strong>a single global weight vector</strong> \(w\) that interacts with \(\psi(x, i)\) to compute scores for all classes:</p> \[h(x, i) = w^\top \psi(x, i) \tag{5}\] <p>This transformation allows us to use a single weight vector for all classes, which significantly reduces memory usage and computational complexity.</p> <p><strong>Concrete Example</strong></p> <p>Let:</p> <ul> <li>Input vector \(x \in \mathbb{R}^2\), e.g.,</li> </ul> \[x = \begin{bmatrix} 1 \\ 2 \end{bmatrix}\] <ul> <li>Number of classes \(k = 3\)</li> </ul> <p>We define \(\psi(x, i)\) as a vector in \(\mathbb{R}^{2k}\) (i.e., 6 dimensions). It places \(x\) into the block corresponding to class \(i\) and zeros elsewhere.</p> <p>For example, for class \(i = 2\):</p> \[\psi(x, 2) = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 2 \\ 0 \\ 0 \\ \end{bmatrix}\] <p>Let \(w \in \mathbb{R}^6\) (since \(x \in \mathbb{R}^2\) and \(k = 3\)):</p> \[w = \begin{bmatrix} 0.5 \\ -1.0 \\ 0.2 \\ 0.3 \\ -0.4 \\ 1.0 \\ \end{bmatrix}\] <p>To compute the score for class 2:</p> \[h(x, 2) = w^\top \psi(x, 2)\] <p>Only the block for class 2 is active:</p> \[h(x, 2) = [0.2, 0.3]^\top \cdot [1, 2] = 0.2 \cdot 1 + 0.3 \cdot 2 = 0.8\] <p>We can now compute scores for all classes:</p> <ul> <li> <p>Class 1 uses block: \([0.5, -1.0]\)</p> \[h(x, 1) = 0.5 \cdot 1 + (-1.0) \cdot 2 = -1.5\] </li> <li> <p>Class 2: (already computed) \(0.8\)</p> </li> <li> <p>Class 3 uses block: \([-0.4, 1.0]\)</p> \[h(x, 3) = -0.4 \cdot 1 + 1.0 \cdot 2 = 1.6\] </li> </ul> <p>For final prediction, we select the class with the highest score:</p> \[\hat{y} = \arg\max_i h(x, i) = 3\] <p>So, for input \(x = [1, 2]\), the predicted class is <strong>3</strong>.</p> <p>And suppose the true label is:</p> \[y = 2\] <p>Since \(\hat{y} \ne y\), the prediction is incorrect.</p> <p>The classic multiclass Perceptron updates:</p> <ul> <li><strong>Promote</strong> the correct class (add input to the correct class block)</li> <li><strong>Demote</strong> the predicted class (subtract input from the predicted class block)</li> </ul> <p>Using the joint feature map:</p> \[w \leftarrow w + \psi(x, y) - \psi(x, \hat{y})\] <p>In our case:</p> <ul> <li> \[\psi(x, y) = \psi(x, 2) = [0, 0, 1, 2, 0, 0]^\top\] </li> <li> \[\psi(x, \hat{y}) = \psi(x, 3) = [0, 0, 0, 0, 1, 2]^\top\] </li> </ul> <p>Then:</p> \[w_{\text{new}} = w + \psi(x, 2) - \psi(x, 3)\] <p>Apply this update:</p> \[w = \begin{bmatrix} 0.5 \\ -1.0 \\ 0.2 \\ 0.3 \\ -0.4 \\ 1.0 \\ \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \\ 1 \\ 2 \\ 0 \\ 0 \\ \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 2 \\ \end{bmatrix} = \begin{bmatrix} 0.5 \\ -1.0 \\ 1.2 \\ 2.3 \\ -1.4 \\ -1.0 \\ \end{bmatrix}\] <p>This update increases the score for the correct class (2) and decreases the score for the incorrect prediction (3), just like the original multiclass Perceptron but using a single shared weight vector and structured feature representation.</p> <hr/> <h5 id="formalising-via-multivector-construction"><strong>Formalising via Multivector Construction</strong></h5> <p>Consider a simple example where \(x \in \mathbb{R}^2\) and we have 3 classes \(Y = \{1, 2, 3\}\). Suppose we stack the weight vectors for each class together in the following way:</p> \[w = \begin{pmatrix} -\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}, 0, 1, \frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2} \end{pmatrix}^\top\] <p>Now, define the feature map \(\Psi(x, y)\) as follows:</p> <ul> <li> \[\Psi(x, 1) = (x_1, x_2, 0, 0, 0, 0)\] </li> <li> \[\Psi(x, 2) = (0, 0, x_1, x_2, 0, 0)\] </li> <li> \[\Psi(x, 3) = (0, 0, 0, 0, x_1, x_2)\] </li> </ul> <p>The dot product between the weight vector \(w\) and the feature map \(\Psi(x, y)\) is then:</p> \[\langle w, \Psi(x, y) \rangle = \langle w_y, x \rangle\] <p>This approach allows us to represent all classes using a single weight vector, which is more efficient and scalable.</p> <p>With the multivector construction in place, the multiclass perceptron algorithm can be rewritten as follows:</p> <ol> <li><strong>Initialize</strong> the weight vector \(w = 0\).</li> <li>For \(T\) iterations, repeat the following for each training example \((x, y)\): <ul> <li>Predict \(\hat{y} = \arg\max_{y'} w^\top \psi(x, y')\) (choose the class with the highest score).</li> <li>If \(\hat{y} \neq y\): <ul> <li>Update the weight vector: \(w \leftarrow w + \psi(x, y)\).</li> <li>Update the weight vector: \(w \leftarrow w - \psi(x, \hat{y})\).</li> </ul> </li> </ul> </li> </ol> <p>This version of the algorithm is computationally efficient and scales well to large datasets.</p> <p><strong>Question</strong>: What is the <strong>base binary classification problem</strong> in multiclass perceptron?</p> <p><strong>Answer</strong>: At each update step, the multiclass Perceptron reduces to a binary classification problem between the correct class \(y\) and the predicted class \(\hat{y}\). The model must adjust the weights so that \(y\) scores higher than \(\hat{y}\) — just like in a binary classification setting where one class must be separated from another.</p> <hr/> <h5 id="feature-engineering-for-multiclass-tasks"><strong>Feature Engineering for Multiclass Tasks</strong></h5> <p>To apply the multivector construction in practice, we need to define meaningful and informative features that capture the relationship between the input and each possible class. This is especially important in structured prediction tasks like <strong>part-of-speech (POS) tagging</strong>.</p> <p>Suppose our input space \(X\) consists of all possible words, and our label space \(Y\) contains the categories {NOUN, VERB, ADJECTIVE, ADVERB, etc.}. Each input word needs to be classified into one of these grammatical categories.</p> <p>We can define features that depend on both the input word and the target label — a natural fit for the joint feature map \(\Psi(x, y)\) introduced earlier. For example, some useful features might include:</p> <ul> <li>Whether the word is exactly a specific token (e.g., “run”, “apple”)</li> <li>Whether the word ends in certain suffixes (e.g., “ly” for adverbs)</li> <li>Capitalization or presence of digits (in named entity recognition)</li> </ul> <p>Here are a few sample features written in the multivector style:</p> <ul> <li> \[\psi_1(x, y) = 1[x = \text{apple} \land y = \text{NOUN}]\] </li> <li> \[\psi_2(x, y) = 1[x = \text{run} \land y = \text{NOUN}]\] </li> <li> \[\psi_3(x, y) = 1[x = \text{run} \land y = \text{VERB}]\] </li> <li> \[\psi_4(x, y) = 1[x \text{ ends in } \text{ly} \land y = \text{ADVERB}]\] </li> </ul> <p>Each of these features “activates” only when both the input word and the predicted class match a certain pattern. This is perfectly aligned with the multivector framework, where the model learns weights for specific combinations of features and labels.</p> <h5 id="feature-templates"><strong>Feature Templates</strong></h5> <p>In real-world applications, especially in natural language processing (NLP), we rarely hand-code features for every word. Instead, we use <strong>feature templates</strong> that automatically generate features from observed patterns.</p> <p><strong>What is a Feature Template?</strong></p> <p>A feature template is a <strong>rule or function</strong> that, given an input and a label, produces one or more binary features of the form \(\psi(x, y)\).</p> <p>Templates help create thousands or even millions of features in a structured and consistent way.</p> <p>Let’s say we want to predict the POS tag for the word <strong>“running”</strong> in the sentence:</p> <blockquote> <p>I am <strong>running</strong> late.</p> </blockquote> <p>We might use the following templates:</p> <hr/> <table> <thead> <tr> <th>Template Description</th> <th>Template Rule</th> <th>Example Feature</th> </tr> </thead> <tbody> <tr> <td>Current word</td> <td>\(\psi(x, y) = 1[x = w \land y = y']\)</td> <td>\(x = \text{"running"}, y = \text{VERB}\)</td> </tr> <tr> <td>Word suffix (3 chars)</td> <td>\(\psi(x, y) = 1[x[-3:] = s \land y = y']\)</td> <td>\(x[-3:] = \text{"ing"}, y = \text{VERB}\)</td> </tr> <tr> <td>Previous word is “am”</td> <td>\(\psi(x, y) = 1[\text{prev}(x) = \text{"am"} \land y = y']\)</td> <td>\(y = \text{VERB}\)</td> </tr> <tr> <td>Is capitalized</td> <td>\(\psi(x, y) = 1[x[0].\text{isupper()} \land y = y']\)</td> <td>—</td> </tr> <tr> <td>Prefix (first 2 letters)</td> <td>\(\psi(x, y) = 1[x[:2] = p \land y = y']\)</td> <td>\(x[:2] = \text{"ru"}, y = \text{VERB}\)</td> </tr> </tbody> </table> <hr/> <p>Each of these templates would produce many feature instances across a dataset — and each instance activates only when the corresponding condition holds.</p> <p><strong>Integration with the Model</strong></p> <p>In the multivector model, we don’t store a giant feature matrix explicitly. Instead, we treat each <strong>feature-label pair</strong> \(\psi(x, y)\) as a key that can be mapped to an <strong>index</strong> in a long feature vector. This is done using either:</p> <ul> <li>A <strong>dictionary lookup</strong>, if we predefine all feature-label pairs, or</li> <li>A <strong>hash function</strong>, if we want to compute the index on the fly (common in online or large-scale settings)</li> </ul> <p><strong>Why is this needed?</strong></p> <p>When \(\psi(x, y)\) is represented as a very large sparse vector (e.g. size 100,000+), we don’t want to store all zeros. So instead, we store only the <strong>nonzero features</strong> — each one identified by its <strong>feature name and associated label</strong>.</p> <p>Say we define a feature template:</p> <ul> <li>“Does the word end with ‘ing’?”</li> </ul> <p>Then for the input word <strong>“running”</strong>, and possible labels:</p> <ul> <li> \[\psi(x = \text{running}, y = \text{VERB}) = 1[\text{suffix} = ing \land y = \text{VERB}]\] </li> <li> \[\psi(x = \text{running}, y = \text{NOUN}) = 1[\text{suffix} = ing \land y = \text{NOUN}]\] </li> </ul> <p>These are <strong>two different features</strong>, because they are tied to different labels.</p> <p>We can assign an index to each:</p> <table> <thead> <tr> <th>Feature Name</th> <th>Label</th> <th>Combined Key</th> <th>Index</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">suffix=ing</code></td> <td>VERB</td> <td><code class="language-plaintext highlighter-rouge">suffix=ing_VERB</code></td> <td>1921</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">suffix=ing</code></td> <td>NOUN</td> <td><code class="language-plaintext highlighter-rouge">suffix=ing_NOUN</code></td> <td>2390</td> </tr> </tbody> </table> <p>So \(\psi(x, y)\) is implemented as:</p> <ul> <li>A vector of size (say) 50,000,</li> <li>With a single non-zero at position 1921 or 2390, depending on the label,</li> <li>And the model’s weight vector \(w\) has learned weights at those positions.</li> <li> <p>During prediction, we compute:</p> \[\hat{y} = \arg\max_{y'} w^\top \psi(x, y')\] </li> </ul> <p>This is how the model can <strong>distinguish between “ing” being a verb signal vs a noun signal</strong>, just by associating label-specific versions of the feature. And this feature-to-index mapping is what makes it possible to use linear classifiers with sparse high-dimensional features efficiently.</p> <p><strong>So, Why Feature Templates Matter?</strong></p> <ul> <li>They <strong>automate</strong> feature construction and ensure consistency across training and test data.</li> <li>They <strong>generalize well</strong> — e.g., instead of memorizing that “running” is a verb, a suffix-based feature can generalize that any word ending in “ing” is likely a verb.</li> <li>They are <strong>language-agnostic</strong> to some extent — and can be extended to other structured tasks like NER, chunking, or even machine translation.</li> </ul> <p>This feature-based view, combined with the multivector construction, gives us a powerful and scalable way to build multiclass classifiers, especially in domains like NLP where feature engineering plays a key role.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>We covered how multiclass classification can be tackled using multiclass loss and perceptron algorithms. We highlighted the importance of feature engineering, specifically through feature templates, which help automatically create relevant features for each class. This approach enables efficient, scalable models, especially in tasks like POS tagging. By mapping feature-label pairs to indices, we can handle large datasets without excessive memory usage.</p> <p>Having seen how to generalize the perceptron algorithm, we’ll now move on to explore how <strong>Support Vector Machines (SVMs)</strong> can be extended to handle multiclass classification. Stay tuned and Take care!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.]]></summary></entry><entry><title type="html">Multiclass Classification - Overview</title><link href="https://monishver11.github.io/blog/2025/multiclass/" rel="alternate" type="text/html" title="Multiclass Classification - Overview"/><published>2025-02-23T02:15:00+00:00</published><updated>2025-02-23T02:15:00+00:00</updated><id>https://monishver11.github.io/blog/2025/multiclass</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/multiclass/"><![CDATA[<h4 id="motivation"><strong>Motivation</strong></h4> <p>So far, most of the classification algorithms we have encountered focus on <strong>binary classification</strong>, where the goal is to distinguish between two classes. For instance, sentiment analysis classifies text as either <strong>positive or negative</strong>, while spam filters differentiate between <strong>spam and non-spam</strong> emails.</p> <p>However, real-world problems often involve more than two categories, making binary classification insufficient. <strong>Document classification</strong> may require labeling articles into over <strong>ten</strong> categories, <strong>object recognition</strong> must identify objects among <strong>thousands of classes</strong>, and <strong>face recognition</strong> must distinguish between <strong>millions</strong> of individuals.</p> <p>As the number of classes grows, several challenges arise. <strong>Computation cost</strong> increases significantly, especially when training separate models for each class. Additionally, some classes might have far fewer examples than others, leading to <strong>class imbalance</strong> issues. Finally, in some cases, <strong>different types of errors</strong> have varying consequences—for example, misidentifying a handwritten “3” as an “8” might be less problematic than confusing a stop sign with a yield sign in an autonomous driving system.</p> <p>Given these challenges, we need effective strategies to extend binary classification techniques to handle multiple classes efficiently.</p> <hr/> <h4 id="reducing-multiclass-to-binary-classification"><strong>Reducing Multiclass to Binary Classification</strong></h4> <p>A natural way to approach multiclass classification is by reducing it to <strong>multiple binary classification problems</strong>. One naive approach is to represent each class using a unique binary code and train a classifier to predict the code (000, 001, 010). Another approach is to apply <strong>regression</strong>, where each class is assigned a numerical value (1.0, 2.0, 3.0), and the model predicts a continuous number that is rounded to the nearest class. However, these methods often fail in practice due to poor generalization.</p> <p>Instead, two well-established techniques are commonly used:</p> <ol> <li><strong>One-vs-All (OvA), also called One-vs-Rest</strong></li> <li><strong>All-vs-All (AvA), also known as One-vs-One</strong></li> </ol> <p>These methods decompose the problem into smaller binary classification tasks while ensuring that the final model can still distinguish between all classes.</p> <hr/> <h4 id="one-vs-all-ova"><strong>One-vs-All (OvA)</strong></h4> <p>The <strong>One-vs-All</strong> (OvA) approach works by training <strong>one binary classifier per class</strong>. Each classifier is responsible for distinguishing a single class from all the others.</p> <h5 id="training"><strong>Training</strong></h5> <p>Given a dataset with \(k\) classes, we train \(k\) separate classifiers:</p> <ul> <li>Each classifier \(h_i\) learns to recognize class \(i\) as <strong>positive (+1)</strong> while treating all other classes as <strong>negative (-1)</strong>.</li> <li>Formally, each classifier is a function \(h_i: X \to \mathbb{R}\), where a higher score indicates a higher likelihood of belonging to class \(i\).</li> </ul> <h5 id="prediction"><strong>Prediction</strong></h5> <p>When a new input \(x\) is given, we evaluate all \(k\) classifiers and select the class with the highest score:</p> \[h(x) = \arg\max_{i \in \{1, \dots, k\}} h_i(x)\] <p>If multiple classifiers output the same score, we can resolve ties arbitrarily.</p> <h5 id="example-3-class-problem"><strong>Example: 3-Class Problem</strong></h5> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multiclass-1-480.webp 480w,/assets/img/Multiclass-1-800.webp 800w,/assets/img/Multiclass-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multiclass-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multiclass-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Consider a classification task with three categories: <strong>cats, dogs, and rabbits</strong>. Using OvA, we train three classifiers:</p> <ol> <li>A classifier that distinguishes <strong>cats</strong> from <strong>dogs and rabbits</strong>.</li> <li>A classifier that distinguishes <strong>dogs</strong> from <strong>cats and rabbits</strong>.</li> <li>A classifier that distinguishes <strong>rabbits</strong> from <strong>cats and dogs</strong>.</li> </ol> <p>At test time, each classifier produces a score, and the class with the highest score is selected.</p> <p>However, this method has some limitations. If the data is <strong>not linearly separable</strong>, the decision boundaries can become ambiguous. Additionally, if one class has far fewer examples than others, the classifier for that class might be undertrained, leading to <strong>class imbalance issues</strong>.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multiclass-2-480.webp 480w,/assets/img/Multiclass-2-800.webp 800w,/assets/img/Multiclass-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multiclass-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multiclass-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h4 id="all-vs-all-ava"><strong>All-vs-All (AvA)</strong></h4> <p>A more refined approach is <strong>All-vs-All (AvA)</strong>, also known as <strong>One-vs-One</strong>. Instead of training one classifier per class, we train a <strong>separate classifier for each pair of classes</strong>.</p> <h5 id="training-1"><strong>Training</strong></h5> <p>For a dataset with \(k\) classes, we train \(\frac{k(k-1)}{2}\) binary classifiers:</p> <ul> <li>Each classifier \(h_{ij}\) is trained to distinguish class \(i\) from class \(j\).</li> <li>Formally, each classifier \(h_{ij}: X \to \mathbb{R}\) outputs a score, where a positive value indicates class \(i\), and a negative value indicates class \(j\).</li> </ul> <h5 id="prediction-1"><strong>Prediction</strong></h5> <p>At test time, each classifier makes a decision between its assigned two classes. Each class receives a <strong>vote</strong> based on the number of times it wins in pairwise comparisons. The final prediction is the class with the most votes:</p> \[h(x) = \arg\max_{i \in \{1, \dots, k\}} \sum_{j \ne i} [ \underbrace{h_{ij}(x)\mathbb{I}\{i &lt; j\}}_{\text{class } i \text{ is } +1} - \underbrace{h_{ji}(x)\mathbb{I}\{j &lt; i\}}_{\text{class } i \text{ is } -1} ]\] <p>Again, in scenarios where multiple classes receive the same number of votes, a tournament-style approach can be used to break ties. Here, classes compete in a series of pairwise matchups, and the winner of each round advances until only one class remains—the final prediction.</p> <h5 id="example-4-class-problem"><strong>Example: 4-Class Problem</strong></h5> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multiclass-3-480.webp 480w,/assets/img/Multiclass-3-800.webp 800w,/assets/img/Multiclass-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multiclass-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multiclass-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Suppose we have four classes. Using AvA, we train six classifiers:</p> <ol> <li>A classifier for <strong>cats vs. dogs</strong></li> <li>A classifier for <strong>cats vs. rabbits</strong></li> <li>A classifier for <strong>cats vs. birds</strong></li> <li>A classifier for <strong>dogs vs. rabbits</strong></li> <li>A classifier for <strong>dogs vs. birds</strong></li> <li>A classifier for <strong>rabbits vs. birds</strong></li> </ol> <p>At test time, each classifier votes for a class, and the class with the most votes is chosen as the final prediction.</p> <p>This method has <strong>better decision boundaries</strong> than OvA because it learns finer distinctions between pairs of classes. However, it is <strong>computationally expensive</strong> for large \(k\), as the number of classifiers grows quadratically.</p> <hr/> <h4 id="ova-vs-ava-trade-offs"><strong>OvA vs. AvA: Trade-offs</strong></h4> <p>Both approaches have their own advantages and limitations. In general:</p> <ul> <li><strong>OvA is simpler</strong> and requires fewer models, but it suffers from <strong>class imbalance issues</strong>.</li> <li><strong>AvA provides better decision boundaries</strong> but is <strong>computationally expensive</strong>, especially for large numbers of classes.</li> </ul> <p>The following table summarizes the computational complexity of both methods:</p> <hr/> <table> <thead> <tr> <th> </th> <th><strong>OvA</strong></th> <th><strong>AvA</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Training Complexity</strong></td> <td>\(O(k B_{\text{train}}(n))\)</td> <td>\(O(k^2 B_{\text{train}}(n/k))\)</td> </tr> <tr> <td><strong>Testing Complexity</strong></td> <td>\(O(k B_{\text{test}})\)</td> <td>\(O(k^2 B_{\text{test}})\)</td> </tr> <tr> <td><strong>Challenges</strong></td> <td>Class imbalance, poor calibration</td> <td>Small training sets, tie-breaking</td> </tr> </tbody> </table> <hr/> <ul> <li> <p><strong>\(k\)</strong>: The <strong>number of classes</strong> in the multiclass classification problem.</p> </li> <li> <p><strong>\(B_{\text{train}}(n)\)</strong>: The <strong>computational cost of training</strong> a binary classifier on <strong>\(n\)</strong> training examples.</p> </li> <li> <p><strong>\(B_{\text{test}}\)</strong>: The <strong>computational cost of testing</strong> a single input using one binary classifier.</p> </li> <li>For <strong>One-vs-All (OvA)</strong>: <ul> <li><strong>Training</strong>: <ul> <li>Train \(k\) classifiers, each on the <strong>entire dataset</strong> with \(n\) examples.</li> <li><strong>Total training cost</strong>: \(O(k \cdot B_{\text{train}}(n))\)</li> </ul> </li> <li><strong>Testing</strong>: <ul> <li>For each new input, all \(k\) classifiers are evaluated.</li> <li><strong>Total testing cost per input</strong>: \(O(k \cdot B_{\text{test}})\)</li> </ul> </li> </ul> </li> <li>For <strong>All-vs-All (AvA)</strong>: <ul> <li><strong>Training</strong>: <ul> <li>Train \(\frac{k(k-1)}{2} \approx O(k^2)\) classifiers.</li> <li>Each classifier uses data from only <strong>two classes</strong>, approximately \(n/k\) examples.</li> <li><strong>Total training cost</strong>: \(O(k^2 \cdot B_{\text{train}}(n/k))\)</li> </ul> </li> <li><strong>Testing</strong>: <ul> <li>For each new input, all \(O(k^2)\) classifiers are evaluated.</li> <li><strong>Total testing cost per input</strong>: \(O(k^2 \cdot B_{\text{test}})\)</li> </ul> </li> </ul> </li> </ul> <p>While these reduction-based approaches work well for <strong>small numbers of classes</strong>, they become impractical when scaling to large datasets.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>We explored two fundamental approaches for handling multiclass classification by <strong>reducing it to multiple binary classification problems</strong>:</p> <ul> <li><strong>One-vs-All (OvA)</strong>: Train one classifier per class.</li> <li><strong>All-vs-All (AvA)</strong>: Train one classifier per pair of classes.</li> </ul> <p>Although these methods are simple and effective for small datasets, they become computationally expensive as the number of classes grows. For example, <strong>ImageNet contains over 20,000 categories, and Wikipedia has over 1 million topics</strong>, making reduction-based methods infeasible.</p> <h5 id="whats-next"><strong>What’s Next?</strong></h5> <p>To overcome these challenges, we need classification algorithms that <strong>directly generalize binary classification to multiple classes</strong> without breaking the problem into smaller binary tasks. In the next post, we’ll explore these approaches and how they scale efficiently to large datasets. Stay tuned and See you! 👋</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.]]></summary></entry><entry><title type="html">Gaussian Regression - A Bayesian Approach to Linear Regression</title><link href="https://monishver11.github.io/blog/2025/gaussian-regression/" rel="alternate" type="text/html" title="Gaussian Regression - A Bayesian Approach to Linear Regression"/><published>2025-02-08T16:59:00+00:00</published><updated>2025-02-08T16:59:00+00:00</updated><id>https://monishver11.github.io/blog/2025/gaussian-regression</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/gaussian-regression/"><![CDATA[<p>Gaussian regression provides a <strong>probabilistic perspective</strong> on linear regression, enabling us to <strong>model uncertainty</strong> in our predictions. Unlike traditional regression techniques that provide only point estimates, Gaussian regression models the <strong>entire distribution</strong> over possible predictions.</p> <p>In this post, we will explore Gaussian regression through a <strong>step-by-step example in one dimension</strong>, gradually building intuition before moving to the <strong>Bayesian posterior update</strong> with multiple observations.</p> <hr/> <h5 id="gaussian-regression-in-one-dimension-problem-setup"><strong>Gaussian Regression in One Dimension: Problem Setup</strong></h5> <p>To understand Gaussian regression, let’s consider a simple <strong>one-dimensional regression task</strong>. Suppose our <strong>input space</strong> is:</p> \[X = [-1,1]\] <p>and the <strong>output space</strong> is:</p> \[Y \subseteq \mathbb{R}\] <p>We assume that for a given input \(x\), the corresponding output \(y\) is generated according to a <strong>linear model</strong> with additive Gaussian noise:</p> \[y = w_0 + w_1 x + \varepsilon, \quad \text{where} \quad \varepsilon \sim \mathcal{N}(0, 0.2^2)\] <p>In other words, the output is a <strong>linear function</strong> of \(x\) with coefficients \((w_0, w_1)\) and <strong>Gaussian noise</strong> \(\varepsilon\) with variance \(0.2^2\).</p> <p>Alternatively, we can express this in <strong>probabilistic form</strong> as:</p> \[y \mid x, w_0, w_1 \sim \mathcal{N}(w_0 + w_1 x, 0.2^2)\] <p>where the mean is given by the <strong>linear function</strong> \(w_0 + w_1 x\), and the variance captures the <strong>uncertainty in our observations</strong>.</p> <p>Now, the question is: <strong>how do we model our belief about the parameters \(w_0\) and \(w_1\)?</strong></p> <hr/> <h5 id="prior-distribution-over-parameters"><strong>Prior Distribution Over Parameters</strong></h5> <p>Before observing any data, we assume a <strong>prior belief</strong> about the parameters \(w = (w_0, w_1)\). A common choice is to model the parameters as a <strong>zero-mean Gaussian distribution</strong>:</p> \[w = (w_0, w_1) \sim \mathcal{N}(0, \frac{1}{2} I)\] <p>This prior reflects our initial assumption that \(w_0\) and \(w_1\) are likely to be small, centered around zero, with <strong>independent Gaussian uncertainty</strong>.</p> <h5 id="visualizing-the-prior"><strong>Visualizing the Prior</strong></h5> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Guassian_Regression-1-480.webp 480w,/assets/img/Guassian_Regression-1-800.webp 800w,/assets/img/Guassian_Regression-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Guassian_Regression-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Guassian_Regression-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The <strong>left plot</strong> below illustrates samples drawn from the prior distribution over \(w\).</li> <li>The <strong>right plot</strong> shows <strong>random linear functions</strong> \(y(x) = E[y \mid x, w] = w_0 + w_1 x\) drawn from this prior distribution \((w ∼ p(w))\).</li> </ul> <p>Since no observations have been made yet, these functions represent <strong>potential hypotheses</strong> about the true relationship between \(x\) and \(y\).</p> <hr/> <h5 id="updating-beliefs-posterior-after-one-observation"><strong>Updating Beliefs: Posterior After One Observation</strong></h5> <p>Once we collect data, we can update our belief about \(w\) using <strong>Bayes’ theorem</strong>. Suppose we observe a <strong>single training point</strong> \((x_1, y_1)\).</p> <p>This leads to a <strong>posterior distribution</strong> over \(w\), incorporating the evidence from our observation.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Guassian_Regression-2-480.webp 480w,/assets/img/Guassian_Regression-2-800.webp 800w,/assets/img/Guassian_Regression-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Guassian_Regression-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Guassian_Regression-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The <strong>left plot</strong> below shows the updated posterior distribution over \(w\), where the <strong>white cross</strong> represents the true underlying parameters.</li> <li>The <strong>right plot</strong> now shows <strong>updated predictions</strong> based on this posterior. Blue circle indicates the training observation. The red lines represent sampled functions \(y(x) = E[y \mid x, w]\) drawn from the <strong>posterior</strong> \((w ∼ p(w \vert D) )\) instead of the prior.</li> </ul> <h5 id="key-observations-after-one-data-point"><strong>Key Observations After One Data Point</strong></h5> <ol> <li>The <strong>posterior distribution is more concentrated</strong>, reflecting reduced uncertainty about \(w\).</li> <li>Predictions near the observed point \(x_1\) are more certain, while uncertainty remains high elsewhere.</li> </ol> <hr/> <h5 id="adding-more-observations-improved-predictions"><strong>Adding More Observations: Improved Predictions</strong></h5> <p>Let’s now extend this idea by considering multiple observations.</p> <p><strong>Posterior After Two Observations</strong></p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Guassian_Regression-3-480.webp 480w,/assets/img/Guassian_Regression-3-800.webp 800w,/assets/img/Guassian_Regression-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Guassian_Regression-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Guassian_Regression-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>With two training points, the posterior further <strong>shrinks</strong>, indicating <strong>increased confidence</strong> in our estimate of \(w\).</li> <li>The red curves on the right become more <strong>aligned</strong>, meaning the model has more confidence in its predictions.</li> </ul> <p><strong>Posterior After Twenty Observations</strong></p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Guassian_Regression-4-480.webp 480w,/assets/img/Guassian_Regression-4-800.webp 800w,/assets/img/Guassian_Regression-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Guassian_Regression-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Guassian_Regression-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>When we have <strong>twenty observations</strong>, our uncertainty about \(w\) significantly decreases.</li> <li>The predicted functions become <strong>highly concentrated</strong>, meaning the model has effectively <strong>learned the underlying relationship</strong> between \(x\) and \(y\).</li> </ul> <p>This progression illustrates a fundamental principle in Bayesian regression: <strong>as we collect more data, the posterior distribution sharpens, leading to more confident predictions.</strong></p> <hr/> <p>So, Gaussian regression provides a <strong>Bayesian approach to linear regression</strong>, allowing us to <strong>incorporate prior knowledge</strong> and <strong>update beliefs</strong> as new data arrives.</p> <p><strong>Key takeaways:</strong></p> <ol> <li>We started with a <strong>prior distribution</strong> over parameters \(w\).</li> <li>We incorporated <strong>one observation</strong>, updating our belief using the <strong>posterior distribution</strong>.</li> <li>As we collected <strong>more data</strong>, our predictions became <strong>more confident</strong>, reducing uncertainty in our model.</li> </ol> <p>This framework not only provides <strong>point predictions</strong> but also <strong>quantifies uncertainty</strong>, making it particularly useful for applications where knowing confidence levels is crucial.</p> <p>Next up, we will derive the <strong>closed-form posterior distribution</strong> for Gaussian regression and explore its connection to <strong>ridge regression</strong>.</p> <hr/> <h4 id="deriving-the-general-posterior-distribution-in-gaussian-regression"><strong>Deriving the General Posterior Distribution in Gaussian Regression</strong></h4> <p>We begin by defining the prior belief about our parameters. Suppose we have a <strong>prior distribution</strong> over the weights:</p> \[w \sim \mathcal{N}(0, \Sigma_0)\] <p>where \(\Sigma_0\) represents the prior covariance matrix, encoding our initial uncertainty about \(w\). Given an <strong>observed dataset</strong> \(D = \{(x_i, y_i)\}_{i=1}^{n}\), we assume the following <strong>likelihood model</strong>:</p> \[y_i \mid x_i, w \sim \mathcal{N}(w^T x_i, \sigma^2)\] <p>where:</p> <ul> <li>\(X\) is the <strong>design matrix</strong>, consisting of all input feature vectors \(x_i\),</li> <li>\(y\) is the <strong>response column vector</strong>, representing the observed outputs,</li> <li>\(\sigma^2\) represents the <strong>variance of the noise</strong> in the observations.</li> </ul> <p>Using <strong>Bayes’ theorem</strong>, we update our belief about \(w\) after observing data \(D\), leading to a <strong>Gaussian posterior distribution</strong>:</p> \[w \mid D \sim \mathcal{N}(\mu_P, \Sigma_P)\] <p>where the posterior mean \(\mu_P\) and covariance \(\Sigma_P\) are given by:</p> \[\mu_P = \left( X^T X + \sigma^2 \Sigma_0^{-1} \right)^{-1} X^T y\] \[\Sigma_P = \left( \sigma^{-2} X^T X + \Sigma_0^{-1} \right)^{-1}\] <p>The posterior mean \(\mu_P\) provides the <strong>best estimate</strong> of \(w\) given the data, while the posterior covariance \(\Sigma_P\) captures the <strong>uncertainty</strong> in our estimation.</p> <hr/> <h5 id="maximum-a-posteriori-map-estimation-and-its-connection-to-ridge-regression"><strong>Maximum A Posteriori (MAP) Estimation and Its Connection to Ridge Regression</strong></h5> <p>While the posterior distribution fully describes the uncertainty in \(w\), in practice, we often seek a <strong>point estimate</strong>. The <strong>MAP (Maximum A Posteriori) estimate</strong> of \(w\) is simply the <strong>posterior mean</strong>:</p> \[\hat{w} = \mu_P = \left( X^T X + \sigma^2 \Sigma_0^{-1} \right)^{-1} X^T y\] <p>Now, let’s assume an <strong>isotropic prior</strong> variance:</p> \[\Sigma_0 = \frac{\sigma^2}{\lambda} I\] <p>Plugging this into the MAP estimate simplifies it to:</p> \[\hat{w} = \left( X^T X + \lambda I \right)^{-1} X^T y\] <p>which is precisely the <strong>ridge regression solution</strong>.</p> <p>Thus, ridge regression can be interpreted as a <strong>Bayesian approach</strong> where we assume a Gaussian prior on the weights with variance controlled by \(\lambda\). The larger \(\lambda\), the stronger our prior belief that \(w\) should remain small, leading to <strong>regularization</strong>.</p> <p>This is because the isotropic Gaussian prior on \(w\) has variance \(\frac{\sigma^2}{\lambda}\). As \(\lambda\) increases, the prior variance decreases, implying a stronger belief that the weights \(w\) are closer to zero (more regularization). This results in a simpler model with smaller coefficients, reducing overfitting by penalizing large weights.</p> <hr/> <h5 id="understanding-the-posterior-density-and-ridge-regression"><strong>Understanding the Posterior Density and Ridge Regression</strong></h5> <p>To further illustrate the connection between MAP estimation and ridge regression, let’s look at the <strong>posterior density</strong> of \(w\) under the prior assumption \(\Sigma_0 = \frac{\sigma^2}{\lambda} I\):</p> \[p(w \mid D) \propto \underbrace{\exp \left( - \frac{\lambda}{2 \sigma^2} \|w\|^2 \right)}_{\text{Prior}} \underbrace{\prod_{i=1}^{n} \exp \left( - \frac{(y_i - w^T x_i)^2}{2 \sigma^2} \right)}_{\text{Likelihood}}\] <p>To find the <strong>MAP estimate</strong>, we minimize the <strong>negative log-posterior</strong>:</p> \[\hat{w}_{\text{MAP}} = \arg\min_{w \in \mathbb{R}^d} \left[ -\log p(w \mid D) \right]\] \[\hat{w}_{\text{MAP}} = \arg\min_{w \in \mathbb{R}^d} \left[ \underbrace{\sum_{i=1}^{n} (y_i - w^T x_i)^2}_{\text{Log Likelihood}} + \underbrace{\lambda \|w\|^2}_{\text{Log Prior}} \right]\] <p>This objective function is exactly the <strong>ridge regression loss function</strong>, which balances the <strong>data likelihood</strong> (sum of squared errors) with a <strong>penalty on the weight magnitude</strong>.</p> <p>Thus, we see that <strong>MAP estimation in Bayesian regression is equivalent to ridge regression in frequentist statistics</strong>.</p> <hr/> <h5 id="predictive-posterior-distribution-and-uncertainty-quantification"><strong>Predictive Posterior Distribution and Uncertainty Quantification</strong></h5> <p>Now that we have obtained the posterior distribution of \(w\), we can compute the <strong>predictive distribution</strong> for a <strong>new input point</strong> \(x_{\text{new}}\). Instead of providing just a single point estimate, Bayesian regression gives a <strong>distribution over predictions</strong>, reflecting our confidence.</p> <p>The predictive distribution is given by:</p> \[p(y_{\text{new}} \mid x_{\text{new}}, D) = \int p(y_{\text{new}} \mid x_{\text{new}}, w) p(w \mid D) dw\] <p>Averages over prediction for each \(w\), weighted by posterior distribution.</p> <hr/> <p>The equation represents the <strong>predictive distribution</strong> for a new observation \(y_{\text{new}}\) given a new input \(x_{\text{new}}\) and the data \(D\).</p> <p>It combines:</p> <ol> <li> <p><strong>Likelihood</strong> \(p(y_{\text{new}} \mid x_{\text{new}}, w)\): This term describes how likely the new output \(y_{\text{new}}\) is for a given set of model weights \(w\) and the new input \(x_{\text{new}}\).</p> </li> <li> <p><strong>Posterior</strong> \(p(w \mid D)\): This is the distribution of the model weights \(w\) after observing the data \(D\), representing the uncertainty in the weights.</p> </li> </ol> <p>The integral averages the likelihood of the new observation over all possible values of the model parameters \(w\), weighted by the posterior distribution \(p(w \mid D)\), because we don’t know the exact value of \(w\) — it can vary based on the data \(D\).</p> <p>Here’s why this happens:</p> <ol> <li> <p><strong>Uncertainty in the weights</strong>: In Bayesian regression, we have uncertainty about the model parameters (weights) \(w\), and the posterior distribution \(p(w \mid D)\) reflects our belief about the possible values of \(w\), given the observed data \(D\).</p> </li> <li> <p><strong>Prediction is uncertain</strong>: For a new input \(x_{\text{new}}\), the prediction \(y_{\text{new}}\) depends on the model parameters \(w\), and since we have uncertainty in \(w\), we can’t give a single prediction. Instead, we need to compute the distribution over all possible values of \(y_{\text{new}}\) corresponding to all possible values of \(w\).</p> </li> <li> <p><strong>Integrating over the posterior</strong>: The integral averages over all possible values of \(w\) because we want to account for the uncertainty in \(w\), which is captured by the posterior distribution \(p(w \mid D)\). Each value of \(w\) contributes to the prediction \(y_{\text{new}}\) according to its likelihood, and the posterior distribution tells us how probable each \(w\) is. By integrating, we are essentially weighing each possible prediction by how likely the corresponding weight \(w\) is under the posterior distribution.</p> </li> </ol> <p>Thus, the integral provides the <strong>expected prediction</strong> by combining the likelihood of the new data with the uncertainty about the model parameters. This results in a distribution for the prediction, reflecting both the model’s uncertainty and the uncertainty in the new data.</p> <hr/> <p>Since both the likelihood and posterior are Gaussian, the predictive distribution is also Gaussian:</p> \[y_{\text{new}} \mid x_{\text{new}}, D \sim \mathcal{N} (\eta_{\text{new}}, \sigma_{\text{new}}^2)\] <p>where:</p> \[\eta_{\text{new}} = \mu_P^T x_{\text{new}}\] \[\sigma_{\text{new}}^2 = x_{\text{new}}^T \Sigma_P x_{\text{new}} + \sigma^2\] <p>The predictive variance \(\sigma_{\text{new}}^2\) consists of two terms:</p> <ol> <li>\(x_{\text{new}}^T \Sigma_P x_{\text{new}}\) – <strong>Uncertainty due to finite data</strong>, representing how much we should trust our estimate of \(w\) (or simply uncertainty from the variance of \(w\)).</li> <li>\(\sigma^2\) – <strong>Inherent observation noise</strong>, representing the irreducible error in predicting \(y\).</li> </ol> <p>This decomposition highlights how <strong>Bayesian regression naturally incorporates uncertainty in predictions</strong>.</p> <hr/> <h5 id="why-bayesian-regression-is-powerful"><strong>Why Bayesian Regression is Powerful</strong></h5> <p>One major advantage of Bayesian regression over traditional point estimation methods is that it <strong>provides uncertainty estimates</strong>. So, With predictive distributions, we can give mean prediction with error bands.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Guassian_Regression-5-480.webp 480w,/assets/img/Guassian_Regression-5-800.webp 800w,/assets/img/Guassian_Regression-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Guassian_Regression-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Guassian_Regression-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This is particularly useful in applications where knowing <strong>confidence intervals</strong> is crucial, such as:</p> <ul> <li><strong>Medical diagnostics</strong>, where uncertainty in predictions affects decision-making.</li> <li><strong>Financial modeling</strong>, where risk estimation is key.</li> <li><strong>Autonomous systems</strong>, where knowing when to be uncertain improves safety.</li> </ul> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>In this post, we derived the <strong>posterior distribution</strong> for Gaussian regression, connected <strong>MAP estimation to ridge regression</strong>, and explored the <strong>predictive posterior distribution</strong>, which quantifies uncertainty.</p> <p>Bayesian regression provides a <strong>principled way</strong> to incorporate prior beliefs and quantify uncertainty, making it an essential tool in machine learning.</p> <p>Next, we’ll take a brief pause from probabilistic machine learning methods, as we’ve covered the essentials thoroughly, and shift our focus to a new topic: <strong>Multiclass Classification</strong>. Stay tuned, and keep learning and growing!🚀</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.]]></summary></entry><entry><title type="html">My Understanding of “Efficient Algorithms for Online Decision Problems” Paper</title><link href="https://monishver11.github.io/blog/2025/FPL-proof/" rel="alternate" type="text/html" title="My Understanding of “Efficient Algorithms for Online Decision Problems” Paper"/><published>2025-02-05T04:08:00+00:00</published><updated>2025-02-05T04:08:00+00:00</updated><id>https://monishver11.github.io/blog/2025/FPL-proof</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/FPL-proof/"><![CDATA[<p>Here is the link to the paper - <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/2005-Efficient_Algorithms_for_Online_Decision_Problems.pdf">Efficient Algorithms for Online Decision Problems</a>. I highly recommend reading through the entire paper once or twice before continuing with this blog.</p> <h4 id="1-introduction"><strong>1. Introduction</strong></h4> <p>This paper explores <strong>online decision problems</strong>, where decisions must be made sequentially without knowing future costs. The goal is to ensure that the total cost incurred is <strong>close to the best fixed decision in hindsight</strong>.</p> <h5 id="key-contributions"><strong>Key Contributions:</strong></h5> <ul> <li>Introducing <strong>Follow the Perturbed Leader (FPL)</strong>, a computationally efficient alternative to <strong>exponential weighting</strong> methods like Weighted Majority.</li> <li>Extending FPL to <strong>structured problems</strong> (e.g., <strong>shortest paths</strong>, <strong>tree-based search</strong>, <strong>adaptive Huffman coding</strong>).</li> <li>Demonstrating that FPL achieves <strong>low regret</strong> while remaining computationally efficient.</li> <li>Showing how FPL can be applied <strong>even when exact offline solutions are infeasible</strong>, by leveraging approximation algorithms.</li> </ul> <h5 id="2-online-decision-problem-setup"><strong>2. Online Decision Problem Setup</strong></h5> <p><strong>Problem Definition</strong></p> <ul> <li>At each time step \(t\), a <strong>cost vector</strong> \(s_t\) is revealed.</li> <li>The algorithm picks a <strong>decision \(d_t\)</strong> from a set \(D\).</li> <li> <p>The objective is to minimize the cumulative cost: \(\sum_{t=1}^{T} d_t \cdot s_t\) compared to the <strong>best single decision in hindsight</strong>:</p> \[\min_{d \in D} \sum_{t=1}^{T} d \cdot s_t.\] </li> </ul> <p><strong>Example: The Experts Problem</strong></p> <ul> <li>Suppose there are <strong>\(n\) experts</strong> providing recommendations.</li> <li>Each expert incurs a cost at each time step.</li> <li>The goal is to perform <strong>as well as the best expert</strong>.</li> </ul> <p>Traditional solutions use <strong>exponential weighting</strong> but are computationally expensive. <strong>FPL</strong> provides a <strong>simpler and faster</strong> alternative.</p> <h5 id="3-follow-the-perturbed-leader-fpl-algorithm"><strong>3. Follow the Perturbed Leader (FPL) Algorithm</strong></h5> <p><strong>Key Idea</strong></p> <ul> <li>Instead of following the exact best decision so far (<strong>Follow the Leader, FTL</strong>), which can lead to excessive switching, we introduce <strong>random perturbations</strong>.</li> <li>This smooths decision-making and prevents adversarial manipulation.</li> </ul> <p><strong>Algorithm</strong></p> <ol> <li>Compute cumulative costs for each decision: \(c_t(e) = \sum_{\tau=1}^{t} s_\tau(e)\)</li> <li>Add <strong>random perturbation</strong> \(p_t(e)\) drawn from an exponential distribution: \(\tilde{c}_t(e) = c_t(e) + p_t(e).\)</li> <li>Choose the decision <strong>with the lowest perturbed cost</strong>.</li> </ol> <p><strong>Why Does This Work?</strong></p> <ul> <li>Reduces <strong>frequent switching</strong> of decisions.</li> <li>Makes the algorithm <strong>less predictable</strong> to adversarial environments.</li> <li>Maintains a <strong>low regret bound</strong>.</li> </ul> <h5 id="4-regret-analysis-of-fpl"><strong>4. Regret Analysis of FPL</strong></h5> <p>The regret measures how much worse FPL is compared to the <strong>best decision in hindsight</strong>.</p> \[E[\text{Cost of FPL}] - \min_{d \in D} \sum_{t=1}^{T} d \cdot s_t.\] <p><strong>Step 1: The “Be the Leader” Algorithm</strong></p> <ul> <li>The <strong>hypothetical</strong> “Be the Leader” algorithm always picks the best decision so far: \(d_t = M(s_{1:t})\)</li> <li>It incurs <strong>zero regret</strong>: \(\sum_{t=1}^{T} M(s_{1:t}) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T}.\)</li> <li>However, it <strong>switches too often</strong>.</li> </ul> <p><strong>Step 2: Introducing Perturbations</strong></p> <p>FPL <strong>adds perturbations</strong> to cumulative costs before selecting the leader: \(M(s_{1:t-1} + p_t).\) The regret bound becomes:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + \eta R A T + D / \eta.\] <p><strong>Step 3: Choosing the Optimal Perturbation Scale</strong></p> <p>To balance stability and adaptation, we set: \(\eta = \sqrt{\frac{D}{RAT}}.\) This leads to the <strong>final regret bound</strong>: \(O(\sqrt{T}).\) This ensures <strong>sublinear regret</strong>, meaning FPL performs nearly as well as the best expert over time.</p> <h5 id="5-applying-fpl-to-structured-problems"><strong>5. Applying FPL to Structured Problems</strong></h5> <p>FPL can be extended to <strong>problems beyond expert selection</strong>, such as <strong>shortest paths</strong>.</p> <p><strong>Online Shortest Paths Problem</strong></p> <ul> <li>Given a <strong>graph</strong> with edge costs changing over time.</li> <li>Each round, the algorithm must select a <strong>path from source \(s\) to destination \(t\)</strong>.</li> <li>The naive approach (treating paths as independent experts) is <strong>computationally infeasible</strong>.</li> </ul> <p><strong>Efficient Approach: FPL at the Edge Level</strong></p> <ol> <li>Instead of applying FPL to entire paths, apply <strong>perturbations at the edge level</strong>.</li> <li>Compute <strong>perturbed edge costs</strong>: \(\tilde{c}_t(e) = c_t(e) + p_t(e).\)</li> <li><strong>Compute the shortest path</strong> based on these perturbed edge costs.</li> <li>Select the <strong>shortest path</strong>.</li> </ol> <p><strong>Why Does This Work?</strong></p> <ul> <li><strong>Avoids exponential complexity</strong> by working at the edge level.</li> <li><strong>Efficiently computed</strong> using shortest path algorithms (e.g., Dijkstra’s).</li> <li><strong>Regret bound remains low</strong>: \((1 + \epsilon) \times (\text{Best Path Cost}) + O(m n \log n).\)</li> </ul> <p><strong>Other Structured Problems</strong></p> <p>FPL has also been applied to:</p> <ul> <li><strong>Tree search</strong>: Efficiently updating search trees.</li> <li><strong>Adaptive Huffman coding</strong>: Dynamically optimizing prefix codes.</li> <li><strong>Online approximation algorithms</strong>: Extending FPL when exact offline solutions are infeasible.</li> </ul> <h5 id="6-summary"><strong>6. Summary</strong></h5> <ul> <li>FPL is a simple, efficient alternative to exponential weighting methods.</li> <li>It achieves regret \(O(\sqrt{T})\), ensuring performance close to the best decision in hindsight.</li> <li>The perturbation method generalizes to structured problems like shortest paths.</li> <li>FPL is computationally feasible even when the number of decisions is large.</li> </ul> <hr/> <blockquote> <p>Follow Up Questions;</p> </blockquote> <h4 id="understanding-the-additive-analysis-and-regret-bounds"><strong>Understanding the Additive Analysis and Regret Bounds</strong></h4> <p>The <strong>Additive Analysis</strong> section in the paper derives a regret bound for the <strong>Follow the Perturbed Leader (FPL)</strong> algorithm. The key goal is to compare the <strong>cumulative cost of FPL</strong> to the <strong>best fixed decision in hindsight</strong>.</p> <h5 id="key-notation"><strong>Key Notation</strong></h5> <p>Before deriving the regret bound, let’s clarify the notation:</p> <ul> <li>\(s_t\): <strong>State (cost vector) at time \(t\)</strong> <ul> <li>At each time step, we observe a cost vector \(s_t\), where each component represents the cost of a different decision.</li> </ul> </li> <li>\(d_t\): <strong>Decision made at time \(t\)</strong> <ul> <li>The action chosen by the algorithm at time \(t\).</li> </ul> </li> <li>\(M(x)\): <strong>Best fixed decision in hindsight</strong> <ul> <li>Given a total cost vector \(x\), \(M(x)\) returns the best decision: \(M(x) = \arg\min_{d \in D} d \cdot x\)</li> </ul> </li> <li>\(s_{1:T}\): <strong>Total cost vector over all \(T\) time steps</strong> <ul> <li>This is simply the sum of all cost vectors: \(s_{1:T} = s_1 + s_2 + \dots + s_T\)</li> </ul> </li> <li>\(p_t\): <strong>Random perturbation added at time \(t\)</strong> <ul> <li>Introduced to smooth out decision-making and prevent frequent switches.</li> </ul> </li> </ul> <h5 id="goal-regret-minimization"><strong>Goal: Regret Minimization</strong></h5> <p>We define regret as the difference between FPL’s total cost and the best fixed decision in hindsight:</p> \[E \left[ \sum_{t=1}^{T} d_t \cdot s_t \right] - \min_{d \in D} \sum_{t=1}^{T} d \cdot s_t.\] <p><strong>Step 1: The Hypothetical “Be the Leader” Algorithm</strong></p> <p>To analyze FPL, we first consider a <strong>hypothetical “Be the Leader” algorithm</strong>, which always picks the <strong>best decision so far</strong>:</p> \[d_t = M(s_{1:t}).\] <p>The key property of this algorithm is:</p> \[\sum_{t=1}^{T} M(s_{1:t}) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T}.\] <p><strong>Why Does This Hold?</strong></p> <ul> <li>The <strong>best decision in hindsight</strong> is optimal for the full sequence.</li> <li>If we could always “be the leader,” we would incur <strong>zero regret</strong>.</li> <li>However, this algorithm <strong>switches too frequently</strong>, making it unstable.</li> </ul> <p><strong>Step 2: Introducing Perturbations</strong></p> <p>FPL smooths out decision-making by adding <strong>random perturbations</strong> before selecting the leader:</p> \[M(s_{1:t-1} + p_t).\] <p>The analysis shows:</p> \[\sum_{t=1}^{T} M(s_{1:t} + p_t) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T} + D \sum_{t=1}^{T} |p_t - p_{t-1}|_\infty.\] <p><strong>Key Insight</strong></p> <ul> <li>The first term on the RHS is the cost of the <strong>best decision in hindsight</strong>.</li> <li>The second term represents the <strong>additional cost due to perturbations</strong>.</li> <li>This term grows at most as \(O(\sqrt{T})\), ensuring that regret remains <strong>sublinear</strong>.</li> </ul> <p><strong>Step 3: Bounding the Impact of Perturbations</strong></p> <p>The final step is to bound the effect of perturbations:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + \eta R A T + D / \eta.\] <p>where:</p> <ul> <li>\(\eta\) is a tuning parameter controlling the size of perturbations.</li> <li>\(R, A, D\) are problem-dependent constants.</li> </ul> <p><strong>Choosing the Optimal \(\eta\)</strong></p> <p>To minimize regret, we set:</p> \[\eta = \sqrt{\frac{D}{RAT}}.\] <p>Plugging this into the regret bound:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + 2 \sqrt{DRAT}.\] <p><strong>Final Regret Bound</strong> \(O(\sqrt{T}).\)</p> <p>This ensures <strong>sublinear regret</strong>, meaning that over time, <strong>FPL performs nearly as well as the best fixed decision</strong>.</p> <p><strong>Key Takeaways</strong></p> <ol> <li>“Be the Leader” is optimal but switches too often.</li> <li>FPL adds perturbations to smooth decision-making.</li> <li>The regret bound is controlled by the trade-off between making stable choices and avoiding excessive randomness.</li> <li>Choosing \(\eta\) optimally gives a regret bound of \(O(\sqrt{T})\), ensuring long-term efficiency.</li> </ol> <hr/> <h4 id="understanding-the-key-notation-and-be-the-leader-algorithm"><strong>Understanding the Key Notation and “Be the Leader” Algorithm</strong></h4> <p>This section provides a clear explanation of the <strong>Key Notation</strong> and the <strong>“Be the Leader” algorithm</strong> used in the <strong>Additive Analysis</strong> of the paper.</p> <h5 id="1-key-notation-clarified"><strong>1. Key Notation (Clarified)</strong></h5> <p>To understand the regret bound derivation, let’s first clarify the key mathematical symbols.</p> <h5 id="online-decision-problem-setup"><strong>Online Decision Problem Setup</strong></h5> <p>In an <strong>online decision problem</strong>, we repeatedly make decisions without knowing future costs. Our goal is to minimize <strong>total cost</strong> over time.</p> <hr/> <table> <thead> <tr> <th>Symbol</th> <th>Definition</th> </tr> </thead> <tbody> <tr> <td>\(s_t\)</td> <td><strong>State (cost vector) at time \(t\)</strong>, representing the cost of each decision at step \(t\).</td> </tr> <tr> <td>\(d_t\)</td> <td><strong>Decision chosen at time \(t\)</strong> (e.g., selecting an expert or a path).</td> </tr> <tr> <td>\(M(x)\)</td> <td><strong>Best fixed decision in hindsight</strong>, meaning the best decision if we knew all costs in advance.</td> </tr> <tr> <td>\(s_{1:T}\)</td> <td><strong>Total cost vector over \(T\) rounds</strong>, defined as \(s_{1:T} = \sum_{t=1}^{T} s_t\).</td> </tr> <tr> <td>\(p_t\)</td> <td><strong>Random perturbation added at time \(t\)</strong> to smooth decision-making.</td> </tr> </tbody> </table> <hr/> <h5 id="example-experts-problem"><strong>Example: Experts Problem</strong></h5> <ul> <li>Suppose we are choosing between <strong>two experts</strong> (A and B).</li> <li>Each expert has a different cost at each time step.</li> <li>We want to <strong>pick the expert that minimizes the total cost over time</strong>.</li> </ul> <hr/> <table> <thead> <tr> <th>Time \(t\)</th> <th>Expert A’s Cost \(s_t(A)\)</th> <th>Expert B’s Cost \(s_t(B)\)</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> </tr> <tr> <td>\(t = 2\)</td> <td>0.5</td> <td>0.2</td> </tr> <tr> <td>\(t = 3\)</td> <td>0.4</td> <td>0.5</td> </tr> <tr> <td>\(t = 4\)</td> <td>0.2</td> <td>0.3</td> </tr> </tbody> </table> <hr/> <ul> <li><strong>Without perturbations</strong>, we would always select the expert with the lowest cumulative cost.</li> <li>However, <strong>this can cause excessive switching</strong>, which leads to instability.</li> </ul> <p><strong>2. Step 1: The “Be the Leader” Algorithm</strong></p> <p>The <strong>“Be the Leader” algorithm</strong> is a <strong>hypothetical strategy</strong> where we always choose the <strong>best decision so far</strong>.</p> <p><strong>How It Works</strong></p> <p>At time \(t\), select:</p> \[d_t = M(s_{1:t}),\] <p>meaning:</p> <ul> <li>Choose the <strong>decision that has had the lowest total cost so far</strong>.</li> <li>This ensures <strong>no regret</strong> because we are always picking the best option up to that point.</li> </ul> <p><strong>Key Property</strong></p> \[\sum_{t=1}^{T} M(s_{1:t}) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T}.\] <p><strong>Why is this true?</strong></p> <ol> <li>The <strong>best decision in hindsight</strong> is optimal for the full sequence.</li> <li>If we could always “be the leader,” we would incur <strong>zero regret</strong>.</li> <li>However, this algorithm <strong>switches decisions too frequently</strong>, making it unstable.</li> </ol> <p><strong>Example Calculation</strong></p> <hr/> <table> <thead> <tr> <th>Time \(t\)</th> <th>Cumulative Cost \(A\)</th> <th>Cumulative Cost \(B\)</th> <th>Chosen Expert</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> <td><strong>A</strong></td> </tr> <tr> <td>\(t = 2\)</td> <td>0.8</td> <td>0.6</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 3\)</td> <td>1.2</td> <td>1.1</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 4\)</td> <td>1.4</td> <td>1.4</td> <td><strong>Switching rapidly!</strong></td> </tr> </tbody> </table> <hr/> <ul> <li>The leader <strong>switches frequently</strong> whenever cumulative costs change slightly.</li> <li>This is problematic, especially in <strong>adversarial settings</strong>, because an adversary can force unnecessary switches.</li> </ul> <p><strong>3. Why Do We Need Perturbations?</strong></p> <p>The <strong>“Be the Leader”</strong> algorithm <strong>switches too often</strong>, making it inefficient.</p> <p><strong>Follow the Perturbed Leader (FPL) Fixes This</strong></p> <p>To avoid excessive switching, <strong>FPL adds random perturbations</strong> before selecting the leader:</p> \[M(s_{1:t-1} + p_t).\] <p>This <strong>smooths out decision-making</strong>:</p> <ul> <li><strong>Prevents rapid switches</strong> caused by small cost changes.</li> <li><strong>Balances stability and adaptability</strong>.</li> <li><strong>Maintains a low regret bound</strong>.</li> </ul> <p><strong>Key Insight</strong></p> <ul> <li><strong>FPL ensures that decisions do not fluctuate excessively</strong>.</li> <li><strong>Adding perturbations leads to a regret bound of \(O(\sqrt{T})\), ensuring long-term efficiency</strong>.</li> </ul> <hr/> <h5 id="understanding-step-2-and-step-3-in-additive-analysis"><strong>Understanding Step 2 and Step 3 in Additive Analysis</strong></h5> <p>In the <strong>Additive Analysis</strong> section, Steps 2 and 3 are crucial for deriving the regret bound for <strong>Follow the Perturbed Leader (FPL)</strong>. These steps show how perturbations help smooth decision-making while maintaining low regret.</p> <hr/> <p><strong>Step 2: Introducing Perturbations</strong></p> <p>The issue with the <strong>“Be the Leader”</strong> algorithm is that it <strong>switches too frequently</strong>, leading to instability. To fix this, <strong>Follow the Perturbed Leader (FPL)</strong> <strong>adds small random perturbations</strong> to past costs before selecting the leader:</p> \[d_t = M(s_{1:t-1} + p_t).\] <p><strong>Effect of Perturbations</strong></p> <p>Instead of choosing the decision with the exact lowest cumulative cost, FPL selects the decision <strong>with the lowest perturbed cost</strong>:</p> \[\tilde{c}_t(e) = c_t(e) + p_t(e).\] <p>This ensures:</p> <ol> <li><strong>Fewer unnecessary switches</strong>: Small cost fluctuations no longer cause frequent decision changes.</li> <li><strong>Better robustness against adversarial cost sequences</strong>.</li> </ol> <p><strong>Key Inequality</strong></p> <p>The analysis shows:</p> \[\sum_{t=1}^{T} M(s_{1:t} + p_t) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T} + D \sum_{t=1}^{T} |p_t - p_{t-1}|_\infty.\] <p><strong>Breaking It Down</strong></p> <ul> <li><strong>LHS</strong>: The total cost incurred by FPL.</li> <li><strong>RHS</strong>: The total cost of the best decision in hindsight <strong>plus an extra term due to perturbations</strong>.</li> <li><strong>The second term</strong> captures the additional cost introduced by randomness.</li> </ul> <p>Since perturbations are drawn from a well-chosen distribution, their effect remains <strong>small</strong> (bounded by \(O(\sqrt{T})\)).</p> <p><strong>Step 3: Bounding the Impact of Perturbations</strong></p> <p>The final step is to <strong>quantify how much extra cost perturbations introduce</strong>.</p> <p>The key regret bound derived is:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + \eta R A T + D / \eta.\] <p>where:</p> <ul> <li>\(\eta\) controls <strong>how large the perturbations are</strong>.</li> <li>\(R, A, D\) are constants depending on the problem setup.</li> </ul> <p><strong>Choosing the Optimal Perturbation Scale</strong></p> <p>To balance stability and adaptation, they choose:</p> \[\eta = \sqrt{\frac{D}{RAT}}.\] <p>Plugging this into the regret bound:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + 2 \sqrt{DRAT}.\] <p><strong>Final Regret Bound</strong> \(O(\sqrt{T}).\)</p> <p>This means that <strong>the regret grows sublinearly</strong>, ensuring that over time, <strong>FPL performs nearly as well as the best fixed decision</strong>.</p> <hr/> <h5 id="understanding-fpl-with-a-worked-out-example"><strong>Understanding FPL with a Worked-Out Example</strong></h5> <p>To better understand <strong>Follow the Perturbed Leader (FPL)</strong>, let’s go through a <strong>step-by-step numerical example</strong>.</p> <p><strong>Problem Setup</strong></p> <ul> <li>We have <strong>two experts</strong>: <strong>A</strong> and <strong>B</strong>.</li> <li>Each expert incurs a cost at each time step.</li> <li>We must <strong>pick one expert per round</strong> without knowing future costs.</li> <li>Our goal is to minimize <strong>total cost over \(T = 4\) rounds</strong>.</li> </ul> <p><strong>Cost Sequence</strong></p> <hr/> <table> <thead> <tr> <th>Time \(t\)</th> <th>Expert A’s Cost \(s_t(A)\)</th> <th>Expert B’s Cost \(s_t(B)\)</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> </tr> <tr> <td>\(t = 2\)</td> <td>0.5</td> <td>0.2</td> </tr> <tr> <td>\(t = 3\)</td> <td>0.4</td> <td>0.5</td> </tr> <tr> <td>\(t = 4\)</td> <td>0.2</td> <td>0.3</td> </tr> </tbody> </table> <hr/> <ul> <li><strong>Without perturbations</strong>, FPL would always pick the expert with the lowest cumulative cost.</li> <li>However, this leads to <strong>frequent switching</strong>.</li> </ul> <p><strong>Step 1: Follow the Leader (FTL) - No Perturbation</strong></p> <p>If we naively follow the leader <strong>without perturbations</strong>, we get:</p> <table> <thead> <tr> <th>Time \(t\)</th> <th>Cumulative Cost \(A\)</th> <th>Cumulative Cost \(B\)</th> <th>Chosen Expert</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> <td><strong>A</strong></td> </tr> <tr> <td>\(t = 2\)</td> <td>0.8</td> <td>0.6</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 3\)</td> <td>1.2</td> <td>1.1</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 4\)</td> <td>1.4</td> <td>1.4</td> <td><strong>Switching rapidly!</strong></td> </tr> </tbody> </table> <p><strong>Problem with FTL</strong></p> <ul> <li>The algorithm <strong>switches too frequently</strong>, making it unstable.</li> <li>Small cost differences cause unnecessary <strong>leader changes</strong>.</li> <li>This is <strong>bad in adversarial settings</strong>, where cost sequences can be manipulated.</li> </ul> <p><strong>Step 2: Follow the Perturbed Leader (FPL) - Adding Perturbations</strong></p> <p>Now, let’s <strong>add perturbations</strong>.</p> <ul> <li>We <strong>randomly sample perturbations</strong> \(p_t(A)\) and \(p_t(B)\) from an <strong>exponential distribution</strong>.</li> <li>Suppose we get: \(p_1(A) = 0.1, \quad p_1(B) = 0.2\)</li> </ul> <p><strong>Step 2.1: Compute Perturbed Costs</strong></p> <hr/> <table> <thead> <tr> <th>Time \(t\)</th> <th>Cumulative Cost \(A\)</th> <th>Cumulative Cost \(B\)</th> <th>Perturbed \(A\)</th> <th>Perturbed \(B\)</th> <th>Chosen Expert</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> <td><strong>0.4</strong></td> <td><strong>0.6</strong></td> <td><strong>A</strong></td> </tr> <tr> <td>\(t = 2\)</td> <td>0.8</td> <td>0.6</td> <td><strong>0.9</strong></td> <td><strong>0.8</strong></td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 3\)</td> <td>1.2</td> <td>1.1</td> <td><strong>1.3</strong></td> <td><strong>1.2</strong></td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 4\)</td> <td>1.4</td> <td>1.4</td> <td><strong>1.5</strong></td> <td><strong>1.6</strong></td> <td><strong>A</strong></td> </tr> </tbody> </table> <hr/> <p><strong>Step 2.2: What Changed?</strong></p> <ul> <li><strong>FPL smooths out decisions</strong>: The perturbations prevent unnecessary switching.</li> <li><strong>Perturbations reduce instability</strong>: Instead of switching too frequently (like FTL), FPL <strong>stabilizes</strong>.</li> <li><strong>Better stability → Lower regret!</strong></li> </ul> <p><strong>Step 3: Computing the Regret</strong></p> <p><strong>Step 3.1: Compute Cost of FPL</strong></p> <p>The total cost incurred by <strong>FPL</strong>: \(\sum_{t=1}^{T} \text{Chosen Expert's Cost}\) Using the decisions above:</p> \[\text{Total Cost of FPL} = 0.3 + 0.2 + 0.5 + 0.2 = 1.2.\] <p><strong>Step 3.2: Compute Cost of Best Expert in Hindsight</strong></p> <p>If we had <strong>perfect hindsight</strong>, we would choose the best expert who had the <strong>lowest total cost over all \(T\) rounds</strong>.</p> \[\text{Total Cost of Best Expert} = \min(1.4, 1.4) = 1.4.\] <p><strong>Step 3.3: Compute Regret</strong></p> <p>Regret is the difference between FPL and the best fixed decision:</p> \[\text{Regret} = E[\text{Cost of FPL}] - \text{Cost of Best Expert}.\] <p>Since FPL <strong>performs better than the best single expert</strong>, it actually has <strong>negative regret in this case!</strong> In general, the regret is <strong>bounded by \(O(\sqrt{T})\), ensuring FPL converges to the best decision in hindsight over time.</strong></p> <hr/> <h5 id="extending-fpl-to-structured-problems-online-shortest-paths"><strong>Extending FPL to Structured Problems: Online Shortest Paths</strong></h5> <p>One of the major contributions of the paper is extending <strong>Follow the Perturbed Leader (FPL)</strong> beyond the <strong>experts setting</strong> to more <strong>structured problems</strong>, such as <strong>online shortest paths</strong>. This is important because treating every possible path as an independent expert is computationally infeasible when the number of paths is exponential in the number of edges.</p> <p><strong>1. The Online Shortest Path Problem</strong></p> <p><strong>Problem Setup</strong></p> <ul> <li>Given a <strong>directed graph</strong> with \(n\) nodes and \(m\) edges.</li> <li>Each edge \(e\) has a time cost \(c_t(e)\) at each time step \(t\).</li> <li>The goal is to <strong>select a path from source \(s\) to destination \(t\)</strong> at each time step without knowing future costs.</li> <li>After selecting a path, we observe the costs of all edges.</li> </ul> <p><strong>Objective</strong></p> <p>We want to ensure that the total travel time over \(T\) rounds is close to the best <strong>single</strong> path in hindsight.</p> <p><strong>Challenges</strong></p> <ul> <li>The number of possible paths grows exponentially with the number of nodes, making <strong>treating paths as experts infeasible</strong>.</li> <li>Instead of treating whole paths as independent decisions, we need a way to <strong>apply FPL efficiently at the edge level</strong>.</li> </ul> <p><strong>2. Applying FPL to Online Shortest Paths</strong></p> <p><strong>Naïve Approach (Infeasible)</strong></p> <ul> <li>If we were to apply <strong>vanilla FPL</strong> directly, we would: <ol> <li>Treat each <strong>entire path</strong> as an “expert.”</li> <li>Maintain cumulative travel time for every path.</li> <li>Apply perturbations to total path costs.</li> <li>Choose the best path.</li> </ol> </li> <li><strong>Problem:</strong> The number of paths grows exponentially, making this computationally <strong>infeasible</strong>.</li> </ul> <p><strong>Efficient Approach: FPL at the Edge Level</strong></p> <p>To make FPL work efficiently, we <strong>apply perturbations to edges instead of whole paths</strong>:</p> <p><strong>Follow the Perturbed Leading Path (FPL for Paths)</strong></p> <p><strong>At each time step \(t\):</strong></p> <ol> <li><strong>For each edge \(e\)</strong>, draw a random perturbation \(p_t(e)\) from an exponential distribution.</li> <li>Compute <strong>perturbed edge costs</strong>: \(\tilde{c}_t(e) = c_t(e) + p_t(e)\)</li> <li><strong>Find the shortest path</strong> using these perturbed edge costs.</li> <li>Choose the <strong>shortest path</strong> in the graph based on the perturbed edge weights.</li> </ol> <p><strong>Why Does This Work?</strong></p> <ul> <li>Since perturbations are applied <strong>at the edge level</strong>, we avoid maintaining explicit costs for all paths.</li> <li>The standard <strong>shortest path algorithm (e.g., Dijkstra’s)</strong> can efficiently compute the best path at each step.</li> <li> <p>Theoretical guarantees remain valid: The expected regret is at most:</p> \[(1 + \epsilon) \times (\text{Best Path Cost}) + O(m n \log n)\] <p>where \(m\) is the number of edges and \(n\) is the number of nodes.</p> </li> </ul> <p><strong>3. Understanding the Regret Bound</strong></p> <p>We now derive the <strong>regret bound</strong> for online shortest paths.</p> <p><strong>Key Definitions</strong></p> <ul> <li>Let \(P_t\) be the path chosen by FPL at time \(t\).</li> <li> <p>Let \(P^*\) be the best fixed path in hindsight, i.e., the path with the lowest total cost over \(T\) rounds:</p> \[P^* = \arg\min_{P} \sum_{t=1}^{T} c_t(P).\] </li> <li> <p>The regret of FPL is:</p> \[\sum_{t=1}^{T} c_t(P_t) - \sum_{t=1}^{T} c_t(P^*).\] </li> </ul> <p><strong>Applying FPL Analysis</strong></p> <ul> <li>The perturbation ensures that <strong>bad paths are not chosen too often</strong> and <strong>good paths are discovered quickly</strong>.</li> <li>The additional regret due to perturbation grows as \(O(m n \log n)\), meaning it is still <strong>sublinear in \(T\)</strong>.</li> </ul> <p>Thus, FPL guarantees:</p> \[E[\text{Total Cost of FPL}] \leq (1 + \epsilon) \times \text{Best Path Cost} + O(m n \log n).\] <p><strong>4. Why is This Important?</strong></p> <p><strong>1. Generalization to Graph-Structured Problems</strong></p> <ul> <li>This method can be applied to <strong>any structured problem where the decision space is large</strong>.</li> <li>Example: Instead of treating each full <strong>decision tree</strong> as an expert, perturbations can be applied at <strong>the node level</strong>.</li> </ul> <p><strong>2. Computational Efficiency</strong></p> <ul> <li>Unlike <strong>exponential weighting algorithms</strong> (which require maintaining weights for each path), this approach <strong>only requires standard shortest-path computations</strong>.</li> <li><strong>Time complexity:</strong> Runs in <strong>\(O(m)\)</strong> (if using Bellman-Ford) or <strong>\(O(m + n \log n)\)</strong> (if using Dijkstra’s).</li> </ul> <p><strong>3. Practical Use Cases</strong></p> <ul> <li><strong>Network Routing:</strong> Selecting optimal paths in a <strong>dynamic network</strong>.</li> <li><strong>Robot Navigation:</strong> Choosing paths in a changing environment.</li> <li><strong>Traffic Prediction:</strong> Adjusting routes based on real-time conditions.</li> </ul> <p><strong>5. Summary</strong></p> <p><strong>Problem</strong>: Online shortest path selection where edge costs change over time.<br/> <strong>FPL Extension</strong>: Instead of treating full paths as experts, <strong>apply perturbations to edges</strong>.<br/> <strong>Algorithm</strong>:</p> <ol> <li>Add <strong>random noise</strong> to <strong>edge costs</strong>.</li> <li>Compute the <strong>shortest path</strong> with the perturbed costs.</li> <li>Follow that path.<br/> <strong>Regret Bound</strong>: <ul> <li><strong>Competitive with the best fixed path</strong> in hindsight.</li> <li><strong>Extra cost</strong> due to perturbations is <strong>small</strong> (only \(O(m n \log n)\)).<br/> <strong>Key Benefit</strong>: <strong>Works efficiently</strong> even when the number of paths is exponential.</li> </ul> </li> </ol> <hr/>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Paper"/><summary type="html"><![CDATA[A breakdown of Follow the Perturbed Leader (FPL) from Kalai & Vempala’s (2005) paper, "Efficient Algorithms for Online Decision Problems." This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.]]></summary></entry></feed>