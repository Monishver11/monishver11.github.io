<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-26T02:48:24+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A clear, theory-focused approach to machine learning, designed to take you beyond the basics. </subtitle><entry><title type="html">Exponential Weighted Average Algorithm</title><link href="https://monishver11.github.io/blog/2025/EWA/" rel="alternate" type="text/html" title="Exponential Weighted Average Algorithm"/><published>2025-01-25T05:28:00+00:00</published><updated>2025-01-25T05:28:00+00:00</updated><id>https://monishver11.github.io/blog/2025/EWA</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/EWA/"><![CDATA[<p>The <strong>Exponential Weighted Average Algorithm (EWAA)</strong> is an online learning algorithm that provides elegant guarantees for minimizing regret in adversarial settings. It extends the principles of the Weighted Majority Algorithm by incorporating exponential weight updates, making it particularly effective for handling convex loss functions.</p> <h4 id="how-the-exponential-weighted-average-algorithm-works"><strong>How the Exponential Weighted Average Algorithm Works</strong></h4> <p>At its core, the EWAA maintains and updates weights for a set of experts, similar to the Weighted Majority Algorithm. However, it uses an <strong>exponential weighting scheme</strong> to achieve better bounds on regret, especially for convex losses.</p> <h5 id="steps-of-the-algorithm"><strong>Steps of the Algorithm</strong></h5> <p><strong>Initialization</strong> Set initial weights for all \(N\) experts:</p> \[w_{1,i} = 1, \quad \forall i \in \{1, \dots, N\}\] <p><strong>Prediction at Round \(t\)</strong></p> <ul> <li>Observe the predictions \(\hat{y}_{t,i}\) from all experts.</li> <li>Compute the aggregate prediction as a weighted average:</li> </ul> \[\hat{y}_t = \frac{\sum_{i=1}^N w_{t,i} \cdot \hat{y}_{t,i}}{\sum_{i=1}^N w_{t,i}}\] <p><strong>Update Weights</strong></p> <ul> <li>After receiving the true outcome \(y_t\), update the weights for the next round:</li> </ul> \[w_{t+1,i} = w_{t,i} \cdot e^{-\eta L(\hat{y}_{t,i}, y_t)}\] <ul> <li>The weight update can also be expressed directly using the <strong>cumulative loss</strong> \(L_t(i)\) of expert \(i\) after \(t\) rounds:</li> </ul> \[w_{t+1,i} = e^{-\eta L_t(i)}\] <p><strong><mark>Key Points to Highlight:</mark></strong></p> <p><strong>Simplification of Weight Updates</strong> While the equation appears to involve \(w_{t,i}\) in the update, the final weight \(w_{t+1,i}\) depends only on the cumulative loss \(L_t(i)\) and not on previous weights.<br/> This is why it shows \(w_{t+1,i} = e^{-\eta L_t(i)}\), as the weights can be normalized afterward.</p> <p><strong>Interpretation of \(e^{-x}\)</strong> The term \(e^{-\eta x}\) decreases exponentially as \(x\) (loss) increases.<br/> This ensures poorly performing experts are rapidly down-weighted. A plot of \(e^{-x}\) can visually illustrate this decay.</p> <p>If you‚Äôre still unclear about the final weight update rule, keep reading ‚Äî the explanation below should clarify things.</p> <hr/> <p>We start from the original weight update equation and simplify it step by step to express it in terms of the <strong>cumulative loss</strong>.</p> <p><strong>1. Original Weight Update</strong> The weight update at time \(t+1\) is defined as:</p> \[w_{t+1,i} = w_{t,i} \cdot e^{-\eta L(\hat{y}_{t,i}, y_t)}\] <p><strong>2. Recursive Application</strong> Expanding the recursion over all previous rounds \(1, \dots, t\):</p> \[w_{t+1,i} = w_{1,i} \cdot e^{-\eta L(\hat{y}_{1,i}, y_1)} \cdot e^{-\eta L(\hat{y}_{2,i}, y_2)} \cdots e^{-\eta L(\hat{y}_{t,i}, y_t)}\] <p><strong>3. Simplify the Product</strong> Using the property of exponents \(a^x \cdot a^y = a^{x+y}\):</p> \[w_{t+1,i} = w_{1,i} \cdot e^{-\eta \sum_{s=1}^t L(\hat{y}_{s,i}, y_s)}\] <p><strong>4. Initial Weights</strong> Since the initial weights are set to \(w_{1,i} = 1\) for all experts, this simplifies to:</p> \[w_{t+1,i} = e^{-\eta \sum_{s=1}^t L(\hat{y}_{s,i}, y_s)}\] <p><strong>5. Cumulative Loss Definition</strong> Define the <strong>cumulative loss</strong> of expert \(i\) after \(t\) rounds as:</p> \[L_t(i) = \sum_{s=1}^t L(\hat{y}_{s,i}, y_s)\] <p><strong>6. Final Simplified Form</strong> Substituting \(L_t(i)\) into the equation gives the simplified weight update:</p> \[w_{t+1,i} = e^{-\eta L_t(i)}\] <p><strong><mark>Key Insight:</mark></strong></p> <ul> <li>The weight at time \(t+1\) depends only on the <strong>cumulative loss</strong> \(L_t(i)\), not on the individual losses at previous rounds or the intermediate weights.</li> <li>This simplification is possible because the update rule is <strong>multiplicative</strong>, and the cumulative loss naturally aggregates all penalties from previous rounds.</li> </ul> <hr/> <h5 id="theorem-regret-bound-for-ewaa"><strong>Theorem: Regret Bound for EWAA</strong></h5> <p>Let \(L(y, y')\) be a convex loss function in its first argument, taking values in \([0, 1]\). For any \(\eta &gt; 0\) and any sequence of labels \(y_1, \dots, y_T \in \mathcal{Y}\), the regret of the Exponential Weighted Average Algorithm satisfies:</p> \[R_T \leq \frac{\log N}{\eta} + \frac{\eta T}{8}\] <p>Here, the regret is defined as the difference between the total loss of the algorithm and the loss of the best expert:</p> \[R_T = \sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i \in [N]} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p><strong>Optimized Learning Rate</strong></p> <p>By choosing \(\eta = \sqrt{\frac{8 \log N}{T}}\), we minimize the regret bound, resulting in: \(R_T \leq \sqrt{\frac{T}{2} \log N}\)</p> <p>This demonstrates that the regret grows logarithmically with the number of experts \(N\) and sublinearly with the number of time steps \(T\), indicating the efficiency of EWAA.</p> <p><strong>Convex Loss Function in Its First Argument</strong></p> <p>Before we dive deeper, let‚Äôs clarify what we mean by a <strong>convex loss function in its first argument</strong>. In this context, the phrase refers to the loss function \(L(y, y')\) being convex with respect to its first argument, \(y\) (which could be the true label or the model output).</p> <p>To break it down:</p> <ul> <li>The loss function \(L(y, y')\) measures the difference between the true label \(y\) and the predicted label \(y'\).</li> <li><strong>Convexity in the first argument</strong> means that for any fixed value of \(y'\), the function \(L(y, y')\) is convex in \(y\). This implies that as you vary the predicted label \(y'\), the loss function increases in a ‚Äúbowl-shaped‚Äù manner when considering its first argument \(y\). This property is important for optimization because convex functions are easier to minimize, ensuring that algorithms like EWAA can efficiently adjust to minimize cumulative loss over time.</li> </ul> <p>In mathematical terms, for any fixed \(y'\), the function \(L(y, y')\) satisfies the condition of convexity:</p> \[L(\lambda y_1 + (1-\lambda) y_2, y') \leq \lambda L(y_1, y') + (1-\lambda) L(y_2, y')\] <p>for any \(y_1, y_2 \in \mathcal{Y}\) and \(\lambda \in [0, 1]\).</p> <hr/> <h4 id="proof-of-the-regret-bound"><strong>Proof of the Regret Bound</strong></h4> <p>Define the <strong>potential function</strong>:</p> \[\Phi_t = \log \left( \sum_{i=1}^N w_{t,i} \right)\] <p>The goal is to derive an upper bound and a lower bound for \(\Phi_t\), and then combine them to establish the regret bound.</p> <h5 id="upper-bound"><strong>Upper Bound</strong></h5> <p><strong>Step 1: Change in Potential Function</strong></p> <p>From the weight update rule:</p> \[w_{t+1,i} = w_{t,i} e^{-\eta L(\hat{y}_{t,i}, y_t)},\] <p>we can write the change in the potential function as:</p> \[\Phi_{t+1} - \Phi_t = \log \left( \frac{\sum_{i=1}^N w_{t,i} e^{-\eta L(\hat{y}_{t,i}, y_t)}}{\sum_{i=1}^N w_{t,i}} \right) = \log \left( \mathbb{E}_{p_t}[e^{-\eta X}] \right),\] <p>where \(X = -L(\hat{y}_{t,i}, y_t) \in [-1, 0]\) and \(p_t(i) = \frac{w_{t,i}}{\sum_{j=1}^N w_{t,j}}\) is the probability distribution over experts.</p> <p><strong>Step 2: Centering the Random Variable</strong></p> <p>Define \(X = -L(\hat{y}_{t,i}, y_t)\), where \(X \in [-1, 0]\). The expectation can be centered around its mean:</p> \[\mathbb{E}_{p_t} \left[ e^{-\eta L(\hat{y}_{t,i}, y_t)} \right] = \mathbb{E}_{p_t} \left[ e^{\eta (X - \mathbb{E}_{p_t}[X])} \right] e^{\eta \mathbb{E}_{p_t}[X]}\] <p>Substituting this back, we have:</p> \[\Phi_{t+1} - \Phi_t = \log \mathbb{E}_{p_t} \left[ e^{\eta (X - \mathbb{E}_{p_t}[X])} \right] + \eta \mathbb{E}_{p_t}[X]\] <p><strong>Step 3: Applying Hoeffding‚Äôs Lemma</strong></p> <p>By Hoeffding‚Äôs Lemma, for any centered random variable \(X - \mathbb{E}_{p_t}[X]\) bounded in \([-1, 0]\):</p> \[\log \mathbb{E}_{p_t} \left[ e^{\eta (X - \mathbb{E}_{p_t}[X])} \right] \leq \frac{\eta^2}{8}\] <p>Substituting this bound:</p> \[\Phi_{t+1} - \Phi_t \leq \eta \mathbb{E}_{p_t}[X] + \frac{\eta^2}{8}\] <p><strong>Step 4: Substituting \(X = -L(\hat{y}_{t,i}, y_t)\)</strong></p> <p>Recall that \(X = -L(\hat{y}_{t,i}, y_t)\), so \(\mathbb{E}_{p_t}[X] = -\mathbb{E}_{p_t}[L(\hat{y}_{t,i}, y_t)]\). Substituting:</p> \[\Phi_{t+1} - \Phi_t \leq -\eta \mathbb{E}_{p_t}[L(\hat{y}_{t,i}, y_t)] + \frac{\eta^2}{8}\] <p><strong>Step 5: Applying Convexity of \(L\)</strong></p> <p>By the convexity of \(L\) in its first argument:</p> \[\mathbb{E}_{p_t}[L(\hat{y}_{t,i}, y_t)] \geq L(\mathbb{E}_{p_t}[\hat{y}_{t,i}], y_t) = L(\hat{y}_t, y_t),\] <p>where \(\hat{y}_t = \mathbb{E}_{p_t}[\hat{y}_{t,i}]\) is the prediction of the algorithm. Using this:</p> \[\Phi_{t+1} - \Phi_t \leq -\eta L(\hat{y}_t, y_t) + \frac{\eta^2}{8}\] <p>[How? - Unclear]</p> <p><strong>Step 6: Summing Over \(t = 1, \dots, T\)</strong></p> <p>Summing this inequality over all time steps:</p> \[\sum_{t=1}^T (\Phi_{t+1} - \Phi_t) \leq -\eta \sum_{t=1}^T L(\hat{y}_t, y_t) + \frac{\eta^2 T}{8}\] <p>The left-hand side telescopes:</p> \[\Phi_{T+1} - \Phi_1 \leq -\eta \sum_{t=1}^T L(\hat{y}_t, y_t) + \frac{\eta^2 T}{8}\] <p><strong>Final Upper Bound</strong></p> <p>This establishes the <strong>upper bound</strong> for the change in potential:</p> \[\Phi_{T+1} - \Phi_1 \leq -\eta \sum_{t=1}^T L(\hat{y}_t, y_t) + \frac{\eta^2 T}{8}\] <hr/> <h5 id="lower-bound"><strong>Lower Bound</strong></h5> <p><strong>Step 1: Potential Function at \(T+1\)</strong></p> <p>From the definition of the potential function:</p> \[\Phi_{T+1} = \log \left( \sum_{i=1}^N w_{T+1,i} \right),\] <p>where \(w_{T+1,i}\) is the weight of expert \(i\) at time \(T+1\).</p> <p><strong>Step 2: Weight Update Rule</strong></p> <p>Using the weight update rule:</p> \[w_{T+1,i} = e^{-\eta L_{T,i}},\] <p>where \(L_{T,i} = \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\) is the <strong>cumulative loss</strong> of expert \(i\) up to time \(T\).</p> <p>Substituting into the potential function:</p> \[\Phi_{T+1} = \log \left( \sum_{i=1}^N e^{-\eta L_{T,i}} \right).\] <p><strong>Step 3: Lower Bound for Log-Sum-Exp</strong></p> <p>Applying the <strong>lower bound for the log-sum-exp function</strong>:</p> \[\log \left( \sum_{i=1}^N e^{-\eta L_{T,i}} \right) \geq \max_{i \in [N]} \left( -\eta L_{T,i} \right) + \log N.\] <p>Rewriting:</p> \[\Phi_{T+1} \geq -\eta \min_{i \in [N]} L_{T,i} + \log N,\] <p>where \(\min_{i \in [N]} L_{T,i}\) is the smallest cumulative loss among all experts.</p> <p><strong>Note:</strong> If this isn‚Äôt clear, refer to the <a href="https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/">log-sum-exp</a> trick‚Äîit‚Äôs essentially the same approach we‚Äôve used here.</p> <p><strong>Step 4: Initial Potential Function</strong></p> <p>From the initial condition, the potential function at \(t = 1\) is:</p> \[\Phi_1 = \log N\] <p><strong>Step 5: Combining Results</strong></p> <p>Combining the expressions for \(\Phi_{T+1}\) and \(\Phi_1\), we obtain:</p> \[\Phi_{T+1} - \Phi_1 \geq -\eta \min_{i \in [N]} L_{T,i}\] <p><strong>Final Lower Bound</strong></p> <p>Thus, the lower bound for the change in potential is:</p> \[\Phi_{T+1} - \Phi_1 \geq -\eta \min_{i \in [N]} L_{T,i}\] <hr/> <h5 id="combining-bounds"><strong>Combining Bounds</strong></h5> <p>From the upper and lower bounds:</p> \[-\eta \min_{i \in [N]} L_{T,i} \leq -\eta \sum_{t=1}^T L(\hat{y}_t, y_t) + \frac{\eta^2 T}{8}\] <p>Rearranging terms:</p> \[\sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i \in [N]} L_{T,i} \leq \frac{\log N}{\eta} + \frac{\eta T}{8}\] <p>Thus, the regret satisfies:</p> \[R_T \leq \frac{\log N}{\eta} + \frac{\eta T}{8}\] <p><strong>Note:</strong> <strong>\(\min_{i \in [N]} L_{T,i}\) and Its Meaning</strong></p> <p>The term \(\min_{i \in [N]} L_{T,i}\) refers to the <strong>minimum cumulative loss</strong> among all the experts (or models) after \(T\) rounds. Specifically:</p> <ul> <li>\(L_{T,i}\) is the cumulative loss of expert (or model) \(i\) after \(T\) rounds.</li> <li>\(\min_{i \in [N]} L_{T,i}\) represents the smallest cumulative loss incurred by any expert over the \(T\) rounds.</li> </ul> <p>This is the term we need in our calculation to compute the regret, right!</p> <p><strong><mark>Key Takeaways:</mark></strong></p> <ul> <li>The <strong>regret bound</strong> of the EWAA is a function of both the learning rate \(\eta\) and the time horizon \(T\).</li> <li>By choosing \(\eta = \sqrt{\frac{8 \log N}{T}}\), the regret grows sublinearly with \(T\) and logarithmically with \(N\), ensuring the algorithm‚Äôs efficiency.</li> </ul> <hr/> <h5 id="advantages-and-disadvantages-of-ewaa"><strong>Advantages and Disadvantages of EWAA</strong></h5> <p><strong>Advantages</strong></p> <ol> <li><strong>Strong Theoretical Guarantees</strong>: <ul> <li>The regret bound for the <strong>(EWAA)</strong> is logarithmic in the number of experts \(N\) and sublinear in the number of time steps \(T\). This means that even as the number of experts or rounds increases, the regret grows slowly, offering a strong theoretical guarantee on performance. [Why? - Think]</li> </ul> </li> <li><strong>Applicability to Convex Losses</strong>: <ul> <li>Unlike algorithms specifically tailored for binary losses, EWAA can handle <strong>convex loss functions</strong>. This makes it a more versatile algorithm since convex losses are more general and can cover a wider range of applications beyond binary classification.</li> </ul> </li> <li><strong>Weight Adaptivity</strong>: <ul> <li>The <strong>exponential weight updates</strong> in EWAA ensure that poor-performing experts are penalized efficiently over time. This adaptive mechanism allows the algorithm to focus more on better-performing experts, while discouraging the influence of worse-performing ones, improving its overall performance.</li> </ul> </li> </ol> <p><strong>Disadvantages</strong></p> <ul> <li><strong>Requires Knowledge of Horizon \(T\)</strong>: <ul> <li>A disadvantage of the EWAA is that it requires knowledge of the <strong>horizon</strong> \(T\), which refers to the total number of rounds or time steps the algorithm will run. Specifically, the learning rate \(\eta\) in the regret bound often depends on \(T\) (for example, \(\eta\) might be chosen as \(\frac{1}{\sqrt{T}}\)).</li> <li>This means that to optimize the regret bound, you need to have some insight or knowledge about the total number of rounds \(T\) in advance. This can be a significant limitation in practical applications, where \(T\) is not always known or fixed in advance. In real-world scenarios, you might need to adapt to changing environments without prior knowledge of how long the process will last, making it challenging to choose the best parameters like \(\eta\).</li> </ul> </li> </ul> <hr/> <p>Before we wrap up, let‚Äôs take a step back and get a clearer picture of the whole thing:</p> <h5 id="why-convexity-helps-in-ewaa"><strong><mark>Why Convexity Helps in EWAA</mark></strong></h5> <ol> <li><strong>Optimization and Regret Minimization</strong>: <ul> <li>The convexity of the loss function with respect to the predicted labels \(y\) (the first argument) ensures that the algorithm can effectively minimize cumulative loss. Since convex functions have a single global minimum, optimization is straightforward and guarantees convergence toward a solution with low regret.</li> </ul> </li> <li><strong>Exponential Weight Updates</strong>: <ul> <li>In EWAA, the weight updates are based on the <strong>exponential</strong> of the loss, and convexity allows these updates to be well-behaved. Specifically, since the loss function increases in a convex manner as the difference between \(y\) and \(y'\) increases, the exponential weight updates ensure that poorly performing experts are penalized more heavily. This ensures that the algorithm focuses on the most promising experts while reducing the influence of poor ones.</li> </ul> </li> <li><strong>Efficient Learning</strong>: <ul> <li>Convexity ensures that the loss function grows in a predictable manner, which helps in adjusting the weights efficiently across time steps. This is important for the overall performance of the algorithm, as it leads to effective adaptation and faster convergence to a good solution.</li> </ul> </li> <li><strong>Theoretical Guarantees</strong>: <ul> <li>The convexity property allows the <strong>theoretical regret bounds</strong> for EWAA to be derived more easily. Since convex functions have well-defined gradients and curvature properties, we can make rigorous claims about the regret bound, such as the logarithmic growth in the number of experts \(N\) and sublinear growth in the number of time steps \(T\). Without convexity, such guarantees would not be as strong or as easily established.</li> </ul> </li> </ol> <p>And, If you‚Äôre unsure about the difference between linear and sublinear growth, here‚Äôs a quick clarification:</p> <ul> <li><strong>Linear growth</strong> means the value grows at a constant rate (proportional to the parameter). Mathematically: \(f(x) = O(x)\).</li> <li><strong>Sublinear growth</strong> means the value grows at a slower rate than the parameter, such that the output doesn‚Äôt keep up at the same pace. Mathematically: \(f(x) = O(x^a)\) where \(0 &lt; a &lt; 1\), or \(f(x) = O(\log(x))\).</li> </ul> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>The Exponential Weighted Average Algorithm provides strong guarantees for regret minimization with convex loss functions. Its use of exponential weight updates makes it both adaptable and theoretically elegant, though its dependence on the time horizon \(T\) can present practical challenges.</p> <p>In the next post, we‚Äôll dive into the doubling trick for selecting \(\eta\) and how it improves regret bounds. Stay tuned‚Äîsee you in the next one!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/21.pdf">Online Learning: Halving Algorithm and Exponential Weights(Notes)</a></li> </ul>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.]]></summary></entry><entry><title type="html">Bayesian Machine Learning - Mathematical Foundations</title><link href="https://monishver11.github.io/blog/2025/Bayesian-ML/" rel="alternate" type="text/html" title="Bayesian Machine Learning - Mathematical Foundations"/><published>2025-01-24T14:56:00+00:00</published><updated>2025-01-24T14:56:00+00:00</updated><id>https://monishver11.github.io/blog/2025/Bayesian-ML</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/Bayesian-ML/"><![CDATA[<p>When working with machine learning models, it‚Äôs crucial to understand the underlying statistical principles that drive our methods. Whether you‚Äôre a frequentist or a Bayesian, the starting point often involves a <strong>parametric family of densities</strong>. This concept forms the foundation for inference and is used to model the data we observe.</p> <h4 id="parametric-family-of-densities"><strong>Parametric Family of Densities</strong></h4> <p>A <strong>parametric family of densities</strong> is defined as a set</p> \[\{p(y \mid \theta) : \theta \in \Theta\},\] <p>where \(p(y \mid \theta)\) is a density function over some sample space \(Y\), and \(\theta\) represents a parameter in a finite-dimensional parameter space \(\Theta\).</p> <p>In simpler terms, this is a collection of probability distributions, each associated with a specific value of the parameter \(\theta\). When we refer to ‚Äúdensity,‚Äù it‚Äôs worth noting that this can be replaced with ‚Äúmass function‚Äù if we‚Äôre dealing with discrete random variables. Similarly, integrals can be replaced with summations in such cases.</p> <p>This framework is the common starting point for both <strong>classical statistics</strong> and <strong>Bayesian statistics</strong>, as it provides a structured way to think about modeling the data.</p> <h5 id="frequentist-or-classical-statistics"><strong>Frequentist or ‚ÄúClassical‚Äù Statistics</strong></h5> <p>In frequentist statistics, we also work with the parametric family of densities \(\{p(y \mid \theta) : \theta \in \Theta\}\), assuming that the true distribution \(p(y \mid \theta)\) governs the world we observe. This means there exists some unknown parameter \(\theta \in \Theta\) that determines the true nature of the data.</p> <p>If we had direct access to this true parameter \(\theta\), we wouldn‚Äôt need statistics at all! However, in practice, we only have a dataset, denoted as</p> \[D = \{y_1, y_2, \dots, y_n\},\] <p>where each \(y_i\) is sampled independently from the true distribution \(p(y \mid \theta)\).</p> <p>This brings us to the heart of statistics: <strong>how do we make inferences about the unknown parameter \(\theta\) using only the observed data \(D\)?</strong></p> <h5 id="point-estimation"><strong>Point Estimation</strong></h5> <p>One fundamental problem in statistics is <strong>point estimation</strong>, where the goal is to estimate the true value of the parameter \(\theta\) as accurately as possible.</p> <p>To do this, we use a <strong>statistic</strong>, denoted as \(s = s(D)\), which is simply a function of the observed data. When this statistic is designed to estimate \(\theta\), we call it a <strong>point estimator</strong>, represented as \(\hat{\theta} = \hat{\theta}(D)\).</p> <p>A <strong>good point estimator</strong> is one that is both:</p> <ul> <li><strong>Consistent</strong>: As the sample size \(n\) grows larger, the estimator \(\hat{\theta}_n\) converges to the true parameter \(\theta\).</li> <li><strong>Efficient</strong>: The estimator \(\hat{\theta}_n\) extracts the maximum amount of information about \(\theta\) from the data, achieving the best possible accuracy for a given sample size.</li> </ul> <p>One of the most popular methods for point estimation is the <strong>maximum likelihood estimator (MLE)</strong>. While we‚Äôve already covered it, let‚Äôs revisit it through a concrete example to reinforce our understanding.</p> <h5 id="example-coin-flipping-and-maximum-likelihood-estimation"><strong>Example: Coin Flipping and Maximum Likelihood Estimation</strong></h5> <p>Let‚Äôs consider the simple yet illustrative problem of estimating the probability of a coin landing on heads.</p> <p>Here, the parametric family of mass functions is given by:</p> \[p(\text{Heads} \mid \theta) = \theta, \quad \text{where } \theta \in \Theta = (0, 1).\] <p>The parameter \(\theta\) represents the probability of the coin landing on heads. Our goal is to estimate this parameter based on observed data.</p> <p>If this seems a bit confusing, seeing \(\theta\) in two places, let‚Äôs clarify it first.</p> <p>Imagine you have a coin, and you‚Äôre curious about how ‚Äúfair‚Äù it is. A perfectly fair coin has a 50% chance of landing heads or tails, but your coin might be biased. To capture this bias mathematically, you introduce a parameter, \(\theta\), which represents the probability of the coin landing on heads.</p> <p>We write this as:</p> \[p(\text{Heads} \mid \theta) = \theta\] <p>Let‚Äôs break this down with intuition:</p> <ol> <li><strong>What does \(\theta\) mean?</strong><br/> \(\theta\) is the coin‚Äôs ‚Äúpersonality.‚Äù For example: <ul> <li>If \(\theta = 0.8\), it means the coin ‚Äúloves‚Äù heads, and there‚Äôs an 80% chance it will land heads on any given flip.</li> <li>If \(\theta = 0.3\), the coin is biased toward tails, and there‚Äôs only a 30% chance of heads.</li> </ul> </li> <li> <p><strong>What does \(p(\text{Heads} \mid \theta) = \theta\) mean?</strong><br/> This equation ties the probability of getting heads to the parameter \(\theta\). It‚Äôs like saying: ‚ÄúThe parameter \(\theta\) <em>is</em> the probability of heads.‚Äù For every coin flip, \(\theta\) directly determines the likelihood of heads.</p> </li> <li><strong>Why is this useful?</strong><br/> It simplifies modeling. Instead of treating each flip as random and unconnected, we assume there‚Äôs a fixed bias, \(\theta\), that governs the coin‚Äôs behavior. Once we observe enough flips (data), we can estimate \(\theta\) and predict future outcomes.</li> </ol> <p><strong>A relatable example might be‚Ä¶</strong></p> <p>Imagine a factory making coins with varying biases. Each coin is labeled with its bias, \(\theta\), ranging between 0 (always tails) and 1 (always heads). If you‚Äôre handed a coin without a label, your job is to figure out its bias by flipping it multiple times and observing the outcomes.</p> <p>This is the setup for the equation \(p(\text{Heads} \mid \theta) = \theta\). It tells us the coin‚Äôs behavior is entirely controlled by its bias, \(\theta\), and allows us to estimate it from observed data. <strong>Data and Likelihood Function</strong></p> <p>I hope that clears things up, and we‚Äôre good to proceed!</p> <hr/> <p>Suppose we observe the outcomes of \(n\) independent coin flips, represented as:</p> \[D = (\text{H, H, T, T, T, T, T, H, ... , T}),\] <p>where \(n_h\) is the number of heads, and \(n_t\) is the number of tails. Since each flip is independent, the likelihood function for the observed data is:</p> \[L_D(\theta) = p(D \mid \theta) = \theta^{n_h} (1 - \theta)^{n_t}.\] <p><strong>Log-Likelihood and Optimization</strong></p> <p>Rather than working directly with the likelihood function, which involves products and can become cumbersome, we typically maximize the <strong>log-likelihood function</strong> for computational simplicity. The log-likelihood is:</p> \[\log L_D(\theta) = n_h \log \theta + n_t \log (1 - \theta).\] <p>The <strong>maximum likelihood estimate (MLE)</strong> of \(\theta\) is the value that maximizes this log-likelihood:</p> \[\hat{\theta}_{\text{MLE}} = \underset{\theta \in \Theta}{\text{argmax}} \, \log L_D(\theta).\] <p><strong>Derivation of the MLE</strong></p> <p>To find the MLE, we compute the derivative of the log-likelihood with respect to \(\theta\), set it to zero, and solve for \(\theta\):</p> \[\frac{\partial}{\partial \theta} \big[ n_h \log \theta + n_t \log (1 - \theta) \big] = \frac{n_h}{\theta} - \frac{n_t}{1 - \theta}.\] <p>Setting this derivative to zero:</p> \[\frac{n_h}{\theta} = \frac{n_t}{1 - \theta}.\] <p>Simplifying this equation gives:</p> \[\theta = \frac{n_h}{n_h + n_t}.\] <p>Thus, the MLE for \(\theta\) is:</p> \[\hat{\theta}_{\text{MLE}} = \frac{n_h}{n_h + n_t}.\] <p><strong>Intuition Behind the MLE</strong></p> <p>The result makes intuitive sense: the MLE simply calculates the proportion of heads observed in the data. It uses the empirical frequency as the best estimate of the true probability of heads, given the observed outcomes.</p> <hr/> <p>While frequentist approaches like MLE provide a single ‚Äúbest‚Äù estimate for \(\theta\), Bayesian methods take a different perspective. Instead of finding a point estimate, Bayesian inference quantifies uncertainty about \(\theta\) using probability distributions. This leads to the concepts of <strong>prior distributions</strong> and <strong>posterior inference</strong>, which is what we‚Äôre going to explore next.</p> <h4 id="bayesian-statistics-an-introduction"><strong>Bayesian Statistics: An Introduction</strong></h4> <p>In the frequentist framework, the goal is to estimate the true parameter \(\theta\) using the observed data. However, <strong>Bayesian statistics</strong> takes a fundamentally different approach by introducing an important concept: the <strong>prior distribution</strong>. This addition allows us to explicitly incorporate prior beliefs about the parameter into our analysis and update them rationally as we observe new data.</p> <h5 id="the-prior-distribution-reflecting-prior-beliefs"><strong>The Prior Distribution: Reflecting Prior Beliefs</strong></h5> <p>A <strong>prior distribution</strong>, denoted as \(p(\theta)\), is a probability distribution over the parameter space \(\Theta\). It represents our belief about the value of \(\theta\) <strong>before</strong> observing any data. For instance, if we believe that \(\theta\) is more likely to lie in a specific range, we can encode this belief directly into the prior.</p> <h5 id="a-bayesian-model-combining-prior-and-data"><strong>A Bayesian Model: Combining Prior and Data</strong></h5> <p>A <strong>[parametric] Bayesian model</strong> is constructed from two key components:</p> <ol> <li>A <strong>parametric family of densities</strong> \(\{p(D \mid \theta) : \theta \in \Theta\}\) that models the likelihood of the observed data \(D\) given \(\theta\).</li> <li>A <strong>prior distribution</strong> \(p(\theta)\) on the parameter space \(\Theta\).</li> </ol> <p>These two components combine to form a <strong>joint density</strong> over \(\theta\) and \(D\):</p> \[p(D, \theta) = p(D \mid \theta) p(\theta).\] <p>This joint density encapsulates both the likelihood of the data and our prior beliefs about the parameter.</p> <h5 id="posterior-distribution-updating-beliefs"><strong>Posterior Distribution: Updating Beliefs</strong></h5> <p>The real power of Bayesian statistics lies in the ability to <strong>update prior beliefs</strong> after observing data. This is achieved through the <strong>posterior distribution</strong>, denoted as \(p(\theta \mid D)\).</p> <ul> <li>The <strong>prior distribution</strong> \(p(\theta)\) captures our initial beliefs about \(\theta\).</li> <li>The <strong>posterior distribution</strong> \(p(\theta \mid D)\) reflects our updated beliefs after observing the data \(D\).</li> </ul> <p>By applying <strong>Bayes‚Äô rule</strong>, we can express the posterior distribution as:</p> \[p(\theta \mid D) = \frac{p(D \mid \theta) p(\theta)}{p(D)},\] <p>where:</p> <ul> <li>\(p(D \mid \theta)\) is the <strong>likelihood</strong>, capturing how well \(\theta\) explains the observed data.</li> <li>\(p(\theta)\) is the <strong>prior</strong>, encoding our initial beliefs about \(\theta\).</li> <li>\(p(D)\) is a normalizing constant, ensuring the posterior integrates to 1.</li> </ul> <h5 id="simplifying-the-posterior"><strong>Simplifying the Posterior</strong></h5> <p>When analyzing the posterior distribution, we often focus on terms that depend on \(\theta\). Dropping constant factors that are independent of \(\theta\), we write:</p> \[p(\theta \mid D) \propto p(D \mid \theta) \cdot p(\theta),\] <p>where \(\propto\) denotes proportionality.</p> <p>In practice, this allows us to analyze and work with the posterior distribution more efficiently. For instance, the <strong>maximum a posteriori (MAP) estimate</strong> of \(\theta\) is given by:</p> \[\hat{\theta}_{\text{MAP}} = \underset{\theta \in \Theta}{\text{argmax}} \, p(\theta \mid D).\] <p><strong>A Way to Think About It:</strong></p> <p>A helpful way to think of Bayesian methods is to imagine you‚Äôre trying to predict the outcome of an event, but you have some prior knowledge (or beliefs) about it. For example, let‚Äôs say you‚Äôre predicting whether a student will pass an exam, and you have prior knowledge that most students in the class have been doing well. This prior belief can be represented as a probability distribution, which reflects how confident you are about the parameter (like the likelihood of passing).</p> <p>As you collect more data (say, the student‚Äôs past performance or study hours), Bayesian methods update your belief (the prior) to form a new, updated belief, called the <strong>posterior distribution</strong>. The more data you have, the more confident the posterior becomes about the true outcome.</p> <p>So, in essence:</p> <ul> <li><strong>Prior distribution</strong> = What you believe before observing data (your initial guess).</li> <li><strong>Likelihood</strong> = How the observed data might be related to your belief.</li> <li><strong>Posterior distribution</strong> = Your updated belief after observing the data.</li> </ul> <p>In Bayesian inference, the goal is to calculate the posterior, which balances the prior belief with the observed data.</p> <hr/> <h4 id="example-bayesian-coin-flipping"><strong>Example: Bayesian Coin Flipping</strong></h4> <p>Let‚Äôs revisit the coin-flipping example, but this time from a Bayesian perspective. We start with the parametric family of mass functions:</p> \[p(\text{Heads} \mid \theta) = \theta, \quad \text{where } \theta \in \Theta = (0, 1).\] <p>To complete our Bayesian model, we also need to specify a <strong>prior distribution</strong> over \(\theta\). One common choice is the <strong>Beta distribution</strong>, which is particularly convenient for this problem.</p> <h5 id="beta-prior-distribution"><strong>Beta Prior Distribution</strong></h5> <p>The Beta distribution, denoted as \(\text{Beta}(\alpha, \beta)\), is a flexible family of distributions defined on the interval \((0, 1)\). Its density function is:</p> \[p(\theta) \propto \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}.\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Beta_Distribution-480.webp 480w,/assets/img/Beta_Distribution-800.webp 800w,/assets/img/Beta_Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Beta_Distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Beta_Distribution" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For our coin-flipping example, we can use:</p> \[p(\theta) \propto \theta^{h - 1} (1 - \theta)^{t - 1},\] <p>where \(h\) and \(t\) represent our prior ‚Äúcounts‚Äù of heads and tails, respectively.</p> <p>The <strong>mean</strong> of the Beta distribution is:</p> \[\mathbb{E}[\theta] = \frac{h}{h + t},\] <p>and its <strong>mode</strong> (for \(h, t &gt; 1\)) is:</p> \[\text{Mode} = \frac{h - 1}{h + t - 2}.\] <p><strong>A Way to Think of This Distribution:</strong></p> <p>Imagine you‚Äôre trying to estimate the probability of rain on a given day in a city you‚Äôve never visited. You don‚Äôt have any direct weather data yet, but you do have some general knowledge about the region. Based on this, you form an initial belief about how likely it is to rain‚Äîmaybe you‚Äôre unsure, so you assume it‚Äôs equally likely to rain or not, or maybe you‚Äôve heard that it‚Äôs usually dry there.</p> <ul> <li><strong>The Beta distribution</strong> helps you represent this uncertainty. It‚Äôs like a flexible tool that encodes your prior beliefs about the probability of rain, and you can adjust these beliefs based on what you know or expect. <ul> <li>If you‚Äôre totally uncertain, you might use a <strong>uniform prior</strong> (where \(\alpha = \beta = 1\)), meaning you‚Äôre equally unsure whether rain is more likely or not.</li> <li>If you‚Äôve already heard that it tends to rain more often, say 70% of the time, you could choose \(\alpha = 7\) and \(\beta = 3\) to reflect this prior information.</li> </ul> </li> </ul> <p>As you gather more data‚Äîsay, after several days of weather observations‚Äîyou can update your beliefs about the likelihood of rain. Each new observation (rain or no rain) ‚Äúshapes‚Äù your distribution.</p> <ul> <li> <p><strong>The mean</strong> \(\mathbb{E}[\theta] = \frac{h}{h + t}\) represents the average likelihood of rain after considering all your prior knowledge and the observed days. This is your updated best guess about how likely it is to rain on any given day.</p> </li> <li> <p><strong>The mode</strong> \(\text{Mode} = \frac{h - 1}{h + t - 2}\), which reflects the most probable value of \(\theta\) after observing data, might give you a better estimate if the weather has shown a clear tendency over time (e.g., if it‚Äôs rained most days).</p> </li> </ul> <p>In essence, the Beta distribution allows you to start with an initial belief (or no belief) about the probability of rain, and as you observe more data, you continuously refine that belief. This is what makes Bayesian inference powerful‚Äîit enables you to <strong>update</strong> your beliefs rationally based on new evidence.</p> <p><strong>Why Use the Beta Prior Distribution in this Coin Flipping Problem?</strong></p> <p>The <strong>Beta distribution</strong> is particularly well-suited for modeling probabilities in Bayesian statistics, especially in problems like coin flipping. Here are a few reasons why it‚Äôs a good choice:</p> <ol> <li> <p><strong>Support on (0, 1):</strong> The Beta distribution is defined over the interval \(\theta \in (0, 1)\), which matches the range of possible values for \(\theta\) in the coin-flipping example. Since \(\theta\) represents the probability of getting heads, it must lie between 0 and 1.</p> </li> <li><strong>Flexibility:</strong> The Beta distribution is very flexible in shaping its probability density. By adjusting the parameters \(\alpha\) and \(\beta\), we can model a wide variety of prior beliefs about \(\theta\): <ul> <li>When \(\alpha = \beta = 1\), the Beta distribution is uniform, indicating that we have no strong prior belief about whether heads or tails is more likely.</li> <li>When \(\alpha &gt; \beta\), the distribution is biased towards heads, and when \(\alpha &lt; \beta\), it is biased towards tails.</li> <li>The parameters can also reflect <strong>observed data</strong>: if you‚Äôve already seen \(h\) heads and \(t\) tails, the Beta distribution can be chosen with \(\alpha = h + 1\) and \(\beta = t + 1\), which matches the idea of ‚Äúupdating‚Äù your beliefs based on the data you observe.</li> </ul> </li> <li><strong>Intuitive Interpretation:</strong> The Beta distribution is easy to interpret in terms of prior knowledge. The parameters \(\alpha\) and \(\beta\) can be seen as counts of prior observations of heads and tails, respectively. This makes it a natural choice when we have prior information or beliefs about the likelihood of different outcomes, and want to update them as new data comes in.</li> </ol> <p><strong>Note:</strong> I highly suggest taking a look at the Beta distribution graph. As \(\alpha\) increases, the distribution tends to skew towards higher values of \(\theta\) (closer to 1), reflecting a higher likelihood of success. On the other hand, as \(\beta\) increases, the distribution skews towards lower values of \(\theta\) (closer to 0), indicating a higher likelihood of failure. If \(\alpha\) and \(\beta\) are equal, the distribution is symmetric and uniform, reflecting no prior preference between the two outcomes.</p> <hr/> <p>After observing data \(D = (\text{H, H, T, T, T, H, ...})\), where \(n_h\) is the number of heads and \(n_t\) is the number of tails, we combine the <strong>prior</strong> and <strong>likelihood</strong> to obtain the <strong>posterior distribution</strong>.</p> <p>The likelihood function, based on the observed data, is:</p> \[L(\theta) = p(D \mid \theta) = \theta^{n_h} (1 - \theta)^{n_t}.\] <p>Combining the prior and likelihood, the posterior density is:</p> \[p(\theta \mid D) \propto p(\theta) \cdot L(\theta),\] <p>which simplifies to:</p> \[p(\theta \mid D) \propto \theta^{h - 1} (1 - \theta)^{t - 1} \cdot \theta^{n_h} (1 - \theta)^{n_t}.\] <p>Simplifying further, we get:</p> \[p(\theta \mid D) \propto \theta^{h - 1 + n_h} (1 - \theta)^{t - 1 + n_t}.\] <p>This posterior distribution is also a Beta distribution:</p> \[\theta \mid D \sim \text{Beta}(h + n_h, t + n_t).\] <h5 id="interpreting-the-posterior"><strong>Interpreting the Posterior</strong></h5> <p>The posterior distribution shows how our prior beliefs are updated by the observed data:</p> <ul> <li>The prior \(\text{Beta}(h, t)\) initializes our counts with \(h\) heads and \(t\) tails.</li> <li>The posterior \(\text{Beta}(h + n_h, t + n_t)\) updates these counts by adding the observed \(n_h\) heads and \(n_t\) tails.</li> </ul> <p>For example, if our prior belief was \(\text{Beta}(2, 2)\) (a uniform prior), and we observed \(n_h = 3\) heads and \(n_t = 1\) tails, the posterior would be:</p> \[\text{Beta}(2 + 3, 2 + 1) = \text{Beta}(5, 3).\] <p>This reflects our updated belief about the probability of heads after observing the data.</p> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>In this blog, we explored the essence of Bayesian statistics, focusing on how priors, likelihoods, and posteriors interact to update our beliefs. Using the coin-flipping example, we demonstrated key Bayesian tools like the Beta distribution and how to compute posterior updates. Also, as we mentioned, there‚Äôs one more important reason for choosing the Beta distribution‚Äîits technical term is <strong>conjugate priors</strong>. In the next blog, we‚Äôll dive deeper into this concept and explore Bayesian point estimates, comparing them to the frequentist MLE estimate. Stay tuned as we continue to build intuition and delve further into Bayesian inference! üëã</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Bayesian Statistics</li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.]]></summary></entry><entry><title type="html">Understanding the Weighted Majority Algorithm in Online Learning</title><link href="https://monishver11.github.io/blog/2025/WMA/" rel="alternate" type="text/html" title="Understanding the Weighted Majority Algorithm in Online Learning"/><published>2025-01-23T16:33:00+00:00</published><updated>2025-01-23T16:33:00+00:00</updated><id>https://monishver11.github.io/blog/2025/WMA</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/WMA/"><![CDATA[<p><strong>The Weighted Majority Algorithm: A Powerful Online Learning Technique</strong></p> <p>In the continuation of our exploration into online learning, we turn to the <strong>Weighted Majority Algorithm (WMA)</strong>, an influential approach introduced by Littlestone and Warmuth in 1988. This algorithm builds upon the foundational principles of online learning and offers remarkable theoretical guarantees for handling adversarial scenarios.</p> <p>Let‚Äôs dive into the workings of the Weighted Majority Algorithm, analyze its performance, and understand its strengths and limitations.</p> <h4 id="the-weighted-majority-algorithm"><strong>The Weighted Majority Algorithm</strong></h4> <p>The Weighted Majority Algorithm operates in a framework where predictions are made by combining the advice of multiple experts. Unlike the Halving Algorithm, which outright eliminates incorrect hypotheses, WMA assigns and updates weights to experts based on their performance, ensuring a more adaptive approach.</p> <h5 id="the-algorithm-steps"><strong>The Algorithm Steps</strong></h5> <ol> <li><strong>Initialization</strong>: <ul> <li>Start with \(N\) experts, each assigned an initial weight of 1:<br/> \(w_{1,i} = 1 \quad \text{for } i = 1, 2, \dots, N\)</li> </ul> </li> <li><strong>Prediction</strong>: <ul> <li>At each time step \(t\): <ul> <li>Receive the instance \(x_t\).</li> <li>Predict the label \(\hat{y}_t\) using a <strong>weighted majority vote</strong>: \(\hat{y}_t = \begin{cases} 1 &amp; \text{if } \sum_{i: y_{t,i}=1}^{N} w_{t,i} &gt; \sum_{i: y_{t,i}=0}^{N} w_{t,i}, \\ 0 &amp; \text{otherwise.} \end{cases}\)</li> </ul> </li> </ul> </li> <li><strong>Update Weights</strong>: <ul> <li>After receiving the true label \(y_t\), update the weights of the experts: <ul> <li>For each expert \(i\): \(w_{t+1,i} = \begin{cases} \beta w_{t,i} &amp; \text{if } y_{t,i} \neq y_t, \\ w_{t,i} &amp; \text{otherwise,} \end{cases}\) where \(\beta \in [0,1)\) is a parameter that reduces the weight of experts who make incorrect predictions.</li> </ul> </li> </ul> </li> </ol> <p>[The condition check matters here right??, 2 cases: 1. if the majority vote is correct, then no update for the experts that did mistake. 2. If the majoriy vote is wrong, then we update correctly for experts that are wrong. So, its better to update without checking if the majority vote gives correct result right? ]</p> <ol> <li><strong>Termination</strong>: <ul> <li>After \(T\) iterations, return the final weights of all experts.</li> </ul> </li> </ol> <h4 id="theoretical-performance-of-the-weighted-majority-algorithm"><strong>Theoretical Performance of the Weighted Majority Algorithm</strong></h4> <h5 id="mistake-bound"><strong>Mistake Bound</strong></h5> <p><strong>Theorem:</strong> The Weighted Majority (WM) algorithm guarantees a bound on the number of mistakes it makes compared to the best expert. Let:</p> <ul> <li>\(m_t\): Total number of mistakes made by the WM algorithm up to time \(t\),</li> <li>\(m_t^*\): Number of mistakes made by the best expert up to time \(t\),</li> <li>\(N\): Total number of experts,</li> <li>\(\beta\): Parameter controlling the weight decay for incorrect experts.</li> </ul> <p>The mistake bound is given by:</p> \[m_t \leq \frac{\log N + m_t^* \log \frac{1}{\beta}}{\log \frac{2}{1+\beta}}\] <p><strong>Interpretation of the Bound:</strong></p> <ol> <li><strong>First Term (\(\log N\))</strong>: <ul> <li>This term accounts for the initial uncertainty due to having \(N\) experts. The algorithm needs to explore to identify the best expert, and the logarithmic dependence on \(N\) ensures scalability.</li> </ul> </li> <li><strong>Second Term (\(m_t^* \log \frac{1}{\beta}\))</strong>: <ul> <li>This term reflects the cost of following the best expert, scaled by \(\log \frac{1}{\beta}\). A smaller \(\beta\) increases the penalty for mistakes, leading to slower adaptation but potentially fewer overall mistakes. [<strong>Think it through, Why?</strong>]</li> </ul> </li> <li><strong>Denominator (\(\log \frac{2}{1+\beta}\))</strong>: <ul> <li>This represents the efficiency of the weight adjustment. When \(\beta\) is close to 0 (as in the Halving Algorithm), the denominator becomes larger, leading to tighter bounds.</li> </ul> </li> </ol> <p><strong>Special Cases:</strong></p> <ol> <li><strong>Realizable Case (\(m_t^* = 0\))</strong>: <ul> <li>If there exists an expert with zero mistakes, the bound simplifies to: \(m_t \leq \frac{\log N}{\log \frac{2}{1+\beta}}\) <ul> <li>For the Halving Algorithm (\(\beta = 0\)), this further simplifies to: \(m_t \leq \log N\)</li> </ul> </li> </ul> </li> <li><strong>General Case</strong>: <ul> <li>When no expert is perfect (\(m_t^* &gt; 0\)), the algorithm incurs an additional cost proportional to \(m_t^* \log \frac{1}{\beta}\). This reflects the cost of distributing weights among experts.</li> </ul> </li> </ol> <p><strong>Intuition:</strong></p> <ul> <li>The Weighted Majority Algorithm balances exploration (trying all experts) and exploitation (focusing on the best expert). The logarithmic terms indicate that the algorithm adapts efficiently, even with a large number of experts.</li> <li>By tuning \(\beta\), the algorithm can trade off between faster adaptation and resilience to noise. A smaller \(\beta\) (e.g., \(\beta = 0\)) emphasizes rapid adaptation, while a larger \(\beta\) smoothens weight updates.</li> </ul> <p><strong>Note:</strong> If this isn‚Äôt clear, I can simplify it further, but this time, I encourage you to try and understand it on your own. There‚Äôs a reason behind this‚Äîit helps you build a mental model of how the algorithm works and strengthens your intuition. This kind of reinforcement is essential as we dive deeper into advanced ML concepts. I hope this makes sense. If you‚Äôre still struggling, consider using tools like ChatGPT or Perplexity to gain additional clarity.</p> <hr/> <h4 id="proof-of-the-mistake-bound"><strong>Proof of the Mistake Bound</strong></h4> <p><mark>A general method for deducing bounds and guarantees involves defining a potential function, establishing both upper and lower bounds for it, and deriving results from the resulting inequality.</mark> This powerful approach is central to deducing several proofs. Specifically, the proof of the mistake bound relies on defining a potential function that tracks the total weight of all experts over time.</p> <h5 id="potential-function"><strong>Potential Function</strong></h5> <p>The potential function at time \(t\) is defined as:</p> \[\Phi_t = \sum_{i=1}^N w_{t,i},\] <p>where \(w_{t,i}\) is the weight of expert \(i\) at time \(t\).</p> <h5 id="upper-bound-on-potential"><strong>Upper Bound on Potential</strong></h5> <p>Initially, the weights of all \(N\) experts sum up to \(\Phi_0 = N\) since each expert starts with a weight of 1.</p> <p><strong>Step 1: Effect of a Mistake</strong></p> <p>When the Weighted Majority Algorithm makes a mistake, the weights of the experts who predicted incorrectly are reduced by a factor of \(\beta\), where \(0 &lt; \beta &lt; 1\). This means the total weight at the next time step, \(\Phi_{t+1}\), will be less than or equal to the weighted average of the weights of the correct and incorrect experts.</p> <p><strong>Step 2: Fraction of Correct and Incorrect Experts</strong></p> <p>Let‚Äôs say a fraction \(p\) of the total weight belongs to the correct experts, and a fraction \(1 - p\) belongs to the incorrect experts. After the mistake, the incorrect experts‚Äô weights are scaled by \(\beta\). Thus, the new potential is:</p> \[\Phi_{t+1} = p \Phi_t + \beta (1 - p) \Phi_t\] <p><strong>Step 3: Simplifying the Expression</strong></p> <p>Factor out \(\Phi_t\) from the equation:</p> \[\Phi_{t+1} = \Phi_t \left[ p + \beta (1 - p) \right]\] <p>Rewriting \(p + \beta (1 - p)\):</p> \[\Phi_{t+1} = \Phi_t \left[ 1 - (1 - \beta)(1 - p) \right]\] <p>Since \(p + (1 - p) = 1\), this simplifies to:</p> \[\Phi_{t+1} \leq \Phi_t \left[ 1 + \frac{\beta}{2} \right]\] <p>This inequality holds because the worst-case scenario assumes \(p = \frac{1}{2}\), where half the weight comes from correct predictions and half from incorrect predictions.</p> <p><strong>Step 4: Over Multiple Mistakes</strong></p> <p>If the algorithm makes \(m_t\) mistakes, the inequality applies iteratively. After \(m_t\) mistakes, the potential becomes:</p> \[\Phi_t \leq \Phi_0 \left[ 1 + \frac{\beta}{2} \right]^{m_t}\] <p>Substituting \(\Phi_0 = N\) (the initial total weight):</p> \[\Phi_t \leq N \left[ 1 + \frac{\beta}{2} \right]^{m_t}\] <p><strong>Intuition:</strong> The factor \(1 + \frac{\beta}{2}\) reflects the reduction in potential after each mistake. As \(\beta\) decreases, the penalty for incorrect experts increases, leading to a faster reduction in \(\Phi_t\). This bound shows how the potential decreases exponentially with the number of mistakes \(m_t\).</p> <hr/> <h5 id="lower-bound-on-potential"><strong>Lower Bound on Potential</strong></h5> <p>The <strong>lower bound</strong> on the potential is based on the performance of the best expert in the algorithm.</p> <p>Let‚Äôs define \(w_{t,i}\) as the weight of expert \(i\) at time \(t\). The total potential \(\Phi_t\) is the sum of the weights of all experts. Since the best expert is the one with the highest weight at any time, we can state that:</p> \[\Phi_t \geq w_{t,i^*}\] <p>where \(i^*\) is the index of the best expert.</p> <p>Now, the key point is that the weight of the best expert, \(w_{t,i^*}\), decays over time as it makes mistakes. Let \(m_t^*\) be the number of mistakes made by the best expert up to time \(t\). Since each mistake reduces the weight of the best expert by a factor of \(\beta\), the weight of the best expert at time \(t\) is given by:</p> \[w_{t,i^*} = \beta^{m_t^*}\] <p>Therefore, the potential at time \(t\) is at least the weight of the best expert:</p> \[\Phi_t \geq \beta^{m_t^*}\] <p>This provides a <strong>lower bound</strong> on the potential.</p> <h5 id="combining-the-upper-and-lower-bounds"><strong>Combining the Upper and Lower Bounds</strong></h5> <p>Now that we have both an upper bound and a lower bound on the potential, we can combine them to get a more useful inequality.</p> <p>From the upper bound, we know:</p> \[\Phi_t \leq \left[\frac{1 + \beta}{2}\right]^{m_t} N\] <p>From the lower bound, we know:</p> \[\Phi_t \geq \beta^{m_t^*}\] <p>By combining these two inequalities, we get:</p> \[\beta^{m_t^*} \leq \left[\frac{1 + \beta}{2}\right]^{m_t} N\] <p>This inequality tells us that the weight of the best expert at time \(t\), \(\beta^{m_t^*}\), is less than or equal to the total potential \(\Phi_t\) after \(m_t\) mistakes.</p> <p>To solve for \(m_t\), we take the logarithm of both sides of the inequality:</p> \[\log \left( \beta^{m_t^*} \right) \leq \log \left( \left[\frac{1 + \beta}{2}\right]^{m_t} N \right)\] <p>Using the logarithmic property \(\log(a^b) = b \log(a)\), we simplify both sides:</p> \[m_t^* \log \beta \leq m_t \log \left[\frac{1 + \beta}{2}\right] + \log N\] <p>Now, we want to isolate \(m_t\) on one side of the inequality. First, subtract \(\log N\) from both sides:</p> \[m_t^* \log \beta - \log N \leq m_t \log \left[\frac{1 + \beta}{2}\right].\] <p>Now, divide both sides by \(\log \left[\frac{1 + \beta}{2}\right]\). Note that \(\log \left[\frac{1 + \beta}{2}\right]\) is negative because \(\frac{1 + \beta}{2} &lt; 1\), so dividing by it reverses the inequality:</p> \[m_t \geq \frac{m_t^* \log \beta - \log N}{\log \left[\frac{1 + \beta}{2}\right]}.\] <p>We can simplify the expression further by recognizing that \(\log \frac{1}{\beta} = -\log \beta\). This gives us:</p> \[m_t \leq \frac{\log N + m_t^* \log \frac{1}{\beta}}{\log \frac{2}{1 + \beta}}.\] <p>This is the final inequality, which gives a bound on the number of mistakes \(m_t\) made by the algorithm in terms of the number of mistakes \(m_t^*\) made by the best expert, the total number of experts \(N\), and the factor \(\beta\).</p> <p><strong>Note:</strong> The <strong>less than or equal to (‚â§)</strong> sign appears because of the <strong>inequality reversal</strong> when dividing by a negative quantity. This is why the final inequality has the \(\leq\) sign instead of \(\geq\).</p> <p>This completes the proof of the mistake bound. The inequality shows that the number of mistakes \(m_t\) made by the algorithm is related to the mistakes \(m_t^*\) made by the best expert, and that the algorithm‚Äôs performance improves as \(\beta\) decreases.</p> <p>[Self - Understand it better, think it through]</p> <hr/> <h5 id="strengths-and-weaknesses-of-the-weighted-majority-algorithm"><strong>Strengths and Weaknesses of the Weighted Majority Algorithm</strong></h5> <p><strong>Advantages</strong></p> <ul> <li><strong>Strong Theoretical Bound</strong>: <ul> <li>The Weighted Majority Algorithm (WMA) achieves a remarkable theoretical bound on regret without requiring any assumptions about the data distribution or the performance of individual experts.</li> <li>This makes it robust, particularly in adversarial environments.</li> </ul> </li> </ul> <p><strong>Disadvantages</strong></p> <ul> <li><strong>Limitation with Binary Loss</strong>: <ul> <li>For binary loss, no deterministic algorithm (including WMA) can achieve a regret \(R_T = o(T)\).</li> <li>This means deterministic WMA cannot guarantee sublinear regret in such settings.</li> </ul> </li> </ul> <p>In the context of binary loss, where predictions are either correct or incorrect (0 or 1), deterministic algorithms like the Weighted Majority Algorithm (WMA) face a fundamental limitation. Regret, which measures how much worse the algorithm performs compared to the best expert in hindsight, ideally should grow sublinearly with the number of rounds (\(T\)). However, deterministic WMA cannot achieve this in adversarial environments. The fixed and predictable nature of deterministic strategies allows adversaries to exploit the algorithm, forcing it into repeated mistakes. As a result, the regret \(R_T\) grows at least linearly with \(T\), meaning the algorithm‚Äôs performance does not improve relative to the best expert over time. This inability to achieve sublinear regret (\(R_T = o(T)\)) under binary loss is a significant disadvantage of deterministic WMA, necessitating alternative approaches like randomization to overcome this limitation.</p> <p>[Still, Not clear of this limitation, How and Why?]</p> <p>In the following cases, the Weighted Majority Algorithm offers improved guarantees:</p> <ol> <li><strong>Randomization Improves Regret Bounds</strong>: <ul> <li><strong>Randomized versions</strong> of WMA (e.g., RWMA) can improve the regret bound by introducing randomness, which helps the algorithm avoid getting stuck with poor-performing experts.</li> </ul> </li> <li><strong>Extensions for Convex Losses</strong>: <ul> <li>WMA can be adapted to handle <strong>convex loss functions</strong>, such as in regression tasks. In these cases, the algorithm provides improved theoretical guarantees, ensuring more reliable and efficient performance.</li> </ul> </li> </ol> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>The Weighted Majority Algorithm offers an elegant and efficient approach to handling adversarial settings. By adaptively updating the weights of experts, it ensures robust performance with minimal assumptions.</p> <p>In the next post, we‚Äôll dive into alternative versions of WMA and explore powerful algorithms designed for online learning. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li></li> </ul>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.]]></summary></entry><entry><title type="html">Online Learning in ML - A Beginner‚Äôs Guide to Adaptive Learning</title><link href="https://monishver11.github.io/blog/2025/Online-Learning/" rel="alternate" type="text/html" title="Online Learning in ML - A Beginner‚Äôs Guide to Adaptive Learning"/><published>2025-01-23T14:45:00+00:00</published><updated>2025-01-23T14:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/Online-Learning</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/Online-Learning/"><![CDATA[<p>In this post, we‚Äôll dive into one of the foundational topics that was discussed in the first class of my advanced machine learning course: online learning. This course is centered on theoretical insights and encourages students to think critically, experiment fearlessly, and embrace confusion as a stepping stone toward innovation. Let‚Äôs explore how online learning fits into the broader landscape of machine learning and why it‚Äôs such a powerful concept.</p> <h5 id="the-advanced-ml-course-a-brief-overview"><strong>The Advanced ML Course: A Brief Overview</strong></h5> <p>The course takes a deeply theoretical approach, focusing on critical analysis and research to build innovative machine learning algorithms. It‚Äôs not just about solving problems but about challenging established ideas, learning from mistakes, and having the courage to be wrong. Confusion, in this context, is not a roadblock‚Äîit‚Äôs a catalyst for deeper thinking.</p> <p>Two primary domains form the core of present ML/AI space:</p> <ol> <li><strong>Neural Networks</strong>: The cornerstone of modern machine learning.</li> <li><strong>Online Learning</strong>: A versatile and influential approach with deep connections to game theory and optimization.</li> </ol> <h4 id="what-is-online-learning"><strong>What is Online Learning?</strong></h4> <p>Online learning stands out as an area of machine learning with rich literature and numerous practical applications. It bridges the gap between supervised learning and game-theoretic optimization while offering efficient solutions for large-scale problems.</p> <p>Unlike traditional batch learning, where algorithms process the entire dataset at once, online learning operates iteratively, processing one sample at a time. This makes it computationally efficient and ideal for large datasets. Moreover, online learning does not rely on the common assumption that data points are independent and identically distributed (i.i.d.). Instead, it is designed to handle adversarial scenarios, making it incredibly flexible and applicable to situations where data distributions are unknown or variable.</p> <p>Even though these algorithms are inherently designed for adversarial settings, they can, under specific conditions, yield accurate predictions in scenarios where data does follow a distribution.</p> <p><strong>What do we mean by adversarial in the context of online learning?</strong></p> <p>In online learning, ‚Äúadversarial‚Äù refers to settings where data is not assumed to follow a fixed probabilistic distribution. Instead, the data sequence might be unpredictable, dependent, or deliberately chosen to challenge the algorithm. This flexibility makes online learning particularly robust in real-world applications.</p> <ul> <li><strong>Real-world Examples</strong>: <ul> <li><strong>Financial Models</strong>: Adapting to volatile or externally influenced stock price movements.</li> <li><strong>Recommendation Systems</strong>: Managing biased or strategically influenced user feedback.</li> <li><strong>Security Systems</strong>: Responding to data manipulations or malicious attacks.</li> </ul> </li> </ul> <p>By focusing on resilience and adaptability, online learning algorithms excel in handling evolving data and challenging environments, making them indispensable for a wide range of applications.</p> <h5 id="why-online-learning"><strong>Why Online Learning?</strong></h5> <p>Traditional machine learning approaches often rely on the PAC (Probably Approximately Correct) framework, where:</p> <ul> <li>The data distribution remains fixed over time.</li> <li>Both training and testing data are assumed to follow the same i.i.d. distribution.</li> </ul> <p><strong>What is the PAC Learning Framework?</strong></p> <p>The PAC learning framework provides a theoretical foundation for understanding the feasibility of learning in a probabilistic setting. Under this framework:</p> <ul> <li>The algorithm‚Äôs goal is to find a hypothesis that is <em>probably approximately correct</em>, meaning it performs well on the training data and generalizes to unseen data with high probability.</li> <li>It assumes that data points are drawn independently and identically distributed (i.i.d.) from a fixed, unknown distribution.</li> <li>Key metrics include the error rate of the hypothesis on future samples and its convergence to the true distribution as more data is provided.</li> </ul> <p>While this framework is powerful for traditional batch learning, it relies on strong assumptions about the stability and predictability of the data distribution, making it less suitable for dynamic or adversarial scenarios.</p> <p>In contrast, online learning assumes no such distributional stability. It operates under the following key principles:</p> <ul> <li><strong>No Assumptions on Data Distribution</strong>: The data can follow any sequence, including adversarially generated ones. This flexibility allows online learning to adapt to real-world scenarios where data patterns may shift unpredictably.</li> <li><strong>Mixed Training and Testing</strong>: Training and testing are not separate phases but occur simultaneously, enabling the algorithm to continuously learn and improve from new data.</li> <li><strong>Worst-Case Analysis</strong>: Algorithms are designed to perform well even under the most challenging conditions, ensuring robustness in unpredictable environments.</li> <li><strong>Performance Metrics</strong>: Instead of accuracy or loss functions commonly used in batch learning, online learning evaluates performance using measures like: <ul> <li><strong><em>Mistake Model</em></strong>: The total number of incorrect predictions made during the learning process.</li> <li><strong><em>Regret</em></strong>: The difference between the cumulative loss of the algorithm and the loss of the best possible strategy in hindsight.</li> </ul> </li> </ul> <p><strong>What are some practical applications of online learning?</strong></p> <p>Online learning has proven invaluable in a variety of real-world domains where data is dynamic, unpredictable, or arrives sequentially:</p> <ol> <li><strong>Stock Market Predictions</strong>: Continuously adapting to ever-changing financial data, helping traders and financial systems make real-time decisions.</li> <li><strong>Online Advertising</strong>: Personalizing ads based on user behavior that evolves with every click or interaction.</li> <li><strong>Recommendation Systems</strong>: Adapting suggestions in real time as users interact with platforms like Netflix, Amazon, or YouTube.</li> <li><strong>Autonomous Systems</strong>: Enabling self-driving cars or robots to learn and adapt to new scenarios as they encounter them.</li> <li><strong>Spam Filtering</strong>: Continuously updating filters to catch new spam types as they emerge.</li> <li><strong>Security Systems</strong>: Responding to cyberattacks or new threats by learning and adapting on the fly.</li> </ol> <p>This shift in perspective allows online learning to address a broader range of real-world problems. For now, if all of this feels a bit abstract, don‚Äôt worry‚Äîhang tight! We‚Äôll dive deeper and make sure to explore it thoroughly, leaving no stone unturned.</p> <h4 id="the-general-online-learning-framework"><strong>The General Online Learning Framework</strong></h4> <p>The online learning process follows a simple yet powerful framework. At each step:</p> <ol> <li>The algorithm receives an instance, denoted as \(x_t\).</li> <li>It makes a prediction, \(\hat{y}_t\).</li> <li>The true label, \(y_t\), is revealed.</li> <li>A loss is incurred, calculated as \(L(\hat{y}_t, y_t)\), which quantifies the prediction error.</li> </ol> <p>The overarching goal of online learning is to minimize the total loss over a sequence of predictions: \(\sum_{t=1}^T L(\hat{y}_t, y_t)\)</p> <p>For classification tasks, a common choice of loss is the 0-1 loss: \(L(\hat{y}_t, y_t) = \mathbb{1}(\hat{y}_t \neq y_t) \; or \; \vert \hat{y}_t - y_t \vert\) For regression tasks, the squared loss is often used: \(L(\hat{y}_t, y_t) = (\hat{y}_t - y_t)^2\)</p> <hr/> <h4 id="prediction-with-expert-advice"><strong>Prediction with Expert Advice</strong></h4> <p>One particularly compelling framework in online learning is <strong>Prediction with Expert Advice</strong>. Imagine you have multiple ‚Äúexperts,‚Äù each providing advice on how to predict the label for a given instance. The challenge lies in aggregating their advice to make accurate predictions while minimizing the regret associated with poor decisions.</p> <p>The process unfolds as follows:</p> <ol> <li>At each time step, the algorithm receives an instance, \(x_t\), and predictions from \(N\) experts, \(\{y_{t,1}, y_{t,2}, \dots, y_{t,N}\}\).</li> <li>Based on this advice, the algorithm predicts \(\hat{y}_t\).</li> <li>The true label, \(y_t\), is revealed, and the loss, \(L(\hat{y}_t, y_t)\), is incurred.</li> </ol> <p>The performance of the algorithm is measured by its <strong><em>regret</em></strong>, which is the difference between the total loss incurred by the algorithm and the total loss of the best-performing expert:</p> \[\text{Regret}(T) = \sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i=1, \dots, N} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p>Minimizing regret ensures that the algorithm‚Äôs predictions improve over time and closely approximate the performance of the best expert.</p> <p><strong>What does the regret equation convey and how do we interpret it?</strong></p> <p>The regret equation provides a way to evaluate the algorithm‚Äôs performance in hindsight by comparing it to the best expert. Here‚Äôs what each term means:</p> <ol> <li> <p><strong>Algorithm‚Äôs Loss</strong> (\(\sum_{t=1}^T L(\hat{y}_t, y_t)\)):<br/> This is the cumulative loss incurred by the algorithm over \(T\) time steps. It reflects how well the algorithm performs when making predictions based on the aggregated advice of all experts.</p> </li> <li> <p><strong>Best Expert‚Äôs Loss</strong> (\(\min_{i=1, \dots, N} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\)):<br/> This represents the cumulative loss of the single best-performing expert in hindsight. Note that the best expert is identified after observing all \(T\) instances, which gives it an advantage over the algorithm that has to predict in real time.</p> </li> <li> <p><strong>Regret</strong>:<br/> The difference between these two terms quantifies how much worse the algorithm performs compared to the best expert.</p> <ul> <li><strong>Low regret</strong> indicates that the algorithm‚Äôs predictions are close to those of the best expert, demonstrating effective learning.</li> <li><strong>High regret</strong> suggests that the algorithm is failing to learn effectively from the experts‚Äô advice.</li> </ul> </li> </ol> <p><strong>Why is regret important?</strong></p> <p>Regret is a crucial metric in online learning because:</p> <ul> <li>It provides a measure of how well the algorithm adapts to the expert advice over time.</li> <li>It ensures that, as the number of time steps \(T\) increases, the algorithm‚Äôs performance converges to that of the best expert (ideally achieving sublinear regret, such as \(O(\sqrt{T})\) or better).</li> <li>It accounts for the dynamic nature of predictions, focusing on learning improvement rather than static accuracy.</li> </ul> <p>A few more questions to make our understadning better.</p> <p><strong>How to Calculate the Best Expert‚Äôs Loss?</strong></p> <p>The <strong>Best Expert‚Äôs Loss</strong> is the cumulative loss of the single expert that performs best over the entire sequence of predictions, \(T\). Here‚Äôs how to calculate it:</p> <ol> <li> <p><strong>Track each expert‚Äôs cumulative loss</strong>:<br/> For each expert \(i\), maintain a running sum of their losses over the rounds:</p> \[L_{\text{expert } i} = \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p>Here, \(\hat{y}_{t,i}\) is the prediction made by expert \(i\) at time \(t\), and \(y_t\) is the true label. \(L\) could represent any loss function, such as zero-one loss or squared loss.</p> </li> <li> <p><strong>Find the expert with the minimum cumulative loss</strong>:<br/> After summing the losses for all \(N\) experts over \(T\) rounds, identify the expert whose cumulative loss is the smallest:</p> \[\min_{i=1, \dots, N} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p>This value represents the <strong>Best Expert‚Äôs Loss</strong>, which serves as the benchmark for evaluating the algorithm‚Äôs regret.</p> </li> </ol> <p><strong>Do We Pick the Best Expert After Each Round?</strong></p> <p>No, the <strong>Best Expert‚Äôs Loss</strong> is determined in hindsight, <strong><em>after</em></strong> observing the entire sequence of \(T\) rounds. The algorithm does not know in advance which expert is the best. Instead, it aggregates predictions from all experts during the process (e.g., using techniques like weighted averaging).</p> <ul> <li>The <strong>best expert</strong> is identified retrospectively after all rounds.</li> <li>The cumulative loss of this best expert is used to compute regret.</li> </ul> <p><strong>Example:</strong></p> <p>Suppose we have 3 experts, and their losses over 5 rounds are:</p> <table> <thead> <tr> <th>Round (t)</th> <th>Expert 1 Loss</th> <th>Expert 2 Loss</th> <th>Expert 3 Loss</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>0.2</td> <td>0.3</td> <td>0.1</td> </tr> <tr> <td>2</td> <td>0.1</td> <td>0.4</td> <td>0.2</td> </tr> <tr> <td>3</td> <td>0.3</td> <td>0.2</td> <td>0.3</td> </tr> <tr> <td>4</td> <td>0.4</td> <td>0.1</td> <td>0.3</td> </tr> <tr> <td>5</td> <td>0.2</td> <td>0.5</td> <td>0.1</td> </tr> </tbody> </table> <ol> <li><strong>Calculate the cumulative loss for each expert</strong>: <ul> <li><strong>Expert 1</strong>: \(0.2 + 0.1 + 0.3 + 0.4 + 0.2 = 1.2\)</li> <li><strong>Expert 2</strong>: \(0.3 + 0.4 + 0.2 + 0.1 + 0.5 = 1.5\)</li> <li><strong>Expert 3</strong>: \(0.1 + 0.2 + 0.3 + 0.3 + 0.1 = 1.0\)</li> </ul> </li> <li> <p><strong>Find the minimum cumulative loss</strong>: \(\min(1.2, 1.5, 1.0) = 1.0\)</p> <p>Hence, the <strong>Best Expert‚Äôs Loss</strong> is <strong>1.0</strong>, achieved by Expert 3.</p> </li> </ol> <p>Prediction with Expert Advice is a powerful framework for dynamic environments where multiple sources of information or strategies need to be combined effectively. It ensures robustness and adaptability by iteratively improving predictions while minimizing regret.</p> <hr/> <h4 id="the-halving-algorithm-simple-and-powerful"><strong>The Halving Algorithm: Simple and Powerful</strong></h4> <p>The <strong>Halving Algorithm</strong> is a simple yet effective online learning algorithm designed to minimize mistakes. It works by maintaining a set of hypotheses (or experts) and systematically eliminating those that make incorrect predictions.</p> <p>Here‚Äôs how it works:</p> <ol> <li><strong>Initialization</strong>: Start with a set of hypotheses, \(H_1 = H\).</li> <li><strong>Iteration</strong>: At each time step, \(t\): <ul> <li>Receive an instance, \(x_t\).</li> <li>Predict the label, \(\hat{y}_t\), using majority voting among the hypotheses in \(H_t\).</li> <li>Receive the true label, \(y_t\).</li> <li>If \(\hat{y}_t \neq y_t\), update the hypothesis set: \(H_{t+1} = \{h \in H_t : h(x_t) = y_t\}\)</li> </ul> </li> <li><strong>Termination</strong>: After all iterations, return the final hypothesis set, \(H_{T+1}\).</li> </ol> <h5 id="mistake-bound-for-the-halving-algorithm"><strong>Mistake Bound for the Halving Algorithm</strong></h5> <p><strong>Theorem</strong>: If the initial hypothesis set \(H\) is finite, the number of mistakes made by the Halving Algorithm is bounded by:</p> \[M_{Halving(H)} \leq \log_2 |H|\] <p><strong>Proof Outline</strong>:</p> <ul> <li>Each mistake reduces the size of the hypothesis set by at least half: \(|H_{t+1}| \leq \frac{|H_t|}{2}\)</li> <li>Initially, \(|H_1| = |H|\). After \(M\) mistakes: \(|H_{M+1}| \leq \frac{|H|}{2^M}\)</li> <li>To ensure \(|H_{M+1}| \geq 1\) (at least one hypothesis remains), we require: \(M \leq \log_2 |H|\)</li> </ul> <p>This logarithmic bound demonstrates the efficiency of the Halving Algorithm, even in adversarial settings.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>Online learning offers a powerful framework for making predictions in dynamic and adversarial environments. Its ability to adapt, operate under minimal assumptions, and deliver robust performance makes it a cornerstone of modern machine learning research. The Halving Algorithm provides a concrete example of how online learning methods can be both intuitive and theoretically grounded.</p> <p>In upcoming posts, we‚Äôll delve deeper into other online learning algorithms and explore their theoretical guarantees, practical applications, and connections to broader machine learning principles. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>PAC</li> <li>Online Learning Intro</li> </ul>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.]]></summary></entry><entry><title type="html">Multivariate Gaussian Distribution and Naive Bayes</title><link href="https://monishver11.github.io/blog/2025/Multivariate-GNB/" rel="alternate" type="text/html" title="Multivariate Gaussian Distribution and Naive Bayes"/><published>2025-01-23T03:01:00+00:00</published><updated>2025-01-23T03:01:00+00:00</updated><id>https://monishver11.github.io/blog/2025/Multivariate-GNB</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/Multivariate-GNB/"><![CDATA[<p>When analyzing data in higher dimensions, we often encounter scenarios where input features are not independent. In such cases, the <strong>Multivariate Gaussian Distribution</strong> provides a robust probabilistic framework to model these relationships. It extends the familiar univariate Gaussian distribution to multiple dimensions, enabling us to capture dependencies and correlations between variables effectively.</p> <h4 id="understanding-the-multivariate-gaussian-distribution"><strong>Understanding the Multivariate Gaussian Distribution</strong></h4> <p>A multivariate Gaussian distribution is defined as:</p> \[x \sim \mathcal{N}(\mu, \Sigma),\] <p>where \(\mu\) is the mean vector, and \(\Sigma\) is the covariance matrix. Its probability density function is given by:</p> \[p(x) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu)^\top \Sigma^{-1} (x - \mu)\right),\] <p>Here, \(d\) represents the dimensionality of the input \(x\), \(\vert \Sigma \vert\) denotes the determinant of the covariance matrix, and \(\Sigma^{-1}\) is its inverse.</p> <p>The term \((x - \mu)^\top \Sigma^{-1} (x - \mu)\) is referred to as the <strong>Mahalanobis distance</strong>, which measures the distance of a point \(x\) from the mean \(\mu\). Unlike the Euclidean distance, the Mahalanobis distance normalizes for differences in variances and accounts for correlations between the dimensions. This normalization makes it particularly useful in multivariate data analysis.</p> <h5 id="intuition-and-analogy-for-multivariate-gaussian"><strong>Intuition and Analogy for Multivariate Gaussian</strong></h5> <p>Think of the multivariate Gaussian distribution as a <strong>3D bell-shaped curve</strong> (or higher-dimensional equivalent) where:</p> <ul> <li>The <strong>peak</strong> of the bell is at \(\mu\), the mean vector.</li> <li>The <strong>spread</strong> of the bell in different directions is determined by \(\Sigma\), the covariance matrix. It stretches or compresses the curve along certain axes depending on the variances and correlations.</li> </ul> <h6 id="analogy-a-weighted-balloon"><strong>Analogy: A Weighted Balloon</strong></h6> <p>Imagine a balloon filled with air. If the balloon is perfectly spherical, it represents a distribution where all dimensions are independent and have the same variance (this corresponds to \(\Sigma\) being a diagonal matrix with equal entries).</p> <p>Now, if you squeeze the balloon in one direction:</p> <ul> <li>It elongates in one direction and compresses in another. This reflects <strong>correlations</strong> between dimensions in the data, encoded by the off-diagonal elements of \(\Sigma\).</li> <li>The shape of the balloon changes, and distances (like Mahalanobis distance) now account for these correlations, unlike Euclidean distance.</li> </ul> <h5 id="how-to-think-about-mahalanobis-distance"><strong>How to Think About Mahalanobis Distance</strong></h5> <p>The Mahalanobis distance:</p> \[d_M(x) = \sqrt{(x - \mu)^\top \Sigma^{-1} (x - \mu)}\] <p>can be understood as the distance from a point \(x\) to the center \(\mu\), scaled by the shape and orientation of the distribution:</p> <ol> <li> <p><strong>Scaling by Variance</strong>: In directions where the variance is large (the distribution is ‚Äúspread out‚Äù), the Mahalanobis distance will consider points farther from the mean as less unusual. Conversely, in directions where variance is small, even small deviations from the mean are considered significant.</p> </li> <li> <p><strong>Accounting for Correlations</strong>: If two dimensions are correlated, the Mahalanobis distance adjusts for this by using the covariance matrix \(\Sigma\). The covariance matrix captures both the variances of individual dimensions and the relationships (correlations) between them.</p> </li> </ol> <p><strong>Role of the Inverse Covariance Matrix:</strong></p> <p>The term \(\Sigma^{-1}\) (the inverse of the covariance matrix) in the Mahalanobis distance ensures that the contribution of each dimension is scaled appropriately. For example:</p> <ul> <li>If two dimensions are strongly correlated, deviations along one dimension are partially ‚Äúexplained‚Äù by deviations along the other. The Mahalanobis distance reduces the weight of such deviations, treating them as less unusual.</li> <li>Conversely, if two dimensions are uncorrelated, the deviations are treated independently.</li> </ul> <p><strong>Example:</strong></p> <p>In a dataset of height and weight, a taller-than-average person is likely to weigh more than average. The covariance matrix captures this relationship, and \(\Sigma^{-1}\) adjusts the distance calculation to reflect that such deviations are expected. Without this adjustment (as in Euclidean distance), the relationship would be ignored, leading to an overestimation of the ‚Äúunusualness‚Äù of the point.</p> <p><strong>Returning to the Balloon Analogy,</strong></p> <p>The Mahalanobis distance incorporates the ‚Äúshape‚Äù of the balloon (determined by \(\Sigma\)) to measure distances:</p> <p><strong>Shape and Scaling:</strong></p> <ul> <li>A spherical balloon corresponds to a covariance matrix where all dimensions are independent and have equal variance. In this case, the Mahalanobis distance reduces to the Euclidean distance.</li> <li>A stretched or compressed balloon reflects correlations or differences in variance. The Mahalanobis distance scales the contribution of each dimension based on the covariance structure, ensuring that distances are measured relative to the shape of the distribution.</li> </ul> <p><strong>How It Works:</strong></p> <ul> <li>Points on the surface of the balloon correspond to a Mahalanobis distance of 1, regardless of the balloon‚Äôs shape. This is because the Mahalanobis distance normalizes for the stretching or compressing of the balloon in different directions.</li> <li>Mathematically, this is achieved by transforming the space using \(\Sigma^{-1}\), effectively ‚Äúflattening‚Äù the correlations and variances. In this transformed space, the balloon becomes a perfect sphere, and distances are measured uniformly.</li> </ul> <p>These adjustments make the Mahalanobis distance a powerful metric for detecting outliers and understanding the distribution of data in a multivariate context.</p> <hr/> <p>If you‚Äôre still unsure about the concept, let‚Äôs walk through an example and explore it together.</p> <h6 id="1-covariance-matrix"><strong>1. Covariance Matrix</strong>:</h6> <p>For a dataset with two variables, say <strong>height</strong> (\(x_1\)) and <strong>weight</strong> (\(x_2\)), the covariance matrix \(\Sigma\) looks like this:</p> \[\Sigma = \begin{pmatrix} \sigma_{11} &amp; \sigma_{12} \\ \sigma_{21} &amp; \sigma_{22} \end{pmatrix}\] <p>Where:</p> <ul> <li>\(\sigma_{11}\) is the variance of height (\(x_1\)).</li> <li>\(\sigma_{22}\) is the variance of weight (\(x_2\)).</li> <li>\(\sigma_{12} = \sigma_{21}\) is the covariance between height and weight.</li> </ul> <h6 id="2-inverse-covariance-matrix"><strong>2. Inverse Covariance Matrix</strong>:</h6> <p>The inverse of the covariance matrix \(\Sigma^{-1}\) is used to ‚Äúnormalize‚Äù the data and account for correlations. The inverse of a 2x2 matrix is given by:</p> \[\Sigma^{-1} = \frac{1}{\text{det}(\Sigma)} \begin{pmatrix} \sigma_{22} &amp; -\sigma_{12} \\ -\sigma_{21} &amp; \sigma_{11} \end{pmatrix}\] <p>Where the determinant of the covariance matrix is:</p> \[\text{det}(\Sigma) = \sigma_{11} \sigma_{22} - \sigma_{12}^2\] <h6 id="3-example-correlated-data-height-and-weight"><strong>3. Example: Correlated Data (Height and Weight)</strong></h6> <p>Suppose we have a dataset of heights and weights, and the covariance matrix looks like this:</p> \[\Sigma = \begin{pmatrix} 100 &amp; 80 \\ 80 &amp; 200 \end{pmatrix}\] <p>This means:</p> <ul> <li>The variance of height (\(\sigma_{11}\)) is 100.</li> <li>The variance of weight (\(\sigma_{22}\)) is 200.</li> <li>The covariance between height and weight (\(\sigma_{12} = \sigma_{21}\)) is 80, indicating a strong positive correlation between height and weight.</li> </ul> <p>Now, let‚Äôs say we have a data point:</p> \[x = \begin{pmatrix} 180 \\ 75 \end{pmatrix}\] <p>This means the person is 180 cm tall and weighs 75 kg. The mean of the dataset is:</p> \[\mu = \begin{pmatrix} 170 \\ 70 \end{pmatrix}\] <h6 id="31-euclidean-distance-without-accounting-for-correlation"><strong>3.1. Euclidean Distance</strong> (Without Accounting for Correlation)</h6> <p>The Euclidean distance between the data point \(x\) and the mean \(\mu\) is:</p> \[D_E(x) = \sqrt{(x_1 - \mu_1)^2 + (x_2 - \mu_2)^2}\] <p>Substituting the values:</p> \[D_E(x) = \sqrt{(180 - 170)^2 + (75 - 70)^2} = \sqrt{10^2 + 5^2} = \sqrt{100 + 25} = \sqrt{125} \approx 11.18\] <p>This distance doesn‚Äôt account for the correlation between height and weight. It treats the two dimensions as if they are independent, and gives a straightforward measure of how far the point is from the mean in Euclidean space.</p> <h6 id="32-mahalanobis-distance-with-covariance-adjustment"><strong>3.2. Mahalanobis Distance</strong> (With Covariance Adjustment)</h6> <p>Now, let‚Äôs compute the Mahalanobis distance. First, we need to compute the inverse of the covariance matrix \(\Sigma^{-1}\).</p> <p>The determinant of \(\Sigma\) is:</p> \[\text{det}(\Sigma) = 100 \times 200 - 80^2 = 20000 - 6400 = 13600\] <p>So, the inverse covariance matrix is:</p> \[\Sigma^{-1} = \frac{1}{13600} \begin{pmatrix} 200 &amp; -80 \\ -80 &amp; 100 \end{pmatrix} = \begin{pmatrix} 0.0147 &amp; -0.0059 \\ -0.0059 &amp; 0.0074 \end{pmatrix}\] <p>Now, we compute the Mahalanobis distance:</p> \[D_M(x) = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)}\] <p>Substituting the values:</p> \[x - \mu = \begin{pmatrix} 180 - 170 \\ 75 - 70 \end{pmatrix} = \begin{pmatrix} 10 \\ 5 \end{pmatrix}\] <p>Now, calculate the Mahalanobis distance:</p> \[D_M(x) = \sqrt{\begin{pmatrix} 10 &amp; 5 \end{pmatrix} \begin{pmatrix} 0.0147 &amp; -0.0059 \\ -0.0059 &amp; 0.0074 \end{pmatrix} \begin{pmatrix} 10 \\ 5 \end{pmatrix}}\] <p>First, multiply the vectors:</p> \[\begin{pmatrix} 10 &amp; 5 \end{pmatrix} \begin{pmatrix} 0.0147 &amp; -0.0059 \\ -0.0059 &amp; 0.0074 \end{pmatrix} = \begin{pmatrix} 10 \times 0.0147 + 5 \times (-0.0059) \\ 10 \times (-0.0059) + 5 \times 0.0074 \end{pmatrix} = \begin{pmatrix} 0.147 - 0.0295 \\ -0.059 + 0.037 \end{pmatrix} = \begin{pmatrix} 0.1175 \\ -0.022 \end{pmatrix}\] <p>Now, multiply this result by the vector \(\begin{pmatrix} 10 \\ 5 \end{pmatrix}\):</p> \[\begin{pmatrix} 0.1175 &amp; -0.022 \end{pmatrix} \begin{pmatrix} 10 \\ 5 \end{pmatrix} = 0.1175 \times 10 + (-0.022) \times 5 = 1.175 - 0.11 = 1.065\] <p>Thus, the Mahalanobis distance is:</p> \[D_M(x) = \sqrt{1.065} \approx 1.03\] <h6 id="4-interpretation-of-the-results"><strong>4. Interpretation of the Results</strong>:</h6> <ul> <li>The <strong>Euclidean distance</strong> between the point and the mean was approximately <strong>11.18</strong>. This suggests that the point is far from the mean, without considering the correlation between height and weight.</li> <li>The <strong>Mahalanobis distance</strong> is <strong>1.03</strong>, which is much smaller. This is because the Mahalanobis distance accounts for the fact that height and weight are correlated. The deviation in weight is expected given the deviation in height, so the Mahalanobis distance treats this as less ‚Äúunusual.‚Äù</li> </ul> <h6 id="takeaways"><strong>Takeaways:</strong></h6> <ul> <li><strong>Euclidean distance</strong> treats each dimension as independent, ignoring correlations, which can lead to an overestimation of how unusual a point is.</li> <li><strong>Mahalanobis distance</strong>, by using the inverse covariance matrix \(\Sigma^{-1}\), adjusts for correlations and scales the deviations accordingly. This results in a more accurate measure of how far a point is from the mean, considering the underlying structure of the data (e.g., the correlation between height and weight in this example).</li> </ul> <hr/> <h5 id="grasping-better-with-bivariate-normal-distributions"><strong>Grasping Better with Bivariate Normal Distributions</strong></h5> <p>To build a deeper understanding, let‚Äôs focus on a specific case: the two-dimensional Gaussian, commonly referred to as the <strong>bivariate normal distribution</strong>.</p> <h6 id="case-1-identity-covariance-matrix"><strong>Case 1: Identity Covariance Matrix</strong></h6> <p>Suppose the covariance matrix is given as:</p> \[\Sigma = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}.\] <p>In this scenario, the contours of the distribution are circular. This indicates that there is no correlation between the two variables, and both have equal variances. The shape of the contours reflects the isotropic nature of the distribution.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-1-480.webp 480w,/assets/img/Multivariate-GNB-1-800.webp 800w,/assets/img/Multivariate-GNB-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h6 id="case-2-scaled-identity-covariance"><strong>Case 2: Scaled Identity Covariance</strong></h6> <p>If we scale the covariance matrix, say:</p> \[\Sigma = 0.5 \cdot \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix},\] <p>the variances of both variables decrease, resulting in smaller circular contours. Conversely, if we scale it up:</p> \[\Sigma = 2 \cdot \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix},\] <p>the variances increase, leading to larger circular contours. This demonstrates how scaling the covariance matrix affects the spread of the distribution.</p> <h6 id="case-3-anisotropic-variance"><strong>Case 3: Anisotropic Variance</strong></h6> <p>When the variances of the variables are different, such as when \(\text{var}(x_1) \neq \text{var}(x_2)\), the contours take on an elliptical shape. The orientation and eccentricity of the ellipse are determined by the relative variances along each axis.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-2-480.webp 480w,/assets/img/Multivariate-GNB-2-800.webp 800w,/assets/img/Multivariate-GNB-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h6 id="case-4-correlated-variables"><strong>Case 4: Correlated Variables</strong></h6> <p>Correlation between variables introduces an additional layer of complexity.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-3-480.webp 480w,/assets/img/Multivariate-GNB-3-800.webp 800w,/assets/img/Multivariate-GNB-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For instance, if:</p> \[\Sigma = \begin{bmatrix} 1 &amp; \rho \\ \rho &amp; 1 \end{bmatrix},\] <p>where \(\rho\) is the correlation coefficient:</p> <ul> <li>When \(\rho &gt; 0\), the variables are positively correlated, and the ellipse tilts along the diagonal.</li> <li>When \(\rho &lt; 0\), the variables are negatively correlated, and the ellipse tilts in the opposite direction.</li> <li>When \(\rho = 0\), the variables remain uncorrelated, resulting in circular or axis-aligned ellipses.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-4-480.webp 480w,/assets/img/Multivariate-GNB-4-800.webp 800w,/assets/img/Multivariate-GNB-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h4 id="gaussian-bayes-classifier"><strong>Gaussian Bayes Classifier</strong></h4> <p>The <strong>Gaussian Bayes Classifier (GBC)</strong> extends the Gaussian framework to classification tasks. It assumes that the conditional distribution \(p(x \vert y)\) follows a multivariate Gaussian distribution. Mathematically, for a class \(k\):</p> \[p(x|t = k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_k)^\top \Sigma_k^{-1} (x - \mu_k)\right),\] <p>where each class \(k\) has its own mean vector \(\mu_k\) and covariance matrix \(\Sigma_k\). The determinant \(\vert \Sigma_k \vert\) and the inverse \(\Sigma_k^{-1}\) are crucial components for computing probabilities.</p> <p>Estimating the parameters for each class becomes computationally challenging in high dimensions, as the covariance matrix has \(O(d^2)\) parameters. This complexity often necessitates simplifying assumptions to make the model tractable.</p> <p><strong>How do we arrive at this complexity, and why is it considered computationally challenging?</strong></p> <p>In the Gaussian Bayes Classifier, each class \(k\) has its own covariance matrix \(\Sigma_k\), which is a \(d \times d\) matrix where \(d\) is the dimensionality of the feature space. For a single class, this covariance matrix has \(\frac{d(d+1)}{2}\) unique parameters. This is because a covariance matrix is symmetric, meaning that the upper and lower triangular portions are mirrors of each other. Specifically, the diagonal elements represent the variances, while the off-diagonal elements represent the covariances between different features.</p> <p>For \(K\) classes, the total number of parameters required to estimate all the covariance matrices would be: \(K \times \frac{d(d+1)}{2}\)</p> <p>This can become computationally expensive, especially when \(d\) (the number of features) is large.</p> <p>This large number of parameters is the reason why the Gaussian Bayes Classifier faces challenges in high-dimensional settings, as the model needs to estimate these parameters from data, and estimating a large number of parameters requires a substantial amount of data. Moreover, when the dimensionality \(d\) is large relative to the number of training samples, the covariance matrix can become ill-conditioned or singular, which might lead to poor performance.</p> <p>To handle this, simplifications such as assuming diagonal covariance matrices (where off-diagonal covariances are set to zero) or sharing a common covariance matrix across all classes are often made, which reduces the number of parameters that need to be estimated.</p> <hr/> <h5 id="special-cases-of-gaussian-bayes-classifier"><strong>Special Cases of Gaussian Bayes Classifier</strong></h5> <p>To address the computational challenges, we consider the following special cases of the Gaussian Bayes Classifier:</p> <ol> <li> <p><strong>Full Covariance Matrix</strong><br/> Each class has its own covariance matrix \(\Sigma_k\). This allows for flexible modeling of the class distributions, as it can capture correlations between different features. The decision boundary, however, is quadratic, as the posterior probability depends on the quadratic term involving \((x - \mu_k)^\top \Sigma_k^{-1} (x - \mu_k)\).</p> <p><strong>Decision Boundary Derivation</strong>:<br/> The decision rule is based on the ratio of the posterior probabilities:</p> \[\frac{p(x \vert t = k)}{p(x \vert t = l)} &gt; 1\] <p>which leads to a quadratic expression involving the covariance matrices \(\Sigma_k\) and \(\Sigma_l\). This quadratic form creates a curved decision boundary.</p> <p><strong>Insight</strong>:<br/> Since each class can have a different covariance matrix, the decision boundary can bend and adapt to the data‚Äôs true distribution, allowing for accurate classification even in complex scenarios. However, the computational cost is high because each class requires estimating a full covariance matrix with \(O(d^2)\) parameters.</p> </li> <li> <p><strong>Shared Covariance Matrix</strong><br/> If all classes share a common covariance matrix \(\Sigma\), the decision boundary becomes linear. This assumption simplifies the model by treating all classes as having the same spread, reducing the number of parameters to estimate.</p> <p><strong>Decision Boundary Derivation</strong>:<br/> In this case, the likelihood for each class is given by:</p> \[p(x \vert t = k) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_k)^\top \Sigma^{-1} (x - \mu_k)\right)\] <p>The decision rule between two classes \(k\) and \(l\) simplifies to:</p> \[(x - \mu_k)^\top \Sigma^{-1} (x - \mu_k) - (x - \mu_l)^\top \Sigma^{-1} (x - \mu_l) = \text{constant}\] <p>which is linear in \(x\). This results in a linear decision boundary between the classes.</p> <p><strong>Insight</strong>:<br/> By assuming a common covariance matrix, we treat the class distributions as having the same shape and orientation. This simplifies the model and makes the decision boundary linear, leading to reduced computational cost and faster training. However, this may be less flexible if the true distributions of the classes are significantly different.</p> </li> <li> <p><strong>Naive Bayes Assumption</strong><br/> The Naive Bayes classifier assumes that the features are conditionally independent given the class, meaning that the covariance matrix is diagonal. This leads to a model where each feature is treated independently when making class predictions.</p> <p><strong>Decision Boundary Derivation</strong>:<br/> Under the Naive Bayes assumption, the covariance matrix is diagonal, so the likelihood for each class becomes:</p> \[p(x \vert t = k) = \prod_{i=1}^{d} \frac{1}{\sqrt{2\pi \sigma_{k,i}^2}} \exp\left(-\frac{(x_i - \mu_{k,i})^2}{2\sigma_{k,i}^2}\right)\] <p>The decision rule between two classes \(k\) and \(l\) leads to a quadratic expression for each feature, but since the features are independent, the decision boundary remains quadratic overall, as the product of exponentials leads to terms that depend on the squares of the feature values.</p> <p><strong>Insight</strong>:<br/> Even though the features are assumed to be independent, the decision boundary remains quadratic because of the feature-wise variances. The strong independence assumption makes the model computationally efficient, as it reduces the number of parameters to estimate (each class only requires \(d\) variances), but it limits the model‚Äôs flexibility to capture interactions between features.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-5-480.webp 480w,/assets/img/Multivariate-GNB-5-800.webp 800w,/assets/img/Multivariate-GNB-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="gaussian-bayes-classifier-vs-logistic-regression"><strong>Gaussian Bayes Classifier vs. Logistic Regression</strong></h5> <p>One interesting connection between GBC and logistic regression arises when the data is truly Gaussian. If we assume shared covariance matrices, the decision boundaries produced by GBC become identical to those of logistic regression. However, logistic regression is more versatile since it does not rely on Gaussian assumptions and can learn other types of decision boundaries.</p> <p><strong>Note:</strong> Even though both methods produce the same linear decision boundary under the Gaussian assumption with shared covariance, the actual parameter values (weights and means) will be different because they are derived from different models and assumptions.</p> <hr/> <h5 id="final-thoughts"><strong>Final Thoughts</strong></h5> <p>The multivariate Gaussian distribution provides a probabilistic framework for understanding data with correlated features. By extending this to classification tasks, the Gaussian Bayes Classifier offers an elegant and interpretable approach to modeling. However, its reliance on assumptions like Gaussianity and the complexity of covariance estimation in high dimensions present practical challenges.</p> <p>Generative models, like GBC, aim to model the joint distribution \(p(x, y)\), which contrasts with discriminative models, such as logistic regression, that focus directly on \(p(y \vert x)\). While generative models offer a principled way to derive loss functions via maximum likelihood, they can struggle with small datasets, where estimating the joint distribution becomes difficult. <strong>Why?</strong></p> <p>Generative models typically use the product of the likelihood and the prior. Specifically, the likelihood \(p(x \mid y)\), can become challenging with small datasets because:</p> <ul> <li><strong>Insufficient data for accurate parameter estimation</strong>: With limited data, the model may not have enough examples to accurately estimate the parameters of the distribution (such as the means and variances in the case of Gaussian distributions).</li> <li><strong>Overfitting risk</strong>: With small datasets, the model may overfit to noise or specific patterns that don‚Äôt generalize well, leading to poor estimates of the joint distribution.</li> </ul> <p>In contrast, discriminative models like logistic regression focus directly on \(p(y \mid x)\) and are less affected by small sample sizes because they only need to model the decision boundary between classes, making them more robust in such situations.</p> <p>As you delve deeper into probabilistic frameworks, a question worth pondering is: Do generative models have an equivalent form of regularization to mitigate overfitting? This opens up avenues for exploring how these models can be made more robust in practice.</p> <p>To address this question, we‚Äôll next explore <strong>Bayesian Inference</strong>, where generative models can incorporate priors over their parameters. For instance, in a Gaussian Bayes Classifier, instead of relying on point estimates for means and variances, Bayesian methods treat these parameters as distributions. This approach naturally regularizes the model by spreading probability mass over plausible parameter values, reducing the risk of overfitting. Stay tuned‚Äîwe‚Äôll dive into this in the next post!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.]]></summary></entry><entry><title type="html">Gaussian Naive Bayes - A Natural Extension</title><link href="https://monishver11.github.io/blog/2025/NB-continuous-features/" rel="alternate" type="text/html" title="Gaussian Naive Bayes - A Natural Extension"/><published>2025-01-20T20:39:00+00:00</published><updated>2025-01-20T20:39:00+00:00</updated><id>https://monishver11.github.io/blog/2025/NB-continuous-features</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/NB-continuous-features/"><![CDATA[<p>In the previous blog, we explored the Naive Bayes (NB) model for binary features and how it works under the assumption of conditional independence. However, real-world datasets often include continuous features. How can we extend the NB framework to handle such cases? Let‚Äôs dive into Gaussian Naive Bayes (GNB), a variant of NB that uses Gaussian distributions to model continuous inputs.</p> <p><strong>Before we start:</strong> I know this might be challenging to follow just by reading through, especially for this part. So, grab a pen and paper and work through it yourself. You‚Äôll notice that within the summations, all terms except the one you‚Äôre differentiating with respect to are constants and will drop out (i.e., become zero). As you write it out, you‚Äôll also understand why certain steps involve a change in sign. Working through it once will make everything much clearer and easier to grasp.</p> <hr/> <p>Consider a multiclass classification problem where each input feature \(x_i\) is continuous. To model \(p(x_i \mid y)\), we assume that the feature values follow a Gaussian (normal) distribution:</p> \[p(x_i \mid y = k) \sim \mathcal{N}(\mu_{i,k}, \sigma^2_{i,k}),\] <p>where \(\mu_{i,k}\) and \(\sigma^2_{i,k}\) are the mean and variance of \(x_i\) for class \(y = k\), respectively. Additionally, we model the class prior probabilities as:</p> \[p(y = k) = \theta_k.\] <p>With these assumptions, the likelihood of the dataset becomes:</p> \[p(D) = \prod_{n=1}^N p_\theta(x^{(n)}, y^{(n)})\] \[p(D) = \prod_{n=1}^N p(y^{(n)}) \prod_{i=1}^d p(x_i^{(n)} \mid y^{(n)}).\] <p>Substituting the Gaussian distribution for \(p(x_i \mid y)\), we get:</p> \[p(D) = \prod_{n=1}^N \theta_{y^{(n)}} \prod_{i=1}^d \frac{1}{\sqrt{2\pi\sigma_{i,y^{(n)}}^2}} \exp\left(-\frac{\left(x_i^{(n)} - \mu_{i,y^{(n)}}\right)^2}{2\sigma_{i,y^{(n)}}^2}\right).\] <p>It may seem complex at first, but if you look closely, you‚Äôll see that we‚Äôre applying the same principle. The only difference is in the distribution. To visualize this, we‚Äôve essentially applied the distribution to a familiar form \((1)\) once again to obtain the result. Take a moment to reflect on this.</p> \[\hat{y} = \arg\max_{y \in \mathcal{Y}} p(x, y; \theta) = \arg\max_{y} p(y \mid x; \theta) = \arg\max_{y} p(x \mid y; \theta) p(y; \theta) \tag{1}\] <hr/> <h4 id="learning-parameters-with-maximum-likelihood-estimation-mle"><strong>Learning Parameters with Maximum Likelihood Estimation (MLE)</strong></h4> <p>To train the Gaussian Naive Bayes model, we estimate the parameters \(\mu_{i,k}\), \(\sigma^2_{i,k}\), and \(\theta_k\) using MLE.</p> <h5 id="mean-mu_ik"><strong>Mean (\(\mu_{i,k}\)):</strong></h5> <p>The log-likelihood of the data is:</p> \[\ell = \sum_{n=1}^N \log \theta_{y^{(n)}} + \sum_{n=1}^N \sum_{i=1}^d \left[-\frac{1}{2} \log (2\pi \sigma_{i,y^{(n)}}^2) - \frac{\left(x_i^{(n)} - \mu_{i,y^{(n)}}\right)^2}{2\sigma_{i,y^{(n)}}^2}\right]\] <p>Taking the derivative with respect to \(\mu_{j,k}\) and setting it to zero gives:</p> \[\mu_{j,k} = \frac{\sum_{n:y^{(n)}=k} x_j^{(n)}}{\sum_{n:y^{(n)}=k} 1}\] <p>This is simply the sample mean of \(x_j\) for class \(k\).</p> <h5 id="derivation-of-mu_jk-for-gaussian-naive-bayes"><strong>Derivation of \(\mu_{j,k}\) for Gaussian Naive Bayes</strong></h5> <p>To estimate the parameter \(\mu_{j,k}\), the mean of feature \(x_j\) for class \(k\), we maximize the log-likelihood with respect to \(\mu_{j,k}\).</p> <p><strong>Step 1: Compute the Derivative of the Log-Likelihood</strong></p> <p>The log-likelihood is differentiated with respect to \(\mu_{j,k}\):</p> \[\frac{\partial}{\partial \mu_{j,k}} \ell = \frac{\partial}{\partial \mu_{j,k}} \sum_{n: y^{(n)} = k} \left( -\frac{1}{2 \sigma_{j,k}^2} \left( x_j^{(n)} - \mu_{j,k} \right)^2 \right)\] <p>Ignoring irrelevant terms (constants that do not depend on \(\mu_{j,k}\)), this simplifies to:</p> \[\frac{\partial}{\partial \mu_{j,k}} \ell = \sum_{n: y^{(n)} = k} \frac{1}{\sigma_{j,k}^2} \left( x_j^{(n)} - \mu_{j,k} \right)\] <p><strong>Step 2: Set the Derivative to Zero</strong></p> <p>To find the maximum likelihood estimate, set the derivative to zero:</p> \[\sum_{n: y^{(n)} = k} \frac{1}{\sigma_{j,k}^2} \left( x_j^{(n)} - \mu_{j,k} \right) = 0\] <p><strong>Step 3: Solve for \(\mu_{j,k}\)</strong></p> <p>Rearranging terms:</p> \[\sum_{n: y^{(n)} = k} x_j^{(n)} = \mu_{j,k} \sum_{n: y^{(n)} = k} 1\] <p>Divide both sides by \(\sum_{n: y^{(n)} = k} 1\):</p> \[\mu_{j,k} = \frac{\sum_{n: y^{(n)} = k} x_j^{(n)}}{\sum_{n: y^{(n)} = k} 1}\] <p><strong>Final Expression</strong></p> <p>The maximum likelihood estimate of \(\mu_{j,k}\) is:</p> \[\mu_{j,k} = \frac{\sum_{n: y^{(n)} = k} x_j^{(n)}}{\sum_{n: y^{(n)} = k} 1}\] <p><strong>Interpretation:</strong></p> <ul> <li>\(\mu_{j,k}\) is the sample mean of \(x_j\) for all data points in class \(k\).</li> <li>This parameter is essential for defining the Gaussian distribution for feature \(x_j\) given class \(k\) in Gaussian Naive Bayes.</li> </ul> <h5 id="variance-sigma2_ik"><strong>Variance (\(\sigma^2_{i,k}\)):</strong></h5> <p>Similarly, the variance for feature \(x_j\) in class \(k\) is:</p> \[\sigma^2_{j,k} = \frac{\sum_{n:y^{(n)}=k} \left(x_j^{(n)} - \mu_{j,k}\right)^2}{\sum_{n:y^{(n)}=k} 1}\] <h5 id="class-prior-theta_k"><strong>Class Prior (\(\theta_k\)):</strong></h5> <p>The class prior \(\theta_k\) is estimated as the proportion of data points belonging to class \(k\):</p> \[\theta_k = \frac{\sum_{n:y^{(n)}=k} 1}{N}\] <h5 id="derivation-of-sigma_jk2-sample-variance-and-theta_k-class-prior"><strong>Derivation of \(\sigma_{j,k}^2\) (Sample Variance) and \(\theta_k\) (Class Prior)</strong></h5> <p><strong>1. Derivation of \(\sigma_{j,k}^2\) (Sample Variance)</strong></p> <p>To derive the sample variance \(\sigma_{j,k}^2\), we start from the log-likelihood of the Gaussian distribution for feature \(x_j\) within class \(k\):</p> \[\ell = \sum_{n: y^{(n)} = k} \left[ -\frac{1}{2} \log(2\pi \sigma_{j,k}^2) - \frac{\left( x_j^{(n)} - \mu_{j,k} \right)^2}{2\sigma_{j,k}^2} \right]\] <p>We take the derivative of \(\ell\) with respect to \(\sigma_{j,k}^2\) and set it to zero:</p> \[\frac{\partial \ell}{\partial \sigma_{j,k}^2} = \sum_{n: y^{(n)} = k} \left[ -\frac{1}{2\sigma_{j,k}^2} + \frac{\left( x_j^{(n)} - \mu_{j,k} \right)^2}{2\sigma_{j,k}^4} \right] = 0\] <p>Simplify the equation:</p> \[\sum_{n: y^{(n)} = k} \left[ -\sigma_{j,k}^2 + \left( x_j^{(n)} - \mu_{j,k} \right)^2 \right] = 0\] <p>Divide by \(\sigma_{j,k}^2\) and rearrange:</p> \[\sigma_{j,k}^2 = \frac{\sum_{n: y^{(n)} = k} \left( x_j^{(n)} - \mu_{j,k} \right)^2}{\sum_{n: y^{(n)} = k} 1}\] <p>Thus, the MLE for \(\sigma_{j,k}^2\) is:</p> \[\sigma_{j,k}^2 = \frac{\sum_{n: y^{(n)} = k} \left( x_j^{(n)} - \mu_{j,k} \right)^2}{\sum_{n: y^{(n)} = k} 1}\] <p><strong>2. Derivation of \(\theta_k\) (Class Prior)</strong></p> <p>The class prior \(\theta_k\) represents the proportion of data points belonging to class \(k\) in the dataset. It is given by:</p> \[\theta_k = \frac{\sum_{n: y^{(n)} = k} 1}{N}\] <p><strong>Steps:</strong></p> <ol> <li><strong>Numerator</strong>: \(\sum_{n: y^{(n)} = k} 1\) counts the total number of data points that belong to class \(k\).</li> <li><strong>Denominator</strong>: \(N\) is the total number of data points in the entire dataset.</li> </ol> <p><strong>Finally,</strong></p> <ol> <li> <p><strong>Sample Variance</strong>:</p> \[\sigma_{j,k}^2 = \frac{\sum_{n: y^{(n)} = k} \left( x_j^{(n)} - \mu_{j,k} \right)^2}{\sum_{n: y^{(n)} = k} 1}\] </li> <li> <p><strong>Class Prior</strong>:</p> \[\theta_k = \frac{\sum_{n: y^{(n)} = k} 1}{N}\] </li> </ol> <ul> <li>The sample variance \(\sigma_{j,k}^2\) measures the spread of feature \(x_j\) for class \(k\), derived using MLE.</li> <li>The class prior \(\theta_k\) represents the proportion of data points in class \(k\), computed directly from the dataset.</li> </ul> <hr/> <h4 id="decision-boundary-of-the-gaussian-naive-bayes-gnb-model"><strong>Decision Boundary of the Gaussian Naive Bayes (GNB) Model</strong></h4> <p><strong>General Formulation of the Decision Boundary:</strong></p> <p>For binary classification (\(y \in \{0, 1\}\)), the <strong>log odds ratio</strong> is expressed as:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \log \frac{p(x \mid y=1)p(y=1)}{p(x \mid y=0)p(y=0)}.\] <p>If you‚Äôre unclear about what the log odds ratio is, it represents the logarithm of the ratio of the probabilities of the two classes. By setting the log odds ratio to zero, we identify the points where the model is equally likely to classify a sample as belonging to either class.</p> <p>In Gaussian Naive Bayes, this involves substituting the Gaussian distributions for \(p(x \mid y)\), simplifying the expression, and determining whether the resulting decision boundary is quadratic or linear based on the assumptions about the variances.</p> <p>Thus, the log odds ratio serves as a straightforward mathematical tool to derive the decision boundary by locating the regions where the probabilities of the two classes are equal.</p> <p>So, the conditional distributions \(p(x_i \mid y)\) are Gaussian:</p> \[p(x_i \mid y) = \frac{1}{\sqrt{2\pi \sigma_{i,y}^2}} \exp\left(-\frac{(x_i - \mu_{i,y})^2}{2\sigma_{i,y}^2}\right).\] <p>Substituting this into the log odds equation, we get:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \log \frac{\theta_1}{\theta_0} + \sum_{i=1}^d \left[\log \sqrt{\frac{\sigma_{i,0}^2}{\sigma_{i,1}^2}} + \frac{(x_i - \mu_{i,0})^2}{2\sigma_{i,0}^2} - \frac{(x_i - \mu_{i,1})^2}{2\sigma_{i,1}^2}\right].\] <p>This equation represents the <strong>general case</strong> of the GNB decision boundary.</p> <h5 id="linear-vs-quadratic-decision-boundaries"><strong>Linear vs. Quadratic Decision Boundaries</strong></h5> <h6 id="quadratic-decision-boundary"><strong>Quadratic Decision Boundary:</strong></h6> <p>In the general case, where the variances \(\sigma_{i,0}^2\) and \(\sigma_{i,1}^2\) differ between classes, the decision boundary is <strong>quadratic</strong>. This is due to the presence of quadratic terms in the numerator:</p> \[\frac{(x_i - \mu_{i,0})^2}{2\sigma_{i,0}^2} - \frac{(x_i - \mu_{i,1})^2}{2\sigma_{i,1}^2}.\] <h6 id="linear-decision-boundary"><strong>Linear Decision Boundary:</strong></h6> <p>When we assume the variances are equal for both classes \((\sigma_{i,0}^2 = \sigma_{i,1}^2 = \sigma_i^2)\), the quadratic terms cancel out. Simplifying the log odds equation yields:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \sum_{i=1}^d \frac{\mu_{i,1} - \mu_{i,0}}{\sigma_i^2} x_i + \sum_{i=1}^d \frac{\mu_{i,0}^2 - \mu_{i,1}^2}{2\sigma_i^2}.\] <p>In matrix form, this becomes:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \theta^\top x + \theta_0,\] <p>where:</p> <ul> <li> \[\theta_i = \frac{\mu_{i,1} - \mu_{i,0}}{\sigma_i^2}, \quad i \in [1, d]\] </li> <li> \[\theta_0 = \sum_{i=1}^d \frac{\mu_{i,0}^2 - \mu_{i,1}^2}{2\sigma_i^2}.\] </li> </ul> <p>Thus, under the shared variance assumption, the decision boundary is <strong>linear</strong>.</p> <p><strong>Takeaways:</strong></p> <ul> <li><strong>Quadratic Boundary</strong>: The difference in variances between the two classes introduces curvature, resulting in a nonlinear boundary.</li> <li><strong>Linear Boundary</strong>: Equal variances lead to a linear boundary, making the model behave similarly to logistic regression.</li> </ul> <p>This derivation connects Gaussian Naive Bayes to logistic regression and helps to understand its behavior under different assumptions.</p> <hr/> <h4 id="naive-bayes-vs-logistic-regression"><strong>Naive Bayes vs. Logistic Regression</strong></h4> <p>Both Naive Bayes and logistic regression are popular classifiers, but they differ fundamentally in their approach:</p> <hr/> <table> <thead> <tr> <th>¬†</th> <th><strong>Logistic Regression</strong></th> <th><strong>Gaussian Naive Bayes</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Model Type</strong></td> <td>Conditional/Discriminative</td> <td>Generative</td> </tr> <tr> <td><strong>Parametrization</strong></td> <td>\(p(y \mid x)\)</td> <td>\(p(x \mid y), p(y)\)</td> </tr> <tr> <td><strong>Assumptions on Y</strong></td> <td>Bernoulli</td> <td>Bernoulli</td> </tr> <tr> <td><strong>Assumptions on X</strong></td> <td>‚Äî</td> <td>Gaussian</td> </tr> <tr> <td><strong>Decision Boundary</strong></td> <td>\(\theta_{LR}^\top x\)</td> <td>\(\theta_{GNB}^\top x\)</td> </tr> </tbody> </table> <hr/> <ul> <li> <p><strong>Logistic Regression (LR)</strong> is a discriminative model that directly models the conditional probability \(p(y \mid x)\). It does not make assumptions about the distribution of features \(X\) but instead focuses on finding a decision boundary that separates the classes based on the observed data.</p> </li> <li> <p><strong>Gaussian Naive Bayes (GNB)</strong>, on the other hand, is a generative model that explicitly models the joint distribution \(p(x, y)\) by assuming that the features \(X\) are conditionally Gaussian given the class \(y\).</p> </li> </ul> <p>A few questions to address before we wrap up.</p> <p><strong>Question 1:</strong> Given the same training data, is \(\theta_{LR} = \theta_{GNB}\)?</p> <ul> <li>This is a critical question to explore the relationship between discriminative and generative models. While the forms of the decision boundary (e.g., linear) may look similar under certain assumptions (e.g., shared variance in GNB), the parameters \(\theta_{LR}\) and \(\theta_{GNB}\) are generally not the same due to differences in how the two models approach the learning process.</li> </ul> <p><strong>Question 2:</strong> Relationship Between LR and GNB</p> <ul> <li>Logistic regression and Gaussian naive Bayes <strong>converge to the same classifier asymptotically</strong>, assuming the GNB assumptions hold: <ol> <li>Data points are generated from Gaussian distributions for each class.</li> <li>Each dimension of the feature vector is generated independently.</li> <li>Both classes share the same variance for each feature (shared variance assumption).</li> </ol> </li> <li>Under these conditions, the decision boundary derived from GNB becomes identical to that of logistic regression as the amount of data increases.</li> </ul> <p><strong>Question 3:</strong> What Happens if the GNB Assumptions Are Not True?</p> <ul> <li>If the assumptions of GNB are violated (e.g., features are not Gaussian, dimensions are not independent, or variances are not shared), the decision boundary derived by GNB can deviate significantly from the optimal boundary. In such cases: <ul> <li><strong>Logistic Regression</strong> is likely to perform better, as it does not rely on specific assumptions about the feature distributions.</li> <li><strong>GNB</strong> may produce suboptimal results because its assumptions are hardcoded into the model and do not adapt to the true data distribution.</li> </ul> </li> </ul> <p>Thus, the choice between LR and GNB depends heavily on whether the data aligns with GNB‚Äôs assumptions.</p> <hr/> <h4 id="generative-vs-discriminative-models-trade-offs"><strong>Generative vs. Discriminative Models: Trade-offs</strong></h4> <p>The contrast between Naive Bayes and logistic regression highlights the differences between <strong>generative</strong> and <strong>discriminative</strong> models. Generative models like Naive Bayes model the joint distribution \(p(x, y)\), allowing them to generate data as well as make predictions. In contrast, discriminative models like logistic regression focus directly on \(p(y \mid x)\), optimizing for classification accuracy.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Generative_vs_Discriminative_models-480.webp 480w,/assets/img/Generative_vs_Discriminative_models-800.webp 800w,/assets/img/Generative_vs_Discriminative_models-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Generative_vs_Discriminative_models.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Generative_vs_Discriminative_models" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This tradeoff is explored in the classic paper by Ng, A. and Jordan, M. (2002), On discriminative versus generative classifiers: A comparison of logistic regression and naive Bayes., which shows that generative models converge faster but may have higher asymptotic error compared to their discriminative counterparts.</p> <hr/> <p>In the next section, we‚Äôll explore the Multivariate Gaussian Distribution and the Gaussian Bayes Classifier in greater detail. Stay tunedüëã!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.]]></summary></entry><entry><title type="html">An Introduction to Generative Models - Naive Bayes for Binary Features</title><link href="https://monishver11.github.io/blog/2025/generative-models/" rel="alternate" type="text/html" title="An Introduction to Generative Models - Naive Bayes for Binary Features"/><published>2025-01-20T20:29:00+00:00</published><updated>2025-01-20T20:29:00+00:00</updated><id>https://monishver11.github.io/blog/2025/generative-models</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/generative-models/"><![CDATA[<p>Generative models represent a powerful class of machine learning techniques. Unlike methods that directly map inputs \(x\) to outputs \(y\), such as generalized linear models or perceptrons, generative models take a broader approach. They aim to model the <strong>joint distribution</strong> \(p(x, y; \theta)\), which allows us to capture the underlying relationships between inputs and outputs in a holistic manner.</p> <h4 id="generalized-linear-models-vs-generative-models"><strong>Generalized Linear Models vs. Generative Models</strong></h4> <p>To recall, generalized linear models focus on the conditional distribution \(p(y \mid x; \theta)\). By contrast, generative models model \(p(x, y; \theta)\). Once we have the joint distribution, we can predict labels for new data by leveraging the following rule:</p> \[\hat{y} = \arg\max_{y \in \mathcal{Y}} p(x, y; \theta)\] <p>This prediction process connects naturally to conditional distributions. Using <strong>Bayes‚Äô Rule</strong>, we can rewrite \(p(y \mid x)\) as:</p> \[p(y \mid x) = \frac{p(x \mid y) p(y)}{p(x)}\] <p>In practice, we often bypass computing \(p(x)\), as it is independent of \(y\). Instead, predictions simplify to:</p> \[\hat{y} = \arg\max_{y} p(y \mid x) = \arg\max_{y} p(x \mid y) p(y)\] <p>With this foundation, let us explore one of the most straightforward and widely used generative models: <strong>Naive Bayes (NB)</strong>.</p> <p>If you‚Äôre unable to follow the above formulation, here‚Äôs a quick refresher on Bayes‚Äô Rule to help you out.</p> <p>Bayes‚Äô Rule relates conditional probabilities to joint and marginal probabilities. It can be expressed as:</p> \[p(y \mid x) = \frac{p(x, y)}{p(x)} = \frac{p(x \mid y) p(y)}{p(x)},\] <p>where:</p> <ul> <li>\(p(y \mid x)\): Posterior probability of \(y\) given \(x\),</li> <li>\(p(x, y)\): Joint probability of \(x\) and \(y\),</li> <li>\(p(x \mid y)\): Likelihood of \(x\) given \(y\),</li> <li>\(p(y)\): Prior probability of \(y\),</li> <li>\(p(x)\): Marginal probability of \(x\), which ensures proper normalization.</li> </ul> <hr/> <h4 id="naive-bayes-a-simple-and-effective-generative-model"><strong>Naive Bayes: A Simple and Effective Generative Model</strong></h4> <p>To understand Naive Bayes, consider a simple yet practical problem: binary text classification. Imagine we want to classify a document as either a <strong>fake review</strong> or a <strong>genuine review</strong>. This setup offers a clear context to explore the mechanics of generative modeling.</p> <h5 id="representing-documents-as-features"><strong>Representing Documents as Features</strong></h5> <p>To make this task computationally feasible, we use a <strong>bag-of-words representation</strong>. A document is expressed as a binary vector \(x\), where:</p> \[x = [x_1, x_2, \dots, x_d].\] <p>Here, \(d\) represents the vocabulary size, and each \(x_i\) indicates whether the \(i\)-th word in the vocabulary exists in the document (\(x_i = 1\)) or not (\(x_i = 0\)).</p> <h5 id="modeling-the-joint-probability-of-documents-and-labels"><strong>Modeling the Joint Probability of Documents and Labels</strong></h5> <p>For a document \(x\) with label \(y\), the joint probability \(p(x, y)\) can be expressed using the <strong>chain rule of probability</strong>:</p> \[p(x \mid y) = p(x_1, x_2, \dots, x_d \mid y) = p(x_1 \mid y) p(x_2 \mid y, x_1) \cdots p(x_d \mid y, x_{d-1}, \dots, x_1).\] \[p(x \mid y) = \prod_{i=1}^d p(x_i \mid y, x_{&lt;i}),\] <p>However, modeling the dependencies between features (\(x_1, x_2, \dots, x_d\)) becomes intractable as the number of features grows and hard to estimate. This is where Naive Bayes introduces its defining assumption.</p> <h5 id="the-naive-bayes-assumption"><strong>The Naive Bayes Assumption</strong></h5> <p>Naive Bayes simplifies the problem by assuming that <strong>features are conditionally independent given the label \(y\)</strong>. Mathematically, this means:</p> \[p(x \mid y) = \prod_{i=1}^d p(x_i \mid y)\] <p>This assumption significantly reduces computational complexity while often delivering excellent results in practice. While the assumption of conditional independence may not hold in all cases, it is surprisingly effective in many real-world applications.</p> <hr/> <h4 id="parameterizing-the-naive-bayes-model"><strong>Parameterizing the Naive Bayes Model</strong></h4> <p>To make predictions, we need to parameterize the probabilities \(p(x_i \mid y)\) and \(p(y)\).</p> <p><strong><em>Why?</em></strong> Parameterizing these distributions allows us to learn the necessary values (e.g., \(\theta\)) from data in a structured way.</p> <h5 id="binary-features"><strong>Binary Features</strong></h5> <p>For simplicity, let us assume the features \(x_i\) are binary (\(x_i \in \{0, 1\}\)). We model \(p(x_i \mid y)\) as Bernoulli distributions:</p> \[p(x_i = 1 \mid y = 1) = \theta_{i,1}, \quad p(x_i = 1 \mid y = 0) = \theta_{i,0}\] <p>Similarly, the label distribution is modeled as:</p> \[p(y = 1) = \theta_0\] <p><strong><em>How do we arrive at these definitions?</em></strong> These definitions arise from the following assumptions and modeling principles:</p> <ol> <li><strong>Binary Nature of Features</strong>: Since the features \(x_i\) are binary (\(x_i \in \{0, 1\}\)), we need a probability distribution that models the likelihood of binary outcomes. The Bernoulli distribution is a natural choice for this.</li> <li><strong>Parameterization with Bernoulli Distributions</strong>: <ul> <li>For \(p(x_i \mid y)\), the Bernoulli distribution models the probability that \(x_i = 1\) for each possible value of \(y\).</li> <li>We introduce parameters \(\theta_{i,1}\) and \(\theta_{i,0}\), which represent the probability of \(x_i = 1\) given \(y = 1\) and \(y = 0\), respectively.</li> </ul> </li> <li><strong>Label Distribution \(p(y)\)</strong>: <ul> <li>The label \(y\) is also binary (\(y \in \{0, 1\}\)), so we model \(p(y)\) using a Bernoulli distribution with a single parameter \(\theta_0\), where \(\theta_0 = p(y = 1)\).</li> <li>This parameter reflects the prior probability of the positive class.</li> </ul> </li> <li><strong>Learning from Data</strong>: These parameters (\(\theta_{i,1}, \theta_{i,0}, \theta_0\)) are learned from data using methods like Maximum Likelihood Estimation (MLE), ensuring that the model reflects the observed distribution of features and labels in the dataset.</li> </ol> <p>Thus, the definitions provide a straightforward and interpretable way to model binary features and labels within the Naive Bayes framework.</p> <p>With these definitions, the joint probability \(p(x, y)\) can be written as (<strong>with NB assumption</strong>):</p> \[p(x, y) = p(y) \prod_{i=1}^d p(x_i \mid y)\] <p>Substituting the probabilities for binary features:</p> \[p(x, y) = p(y) \prod_{i=1}^d \theta_{i,y}{\mathbb{I}\{x_i = 1\}} + (1 - \theta_{i,y}){\mathbb{I}\{x_i = 0\}}\] <p>Here, \(\mathbb{I}\{\text{condition}\}\) is an indicator function that evaluates to 1 if the condition is true and 0 otherwise.</p> <p><strong><em>How to intuitively understand this equation?</em></strong> This equation represents the joint probability \(p(x, y)\) by combining the prior probability \(p(y)\) with the product of the individual probabilities \(p(x_i \mid y)\) for each feature \(x_i\). Here:</p> <ol> <li>For each feature \(x_i\), the term \(\mathbb{I}\{x_i = 1\}\) ensures that the corresponding parameter \(\theta_{i,y}\) is used if \(x_i = 1\), while \(\mathbb{I}\{x_i = 0\}\) ensures that \((1 - \theta_{i,y})\) is used if \(x_i = 0\).</li> <li>The product \(\prod_{i=1}^d\) combines the contributions of all features under the Naive Bayes assumption of conditional independence.</li> <li>Finally, multiplying by \(p(y)\) incorporates the prior belief about the label \(y\), providing the full joint distribution \(p(x, y)\).</li> </ol> <p>By this decomposition, we can efficiently compute \(p(x, y)\) for classification tasks.</p> <hr/> <h4 id="learning-parameters-with-maximum-likelihood-estimation-mle"><strong>Learning Parameters with Maximum Likelihood Estimation (MLE)</strong></h4> <p>The parameters \(\theta\) of the Naive Bayes model are learned by maximizing the likelihood of the observed data. Given a dataset of \(N\) labeled examples \(\{(x^{(n)}, y^{(n)})\}_{n=1}^N\), the likelihood of the data is:</p> \[\prod_{n=1}^N p_\theta(x^{(n)}, y^{(n)})\] <p>Taking the logarithm of the likelihood to simplify optimization, we obtain the log-likelihood:</p> \[\ell(\theta) = \sum_{n=1}^N \log p_\theta(x^{(n)}, y^{(n)})\] <p>For binary features, substituting the joint probability \(p_\theta(x, y)\) (as defined earlier) gives:</p> \[\ell(\theta) = \sum_{n=1}^N \left[ \sum_{i=1}^d \log ( \mathbb{I}\{x_i^{(n)} = 1\} \theta_{i,y^{(n)}} + \mathbb{I}\{x_i^{(n)} = 0\} (1 - \theta_{i,y^{(n)}}) ) \right] + \log p_\theta(y^{(n)})\] <p>Focusing on a specific feature \(x_j\) and label \(y = 1\), the relevant portion of the log-likelihood is:</p> \[\ell(\theta) = \sum_{n=1}^N \log \left[ \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\} \theta_{j,1} + \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\} (1 - \theta_{j,1}) \right] \tag{1}\] <p><strong>Step 1: Derivative of the Log-Likelihood</strong></p> <p>Taking the derivative of the log-likelihood with respect to \(\theta_{j,1}\):</p> \[\frac{\partial \ell}{\partial \theta_{j,1}} = \sum_{n=1}^N \left[ \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}}{\theta_{j,1}} - \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\}}{1 - \theta_{j,1}} \right] \tag{2}\] <p><strong>Did you follow the derivative?</strong> You might be wondering how derivative \(\log(a + b)\) can be written as \(\frac{1}{a} + \frac{1}{b}\), right? If so, that‚Äôs a completely valid question ‚Äî I had the same thought myself. Here‚Äôs the explanation.</p> <p>The transition from equation (1) to equation (2) involves taking the derivative of the log-likelihood with respect to \(\theta_{j,1}\). Let‚Äôs break it down:</p> \[\frac{\partial}{\partial \theta_{j,1}} \ell = \frac{\partial}{\partial \theta_{j,1}} \sum_{n=1}^{N} \left[ \log \left( \theta_{j,1} I\{ x_j(n) = 1 \} + (1 - \theta_{j,1}) I\{ x_j(n) = 0 \} \right) \right]\] <p>Here, the derivative is applied to the logarithm term. Using the chain rule, we first compute the derivative of the logarithm, which is:</p> \[\frac{\partial}{\partial \theta_{j,1}} \log(f(\theta_{j,1})) = \frac{1}{f(\theta_{j,1})} \cdot \frac{\partial f(\theta_{j,1})}{\partial \theta_{j,1}},\] <p>where</p> \[f(\theta_{j,1}) = \theta_{j,1} I\{ x_j(n) = 1 \} + (1 - \theta_{j,1}) I\{ x_j(n) = 0 \}.\] <p>For a single \(n\), the term inside the logarithm is:</p> \[f(\theta_{j,1}) = \begin{cases} \theta_{j,1}, &amp; \text{if } x_j(n) = 1, \\ 1 - \theta_{j,1}, &amp; \text{if } x_j(n) = 0. \end{cases}\] <p>The derivative of \(f(\theta_{j,1})\) with respect to \(\theta_{j,1}\) is:</p> \[\frac{\partial f(\theta_{j,1})}{\partial \theta_{j,1}} = \begin{cases} 1, &amp; \text{if } x_j(n) = 1, \\ -1, &amp; \text{if } x_j(n) = 0. \end{cases}\] <p>Using the chain rule:</p> \[\frac{\partial}{\partial \theta_{j,1}} \log(f(\theta_{j,1})) = \begin{cases} \frac{1}{\theta_{j,1}}, &amp; \text{if } x_j(n) = 1, \\ \frac{-1}{1 - \theta_{j,1}}, &amp; \text{if } x_j(n) = 0. \end{cases}\] <p>Applying this to the summation over \(N\):</p> \[\frac{\partial}{\partial \theta_{j,1}} \ell = \sum_{n=1}^{N} \left[ I\{ y(n) = 1 \land x_j(n) = 1 \} \frac{1}{\theta_{j,1}} - I\{ y(n) = 1 \land x_j(n) = 0 \} \frac{1}{1 - \theta_{j,1}} \right].\] <p>This is exactly what equation (48) represents, showing the decomposition of the derivative into two terms for \(x_j(n) = 1\) and \(x_j(n) = 0\).</p> <p>The simplification uses the indicator functions \(I\) to select the appropriate cases, where:</p> \[I\{ x_j(n) = 1 \} \quad \text{contributes} \quad \frac{1}{\theta_{j,1}},\] <p>and</p> \[I\{ x_j(n) = 0 \} \quad \text{contributes} \quad \frac{1}{1 - \theta_{j,1}}.\] <p><strong>Key Insight:</strong></p> <p>At each step of the derivation, we are dealing with a <strong>single term</strong> inside the logarithm. As a result, when we take the derivative of the logarithm, the result is simply \(\frac{1}{\text{term}}\), where the term is either \(\theta_{j,1}\) or \(1 - \theta_{j,1}\), depending on the value of \(x_j(n)\).</p> <ul> <li>If \(x_j(n) = 1\), the term inside the log is \(\theta_{j,1}\), and its derivative is \(\frac{1}{\theta_{j,1}}\).</li> <li>If \(x_j(n) = 0\), the term inside the log is \(1 - \theta_{j,1}\), and its derivative is \(\frac{-1}{1 - \theta_{j,1}}\).</li> </ul> <p>Thus, at each \(n\), we compute the derivative as \(\frac{1}{\text{term}}\), with the specific term depending on the value of \(x_j(n)\). This makes the process more straightforward as we apply it term by term across all \(N\) data points.</p> <p>I hope that makes sense now. Let‚Äôs continue.</p> <p><strong>Step 2: Setting the Derivative to Zero</strong></p> <p>To find the maximum likelihood estimate, we set the derivative to zero:</p> \[\sum_{n=1}^N \left[ \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}}{\theta_{j,1}} \right] = \sum_{n=1}^N \left[ \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\}}{1 - \theta_{j,1}} \right]\] <p>Simplifying:</p> \[\theta_{j,1} \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \} = \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}\] <p>The above simplification is quite straightforward. I encourage you to write it out for yourself and work through the steps. Simply <strong>multiply both sides</strong> by \(\theta_{j,1}(1 - \theta_{j,1})\) to eliminate the denominators, then expand both sides and <strong>isolate</strong> \(\theta_{j,1}\).</p> <p><strong>Note</strong>:</p> \[\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\} + \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\} = \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \}\] <p><strong>Step 3: Solving for \(\theta_{j,1}\)</strong></p> <p>Rearranging to isolate \(\theta_{j,1}\):</p> \[\theta_{j,1} = \frac{\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}}{\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1\}}\] <p><strong>Interpretation:</strong> This estimate corresponds to the fraction of examples with \(y = 1\) in which the \(j\)-th feature \(x_j\) is active (i.e., \(x_j = 1\)). Intuitively, it represents the conditional probability \(p(x_j = 1 \mid y = 1)\) under the Naive Bayes assumption.</p> <hr/> <h5 id="next-steps"><strong>Next Steps:</strong></h5> <ol> <li> <p><strong>Compute the other \(\theta_{i,y}\) values</strong>: You should calculate the parameters for all other features in the model (for example, \(\theta_{i,0}\) and \(\theta_{i,1}\) for binary features). These values represent the probability of a feature given a class, so you‚Äôll continue by maximizing the likelihood for each \(i\) and class \(y\) to estimate these parameters.</p> </li> <li> <p><strong>Estimate \(p(y)\)</strong>: You‚Äôll also need to compute the class prior probability \(p(y)\), which is simply the proportion of each class in the training data. This can be done by counting how many times each class label appears and normalizing by the total number of examples.</p> </li> </ol> \[\theta_0 = \frac{\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1\}}{N}\] <ul> <li>\(\theta_0\) is the proportion of samples in the dataset that belong to the class \(y = 1\).</li> <li>It serves as the prior probability of \(y = 1\).</li> </ul> <p><strong>Substituting the Probabilities for Binary Features:</strong></p> <p>The likelihood of the joint probability \(p(x, y)\) can be expressed as:</p> \[p(x, y) = p(y) \prod_{i=1}^d \theta_{i,y} {\mathbb{I}\{x_i = 1\}} + (1 - \theta_{i,y}) {\mathbb{I}\{x_i = 0\}}\] <p>Where \(\mathbb{I}\{x_i = 1\}\) is an indicator function that equals 1 when \(x_i = 1\), and \(\mathbb{I}\{x_i = 0\}\) equals 1 when \(x_i = 0\).</p> <p><strong>Remember this equation; it‚Äôs the one we started with.</strong></p> <p>Once all parameters are estimated, you will have a fully parameterized Naive Bayes model. The model can then be used for prediction by computing the posterior probabilities for each class \(y\) given an input \(x\). For prediction, you would use the formula:</p> \[\hat{y} = \arg\max_{y \in Y} p(y) \prod_{i=1}^d p(x_i \mid y)\] <p>Where \(p(x_i \mid y)\) are the feature likelihoods, and \(p(y)\) is the class prior. The class with the highest posterior probability is chosen as the predicted label. This approach allows Naive Bayes to make efficient, probabilistic predictions based on the learned parameters.</p> <p><strong>So, the fundamental idea is:</strong></p> <p>You are estimating the parameters \(\theta\) for all possible features and classes. Once the parameters are learned, you apply Bayes‚Äô rule to compute the posterior probability for each class \(y\). Finally, you take the class that maximizes the posterior probability using:</p> \[\hat{y} = \arg\max_y p(y \mid x)\] <p>This gives you the predicted class \(\hat{y}\), based on the learned parameters from the training data.</p> <hr/> <h5 id="recipe-for-learning-a-naive-bayes-model"><strong>Recipe for Learning a Naive Bayes Model:</strong></h5> <ol> <li><strong>Choose \(p(x_i \mid y)\)</strong>: Select an appropriate distribution for the features, e.g., Bernoulli distribution for binary features \(x_i\).</li> <li><strong>Choose \(p(y)\)</strong>: Typically, use a categorical distribution for the class labels.</li> <li><strong>Estimate Parameters by MLE</strong>: Use Maximum Likelihood Estimation (MLE) to estimate the parameters, following the same strategy used in conditional models.</li> </ol> <h5 id="where-do-we-go-from-here"><strong>Where Do We Go From Here?</strong></h5> <p>So far, we have focused on modeling binary features. However, many real-world datasets involve continuous features. How can Naive Bayes be extended to handle such cases? In the next blog, we‚Äôll explore Naive Bayes for continuous features and see how this simple model adapts to more complex data types. See you there!</p> ]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.]]></summary></entry><entry><title type="html">Generalized Linear Models Explained - Leveraging MLE for Regression and Classification</title><link href="https://monishver11.github.io/blog/2025/MLE/" rel="alternate" type="text/html" title="Generalized Linear Models Explained - Leveraging MLE for Regression and Classification"/><published>2025-01-18T15:45:00+00:00</published><updated>2025-01-18T15:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/MLE</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/MLE/"><![CDATA[<p>When building machine learning models, one of the most important tasks is estimating the parameters of a model in a way that best explains the observed data. This is where the principle of <strong>Maximum Likelihood Estimation (MLE)</strong> comes into play. MLE provides a rigorous framework for parameter estimation, grounded in probability theory, and is widely used across regression, classification, and beyond.</p> <p>Suppose we have a probabilistic model and a dataset \(D\). The central question is: how do we estimate the parameters \(\theta\) of the model? According to MLE, we should choose \(\theta\) to maximize the likelihood of the observed data. Formally, the likelihood function is defined as:</p> \[L(\theta) \stackrel{\text{def}}{=} p(D; \theta),\] <p>which captures how likely the dataset \(D\) is, given the model parameters \(\theta\).</p> <p>If the dataset consists of \(N\) independent and identically distributed (iid) examples, the likelihood simplifies to a product of individual data likelihoods:</p> \[L(\theta) = \prod_{n=1}^{N} p\left(y^{(n)} \mid x^{(n)}; \theta\right)\] <p>While this expression is mathematically correct, the product of many probabilities can be unwieldy. To simplify the computation, we typically work with the <strong>log-likelihood</strong>, \(\ell(\theta)\), which is simply the natural logarithm of the likelihood:</p> \[\ell(\theta) \stackrel{\text{def}}{=} \log L(\theta) = \sum_{n=1}^{N} \log p\left(y^{(n)} \mid x^{(n)}; \theta\right)\] <p>Maximizing \(\ell(\theta)\) is equivalent to maximizing \(L(\theta)\), as the logarithm is a monotonic function. Alternatively, minimizing the <strong>negative log-likelihood (NLL)</strong> is a common approach, as it frames the problem as a minimization task.</p> <hr/> <h4 id="mle-for-linear-regression"><strong>MLE for Linear Regression</strong></h4> <p>To make these concepts more concrete, let‚Äôs see how MLE applies to a simple and widely known model: linear regression.</p> <p>In linear regression, we assume the output \(Y\) is conditionally Gaussian given the input \(X\). Specifically,</p> \[Y \mid X = x \sim \mathcal{N}(\theta^\top x, \sigma^2),\] <p>where \(\theta^\top x\) is the mean and \(\sigma^2\) is the variance of the Gaussian distribution.</p> <p>The log-likelihood for this model can be written as:</p> \[\ell(\theta) \stackrel{\text{def}}{=} \log L(\theta) = \log \prod_{n=1}^{N} p\left(y^{(n)} \mid x^{(n)}; \theta\right) = \sum_{n=1}^{N} \log p\left(y^{(n)} \mid x^{(n)};\theta\right)\] <p>Substituting the Gaussian probability density function into the equation, we get:</p> \[\ell(\theta) = \sum_{n=1}^{N} \log \left( \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\frac{\left(y^{(n)} - \theta^\top x^{(n)}\right)^2}{2\sigma^2} \right) \right)\] <p>Simplifying further, the log-likelihood becomes:</p> \[\ell(\theta) = N \log \frac{1}{\sqrt{2\pi \sigma^2}} - \frac{1}{2\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right)^2\] <p>Notice that the first term, \(N \log \frac{1}{\sqrt{2\pi \sigma^2}}\), is independent of \(\theta\). This means that to maximize \(\ell(\theta)\), we only need to minimize the second term, which is proportional to the sum of squared residuals.</p> <p>This brings us to an important insight: <strong>maximizing the log-likelihood in linear regression is equivalent to minimizing the squared error.</strong></p> <h5 id="deriving-the-gradient-of-the-log-likelihood"><strong>Deriving the Gradient of the Log-Likelihood</strong></h5> <p>To find the parameters that maximize the log-likelihood, we compute its gradient with respect to \(\theta\) and set it to zero. From our earlier expression for \(\ell(\theta)\):</p> \[\ell(\theta) = N \log \frac{1}{\sqrt{2\pi \sigma^2}} - \frac{1}{2\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right)^2,\] <p>the gradient with respect to \(\theta_i\), the \(i\)-th parameter, is:</p> \[\frac{\partial \ell}{\partial \theta_i} = -\frac{1}{\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right) x_i^{(n)}\] <p>Setting \(\frac{\partial \ell}{\partial \theta_i} = 0\) gives us the familiar <strong>normal equations</strong> for linear regression, which are typically solved to find the optimal \(\theta\).</p> <p>This yields:</p> \[-\frac{1}{\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right) x_i^{(n)} = 0\] <p>Multiplying both sides by \(\sigma^2\) gives:</p> \[\sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right) x_i^{(n)} = 0\] <p>We can write the equation in matrix form as:</p> \[\mathbf{X}^\top \left( \mathbf{y} - \mathbf{X} \theta \right) = 0\] <p>Rewriting the equation:</p> \[\mathbf{X}^\top \mathbf{y} - \mathbf{X}^\top \mathbf{X} \theta = 0\] <p>Rearranging gives the normal equation:</p> \[\mathbf{X}^\top \mathbf{X} \theta = \mathbf{X}^\top \mathbf{y}\] <p>Solving for \(\theta\):</p> \[\theta = \left( \mathbf{X}^\top \mathbf{X} \right)^{-1} \mathbf{X}^\top \mathbf{y}\] <p>Through this derivation, we‚Äôve established a key connection between the probabilistic interpretation of linear regression and the classical squared error minimization. The principle of MLE not only provides a mathematically grounded way to estimate parameters but also reveals the assumptions underlying different models.</p> <p>What‚Äôs fascinating is that this approach generalizes beyond regression. For instance, in classification tasks, MLE leads to the cross-entropy loss. This will be the focus of the next section, where we‚Äôll explore how MLE ties into classification problems and the role of log-loss in optimizing model parameters.</p> <hr/> <h4 id="from-linear-to-logistic-regression-expanding-the-scope-of-mle"><strong>From Linear to Logistic Regression: Expanding the Scope of MLE</strong></h4> <p>In the previous section, we explored how the Maximum Likelihood Estimation (MLE) principle naturally connects with linear regression. We saw that linear regression assumes the target \(Y \vert X = x\) follows a Gaussian distribution, and maximizing the likelihood aligns with minimizing the squared loss.</p> <p>But is the Gaussian assumption always valid? Not necessarily. For example, in classification tasks where \(Y\) takes on discrete values (e.g., 0 or 1), assuming a Gaussian distribution is inappropriate. This raises an important question: <strong>can we use the same MLE-based modeling approach for tasks beyond regression?</strong></p> <p>The answer is yes, and this brings us to <strong>logistic regression</strong>, which is tailored for classification tasks.</p> <h4 id="logistic-regression-assumptions-and-foundations"><strong>Logistic Regression: Assumptions and Foundations</strong></h4> <p>Consider a binary classification problem where the target \(Y \in \{0, 1\}\). What should the conditional distribution of \(Y\) given \(X = x\) look like? For logistic regression, we model \(p(y \mid x)\) using a <strong>Bernoulli distribution</strong>:</p> \[p(y \mid x) = h(x)^y (1 - h(x))^{1-y},\] <p>where \(h(x) \in (0, 1)\) represents the probability \(p(y = 1 \mid x)\).</p> <h5 id="parameterizing-hx"><strong>Parameterizing \(h(x)\):</strong></h5> <p>In linear regression, the mean \(\mathbb{E}[Y \mid X = x]\) was parameterized as \(\theta^\top x\). However, for classification, \(h(x)\) must map the linear predictor \(\theta^\top x\) (which lies in \(\mathbb{R}\)) to the interval \((0, 1)\). To achieve this, we use the <strong>logistic function</strong>:</p> \[f(\eta) = \frac{1}{1 + e^{-\eta}}, \quad \text{where } \eta = \theta^\top x\] <p>Thus, the probability \(p(y \mid x)\) becomes:</p> \[p(y \mid x) = \text{Bernoulli}(f(\theta^\top x)),\] <p>or equivalently:</p> \[p(y = 1 \mid x) = f(\theta^\top x), \quad p(y = 0 \mid x) = 1 - f(\theta^\top x)\] <p>[any reason why bernoulli?]</p> <p><strong>Why do we use the Bernoulli distribution in logistic regression?</strong></p> <p>In logistic regression, the target variable \(Y\) is binary, taking values in \(\{0, 1\}\), making the <strong>Bernoulli distribution</strong> a natural choice. The Bernoulli distribution models the probability of success (1) or failure (0) in a single trial.</p> <p>We model the conditional probability \(p(y \mid x)\) using the logistic function, which ensures that the predicted probabilities lie in the interval \((0, 1)\):</p> \[p(y = 1 \mid x) = f(\theta^\top x), \quad p(y = 0 \mid x) = 1 - f(\theta^\top x),\] <p>where \(f(\eta) = \frac{1}{1 + e^{-\eta}}\) is the logistic function. The Bernoulli distribution is then used to model the binary outcomes given these probabilities.</p> <h5 id="interpreting-the-logistic-function"><strong>Interpreting the Logistic Function</strong></h5> <p>The logistic function is smooth and monotonically increasing, mapping any real-valued input to a value in the range \((0, 1)\). It has the characteristic ‚ÄúS-shape‚Äù and is particularly useful for modeling probabilities.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Logistic_Fn-480.webp 480w,/assets/img/Logistic_Fn-800.webp 800w,/assets/img/Logistic_Fn-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Logistic_Fn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Logistic_Fn" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>One interesting property of the logistic function is its connection to the <strong>log-odds</strong>. For logistic regression:</p> \[\log \frac{p(y = 1 \mid x)}{p(y = 0 \mid x)} = \theta^\top x\] <p>This shows that the log-odds (or logit) of \(y = 1\) depend linearly on the input \(x\). Moreover, the decision boundary, where \(p(y = 1 \mid x) = p(y = 0 \mid x) = 0.5\), is defined by \(\theta^\top x = 0\), making it a <strong>linear decision boundary</strong>.</p> <h5 id="mle-for-logistic-regression"><strong>MLE for Logistic Regression</strong></h5> <p>As with linear regression, the parameters \(\theta\) in logistic regression are estimated by maximizing the conditional log-likelihood:</p> \[\ell(\theta) = \sum_{n=1}^{N} \log p\left(y^{(n)} \mid x^{(n)}; \theta\right)\] <p>For a Bernoulli-distributed \(y\), substituting \(p(y \mid x)\):</p> \[\ell(\theta) = \sum_{n=1}^{N} \left[ y^{(n)} \log f(\theta^\top x^{(n)}) + (1 - y^{(n)}) \log (1 - f(\theta^\top x^{(n)})) \right]\] <p>Unlike linear regression, this log-likelihood does not have a closed-form solution for \(\theta\). However, it is <strong>concave</strong>, meaning that optimization techniques like gradient ascent can efficiently find the unique optimal solution.</p> <h5 id="gradient-ascent-for-logistic-regression"><strong>Gradient Ascent for Logistic Regression</strong></h5> <p>The gradient of the log-likelihood \(\ell(\theta)\) with respect to \(\theta_i\) (the \(i\)-th parameter) is given by:</p> \[\frac{\partial \ell}{\partial \theta_i} = \sum_{n=1}^{N} \left( y^{(n)} - f(\theta^\top x^{(n)}) \right) x_i^{(n)}\] <p><strong>Derivation of the above form:</strong></p> <p>Math Review: Chain Rule</p> <p>If \(z\) depends on \(y\), which itself depends on \(x\), e.g., \(z = (y(x))^2\), then:</p> \[\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}\] <p>Likelihood for a Single Example:</p> \[\ell^n = y^{(n)} \log f(\theta^\top x^{(n)}) + (1 - y^{(n)}) \log(1 - f(\theta^\top x^{(n)}))\] <p>The gradient with respect to \(\theta_i\) is:</p> \[\frac{\partial \ell^n}{\partial \theta_i} = \frac{\partial \ell^n}{\partial f^n} \frac{\partial f^n}{\partial \theta_i}\] <p>Using the chain rule:</p> \[= \left(\frac{y^{(n)}}{f^n} - \frac{1 - y^{(n)}}{1 - f^n}\right) \frac{\partial f^n}{\partial \theta_i}\] <p>Simplify:</p> \[= \left(\frac{y^{(n)}}{f^n} - \frac{1 - y^{(n)}}{1 - f^n}\right) \left(f^n (1 - f^n) x_i^{(n)}\right)\] \[= \left(y^{(n)} - f^n\right) x_i^{(n)}.\] <p>The full gradient is thus:</p> \[\frac{\partial \ell}{\partial \theta_i} = \sum_{n=1}^{N} \left(y^{(n)} - f(\theta^\top x^{(n)})\right) x_i^{(n)}.\] <p>This gradient looks strikingly similar to that of linear regression, except for the presence of the logistic function \(f(\cdot)\).</p> <p>Using this gradient, we iteratively update the parameters using gradient ascent:</p> \[\theta \leftarrow \theta + \alpha \nabla_\theta \ell(\theta),\] <p>where \(\alpha\) is the learning rate.</p> <p><strong>Note the distinction:</strong> since the function is concave, we apply gradient ascent rather than descent.</p> <h5 id="a-comparison-linear-vs-logistic-regression"><strong>A Comparison: Linear vs Logistic Regression</strong></h5> <p>Here‚Äôs a side-by-side comparison to highlight the similarities and differences:</p> <hr/> <table> <thead> <tr> <th>Feature</th> <th>Linear Regression</th> <th>Logistic Regression</th> </tr> </thead> <tbody> <tr> <td>Input combination</td> <td>\(\theta^\top x\) (linear)</td> <td>\(\theta^\top x\) (linear)</td> </tr> <tr> <td>Output</td> <td>Real-valued</td> <td>Categorical (0 or 1)</td> </tr> <tr> <td>Conditional distribution</td> <td>Gaussian</td> <td>Bernoulli</td> </tr> <tr> <td>Transfer function \(f(\theta^\top x)\)</td> <td>Identity</td> <td>Logistic</td> </tr> <tr> <td>Mean \(\mathbb{E}[Y \mid X = x; \theta]\)</td> <td>\(f(\theta^\top x)\)</td> <td>\(f(\theta^\top x)\)</td> </tr> </tbody> </table> <hr/> <p>The main difference lies in the conditional distribution of \(Y\) and the transfer function \(f(\cdot)\), which maps \(\theta^\top x\) to the appropriate range for each model.</p> <h4 id="generalizing-logistic-regression"><strong>Generalizing Logistic Regression</strong></h4> <p>The principles behind logistic regression can be extended to handle other types of outputs, such as counts or probabilities for multiple classes. This generalization leads to the broader family of <strong>generalized linear models (GLMs)</strong>.</p> <h5 id="steps-for-generalized-regression-models"><strong>Steps for Generalized Regression Models</strong></h5> <ol> <li><strong>Task</strong>: Given \(x\), predict \(p(y \mid x)\).</li> <li><strong>Modeling</strong>: <ul> <li>Choose a parametric family of distributions \(p(y; \theta)\) with parameters \(\theta \in \Theta\).</li> <li>Choose a transfer function that maps a linear predictor in \(\mathbb{R}\) to \(\Theta\):</li> </ul> \[x \in \mathbb{R}^d \mapsto w^\top x \in \mathbb{R} \mapsto f(w^\top x) = \theta \in \Theta\] </li> <li> <p><strong>Learning</strong>: Use MLE to estimate the parameters:</p> \[\hat{\theta} = \arg\max_\theta \log p(D; \hat{\theta})\] </li> <li> <p><strong>Inference</strong>: For prediction, map \(x\) through the learned transfer function:</p> \[x \mapsto f(w^\top x)\] </li> </ol> <p>In the next section, we‚Äôll dive deeper into these generalized models, exploring their flexibility and application to diverse prediction tasks.</p> <hr/> <h4 id="extending-generalized-linear-models-from-poisson-to-multinomial-logistic-regression"><strong>Extending Generalized Linear Models: From Poisson to Multinomial Logistic Regression</strong></h4> <p>In our journey through generalized linear models (GLMs), we‚Äôve seen how logistic regression extends MLE principles to classification tasks. Now, let‚Äôs explore other use cases where GLMs shine, including <strong>Poisson regression</strong> for count-based predictions and <strong>multinomial logistic regression</strong> for multiclass classification.</p> <h4 id="example-poisson-regression"><strong>Example: Poisson Regression</strong></h4> <p>Imagine we want to predict the number of people entering a New York restaurant during lunchtime. What features could help? Time of day, day of the week, weather conditions, or nearby events might all be relevant. Importantly, the target variable \(Y\), representing the number of visitors, is a non-negative integer: \(Y \in \{0, 1, 2, \dots\}\).</p> <h5 id="why-use-the-poisson-distribution"><strong>Why Use the Poisson Distribution?</strong></h5> <p>The Poisson distribution is a natural choice for modeling count data. A random variable \(Y \sim \text{Poisson}(\lambda)\) has the probability mass function:</p> \[p(Y = k; \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k \in \{0, 1, 2, \dots\},\] <p>where \(\lambda &gt; 0\) is the rate parameter. The expected value \(\mathbb{E}[Y] = \lambda\), making \(\lambda\) both the mean and variance of \(Y\).</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Poisson_pmf.svg-480.webp 480w,/assets/img/Poisson_pmf.svg-800.webp 800w,/assets/img/Poisson_pmf.svg-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Poisson_pmf.svg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Poisson_pmf" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="what-does-the-poisson-distribution-mean-intuitively"><strong>What Does the Poisson Distribution Mean, Intuitively?</strong></h5> <p>The Poisson distribution can be understood through a simple analogy: imagine standing at a bus stop.</p> <ol> <li><strong>The Events</strong>: Each bus that arrives at the stop is an ‚Äúevent.‚Äù</li> <li><strong>Constant Rate</strong>: On average, buses arrive every 10 minutes, meaning we expect about 6 buses per hour. This average rate, \(\lambda = 6\), is constant.</li> <li><strong>Independence</strong>: The arrival of one bus doesn‚Äôt affect when the next one will come (events are independent).</li> </ol> <p>Now, if you wait at the bus stop for an hour, the Poisson distribution models the probability of seeing exactly 5, 6, or 7 buses in that time. While the average is 6 buses, randomness may cause the actual count to vary, with probabilities decreasing for more extreme deviations (e.g., 0 buses or 12 buses in an hour are unlikely).</p> <p>This shows how the Poisson distribution captures both the expected rate (\(\lambda\)) and the variability in the number of events.</p> <h5 id="constructing-the-poisson-regression-model"><strong>Constructing the Poisson Regression Model</strong></h5> <p>We assume \(Y \mid X = x \sim \text{Poisson}(\lambda)\). The challenge is to ensure \(\lambda\), the rate parameter, is positive. This is achieved using a transfer function \(f\):</p> \[x \mapsto w^\top x \quad \text{(linear predictor in } \mathbb{R} \text{)} \mapsto \lambda = f(w^\top x) \quad \text{(rate parameter in } (0, \infty) \text{)}.\] <p>The standard transfer function is the exponential function:</p> \[f(w^\top x) = e^{w^\top x}\] <h5 id="log-likelihood-for-poisson-regression"><strong>Log-Likelihood for Poisson Regression</strong></h5> <p>Given a dataset \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\), the log-likelihood is:</p> \[\log p(y_i; \lambda_i) = \left[y_i \log \lambda_i - \lambda_i - \log(y_i!)\right]\] \[\log p(D; w) = \sum_{i=1}^{n} \left[ y_i \log f(w^\top x_i) - f(w^\top x_i) - \log(y_i!) \right],\] <p>where \(f(w^\top x_i) = e^{w^\top x_i}\). Substituting \(\lambda_i\), we get:</p> \[\log p(D; w) = \sum_{i=1}^{n} \left[ y_i (w^\top x_i) - e^{w^\top x_i} - \log(y_i!) \right]\] <p>As with logistic regression, the likelihood is concave, so gradient-based methods can efficiently optimize \(w\).</p> <h5 id="gradient-for-poisson-regression"><strong>Gradient for Poisson Regression</strong></h5> <p>To optimize the log-likelihood, we compute its gradient with respect to the weight vector \(w\).</p> <p>The gradient is:</p> \[\frac{\partial \log p(D; w)}{\partial w} = \sum_{i=1}^{n} \left[ y_i x_i - e^{w^\top x_i} x_i \right]\] <p>Factoring out common terms, we get:</p> \[\frac{\partial \log p(D; w)}{\partial w} = \sum_{i=1}^{n} x_i \left[ y_i - e^{w^\top x_i} \right]\] <p>The gradient indicates the update direction for \(w\), with each term capturing the difference between the observed count \(y_i\) and the predicted count \(e^{w^\top x_i}\), weighted by the feature vector \(x_i\). Gradient ascent can then be used to maximize the log-likelihood, as the likelihood is concave for Poisson regression. Again, notice how similar the gradient is to the other problems we‚Äôve explored so far‚Äîthe transfer function differs in each case.</p> <hr/> <h4 id="example-multinomial-logistic-regression"><strong>Example: Multinomial Logistic Regression</strong></h4> <p>Next, let‚Äôs tackle multiclass classification, where the target \(Y \in \{1, 2, \dots, k\}\) spans multiple categories. Logistic regression‚Äôs Bernoulli distribution extends to the <strong>categorical distribution</strong>, which is parameterized by a probability vector \(\theta = (\theta_1, \dots, \theta_k)\). For valid probabilities:</p> \[\sum_{i=1}^{k} \theta_i = 1, \quad \theta_i \geq 0 \text{ for all } i.\] <p>For a given \(y \in \{1, \dots, k\}\):</p> \[p(y) = \theta_y.\] <h5 id="what-does-the-categorical-distribution-mean-intuitively"><strong>What Does the Categorical Distribution Mean, Intuitively?</strong></h5> <p>The categorical distribution assigns a probability to each class. For a given input \(x\), we want to predict the probability of each class \(y\) belonging to the target set. The probability of the class \(y\) is \(\theta_y\), where \(\theta_y\) is the component of the probability vector corresponding to the class \(y\). This allows us to perform multiclass classification by selecting the class with the highest probability.</p> \[p(y) = \theta_y.\] <h5 id="constructing-the-multinomial-logistic-regression-model"><strong>Constructing the Multinomial Logistic Regression Model</strong></h5> <p>The key idea in multinomial logistic regression is to compute a linear score for each class. For a given input vector \(x\), we compute a vector of scores \(s \in \mathbb{R}^k\) for all classes:</p> \[s = (w_1^\top x, \dots, w_k^\top x),\] <p>where \(w_i\) represents the weight vector associated with class \(i\). These scores are then transformed using the <strong>softmax function</strong> to produce valid probabilities. The softmax function is defined as:</p> \[\text{softmax}(s)_i = \frac{e^{s_i}}{\sum_{j=1}^{k} e^{s_j}} \quad \text{for } i = 1, \dots, k.\] <p>The softmax function ensures that the resulting probabilities form a valid probability distribution, satisfying:</p> \[\sum_{i=1}^{k} \theta_i = 1, \quad \theta_i \geq 0 \text{ for all } i.\] <h5 id="log-likelihood-for-multinomial-logistic-regression"><strong>Log-Likelihood for Multinomial Logistic Regression</strong></h5> <p>Given a dataset \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\), the log-likelihood of the model is the sum of the log probabilities for the true classes. The log-likelihood is given by:</p> \[\log p(D; W) = \sum_{i=1}^{n} \log \theta_{y_i},\] <p>where \(\theta_{y_i} = \text{softmax}(W^\top x_i)_{y_i}\). The parameters \(W\) are learned by maximizing the log-likelihood using gradient-based optimization methods.</p> <p><strong>Note:</strong> Don‚Äôt be misled by the use of theta and softmax notations. The way this is mentioned can be confusing. For now, let‚Äôs proceed with caution.</p> <h5 id="gradient-for-multinomial-logistic-regression"><strong>Gradient for Multinomial Logistic Regression</strong></h5> <p>To optimize the parameters \(W\), we compute the gradient of the log-likelihood with respect to \(W\).</p> <h6 id="step-by-step-derivation"><strong>Step-by-Step Derivation</strong></h6> <ol> <li> <p><strong>Log-Likelihood for Multinomial Logistic Regression:</strong></p> <p>The log-likelihood for the dataset \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\) is:</p> \[\log p(D; W) = \sum_{i=1}^{n} \log \theta_{y_i},\] <p>where \(\theta_{y_i} = \text{softmax}(W^\top x_i)_{y_i}\) is the predicted probability of the true class \(y_i\) for the \(i\)-th data point.</p> </li> <li> <p><strong>Softmax Function:</strong></p> <p>The softmax function for class \(j\) is given by:</p> \[\theta_j(x_i; W) = \frac{e^{w_j^\top x_i}}{\sum_{k=1}^{k} e^{w_k^\top x_i}}.\] </li> <li> <p><strong>Log-Likelihood Expansion:</strong></p> <p>Substituting the softmax expression into the log-likelihood:</p> \[\log p(D; W) = \sum_{i=1}^{n} \log \left( \frac{e^{w_{y_i}^\top x_i}}{\sum_{k=1}^{k} e^{w_k^\top x_i}} \right),\] <p>which simplifies to:</p> \[\log p(D; W) = \sum_{i=1}^{n} \left( w_{y_i}^\top x_i - \log \left( \sum_{k=1}^{k} e^{w_k^\top x_i} \right) \right).\] </li> <li> <p><strong>Gradient of the Log-Likelihood:</strong></p> <p>The gradient of the log-likelihood with respect to the weight vector \(w_j\) is computed by differentiating each term in the log-likelihood expression:</p> <ul> <li> <p>The derivative of the first term, \(w_{y_i}^\top x_i\), with respect to \(w_j\) is simply \(x_i\) when \(y_i = j\) and 0 otherwise.</p> </li> <li> <p>The second term involves the <strong>log-sum-exp</strong>, and its derivative with respect to \(w_j\) is:</p> \[\nabla_{w_j} \log \left( \sum_{k=1}^{k} e^{w_k^\top x_i} \right) = \theta_j(x_i; W) \cdot x_i.\] </li> </ul> </li> <li> <p><strong>Final Gradient Expression:</strong></p> <p>Combining the two terms, the gradient of the log-likelihood with respect to \(w_j\) is:</p> \[\nabla_{w_j} \log p(D; W) = \sum_{i=1}^{n} \left( \mathbf{1}_{\{y_i = j\}} - \theta_j(x_i; W) \right) x_i,\] <p>where \(\mathbf{1}_{\{y_i = j\}}\) is the indicator function that is 1 if the true class \(y_i\) equals \(j\), and 0 otherwise.</p> </li> </ol> <p>This gradient is used to adjust the weight vectors during training to improve the model‚Äôs predictions.</p> <hr/> <h4 id="review-recipe-for-conditional-models"><strong>Review: Recipe for Conditional Models</strong></h4> <p>GLMs provide a unified approach for constructing conditional models. Here‚Äôs a quick recipe:</p> <ol> <li><strong>Define the Input and Output Space</strong>: Start with the features \(x\) and target \(y\).</li> <li><strong>Choose an Output Distribution</strong>: Based on the nature of \(y\), select an appropriate probability distribution \(p(y \mid x; \theta)\).</li> <li><strong>Select a Transfer Function</strong>: Map the linear predictor \(w^\top x\) into the required range of the distribution parameters.</li> <li><strong>Optimize via MLE</strong>: Estimate \(\theta\) by maximizing the log-likelihood using gradient-based methods.</li> </ol> <p>This framework, called <strong>generalized linear models</strong>, can be adapted for a wide range of prediction tasks.</p> <hr/> <p>Before we wrap up, did we overlook something? Ah, yes‚Äîlet‚Äôs revisit the log-sum-exp function and its significance.</p> <h4 id="log-sum-exp-function"><strong>Log-Sum-Exp Function</strong></h4> <p>The <strong>log-sum-exp</strong> (LSE) function is a mathematical expression used frequently in machine learning and statistics, particularly in contexts involving probabilities and normalization. It is defined as:</p> \[\text{LSE}(s) = \log \left( \sum_{k=1}^K e^{s_k} \right),\] <p>where \(s = (s_1, s_2, \dots, s_K)\) is a vector of real-valued scores.</p> <h5 id="key-properties-of-log-sum-exp"><strong>Key Properties of Log-Sum-Exp</strong></h5> <ol> <li> <p><strong>Smooth Maximum Approximation</strong><br/> The log-sum-exp function can be thought of as a ‚Äúsoft maximum‚Äù of the elements in \(s\) because, for large values of \(s_k\), the largest term dominates the sum:</p> \[\text{LSE}(s) \approx \max(s_k)\] </li> <li> <p><strong>Numerical Stability</strong><br/> To ensure numerical stability when computing \(e^{s_k}\) for large values of \(s_k\), the log-sum-exp function is often rewritten as:</p> \[\text{LSE}(s) = \log \left( \sum_{k=1}^K e^{s_k - \max(s)} \right) + \max(s),\] <p>where \(\max(s)\) is the maximum value in \(s\). This adjustment ensures that the exponentials remain within a manageable range. Check why it‚Äôs written this way in the referenced resource; I highly recommend it.</p> </li> </ol> <h5 id="log-sum-exp-in-multinomial-logistic-regression"><strong>Log-Sum-Exp in Multinomial Logistic Regression</strong></h5> <p>In multinomial logistic regression, the log-sum-exp term naturally arises when computing the log-likelihood. The predicted probabilities are computed using the softmax function:</p> \[\theta_j(x_i; W) = \frac{e^{w_j^\top x_i}}{\sum_{k=1}^{K} e^{w_k^\top x_i}}\] <p>The denominator of the softmax involves a sum of exponentials. Taking the logarithm of the denominator gives the log-sum-exp term:</p> \[\log \left( \sum_{k=1}^K e^{w_k^\top x_i} \right)\] <p>This term normalizes the probabilities so that they sum to 1 across all classes.</p> <h5 id="derivative-of-log-sum-exp"><strong>Derivative of Log-Sum-Exp</strong></h5> <p>The derivative of the log-sum-exp function with respect to a specific score \(s_j\) is:</p> \[\frac{\partial}{\partial s_j} \log \left( \sum_{k=1}^K e^{s_k} \right) = \frac{e^{s_j}}{\sum_{k=1}^K e^{s_k}}\] <p>This is equivalent to the probability assigned to class \(j\) by the softmax function:</p> \[\frac{\partial}{\partial s_j} \log \left( \sum_{k=1}^K e^{s_k} \right) = \text{softmax}(s)_j.\] <p>In the case of multinomial logistic regression, where \(s_j = w_j^\top x_i\), the derivative with respect to \(w_j\) is:</p> \[\nabla_{w_j} \log \left( \sum_{k=1}^K e^{w_k^\top x_i} \right) = \theta_j(x_i; W) \cdot x_i,\] <p>where \(\theta_j(x_i; W) = \text{softmax}(W^\top x_i)_j\) is the predicted probability for class \(j\).</p> <h5 id="still-why-the-use-of-log-sum-exp"><strong>Still, Why the Use of Log-Sum-Exp</strong></h5> <p>In multinomial logistic regression, we use the <strong>softmax function</strong> to convert the raw class scores (logits) into probabilities. The softmax function itself involves summing the exponentials of the scores, not the log-sum-exp.</p> <p>However, when calculating the <strong>log-likelihood</strong> of the model during optimization, we encounter the <strong>log-sum-exp</strong> function. The log-sum-exp is used in the log-likelihood to handle the sum of the exponentials in a numerically stable way. It‚Äôs primarily used to:</p> <ol> <li><strong>Avoid overflow and underflow</strong>: When working with large or small exponentiated values (as in the softmax function), exponentiation can cause numerical instability. The log-sum-exp helps stabilize these computations.</li> <li><strong>Ensure proper normalization</strong>: In the context of the softmax function, it ensures the sum of the probabilities is 1, making them valid probabilities for classification.</li> </ol> <p>In summary, while softmax uses the sum of exponentials, the <strong>log-sum-exp</strong> appears in the log-likelihood computation to stabilize the logarithmic transformation, enabling proper optimization.</p> <h5 id="analogy-for-the-log-sum-exp-function"><strong>Analogy for the Log-Sum-Exp Function</strong></h5> <p>Imagine you‚Äôre at a sports competition with several players, and their scores are exponentially amplified (think of \(e^{s_k}\) as the ‚Äúhype‚Äù around each player‚Äôs performance). The log-sum-exp function acts like a judge summarizing all the scores into a single value that reflects the overall competition, but with a bias toward the top performers.</p> <ul> <li>The <strong>‚Äúlog‚Äù</strong> compresses the scale, keeping the summary manageable.</li> <li>The <strong>‚Äúsum‚Äù</strong> captures the contributions of <em>all</em> players, not just the best one.</li> <li>The <strong>‚Äúexp‚Äù</strong> amplifies the impact of the highest scores, making it feel like a weighted average that leans toward the standout performers.</li> </ul> <p>In short, the <strong>log-sum-exp is like a ‚Äúsoft maximum‚Äù</strong>: it highlights the best, considers the rest, and ensures the result is stable and interpretable.</p> <p>If you‚Äôre having trouble with this analogy, go through the example in the reference. It‚Äôll help clarify things.</p> <hr/> <p>Alright, it‚Äôs time to wrap this up. In the next section, we‚Äôll dive into another type of probabilistic modeling: generative models. We‚Äôll explore what they are and how they work. Stay tuned, and see you there!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://benhay.es/posts/exploring-distributions/">Exploring Probability Distributions</a></li> <li><a href="https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/">The Log-Sum-Exp Trick</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.]]></summary></entry><entry><title type="html">Unveiling Probabilistic Modeling</title><link href="https://monishver11.github.io/blog/2025/probabilistic-modeling/" rel="alternate" type="text/html" title="Unveiling Probabilistic Modeling"/><published>2025-01-17T04:51:00+00:00</published><updated>2025-01-17T04:51:00+00:00</updated><id>https://monishver11.github.io/blog/2025/probabilistic-modeling</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/probabilistic-modeling/"><![CDATA[<h4 id="why-probabilistic-modeling"><strong>Why Probabilistic Modeling?</strong></h4> <p>Probabilistic modeling offers a unified framework that underpins many machine learning methods, from linear regression to logistic regression and beyond. At its core, probabilistic modeling allows us to handle uncertainty and make informed decisions based on observed data. It provides a principled way to update our beliefs about the data-generating process as new information becomes available.</p> <p>In machine learning, we often think of learning as statistical inference, where the goal is to use data to draw conclusions about the underlying distribution or process that generated it, rather than simply fitting a model to the observed data. In this view, the goal is not just to fit a model to data, but to estimate the underlying distribution that best explains the observed data. Probabilistic methods give us a powerful tool to incorporate our beliefs about the world‚Äîoften referred to as inductive biases‚Äîinto the learning process. This allows us to make more informed predictions and gain deeper insights into the data.</p> <p>For example, in Bayesian inference, prior beliefs are combined with evidence from the data to update our understanding of the underlying process. This principled approach enables us to not only predict outcomes but also quantify our confidence in those predictions, making probabilistic modeling a powerful tool for developing robust, interpretable, and informed machine learning systems.</p> <h5 id="two-ways-of-generating-data"><strong>Two Ways of Generating Data</strong></h5> <p>When we think about how data is generated, there are two main perspectives to consider. The first is through <strong>conditional models</strong>, where we model the likelihood of the output \(y\) given the input \(x\). This is often denoted as \(p(y \vert x)\).</p> <p>The second perspective is through <strong>generative models</strong>, where we model the joint distribution of both the input \(x\) and the output \(y\), denoted as \(p(x, y)\).</p> <p>To understand the distinction between conditional models and generative models, let‚Äôs use the analogy of handwriting recognition:</p> <ul> <li> <p><strong>Conditional Models</strong>: Imagine you are given a handwritten letter and asked to identify the corresponding alphabet. Here, you focus only on the relationship between the handwriting (\(x\)) and the letter it represents (\(y\)). This corresponds to modeling \(p(y \mid x)\), where you predict the output \(y\) (the letter) conditioned on the input \(x\) (the handwriting). Essentially, you‚Äôre answering the question, <em>‚ÄúWhat is the most likely letter given this handwriting?‚Äù</em></p> </li> <li> <p><strong>Generative Models</strong>: Now, imagine that instead of just recognizing handwriting, you also aim to generate realistic handwriting for any letter. To do this, you need to understand how the letters (\(y\)) and handwriting styles (\(x\)) are generated together. This involves modeling the joint distribution \(p(x, y)\), where you learn how inputs and outputs are related as part of a larger generative process. The question here becomes, <em>‚ÄúHow are handwriting (\(x\)) and letters (\(y\)) jointly produced?‚Äù</em></p> </li> </ul> <p>Each approach offers different advantages depending on the context. However, both share the common goal of estimating the parameters of the model, often using a technique called <strong>Maximum Likelihood Estimation (MLE)</strong>.</p> <hr/> <h4 id="conditional-models"><strong>Conditional Models</strong></h4> <p>Conditional models focus on predicting the output given the input. One of the most well-known and widely used conditional models is <strong>linear regression</strong>. Let‚Äôs take a closer look at linear regression and how it fits within this probabilistic framework.</p> <h4 id="linear-regression"><strong>Linear Regression</strong></h4> <p>Linear regression is a fundamental technique in both machine learning and statistics. Its primary goal is to predict a real-valued target \(y\) (also called the response variable) from a vector of features \(x\) (also known as covariates). Linear regression is often used in situations where we want to predict a continuous value, such as:</p> <ul> <li>Predicting house prices based on factors like location, condition, and age of the house.</li> <li>Estimating medical costs of a person based on their age, sex, region, and BMI.</li> <li>Predicting someone‚Äôs age from their photograph.</li> </ul> <h5 id="the-problem-setup"><strong>The Problem Setup</strong></h5> <p>In linear regression, we are given a set of training examples, \(D = \{(x^{(n)}, y^{(n)})\}_{n=1}^N\), where \(x^{(n)} \in \mathbb{R}^d\) represents the features and \(y^{(n)} \in \mathbb{R}\) represents the target. The task is to model the relationship between the features \(x\) and the target \(y\).</p> <p>To do this, we assume that there is a linear relationship between \(x\) and \(y\), which can be expressed as:</p> \[h(x) = \theta^T x = \sum_{i=0}^{d} \theta_i x_i\] <p>Here, \(\theta \in \mathbb{R}^d\) represents the parameters (also known as the weights) of the model, and \(x_0 = 1\) is the bias term. The goal is to find the values of \(\theta\) that best explain the observed data.</p> <blockquote> <p><em>‚ÄúWe use superscript to denote the example id and subscript to denote the dimension id‚Äù</em></p> </blockquote> <h5 id="unveiling-probabilistic-modeling"><strong>Unveiling Probabilistic Modeling</strong></h5> <p>To estimate the parameters \(\theta\), we use the <strong>least squares method</strong>, which involves minimizing the squared loss between the predicted and observed values. The loss function is defined as:</p> \[J(\theta) = \frac{1}{N} \sum_{n=1}^{N} \left( y^{(n)} - \theta^T x^{(n)} \right)^2\] <p>This function represents the <strong>empirical risk</strong>, which quantifies the difference between the predicted and actual values across all the training examples.</p> <h5 id="matrix-formulation"><strong>Matrix Formulation</strong></h5> <p>We can also express this problem in matrix form for efficiency. Let \(X \in \mathbb{R}^{N \times d}\) be the design matrix, whose rows represent the input features for each training example. Let \(y \in \mathbb{R}^N\) be the vector of all target values. The objective is to solve for the parameter vector \(\hat{\theta}\) that minimizes the loss:</p> \[\hat{\theta} = \arg\min_\theta \left( (X\theta - y)^T (X\theta - y) \right)\] <h5 id="closed-form-solution"><strong>Closed-Form Solution</strong></h5> <p>The closed-form solution to this optimization problem is:</p> \[\hat{\theta} = (X^T X)^{-1} X^T y\] <p>This gives us the values for \(\theta\) that minimize the squared loss, and hence, provide the best linear model for the data.</p> <hr/> <p>Before proceeding further, here are a few review questions. Ask yourself these and check the answers.</p> <h5 id="how-do-we-derive-the-solution-for-linear-regression"><strong>How do we derive the solution for linear regression?</strong></h5> <p>The squared loss function in matrix form is:</p> \[J(\theta) = \frac{1}{N} (X\theta - y)^T (X\theta - y)\] <p>To minimize \(J(\theta)\), we compute the gradient with respect to \(\theta\):</p> <ul> <li>Expand the quadratic term:</li> </ul> \[J(\theta) = \frac{1}{N} \left[ \theta^T X^T X \theta - 2y^T X \theta + y^T y \right]\] <ul> <li>Take the derivative with respect to \(\theta\): <ul> <li>Recall that for any vector \(a\), \(b\), and matrix \(A\), the following derivatives are useful: <ul> <li> \[\frac{\partial (a^T b)}{\partial a} = b\] </li> <li> \[\frac{\partial (a^T A a)}{\partial a} = 2A a\] </li> </ul> <p>(when \(A\) is symmetric).</p> </li> </ul> </li> </ul> <p>Applying these rules:</p> \[\nabla_\theta J(\theta) = \frac{1}{N} \left[ 2X^T X \theta - 2X^T y \right].\] <ul> <li>Set the gradient to zero to find the minimizer:</li> </ul> \[X^T X \theta = X^T y\] <ul> <li>Solve for \(\theta\):</li> </ul> \[\theta = (X^T X)^{-1} X^T y,\] <p>provided \(X^T X\) is invertible.</p> <p><strong>Note:</strong> The \(\frac{1}{N}\) normalization factor is constant and cancels out when setting the gradient to zero, so it does not affect the solution for \(\theta\).</p> <h5 id="why-do-transposes-appear-or-disappear"><strong>Why Do Transposes Appear or Disappear?</strong></h5> <ol> <li> <p><strong>Symmetry of Quadratic Terms</strong>:<br/> In the term \(\theta^T X^T X \theta\), note that \(X^T X\) is a symmetric matrix (because \(X^T X = (X^T X)^T\)). This symmetry ensures that when taking the derivative, we don‚Äôt need to explicitly add or remove transposes; they naturally align.</p> </li> <li><strong>Consistency of Vector-Matrix Multiplication</strong>:<br/> When differentiating terms like \(y^T X \theta\), we use the rule \(\frac{\partial (a^T b)}{\partial a} = b\), ensuring dimensions match. This often introduces or removes a transpose based on the structure of the derivative. For example: <ul> <li>\(\nabla_\theta (-2y^T X \theta) = -2X^T y\), where \(X^T\) arises naturally to align dimensions.</li> </ul> </li> <li><strong>Gradient Conventions</strong>:<br/> The transpose changes are necessary to ensure the resulting gradient is a column vector (matching \(\theta\)‚Äôs shape), as gradients are typically represented in the same dimensionality as the parameter being differentiated.</li> </ol> <h5 id="what-happens-if-xt-x-is-not-invertible"><strong>What happens if \(X^T X\) is not invertible?</strong></h5> <p>If \(X^T X\) is not invertible (also called singular or degenerate), the normal equations do not have a unique solution. This happens in cases such as:</p> <ul> <li><strong>Linearly dependent features</strong>: Some columns of \(X\) are linear combinations of others.</li> <li><strong>Too few data points</strong>: If \(N &lt; d\) (more features than samples), \(X^T X\) will not be full rank.</li> </ul> <p>To address this issue, we can:</p> <ol> <li><strong>Add regularization</strong>: Use techniques like Ridge Regression, which modifies the normal equation to include a penalty term:<br/> \(\theta = (X^T X + \lambda I)^{-1} X^T y,\)<br/> where \(\lambda &gt; 0\) is the regularization parameter.</li> <li><strong>Remove redundant features</strong>: Perform feature selection or dimensionality reduction (e.g., PCA) to eliminate linear dependencies.</li> <li><strong>Use pseudo-inverse</strong>: Compute the Moore-Penrose pseudo-inverse of \(X^T X\) to find a solution.</li> </ol> <hr/> <h4 id="understanding-linear-regression-through-a-probabilistic-lens"><strong>Understanding Linear Regression Through a Probabilistic Lens</strong></h4> <p>So far, we‚Äôve discussed how linear regression can be understood as minimizing the squared loss. But why is the squared loss a reasonable choice for regression problems? To answer this, we need to think about the assumptions we are making on the data.</p> <p>Let‚Äôs approach linear regression from a <strong>probabilistic modeling perspective</strong>.</p> <h5 id="assumptions-in-linear-regression"><strong>Assumptions in Linear Regression</strong></h5> <p>In this framework, we assume that the target \(y\) and the features \(x\) are related through a linear function, with an added error term \(\epsilon\):</p> \[y = \theta^T x + \epsilon\] <p>Here, \(\epsilon\) represents the residual error that accounts for all unmodeled effects, such as noise or other sources of variation in the data. We assume that these errors \(\epsilon\) are independent and identically distributed (iid) and follow a normal distribution:</p> \[\epsilon \sim \mathcal{N}(0, \sigma^2)\] <p>Given this assumption, the conditional distribution of \(y\) given \(x\) is a normal distribution with mean \(\theta^T x\) and variance \(\sigma^2\):</p> \[p(y | x; \theta) = \mathcal{N}(\theta^T x, \sigma^2)\] <h5 id="intuition-behind-the-gaussian-distribution"><strong>Intuition Behind the Gaussian Distribution</strong></h5> <p>This distribution suggests that, for each value of \(x\), the output \(y\) is normally distributed around the value predicted by the linear model \(\theta^T x\), with a fixed variance \(\sigma^2\) that captures the uncertainty or noise in the data. In other words, we place a Gaussian ‚Äúbump‚Äù around the output of the linear predictor, reflecting the uncertainty in our prediction.</p> <p>With this, we‚Äôve laid the groundwork for our discussion on Maximum Likelihood Estimation.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>In this post, we introduced how probabilistic modeling can be used for understanding and estimating machine learning models, such as linear regression. By thinking of learning as statistical inference, we can incorporate our prior beliefs about the data-generating process and make more informed predictions.</p> <p>Next, we‚Äôll dive into <strong>Maximum Likelihood Estimation (MLE)</strong> and examine how it can be applied to solve probabilistic linear regression and other machine learning algorithms. We‚Äôll also explore how to formalize this understanding‚Äîstay tuned!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.]]></summary></entry><entry><title type="html">SVM Solution in the Span of the Data</title><link href="https://monishver11.github.io/blog/2025/svm-solution-span-of-data/" rel="alternate" type="text/html" title="SVM Solution in the Span of the Data"/><published>2025-01-16T18:13:00+00:00</published><updated>2025-01-16T18:13:00+00:00</updated><id>https://monishver11.github.io/blog/2025/svm-solution-span-of-data</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/svm-solution-span-of-data/"><![CDATA[<p>Previously, we explored the <strong>kernel trick</strong>, a powerful concept that allows Support Vector Machines (SVMs) to operate efficiently in high-dimensional feature spaces without explicitly computing the coordinates. Building on that foundation, we now turn our attention to an intriguing property of SVM solutions: they lie in the <strong>span of the data</strong>. This observation not only deepens our understanding of the connection between the dual and primal formulations of SVM but also provides a unifying perspective on how solutions in machine learning are inherently tied to the training data.</p> <hr/> <h4 id="svm-dual-problem-a-quick-recap"><strong>SVM Dual Problem: A Quick Recap</strong></h4> <p>To understand this property, let‚Äôs first revisit the SVM dual problem. It is formulated as:</p> \[\sup_{\alpha \in \mathbb{R}^n} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_j^T x_i\] <p>subject to the constraints:</p> <ol> <li> \[\sum_{i=1}^n \alpha_i y_i = 0\] </li> <li> \[\alpha_i \in [0, \frac{c}{n}], \quad i = 1, \dots, n\] </li> </ol> <p>Here, \(\alpha_i\) are the dual variables that correspond to the Lagrange multipliers, and \(c\) is the regularization parameter that controls the margin.</p> <p>The dual problem focuses on maximizing this quadratic function, which involves pairwise interactions between training samples. Once the optimal dual solution \(\alpha^*\) is obtained, it can be used to compute the primal solution as:</p> \[w^* = \sum_{i=1}^n \alpha^*_i y_i x_i\] <p>This equation reveals a critical insight: the primal solution \(w^*\) is expressed as a <strong>linear combination of the training inputs</strong> \(x_1, x_2, \dots, x_n\). This means that \(w^*\) is confined to the span of these inputs, or mathematically:</p> \[w^* \in \text{span}(x_1, \dots, x_n)\] <p>We refer to this phenomenon as ‚Äúthe SVM solution lies in the span of the data.‚Äù It underscores the dependency of \(w^*\) on the training data, aligning it with the geometric intuition of SVMs: the decision boundary is shaped by a subset of data points (the support vectors).</p> <hr/> <h4 id="ridge-regression-another-perspective-on-span-of-the-data"><strong>Ridge Regression: Another Perspective on Span of the Data</strong></h4> <p>Interestingly, this concept is not unique to SVMs. A similar property emerges in <strong>ridge regression</strong>, a linear regression method that incorporates \(\ell_2\) regularization to prevent overfitting. Let‚Äôs delve into this and see how the ridge regression solution also resides in the span of the data.</p> <h5 id="ridge-regression-objective"><strong>Ridge Regression Objective</strong></h5> <p>The objective function for ridge regression, with a regularization parameter \(\lambda &gt; 0\), is given by:</p> \[w^* = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \{ w^T x_i - y_i \}^2 + \lambda \|w\|_2^2\] <p>Here, \(w\) is the weight vector, \(x_i\) are the input data points, and \(y_i\) are the corresponding target values. The regularization term \(\lambda \|w\|_2^2\) penalizes large weights to improve generalization.</p> <h6 id="the-closed-form-solution"><strong>The Closed-Form Solutio</strong>n</h6> <p>The ridge regression problem has a closed-form solution:</p> \[w^* = \left( X^T X + \lambda I \right)^{-1} X^T y\] <p>where \(X\) is the design matrix (rows are \(x_1, \dots, x_n\)), and \(y\) is the vector of target values.</p> <p>At first glance, this expression might seem abstract. However, by rearranging it, we can show that the solution also lies in the span of the training data.</p> <h5 id="showing-the-span-property-for-ridge-regression"><strong>Showing the Span Property for Ridge Regression</strong></h5> <p>To reveal the span property, let‚Äôs rewrite the ridge regression solution. Using matrix algebra:</p> \[w^* = \left( X^T X + \lambda I \right)^{-1} X^T y\] <p>We can express \(w^*\) as:</p> \[w^* = X^T \left[ \frac{1}{\lambda} y - \frac{1}{\lambda} X w^* \right] \tag{1}\] <p>Now, define:</p> \[\alpha^* = \frac{1}{\lambda} y - \frac{1}{\lambda} X w^*\] <p>Substituting this back, we get:</p> \[w^* = X^T \alpha^*\] <p>Expanding further, it becomes:</p> \[w^* = \sum_{i=1}^n \alpha^*_i x_i\] <p>This clearly shows that the ridge regression solution \(w^*\) is also a linear combination of the training inputs. Thus, like SVMs, ridge regression solutions lie in the span of \(x_1, x_2, \dots, x_n\):</p> \[w^* \in \text{span}(x_1, \dots, x_n)\] <p>You may wonder how we arrived at this specific form for \(w^*\) at the start \((1)\). To understand this, we utilized the following lemma and a series of transformations to reframe it accordingly.</p> <h6 id="the-lemma-matrix-inverse-decomposition"><strong>The Lemma: Matrix Inverse Decomposition</strong></h6> <p>We use the following lemma:</p> <p>If \(A\) and \(A + B\) are non-singular, then:</p> \[(A + B)^{-1} = A^{-1} - A^{-1} B (A + B)^{-1}\] <p>This allows us to break down the inverse of a sum of matrices into manageable parts. Let‚Äôs apply it to our problem.</p> <h6 id="applying-the-lemma-to-ridge-regression"><strong>Applying the Lemma to Ridge Regression</strong></h6> <p>Let:</p> <ul> <li>\(A = \lambda I\) (scaled identity matrix),</li> <li>\(B = X^T X\) (Gram matrix).</li> </ul> <p>Substituting into the ridge regression solution:</p> \[w^* = (X^T X + \lambda I)^{-1} X^T y\] <p>Using the lemma, we expand the inverse:</p> \[w^* = \left( \lambda^{-1} - \lambda^{-1} X^T X (X^T X + \lambda I)^{-1} \right) X^T y\] <p>We simplify the terms step by step:</p> <ol> <li> <p><strong>Expand the first term:</strong></p> \[w^* = X^T \lambda^{-1} y - \lambda^{-1} X^T X (X^T X + \lambda I)^{-1} X^T y\] </li> <li> <p><strong>Notice the recursive structure:</strong></p> \[w^* = X^T \lambda^{-1} y - \lambda^{-1} X^T X w^*\] </li> <li> <p><strong>Rearrange to highlight the span of data:</strong></p> \[w^* = X^T \left( \frac{1}{\lambda} y - \frac{1}{\lambda} X w^* \right)\] </li> </ol> <h6 id="supporting-details"><strong>Supporting Details</strong></h6> <p>To solidify our understanding, here‚Äôs how the <strong>Matrix Sum Inverse Lemma</strong> is derived using the Woodbury identity:</p> <p><strong>The Woodbury Identity:</strong></p> \[(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}\] <p>Substituting:</p> <ul> <li>\(C = I\), \(V = I\), and \(U = B\), we get:</li> </ul> \[(A + B)^{-1} = A^{-1} - A^{-1} B (I + A^{-1} B)^{-1} A^{-1}\] <p>Simplify:</p> \[(A + B)^{-1} = A^{-1} - A^{-1} B (A (I + A^{-1} B))^{-1}\] \[(A + B)^{-1} = A^{-1} - A^{-1} B (A + B)^{-1}\] <p>This completes the proof of the lemma and justifies its use in our derivation.</p> <h5 id="core-takeaway"><strong>Core Takeaway:</strong></h5> <p>Both SVMs and ridge regression share the property that their solutions lie in the span of the training data. For SVMs, this emerges naturally from the dual-primal connection, highlighting how support vectors define the decision boundary. In ridge regression, the span property arises through matrix algebra and the closed-form solution.</p> <p>This unifying view provides a deeper understanding of how machine learning models leverage training data to construct solutions. Next, we‚Äôll explore <strong>how this property influences kernelized methods</strong> and its implications for scalability and interpretability in machine learning.</p> <hr/> <h4 id="reparameterizing-optimization-problems-building-on-the-span-property"><strong>Reparameterizing Optimization Problems: Building on the Span Property</strong></h4> <p>In the previous section, we established that both SVM and ridge regression solutions lie in the <strong>span of the training data</strong>. This insight opens up a new avenue: we can <strong>reparameterize the optimization problem</strong> by restricting our search space to this span. Let‚Äôs explore how this simplifies the optimization process and why it‚Äôs particularly useful in high-dimensional settings.</p> <h5 id="reparameterization-of-ridge-regression"><strong>Reparameterization of Ridge Regression</strong></h5> <p>To recap, the ridge regression problem for regularization parameter \(\lambda &gt; 0\) is:</p> \[w^* = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \{ w^T x_i - y_i \}^2 + \lambda \|w\|_2^2\] <p>We know that \(w^* \in \text{span}(x_1, \dots, x_n) \subset \mathbb{R}^d\). Therefore, instead of minimizing over all of \(\mathbb{R}^d\), we can restrict our optimization to the span of the training data:</p> \[w^* = \arg\min_{w \in \text{span}(x_1, \dots, x_n)} \frac{1}{n} \sum_{i=1}^n \{ w^T x_i - y_i \}^2 + \lambda \|w\|_2^2\] <p>Now, let‚Äôs <strong>reparameterize</strong> the objective function. Since \(w \in \text{span}(x_1, \dots, x_n)\), we can express \(w\) as a linear combination of the inputs:</p> \[w = X^T \alpha, \quad \alpha \in \mathbb{R}^n\] <p>Substituting this into the optimization problem gives:</p> <h5 id="reparameterized-objective"><strong>Reparameterized Objective</strong></h5> <p>The original formulation becomes:</p> \[\alpha^* = \arg\min_{\alpha \in \mathbb{R}^n} \frac{1}{n} \sum_{i=1}^n \{ (X^T \alpha)^T x_i - y_i \}^2 + \lambda \|X^T \alpha\|_2^2\] <p>Once \(\alpha^*\) is obtained, the optimal weight vector \(w^*\) can be recovered as:</p> \[w^* = X^T \alpha^*\] <h5 id="why-does-this-matter"><strong>Why Does This Matter?</strong></h5> <p>By reparameterizing, we‚Äôve effectively reduced the dimension of the optimization problem:</p> <ul> <li><strong>Original Problem</strong>: Optimize over \(\mathbb{R}^d\) (where \(d\) is the feature space dimension).</li> <li><strong>Reparameterized Problem</strong>: Optimize over \(\mathbb{R}^n\) (where \(n\) is the number of training examples).</li> </ul> <p>This reduction is significant in scenarios where \(d \gg n\). For instance:</p> <ul> <li><strong>Very Large Feature Space</strong>: Suppose \(d = 300 \, \text{million}\) (e.g., using high-order polynomial interactions).</li> <li><strong>Moderate Training Set Size</strong>: Suppose \(n = 300,000\) examples.</li> </ul> <p>In the original formulation, we solve a \(300 \, \text{million}\)-dimensional optimization problem. After reparameterization, we solve a much smaller \(300,000\)-dimensional problem. This simplification highlights why the span property is crucial, particularly when the number of features vastly exceeds the number of training examples.</p> <hr/> <h4 id="generalization-the-representer-theorem"><strong>Generalization: The Representer Theorem</strong></h4> <p>The span property is not unique to SVM and ridge regression. A powerful result known as the <strong>Representer Theorem</strong> shows that this property applies broadly to all norm-regularized linear models.Here‚Äôs how it works:</p> <h5 id="generalized-objective"><strong>Generalized Objective</strong></h5> <p>We start with a generalized objective for a norm-regularized model:</p> \[w^* = \arg \min_{w \in \mathcal{H}} R(\|w\|) + L\big((\langle w, x_1 \rangle), \dots, (\langle w, x_n \rangle)\big).\] <p>Here:</p> <ul> <li>\(R(\|w\|)\): Regularization term to control model complexity.</li> <li>\(L\): Loss function that measures the fit of the model to the data.</li> <li>\(\mathcal{H}\): Hypothesis space where \(w\) resides.</li> </ul> <h5 id="key-insight-from-the-representer-theorem"><strong>Key Insight from the Representer Theorem</strong></h5> <p>The Representer Theorem tells us that instead of searching for \(w^*\) in the entire hypothesis space \(\mathcal{H}\), we can restrict our search to the span of the training data. Mathematically:</p> \[w^* = \arg \min_{w \in \text{span}(x_1, \dots, x_n)} R(\|w\|) + L\big((\langle w, x_1 \rangle), \dots, (\langle w, x_n \rangle)\big).\] <p>This dramatically reduces the complexity of the optimization problem.</p> <h5 id="reparameterization"><strong>Reparameterization</strong></h5> <p>Using this insight, we can reparameterize the optimization problem as before. Let \(w = \sum_{i=1}^n \alpha_i x_i\), where \(\alpha = (\alpha_1, \dots, \alpha_n) \in \mathbb{R}^n\). Substituting this into the objective:</p> \[\alpha^* = \arg \min_{\alpha \in \mathbb{R}^n} R\left(\left\| \sum_{i=1}^n \alpha_i x_i \right\|\right) + L\Big(\big\langle \sum_{i=1}^n \alpha_i x_i, x_1 \big\rangle, \dots, \big\langle \sum_{i=1}^n \alpha_i x_i, x_n \big\rangle\Big).\] <h5 id="why-this-matters"><strong>Why This Matters</strong></h5> <p>By reparameterizing the problem, we transform the optimization from a potentially infinite-dimensional space \(\mathcal{H}\) to a finite-dimensional space (spanned by the data points). This makes the problem computationally feasible and reveals why the solution lies in the span of the data.</p> <h4 id="implications-kernelization-and-the-kernel-trick"><strong>Implications: Kernelization and the Kernel Trick</strong></h4> <p>The Representer Theorem plays a pivotal role in enabling <strong>kernelization</strong>. Here‚Äôs how it connects:</p> <p>Using the Representer Theorem, we know that the solution \(w^*\) resides in the span of the data. This insight allows us to replace the feature space \(\phi(x)\) with a kernel function \(K(x, x')\), where:</p> \[K(x, x') = \langle \phi(x), \phi(x') \rangle.\] <p>The kernel function computes the inner product in the transformed feature space without explicitly constructing \(\phi(x)\). This process is called <strong>kernelization</strong>.</p> <h5 id="kernelized-representer-theorem"><strong>Kernelized Representer Theorem</strong></h5> <p>The Representer Theorem in the context of kernels can be expressed as:</p> \[w^* = \sum_{i=1}^n \alpha_i \phi(x_i),\] <p>where the coefficients \(\alpha\) are obtained by solving an optimization problem that depends only on the kernel \(K(x_i, x_j)\).</p> <p>The Representer Theorem provides a unifying framework for kernelization. By recognizing that solutions lie in the span of the data, we can seamlessly replace explicit feature mappings with kernel functions. This powerful insight underpins many modern machine learning techniques, making high-dimensional learning tasks computationally feasible.</p> <hr/> <h5 id="summary"><strong>Summary</strong></h5> <ol> <li><strong>Reparameterization</strong>: If a solution lies in the span of the training data, we can reparameterize the optimization problem to reduce its dimensionality, simplifying the computation.</li> <li><strong>High-Dimensional Settings</strong>: This approach is especially useful when \(d \gg n\), where the feature space dimension far exceeds the number of training examples.</li> <li><strong>Representer Theorem</strong>: The span property generalizes to all norm-regularized linear models, forming the theoretical foundation for kernelization and advocates that linear models can be kernelized.</li> <li><strong>Kernel Trick</strong>: By kernelizing models, we can solve complex problems in high-dimensional spaces efficiently and without the need to represent \(\phi(x)\) explicitly.</li> </ol> <p>Understanding the span property and its implications is not just a mathematical curiosity‚Äîit‚Äôs a foundational principle that unifies many machine learning models and opens up practical avenues for efficient computation in challenging scenarios.</p> <p>Next, we‚Äôll delve into specific topics related to SVM that we‚Äôve touched on briefly. We‚Äôll explore them in more depth to build an intuitive understanding of each concept, as many of these form the foundation for more advanced ML techniques. Mastering them is well worth the effort. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Representer Theorem</li> <li>Visualization elements</li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.]]></summary></entry></feed>