<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-13T15:37:33+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A space for journaling my learnings, career, thoughts, and experiences in this amazing ride of life. </subtitle><entry><title type="html">Transformer Block FLOPs &amp;amp; Parameters</title><link href="https://monishver11.github.io/blog/2026/transformer-block-accounting/" rel="alternate" type="text/html" title="Transformer Block FLOPs &amp;amp; Parameters"/><published>2026-02-13T14:31:00+00:00</published><updated>2026-02-13T14:31:00+00:00</updated><id>https://monishver11.github.io/blog/2026/transformer-block-accounting</id><content type="html" xml:base="https://monishver11.github.io/blog/2026/transformer-block-accounting/"><![CDATA[<h1 id="transformer-block-matrix-dimensions-and-flop-calculations">Transformer Block: Matrix Dimensions and FLOP Calculations</h1> <h4 id="notation"><strong>Notation</strong></h4> <ul> <li>\(B\): Batch size</li> <li>\(S\): Sequence length</li> <li>\(D\): Hidden dimension / Model dimension</li> <li>\(N\): Number of attention heads</li> <li>\(H\): Head dimension $(H = D/N)$</li> <li>\(F\): Feed-forward hidden dimension (typically $4D$)</li> <li>\(S_1\)/\(S_2\): Sequence length in attention context, $S_1$=of query &amp; $S_2$=of key (often $S$)</li> <li>\(\text{SiLU}\): Sigmoid Linear Unit activation (also known as Swish)</li> <li>\(\text{RoPE}\): Rotary Position Embeddings</li> <li>\([X \times Y]\): Matrix/tensor dimensions</li> <li>\(@\): Matrix multiplication operator</li> <li>\([X]\cdot[Y]\): Elementwise multiplication/dot-product</li> <li>\(L\): Number of layers</li> </ul> <h4 id="transformer-block-operations"><strong>Transformer Block Operations</strong></h4> <h5 id="input"><strong>Input</strong></h5> \[X = [B \times S \times D] \quad \text{(Input tensor)}\] <h5 id="rms-normalization-1"><strong>RMS Normalization 1</strong></h5> \[\text{Rmsnorm}(x) = \text{normed\_x} * \text{gains}, \quad [B \times S \times D] \cdot [D] \rightarrow \quad [B \times S \times D]\] <h5 id="multi-head-attention-mha"><strong>Multi-Head Attention (MHA)</strong></h5> <p>Attention applied on Rmsnorm output:</p> <ul> <li> <p><strong>(a)</strong> $Q = W_q @ x$, $\quad [D \times D] @ [B \times S \times D] \rightarrow [B \times S \times D]$</p> </li> <li> <p><strong>(b)</strong> $K = W_k @ x$, $\quad [D \times D] @ [B \times S \times D] \rightarrow [B \times S \times D]$</p> </li> <li> <p><strong>(c)</strong> $V = W_v @ x$, $\quad [D \times D] @ [B \times S \times D] \rightarrow [B \times S \times D]$</p> </li> <li> <p><strong>(d)</strong> Rearrange for multi-head attention($Q, K, V$): \([B \times S \times D] \rightarrow [B \times S \times (N \cdot H)] \rightarrow [B \times N \times S \times H]\)</p> </li> <li> <p><strong>(e)</strong> Apply RoPE to $Q$ and $K$ (element-wise) $\rightarrow [B \times N \times S \times H]$</p> </li> <li> <p><strong>(f)</strong> Scaled Dot-Product Attention with Causal Mask: \(\begin{align} QK^T &amp;= [B \times N \times S_1 \times H] @ [B \times N \times S_2 \times H]^T \rightarrow [B \times N \times S_1 \times S_2] \\ \text{output} &amp;= \text{attn-weights} @ V = [B \times N \times S_1 \times S_2] @ [B \times N \times S_2 \times H] \rightarrow [B \times N \times S_1 \times H] \end{align}\)</p> </li> <li> <p><strong>(g)</strong> Rearrange back: \([B \times N \times S \times H] \rightarrow [B \times S \times (N \cdot H)] \rightarrow [B \times S \times D]\)</p> </li> <li> <p><strong>(h)</strong> Output Projection: \(X_{\text{out}} = W_o @ X_{\text{attn}}, \quad [D \times D] @ [B \times S \times D] \rightarrow [B \times S \times D]\)</p> </li> </ul> <h5 id="rms-normalization-2"><strong>RMS Normalization 2</strong></h5> \[\text{Rmsnorm}_2 = [B \times S \times D] \cdot [D] \rightarrow [B \times S \times D]\] <h5 id="feed-forward-network-ffn"><strong>Feed-Forward Network (FFN)</strong></h5> <ol> <li> <p><strong>Gate projection:</strong> \(\text{Gate} = W_1 @ X, \quad [D \times F] @ [B \times S \times D] \rightarrow [B \times S \times F]\)</p> </li> <li> <p><strong>Activation:</strong> \(\text{Gate} = \text{SiLU}(\text{Gate}) \quad \text{(element-wise activation)}\)</p> </li> <li> <p><strong>Linear projection:</strong> \(\text{Linear} = W_3 @ X, \quad [D \times F] @ [B \times S \times D] \rightarrow [B \times S \times F]\)</p> </li> <li> <p><strong>Gated multiplication:</strong> \(\text{Gated} = \text{Gate} \otimes \text{linear} \quad \text{(element-wise multiplication)}\)</p> </li> <li> <p><strong>Output projection:</strong> \(\text{Net} = W_2 @ \text{Gated}, \quad [F \times D] @ [B \times S \times F] \rightarrow [B \times S \times D]\)</p> </li> </ol> <h4 id="flops-and-parameter-count"><strong>FLOPs and Parameter Count</strong></h4> <h5 id="forward-pass-flops"><strong>Forward Pass FLOPs</strong></h5> \[\begin{align} \text{Q, K, V Projections:} &amp;\quad 3 \times [2 \cdot D \cdot (B \cdot S \cdot D)] = 6 \cdot B \cdot S \cdot D^2 \\ \text{Attention QK}^T\text{:} &amp;\quad 2 \cdot B \cdot N \cdot S^2 \cdot H = 2 \cdot B \cdot S^2 \cdot D \\ \text{Attention weights @ V:} &amp;\quad [2 \cdot S \cdot (B \cdot N \cdot S \cdot H)] = 2 \cdot B \cdot S^2 \cdot D \\ \text{O Projections:} &amp;\quad 2 \cdot D \cdot (B \cdot S \cdot D) = 2 \cdot B \cdot S \cdot D^2 \\ \text{FFN Linear 1 (Gate + Linear):} &amp;\quad 2 \times [2 \cdot D \cdot (B \cdot S \cdot F)] = 4 \cdot B \cdot S \cdot D \cdot F \\ \text{FFN Linear 2 (Net):} &amp;\quad 2 \cdot F \cdot (B \cdot S \cdot D) = 2 \cdot B \cdot S \cdot D \cdot F \\ \textbf{Total FLOPs:} &amp;\quad \boxed{8 \cdot B \cdot S \cdot D^2 + 4 \cdot B \cdot S^2 \cdot D + 6 \cdot B \cdot S \cdot D \cdot F} \end{align}\] <h5 id="parameter-count"><strong>Parameter Count</strong></h5> \[\begin{align} \text{Q Projection:} &amp;\quad W_q: [D \times D] = D^2 \\ \text{K Projection:} &amp;\quad W_k: [D \times D] = D^2 \\ \text{V Projection:} &amp;\quad W_v: [D \times D] = D^2 \\ \text{O Projection:} &amp;\quad W_o: [D \times D] = D^2 \\ \text{FFN W}_1\text{:} &amp;\quad [D \times F] = D \cdot F \\ \text{FFN W}_3\text{:} &amp;\quad [D \times F] = D \cdot F \\ \text{FFN W}_2\text{:} &amp;\quad [F \times D] = F \cdot D \\ \text{Total Parameters:} &amp;\quad 4D^2 + 3DF \end{align}\] <h4 id="othersoutside-transformer-block-unembed-projection-lm-output-head"><strong>Others/outside Transformer Block: Unembed Projection (LM Output Head)</strong></h4> \[\begin{align} \text{Unembed:} &amp;\quad W_{\text{out}} @ X, \quad [D \times V] @ [B \times S \times D] \rightarrow [B \times S \times V] \\ \text{FLOPs:} &amp;\quad 2 \cdot D \cdot (B \cdot S \cdot V) = 2 \cdot B \cdot S \cdot D \cdot V \\ \text{Parameters:} &amp;\quad D \cdot V \end{align}\]]]></content><author><name></name></author><category term="LLMR-NYU"/><summary type="html"><![CDATA[Resource accounting for Transformer block]]></summary></entry><entry><title type="html">Distributed Systems - Lecture 1</title><link href="https://monishver11.github.io/blog/2026/ds1/" rel="alternate" type="text/html" title="Distributed Systems - Lecture 1"/><published>2026-01-26T00:35:00+00:00</published><updated>2026-01-26T00:35:00+00:00</updated><id>https://monishver11.github.io/blog/2026/ds1</id><content type="html" xml:base="https://monishver11.github.io/blog/2026/ds1/"><![CDATA[<p>A distributed system consists of one-or-more processes that interact with each other, and implement some functionality. In this class we assume that each process fails independently, that is, the failure of a process \(p\) does not imply failure of process \(q\). Achieving independence in practice is challenging (you might want to think about why), and is something one must consider when deploying distributed systems. For most of this class we will assume that processes communicate by sending and receiving messages over a network<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, which the literature refers to as <strong>message passing</strong>. The behavior of a distributed system is dictated by what protocol it is running. A protocol is analogous to an algorithm, and some of the papers (and perhaps the class) will use the term algorithm and protocol interchangeably.</p> <p>Given this broad description, our model needs to answer the following questions:</p> <ul> <li> <p>What can we say about the relative speeds of processes, or the time it takes for the network to deliver messages? Unless otherwise specified, we will be assuming the <strong>asynchronous model</strong>, which is going to play a crucial part in our analysis throughout the semester.</p> </li> <li> <p>How do we describe the behavior of a process? We will be describing processes as <strong>I/O automaton</strong>, a formal model that was introduced by Lynch and Tuttle in 1987<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>, which we explain below.</p> </li> <li> <p>What guarantees does the network provide? More precisely, we need to answer questions like: Is a message sent by a process \(p\) guaranteed to be eventually delivered? We will use both unreliable and reliable channels throughout the semester.</p> </li> <li> <p>What can fail and how? How many processes? When? What does it mean for a process to fail? The answer to this question will depend on the protocol.</p> </li> </ul> <h4 id="the-asynchronous-model"><strong>The Asynchronous Model</strong></h4> <p>The discussion below is in the context of a distributed system with two processes \(p_0\) and \(p_1\), where process \(p_0\) sends a message \(m\) to process \(p_1\). In this context consider two questions:</p> <ol> <li> <p>How much time does it take for the message to make its way from \(p_0\) to \(p_1\)? Or equivalently, how long before \(p_1\) receives \(m\) (assuming we are sure \(p_1\) will eventually receive \(m\))?</p> </li> <li> <p>How much time does it take message \(m\) to process \(p_1\) once it is received?</p> </li> </ol> <p>Observe that answering either question requires knowledge about the deployment environment, that is, knowledge about where the distributed system is running:</p> <p>a. The time it takes for a message to go from one process to another depends on the distance between the two processes (information can go no faster than the speed of light \(c\)); messages sent by the other processes; the chance that a message is corrupted in transit; etc.</p> <p>b. Similarly, time taken to process a message once it has been received on what other processes (or programs) are running on the same computer, on background tasks that the operating system or runtime might need to perform, etc.</p> <p>Furthermore, these factors that are dictated by the deployment environment change over time: operating systems and processes get updated, networks get older, environmental conditions that affect message corruption vary over time, etc.</p> <p>But when developing and implementing distributed protocols, computer scientists and programmers want to ensure correctness over time and across deployment environments. Therefore, we often build and analyze protocol behavior while making minimal assumptions about message delays and processing times.</p> <p>This set of minimal assumptions is commonly referred to as the <strong>asynchronous model</strong>, and says that:</p> <ul> <li> <p>A message sent from a process can take an unbounded amount of time before it is received by another process.</p> </li> <li> <p>Any computation (e.g., a function) can take an unbounded amount of time, and the time to compute the same function can differ across processes and over time.</p> </li> <li> <p><strong>Fairness</strong>: An event that is enabled infinitely often will be executed infinitely often.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p> </li> </ul> <p>The first two conditions are simple, we will describe fairness in more detail below but an intuitive explanation is that it limits how bad asynchrony can be. For example, consider the scenario set up at the beginning of this section (where \(p_0\) sends a message \(m\) to \(p_1\)), extended with the additional assumption that the network is reliable, that is with the assumption that the network does not drop messages. Fairness ensures that \(p_1\) must eventually receive the message, that is, if \(p_0\) sends \(m\) at time \(t\), there must exit time \(t'\) (\(t' &gt; t\)) when \(p_1\) is received. Without fairness, we need to consider executions (and deployments) where \(p_1\) never receives the message. We will see other uses of the fairness assumption below when we discuss the network model.</p> <h5 id="implications-inability-to-determine-process-failure"><strong>Implications: Inability to determine process failure</strong></h5> <p>The asynchronous model’s lack of assumptions about the deployment environment means that no protocol can determine if a process has failed without additional assumptions. In particular, processes cannot distinguish between the effects of a slow network or process, and the effect of a failed process.</p> <p>Concretely, we define that a process \(p\) has failed at time \(t\) if after time \(t\) process \(p\) does not interact with any other process. In the message passing regime that is our focus, this means that after time \(t\) process \(p\) does not send any messages, nor process a received message.</p> <p>Note that in the message passing setting the only way that a process \(q\) can determine that the state of another process \(p\) (\(p \neq q\)) is by receiving a message from \(q\). This includes determining whether process \(p\) is active (that is, that \(p\) has not failed). Consider a case where process \(q\) has not received any message from \(p\) in the interval \([t, t + \delta]\). Regardless of the value of \(\delta\), process \(q\) cannot distinguish between cases where \(p\) has failed and will never send a message, \(p\) is slow and will eventually send a message, and \(p\) has already sent a message but the network has not yet delivered it. The inability to distinguish between these three possibilities lies at the core of why protocols in the asynchronous model cannot distinguish between failures and delays.</p> <p><strong>You might wonder about what happens in practice?</strong> After all, computers (and thus the processes running on them) do fail, and identifying failed processes is important. We generally deal with this by strengthening assumptions: many of the protocols in use make assumptions about the maximum time the network can take to deliver a message. This assumption is often conservative, and protocols are designed to limit negative effects if these assumptions are violated.</p> <h4 id="processes-and-io-automata"><strong>Processes and I/O Automata</strong></h4> <p>You have likely encountered several ways to describe computation (in pseudocode, actual code, etc.), analyze computation, and write programs that can be executed by a computer. Why then do we need to discuss or figure out a new (or at least a specific) way to discuss computation in distributed protocols?</p> <p>There are two reasons: (a) the protocols are largely driven by receiving messages or timeouts, and the I/O automata model that we describe below focuses on the effect of these events; and (b) most distributed system implementations use and are described in terms of remote-procedure calls (RPCs, a common abstraction for communication) which necessitate the use of multiple threads. The I/O automata model seeks to hide the effect of concurrency, making analysis easier.</p> <p>So how do we use/reason about process logic in the I/O automate model? At its core, you specify the behavior of a process by providing event handling logic (or code) that is run in response to the following three types of events:</p> <p>(a) When a process first starts up (initialization code).</p> <p>(b) When a process receives a message of type \(M_t\).</p> <p>(c) When a timeout occurs.</p> <p>Each event handler can update the process’s state, and send 0 or more messages. For convenience, we often treat timeouts as a special type of message.</p> <p>The model execution model requires that each event handler is executed atomically, that is, it appears as if exactly one event handler is running at a time. Concretely, consider a process specification of the form:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>on initialize:
    a = 0
    b = 0

on receive increment from c:
    a = a + 1
    b = a * 2

on receive decrement from c:
    a = a - 1
    b = a * 2

on receive current_value from c:
    send(c, {a = a, b = b})
</code></pre></div></div> <p>This code receives three types of messages, <code class="language-plaintext highlighter-rouge">increment</code>, <code class="language-plaintext highlighter-rouge">decrement</code> and <code class="language-plaintext highlighter-rouge">current_value</code>. On receiving an <code class="language-plaintext highlighter-rouge">increment</code> or <code class="language-plaintext highlighter-rouge">decrement</code> message the process update the values of <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>, while it handles a <code class="language-plaintext highlighter-rouge">current_value</code> message by sending a message to <code class="language-plaintext highlighter-rouge">c</code> (the sender of the <code class="language-plaintext highlighter-rouge">current_value</code>) message the current values of <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>.</p> <p>Because event handlers are executed atomically, we can assume that whenever the <code class="language-plaintext highlighter-rouge">increment</code>, <code class="language-plaintext highlighter-rouge">decrement</code> or <code class="language-plaintext highlighter-rouge">current_value</code> handlers are run <code class="language-plaintext highlighter-rouge">b = 2*a</code>, and we do not need to consider cases where two handlers are interleaved.</p> <p>We use Elixir for labs in this class and the code you write will be very close to an I/O automata specification. For example, the code above in Elixir would read as:</p> <div class="language-elixir highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="n">start_process</span><span class="p">()</span> <span class="k">do</span>
  <span class="n">process_logic</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="k">end</span>

<span class="k">defp</span> <span class="n">process_logic</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="k">do</span>
  <span class="k">receive</span> <span class="k">do</span>
    <span class="p">{</span><span class="n">_c</span><span class="p">,</span> <span class="ss">:increment</span><span class="p">}</span> <span class="o">-&gt;</span>
      <span class="n">process_logic</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="p">{</span><span class="n">_c</span><span class="p">,</span> <span class="ss">:decrement</span><span class="p">}</span> <span class="o">-&gt;</span>
      <span class="n">process_logic</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="p">{</span><span class="n">c</span><span class="p">,</span> <span class="ss">:current_value</span><span class="p">}</span> <span class="o">-&gt;</span>
      <span class="n">send</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="p">%{</span><span class="n">a</span> <span class="o">=&gt;</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=&gt;</span> <span class="n">b</span><span class="p">})</span>
      <span class="n">process_logic</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div> <p>Observe the close correspondence between the pseudocode/protocol description above and the Elixir code.</p> <h4 id="network-model"><strong>Network Model</strong></h4> <p>The network in a distributed system connects processes and allows them to interact with each other. However, networks themselves are built using wires and devices (switches, etc.) that can have errors or fail. Furthermore, networks must also deal with situations where they need to carry more messages than they can accommodate. The network model captures what processes can expect from a network regardless of failures, errors or a lack of resources.</p> <p>We express the network model in terms of the behavior of a channel (or connection) between two processes. We can characterize connections along two dimensions:</p> <h5 id="reliability-does-it-drop-messages"><strong>Reliability: Does it drop messages</strong></h5> <p>An <strong>unreliable channel</strong> is one that can drop messages. When using an unreliable channel that connects process \(p\) to \(q\) (neither of which fail), a process \(q\) may never receive a message \(m\) sent by \(p\).</p> <p>We require unreliable channels to be fair (in the same sense as we assumed for the asynchronous model): if some process \(p\) sends \(m\) infinitely often to process \(q\) over an unreliable channel, then \(q\) must receive \(m\) infinitely often.</p> <p>A <strong>reliable channel</strong> is one that does not drop messages: if process \(p\) sends message \(m\) to process \(q\), then eventually \(q\) receives \(m\) or \(q\) fails (we cannot ensure messages are received by a failed process).</p> <p>Observe that the fairness assumption allows one to write a distributed protocol that provides reliability (that is, provides the same guarantees as a reliable channel) to processes communicating using an unreliable channel. Indeed, many real networks (including the Internet) provide unreliable channels and programs (or the operating system) use additional protocols for reliability. Unless otherwise stated, we assume unreliable channels.</p> <h5 id="ordering"><strong>Ordering</strong></h5> <p>An <strong>ordered channel</strong> guarantees that messages are delivered in the order sent. Put more precisely, an ordered channel ensure that if process \(q\) receives message \(m_0\) then \(m_1\) (both sent by process \(p\)) then process \(p\) must have sent \(m_0\) before \(m_1\). Note, the ordering guarantees only apply to messages sent by a single process: process \(q\) might receive \(m_0\) from process \(p\) before \(m_1\) from process \(r\) even though \(p\) sent \(m_0\) before \(r\) sent \(m_1\).</p> <p>On the other hand, an <strong>unordered channel</strong> makes no guarantees about the order in which messages are received even when they are sent by the same process.</p> <p>Observe that one can produce a protocol that orders messages sent over an unordered channel. [Doubt, what this means?]</p> <h4 id="failure-model"><strong>Failure Model</strong></h4> <p>Generally one needs to make assumptions about how many processes can fail and the behavior of failed processes. These assumptions are collectively referred to as the <strong>failure model</strong>.</p> <p>The failure model for most protocols specify that no more than \(f\) processes (where \(f\) is some function of the total number of processes \(p\), e.g., \(f = \frac{n}{2} - 1\)). Failure models must also specify the behavior of a process after it has failed, including whether it can recover. In this class we will consider three types of failures:</p> <ul> <li> <p><strong>Fail-stop</strong>: Most of our analysis will assume fail-stop behavior, where after process \(p\) fails it can no longer interact with any other process, that is it can neither send nor receive messages. We assumed this model previously when discussing the implications of the asynchronous model (though note that the inability to distinguish between failures and delays applies to all models). It is important to note that in the fail-stop model once a process has failed it can never recover, that is, once a process fails it will never again send or receive a message.</p> </li> <li> <p><strong>Fail-recover</strong>: The fail-recover model extend the fail-stop model to allow recovery. That is, in the fail-recover model a failed process cannot send or receive any messages. However, if a process \(p\) fails at time \(t\), it might recover (and start sending or receiving messages) at some time \(t' &gt; t\). Observe that a recovered process might have stale or incorrect state, and protocols that assume the fail-recover model usually include logic to check and update the state of recovered processes.</p> </li> <li> <p><strong>Byzantine failures</strong>: The byzantine failure model, which will be our focus later in the semester does not impose any restrictions on how failed-processes behave. In other words, a failed process can stop sending messages (similar to the fail-stop model), send corrupt messages, or send messages that do not conform to the protocol being executed. This is a general failure model that allows reasoning about protocol behavior in the presence of bugs or malicious actors, and is used when designing protocols whose correctness is critical.</p> </li> </ul> <hr/> <h5 id="credits"><strong>Credits</strong>;</h5> <ul> <li>The content of this post represents the exact course notes we received, so all credit goes to the course instructor.</li> </ul> <h5 id="footnotes"><strong>Footnotes</strong></h5> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Processes can also interact with each other using a set of shared registers or shared-memory locations. As we will cover later in class, message passing and shared memory have the same expressiveness, that is, a protocol written for one can be run in the other, albeit doing so affects the protocol’s performance and failure guarantees. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>Hierarchical correctness proofs for distributed algorithms. Lynch and Tuttle, PODC 1987. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>Really the assumption here is strong fairness. In our protocols we often care about liveness, and as we will discuss in a later section we care about turning unreliable channels into reliable channels. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="DS-NYU"/><summary type="html"><![CDATA[Distributed Systems Course at NYU Courant - Personal Notes 1]]></summary></entry><entry><title type="html">LLMR - Lecture 1</title><link href="https://monishver11.github.io/blog/2026/llmr1/" rel="alternate" type="text/html" title="LLMR - Lecture 1"/><published>2026-01-24T22:30:00+00:00</published><updated>2026-01-24T22:30:00+00:00</updated><id>https://monishver11.github.io/blog/2026/llmr1</id><content type="html" xml:base="https://monishver11.github.io/blog/2026/llmr1/"><![CDATA[<h4 id="course-overview"><strong>Course Overview</strong></h4> <p><strong>Goals of this course:</strong> Understand how a language model works all the way down. Build it from scratch to really learn this.</p> <p>According to Percy Liang, three important things to know:</p> <ul> <li><strong>Mechanics:</strong> how things work (Transformer architecture, model parallelism on GPUs, etc.)</li> <li><strong>Mindset:</strong> squeezing the most out of the hardware, taking scale seriously</li> <li><strong>Intuitions:</strong> which data and modeling decisions yield good accuracy</li> </ul> <h4 id="language-models"><strong>Language Models</strong></h4> <p><strong>Language models:</strong> place a distribution \(P(w)\) over strings \(w\) in a language.</p> <p><strong>Autoregressive models:</strong></p> \[P(w) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1, w_2) \cdots\] <h5 id="n-gram-models"><strong>N-gram Models</strong></h5> <p><strong>N-gram models:</strong> distribution of next word is a categorical conditioned on previous \(n-1\) words</p> \[P(w_i|w_1, \ldots, w_{i-1}) = P(w_i|w_{i-n+1}, \ldots, w_{i-1})\] <p><strong>Markov property:</strong> only consider a few previous words</p> <p><strong>Example:</strong> I visited San _____</p> <ul> <li>2-gram: \(P(w \mid \text{San})\)</li> <li>3-gram: \(P(w \mid \text{visited San})\)</li> <li>4-gram: \(P(w \mid \text{I visited San})\)</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li>N-gram LLMs don’t generalize or abstract over related words</li> <li>Don’t handle contexts beyond a few words</li> </ul> <h5 id="neural-language-models"><strong>Neural Language Models</strong></h5> <p><strong>Feedforward networks:</strong> Language models based purely on feedforward networks can abstract over words (using embeddings), but still fail to use large context.</p> <p><strong>Solution:</strong> Need to handle more context</p> <ul> <li>RNNs or CNNs can do this</li> <li>Current best: <strong>Transformers using self-attention</strong></li> </ul> <h4 id="transformers"><strong>Transformers</strong></h4> <h5 id="attention-mechanism"><strong>Attention Mechanism</strong></h5> <p><strong>Attention:</strong> method to access arbitrarily far back in context from this point.</p> <p><strong>Components:</strong></p> <ul> <li><strong>Keys:</strong> embedded versions of the sentence</li> <li><strong>Query:</strong> what we want to find</li> </ul> <p><strong>Attention Process:</strong></p> <p><strong>Step 1:</strong> Compute scores for each key \(s_i = k_i^T q\)</p> <p><strong>Step 2:</strong> Softmax the scores to get probabilities \(\alpha\)</p> <p><strong>Step 3:</strong> Compute output values by multiplying embeddings by alpha and summing \(\text{result} = \sum_i \alpha_i e_i\)</p> <h5 id="making-attention-more-flexible"><strong>Making Attention More Flexible</strong></h5> <p>We can make attention more peaked by not setting keys equal to embeddings. Introduce weight matrices for keys.</p> <h5 id="attention-formally"><strong>Attention, Formally</strong></h5> <p><strong>Original “dot product” attention:</strong> \(s_i = k_i^T q\)</p> <p><strong>Scaled dot product attention:</strong> \(s_i = k_i^T W q\)</p> <p><strong>Equivalent to having two weight matrices:</strong> \(s_i = (W^K k_i)^T (W^Q q)\)</p> <p><strong>Reference:</strong> Jay Alammar, <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p> <h5 id="self-attention"><strong>Self-Attention</strong></h5> <p><strong>Self-attention:</strong> every word is both a key and a query simultaneously</p> <p><strong>Matrices:</strong></p> <ul> <li>\(Q\): seq_len × \(d\) matrix (\(d\) = embedding dimension = 2 for these slides)</li> <li>\(K\): seq_len × \(d\) matrix</li> </ul> <p><strong>Scores:</strong> \(S = QK^T\) where \(S_{ij} = q_i \cdot k_j\)</p> <p>Dimensions: seq_len × seq_len = (seq_len × \(d\)) × (\(d\) × seq_len)</p> <p><strong>Final step:</strong> softmax to get attentions \(A\), then output is \(AE\)</p> <p>*Technically it’s \(A(EW^V)\), using a values matrix \(V = EW^V\)</p> <p>TODO: Add pics from slides 48, 49, 50, 51, 52</p> <h5 id="attention-formalization"><strong>Attention Formalization</strong></h5> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>where:</p> <ul> <li>\(Q = EW^Q\) (queries)</li> <li>\(K = EW^K\) (keys)</li> <li>\(V = EW^V\) (values)</li> </ul> <p><strong>Normalizing by \(\sqrt{d_k}\):</strong> helps control the scale of the softmax, makes it less peaked</p> <p><strong>Multi-head attention:</strong> This is just one head of self-attention — produce multiple heads via randomly initialized parameter matrices</p> <h4 id="architecture"><strong>Architecture</strong></h4> <p>TODO: Add pics for slides 53, 54</p> <h5 id="transformer-block-structure"><strong>Transformer Block Structure</strong></h5> <p><strong>Q: In general, a transformer block contains encoder and decoder right? Here what we’re discussing is the encoder part right?</strong></p> <p><strong>A:</strong> Not quite. What we’re discussing here is a <strong>decoder-only</strong> architecture used for language modeling. The original Transformer (Vaswani et al., 2017) had both encoder and decoder blocks, but modern LLMs like GPT use only decoder blocks stacked together. Each decoder block contains:</p> <ul> <li>Causal (masked) multi-head self-attention with RoPE</li> <li>Layer normalization</li> <li>Position-wise feed-forward network</li> <li>Residual connections (Add operations)</li> </ul> <p>The key difference from an encoder is the <strong>causal masking</strong> in attention, which prevents positions from attending to future positions.</p> <h5 id="transformer-block-components"><strong>Transformer Block Components</strong></h5> <p>A single Transformer block consists of:</p> <ol> <li> <p><strong>Input:</strong> Token embeddings (seq_len × \(d_{model}\))</p> </li> <li><strong>Causal Multi-Head Self-Attention with RoPE:</strong> <ul> <li>Apply RoPE to queries and keys</li> <li>Compute attention with causal mask</li> <li>Multi-head attention allows different heads to learn different attention patterns</li> </ul> </li> <li> <p><strong>Add &amp; Norm:</strong> Residual connection + Layer Normalization</p> </li> <li><strong>Position-Wise Feed-Forward Network:</strong> \(\text{FFN}(x) = \text{SwiGLU}(x, W_1, W_2, W_3) = W_2(\text{SiLU}(W_1 x) \odot W_3 x)\) <ul> <li>Takes each position independently</li> <li>Typically uses a larger hidden dimension \(d_{ff}\)</li> </ul> </li> <li> <p><strong>Add &amp; Norm:</strong> Another residual connection + Layer Normalization</p> </li> <li><strong>Output:</strong> Transformed representations (seq_len × \(d_{model}\))</li> </ol> <p>These blocks are stacked <code class="language-plaintext highlighter-rouge">num_layers</code> times, with the output of one block feeding into the next.</p> <p>TODO: Add pics from slides 55, 56</p> <h5 id="dimensions"><strong>Dimensions</strong></h5> <p><strong>Main vector size:</strong> \(d_{model}\)</p> <p><strong>Queries/keys:</strong> \(d_k\), always smaller than \(d_{model}\), often \(d_{model}/h\) (number of heads)</p> <p><strong>Values:</strong> separate dimension \(d_v\), output is multiplied by \(W^O\) which is \((d_v \times h) \times d_{model}\) so we can get back to \(d_{model}\)</p> <p><strong>FFN:</strong> can use a higher latent dimension \(d_{ff}\)</p> \[\text{FFN}(x) = \text{SwiGLU}(x, W_1, W_2, W_3) = W_2(\text{SiLU}(W_1 x) \odot W_3 x)\] <p><strong>Typical configurations (from GPT-3):</strong></p> <table> <thead> <tr> <th>Model</th> <th>\(n_{params}\)</th> <th>\(n_{layers}\)</th> <th>\(d_{model}\)</th> <th>\(n_{heads}\)</th> <th>\(d_{head}\)</th> </tr> </thead> <tbody> <tr> <td>GPT-3 Small</td> <td>125M</td> <td>12</td> <td>768</td> <td>12</td> <td>64</td> </tr> <tr> <td>GPT-3 Medium</td> <td>350M</td> <td>24</td> <td>1024</td> <td>16</td> <td>64</td> </tr> <tr> <td>GPT-3 Large</td> <td>760M</td> <td>24</td> <td>1536</td> <td>16</td> <td>96</td> </tr> <tr> <td>GPT-3 XL</td> <td>1.3B</td> <td>24</td> <td>2048</td> <td>24</td> <td>128</td> </tr> <tr> <td>GPT-3 2.7B</td> <td>2.7B</td> <td>32</td> <td>2560</td> <td>32</td> <td>80</td> </tr> <tr> <td>GPT-3 6.7B</td> <td>6.7B</td> <td>32</td> <td>4096</td> <td>32</td> <td>128</td> </tr> <tr> <td>GPT-3 13B</td> <td>13.0B</td> <td>40</td> <td>5140</td> <td>40</td> <td>128</td> </tr> <tr> <td>GPT-3 175B</td> <td>175.0B</td> <td>96</td> <td>12288</td> <td>96</td> <td>128</td> </tr> </tbody> </table> <h5 id="flops-distribution"><strong>FLOPs Distribution</strong></h5> <p>As models scale, the proportion of FLOPs changes:</p> <ul> <li><strong>Smaller models (760M):</strong> ~35% MHA, ~44% FFN, ~15% attention computation, ~6% logits</li> <li><strong>Larger models (175B):</strong> ~17% MHA, ~80% FFN, ~3% attention computation, ~0.3% logits</li> </ul> <p>The FFN becomes increasingly dominant in larger models.</p> <h4 id="transformer-language-modeling"><strong>Transformer Language Modeling</strong></h4> <h5 id="training"><strong>Training</strong></h5> \[P(w|\text{context}) = \text{softmax}(Wh_i)\] <p>where \(W\) is a (vocab_size) × (hidden_size) matrix</p> <p><strong>Training setup:</strong></p> <ul> <li>Input is a sequence of words</li> <li>Output is those words shifted by one</li> <li>Allows us to train on predictions across several timesteps simultaneously (similar to batching but this is NOT what we refer to as batching)</li> </ul> <p><strong>Loss:</strong> \(\text{loss} = -\log P(w^*|\text{context})\)</p> <p><strong>Total loss:</strong> sum of negative log likelihoods at each position</p> <p><strong>Important:</strong> Parallel inference across several tokens at training time, but at decoding time, tokens are generated one at a time.</p> <p>TODO: Add pics from slide 61</p> <h5 id="batched-lm-training-detailed-explanation"><strong>Batched LM Training: Detailed Explanation</strong></h5> <p><strong>Batching</strong> in LM training refers to processing multiple sequences simultaneously, which is different from the parallel processing of tokens within a single sequence.</p> <p><strong>Mechanism:</strong></p> <ol> <li><strong>Batch Dimension:</strong> We process multiple independent sequences at once <ul> <li>Each sequence in the batch is a separate training example</li> <li>Sequences are typically padded to the same length for efficient matrix operations</li> </ul> </li> <li><strong>Why Batching?</strong> <ul> <li><strong>Computational Efficiency:</strong> GPUs are optimized for parallel matrix operations. Processing one sequence at a time would waste GPU capacity</li> <li><strong>Gradient Stability:</strong> Averaging gradients over multiple examples provides more stable updates</li> <li><strong>Throughput:</strong> We can process many more tokens per second</li> </ul> </li> <li><strong>The Flow:</strong> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Batch: [seq1, seq2, seq3, ...]
Shape: (batch_size, seq_len, d_model)
   
→ Each sequence processes through transformer independently
→ Attention within each sequence (not across sequences)
→ Loss computed for each sequence
→ Losses averaged across batch
</code></pre></div> </div> </li> <li><strong>Two Levels of Parallelism:</strong> <ul> <li><strong>Within-sequence parallelism:</strong> All tokens in a sequence are processed simultaneously during forward pass (enabled by self-attention)</li> <li><strong>Across-sequence parallelism:</strong> Multiple sequences processed at once (batching)</li> </ul> </li> <li><strong>Practical Considerations:</strong> <ul> <li>Batch size is limited by GPU memory</li> <li>Larger batches → more stable gradients but slower iteration</li> <li>Gradient accumulation can simulate larger batches</li> </ul> </li> </ol> <p>This is distinct from the token-level parallelism during training where we predict all positions simultaneously using teacher forcing.</p> <h5 id="a-small-problem-with-transformer-lms"><strong>A Small Problem with Transformer LMs</strong></h5> <p>This Transformer LM as we’ve described it will easily achieve perfect accuracy. Why?</p> <p>With standard self-attention: “I” attends to “saw” and the model is “cheating”. How do we ensure that this doesn’t happen?</p> <p><strong>Solution: Attention Masking</strong></p> <p>We want to mask out everything in red (an upper triangular matrix)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Query words:     &lt;s&gt;  I  saw  the  dog
Key words:  &lt;s&gt;  [ ]  [X] [X]  [X]  [X]
            I    [ ]  [ ] [X]  [X]  [X]
            saw  [ ]  [ ] [ ]  [X]  [X]
            the  [ ]  [ ] [ ]  [ ]  [X]
            dog  [ ]  [ ] [ ]  [ ]  [ ]
</code></pre></div></div> <p>Where [X] represents masked (forbidden) attention positions.</p> <h4 id="positional-encodings"><strong>Positional Encodings</strong></h4> <h5 id="why-do-we-need-them"><strong>Why Do We Need Them?</strong></h5> <p><strong>Problem:</strong> Self-attention is permutation-invariant without positional information. We need to distinguish:</p> <ul> <li>“B followed by 3 As” from other arrangements</li> <li>Position-dependent patterns</li> </ul> <h5 id="absolute-position-encodings-bert-etc"><strong>Absolute Position Encodings (BERT, etc.)</strong></h5> <p>Encode each sequence position as an integer, add it to the word embedding vector:</p> \[\text{input}_i = \text{emb}(\text{word}_i) + \text{emb}(\text{position}_i)\] <p><strong>Why does this work?</strong> The model learns to use the positional information through training. The embeddings can learn to represent position-dependent features.</p> <h5 id="sinusoidal-position-encodings-vaswani-et-al-2017"><strong>Sinusoidal Position Encodings (Vaswani et al., 2017)</strong></h5> <p>Alternative from Vaswani et al.: sines/cosines of different frequencies</p> <p><strong>Property:</strong> Closer words get higher dot products by default</p> <p>The encoding uses different frequencies for different dimensions:</p> \[PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] \[PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] <p>TODO: Add pics from slide 66, 67</p> <h5 id="rope-rotary-position-embedding---jianlin-su-et-al-2021"><strong>RoPE (Rotary Position Embedding) - Jianlin Su et al., 2021</strong></h5> <p><strong>Core Idea:</strong> Encode positional information by rotating the embedding vectors by an amount that depends on their position.</p> <p><strong>Mechanism:</strong></p> <ol> <li> <p><strong>Break vector into 2D chunks:</strong> Take the \(d\)-dimensional vector and treat it as \(d/2\) pairs of 2D vectors</p> </li> <li> <p><strong>Rotation angle:</strong> For position \(i\) and dimension pair \(k\): \(\theta_{i,k} = \frac{i}{\Theta^{(2k-2)/d}}\)</p> <p>where \(\Theta\) is a base value (typically 10000)</p> </li> <li> <p><strong>Rotation matrix:</strong> For each 2D chunk: \(R_k^i = \begin{bmatrix} \cos(\theta_{i,k}) &amp; -\sin(\theta_{i,k}) \\ \sin(\theta_{i,k}) &amp; \cos(\theta_{i,k}) \end{bmatrix}\)</p> </li> <li> <p><strong>Apply rotation:</strong> Multiply each 2D chunk by its corresponding rotation matrix</p> </li> </ol> <p><strong>What happens as \(i\) increases?</strong> \(\theta\) increases → more rotation</p> <p><strong>What happens as \(k\) increases?</strong> \(\theta\) decreases → less rotation per position</p> <p><strong>Intuition:</strong></p> <ul> <li>Initial positions (small \(k\)) rotate heavily with each position change</li> <li>Later positions (large \(k\)) rotate slowly</li> <li>This creates a multi-scale positional encoding</li> </ul> <p>TODO: Add pics from slide 68, 69</p> <p>TODO: Add pics from slide 71, 72</p> <p><strong>How does RoPE help encode position?</strong></p> <p>The key insight is that RoPE encodes <strong>relative</strong> position information through the geometry of rotations:</p> <ol> <li> <p><strong>Relative position through rotation difference:</strong> When computing attention between positions \(i\) and \(j\), the dot product \(q_i^T k_j\) naturally incorporates the rotation difference \((i-j)\)</p> </li> <li><strong>Scale-dependent locality:</strong> Different frequency bands (\(k\) values) capture different scales of positional relationships: <ul> <li>Low \(k\): sensitive to immediate neighbors</li> <li>High \(k\): captures long-range dependencies</li> </ul> </li> <li><strong>Extrapolation capability:</strong> The rotational nature allows the model to generalize to positions beyond those seen during training (within limits)</li> </ol> <h5 id="where-are-pes-used"><strong>Where are PEs used?</strong></h5> <p><strong>Classical Vaswani et al. Transformer (2017):</strong> Added to input</p> <ul> <li>PE applied once at the bottom of the network</li> <li>Affects all subsequent layers</li> </ul> <p><strong>Modern practice:</strong> Apply RoPE to Qs and Ks right before self-attention</p> <ul> <li>PE applied within each attention layer</li> <li>Only affects the attention computation, not the value vectors</li> <li>Better preserves semantic information in embeddings</li> </ul> <p><strong>How do these methods differ?</strong></p> <ul> <li>Absolute PEs: single application, affects all operations</li> <li>RoPE: applied multiple times, only affects attention mechanism, encodes relative positions</li> </ul> <h5 id="rope-interpolation-and-extrapolation-properties"><strong>RoPE Interpolation and Extrapolation Properties</strong></h5> <p><strong>Position Interpolation (PI):</strong> If RoPE is trained with encodings up to token \(L\), you can expand it to \(L'\) by scaling:</p> \[\theta_{i,k} = \frac{i \cdot (L/L')}{\Theta^{(2k-2)/d}}\] <p>This compresses the positional space, making the model think longer sequences are actually shorter.</p> <p><strong>Wavelength Concept:</strong> The number of tokens needed to do a full rotation at dimension pair \(k\):</p> \[\lambda_k = \frac{2\pi}{\theta_k} = 2\pi \Theta^{2k/|D|}\] <p><strong>YaRN (Yet another RoPE extensioN method) - Bowen Peng et al., 2023:</strong></p> <p>Two key ideas:</p> <ol> <li><strong>Frequency-dependent interpolation:</strong> <ul> <li>If wavelength is small (\(k\) is large): rotations are fast, don’t use position interpolation</li> <li>If wavelength is large (\(k\) is small): rotations are slow, use position interpolation</li> <li>This preserves high-frequency (local) information while extending low-frequency (global) range</li> </ul> </li> <li> <p><strong>Temperature scaling:</strong> Introduce a temperature \(t\) on the attention computation: \(\text{Attention} = \text{softmax}\left(\frac{QK^T}{t\sqrt{d_k}}\right)V\)</p> <p>This compensates for distribution shifts when extending context length</p> </li> </ol> <p><strong>Why does this work?</strong></p> <ul> <li>Different frequency bands in RoPE capture different types of positional relationships</li> <li>Low frequencies (small \(k\)): long-range dependencies, can be interpolated</li> <li>High frequencies (large \(k\)): local patterns, should be preserved</li> <li>Temperature adjustment maintains attention entropy at extended lengths</li> </ul> <h5 id="nope-no-position-embedding---kazemnejad-et-al-2023"><strong>NoPE (No Position Embedding) - Kazemnejad et al., 2023</strong></h5> <p><strong>Question:</strong> Do we <em>actually</em> need positional encodings?</p> <p><strong>Surprising finding:</strong> With causal masking, transformers can learn positional information implicitly!</p> <p><strong>Key difference:</strong></p> <ul> <li><strong>Full attention:</strong> No inherent positional information → requires explicit PEs</li> <li><strong>Causal mask:</strong> The mask structure itself provides positional information <ul> <li>Each position can only attend to earlier positions</li> <li>The attention pattern reveals relative positions</li> </ul> </li> </ul> <p><strong>Result:</strong> In some settings, models can work without explicit positional encodings, though most modern LLMs still use them for better performance.</p>]]></content><author><name></name></author><category term="LLMR-NYU"/><summary type="html"><![CDATA[LLM Reasoners Course at NYU Courant - Personal Notes 1]]></summary></entry><entry><title type="html">Apache Flink</title><link href="https://monishver11.github.io/blog/2025/big-data-11-flink/" rel="alternate" type="text/html" title="Apache Flink"/><published>2025-12-22T23:54:00+00:00</published><updated>2025-12-22T23:54:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-11-flink</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-11-flink/"><![CDATA[<p><strong>Slide 17</strong></p> <p>Stateful stream processing refers to the design of applications that handle continuous, unbounded streams of events (like user clicks, orders, server logs, or sensor readings) while maintaining state—intermediate data that summarizes, aggregates, or tracks information across multiple events. Unlike stateless processing, where each event is handled independently, stateful processing allows the application to remember past events and use that information to compute results for new events.</p> <p>For example, a stateful stream processing application could:</p> <ul> <li>Track the running count of clicks per user on a website.</li> <li>Maintain a windowed sum of sales per product over the last hour.</li> <li>Store sensor readings to compute rolling averages or detect anomalies.</li> </ul> <p>In practice, the application reacts to each incoming event by reading from and/or writing to its state, performing computations that can depend on both the new event and historical information stored in the state. This enables complex operations like aggregations, joins, or pattern detection on real-time streams of data.</p> <p>State here is durable and fault-tolerant (managed by the stream processing framework, e.g., Flink), so even if the application crashes or is restarted, the state can be recovered and processing can continue correctly.</p> <p><strong>Slide 19 &amp; 20</strong></p> <p>Stateful stream processing applications often consume events from an event log such as Kafka. An event log is a durable, append-only store of events, which guarantees that events are written in order and that this order cannot be changed. This design allows multiple consumers to read the same stream independently and ensures that all consumers see events in the exact same order.</p> <p>When a stateful application like Flink connects to an event log, the log provides durable storage and deterministic replay of events. Each incoming event can be processed while updating the application’s state. If the application crashes or fails, Flink can restore the last consistent snapshot of its state from a checkpoint and reset its read position on the event log. It then replays events from that point onward until it catches up to the latest event in the stream.</p> <p>This mechanism ensures fault tolerance and exactly-once semantics: the application can recover from failures without losing data or processing events out of order. It also allows the same input stream to be reprocessed multiple times if needed—for example, to recompute results after a logic update—because the event log persists all events in order.</p> <p>In essence, the combination of stateful processing in Flink and the durable, ordered nature of event logs allows applications to maintain correct state, recover from failures, and guarantee deterministic processing over unbounded streams.</p> <p><strong>Slide 69-72</strong></p> <p>In stream processing, “one minute” can mean different things depending on the time semantics you choose, and this choice has a big impact on correctness and predictability.</p> <p>With processing time, “one minute” refers to one minute on the wall clock of the machine running the operator. A processing-time window simply groups together all events that arrive at the operator within that one-minute interval, regardless of when those events actually occurred in the real world. This makes processing-time windows fast and simple, but also sensitive to system behavior: network delays, backpressure, retries, or slower machines can shift events into different windows, meaning the same input stream may produce different results at different times or under different loads.</p> <p>With event time, “one minute” refers to one minute in the timeline of the events themselves, based on timestamps attached to each event (for example, when a user clicked a button or a sensor recorded a measurement). An event-time window groups events by when they happened, not when they arrived. Because the timestamps are part of the data, event time decouples processing speed from results: whether the stream is processed fast or slow, or events arrive late or out of order, the window computation is based on the same logical time boundaries. As a result, event-time operations are predictable and deterministic, yielding the same output as long as the input events and their timestamps are the same.</p> <p><strong>Slide 73-76</strong></p> <p>Watermarks are how a stream processor like Flink decides when an event-time window is complete and ready to be evaluated. Since events can arrive late or out of order, the system cannot rely on wall-clock time. Instead, a watermark acts as a logical event-time clock that advances as the system gains confidence about how far it has progressed in event time.</p> <p>Conceptually, a watermark with timestamp T means: “I believe that all events with event-time ≤ T have already arrived.” When an operator receives this watermark, it can safely close event-time windows that end at or before T, trigger their computation, and emit results. This makes watermarks essential for event-time windows and for correctly handling out-of-order events, because they tell operators when to stop waiting for older timestamps.</p> <p>Watermarks introduce a tradeoff between latency and correctness (confidence). If watermarks advance aggressively (eager watermarks), results are produced quickly, but there is a higher risk that late events will show up after a window has already been computed. If watermarks advance conservatively (relaxed watermarks), the system waits longer before triggering windows, increasing confidence that all relevant events have arrived—but at the cost of higher end-to-end latency.</p> <p>Because late events can still occur even after a watermark, stream processing systems must define how to handle them. Common strategies include dropping late events, logging or redirecting them for monitoring, or using them to update or correct previously emitted results (for example, via retractions or updates). This is why watermarks alone are not enough—the system’s late-event handling policy is just as important for correctness.</p> <p>In Flink, watermarks are defined by the developer and generated by the source operator of the stream. When a Flink job reads events from a source such as Kafka, the developer configures a watermark strategy that tells Flink how to extract event-time timestamps from each event and how much out-of-orderness (lateness) is allowed. Based on this logic, the source periodically emits watermarks that signal the system’s progress in event time, indicating that no more events with timestamps earlier than a certain time are expected. These watermarks then flow through the entire dataflow alongside the events, and downstream operators such as windows and joins use them to decide when it is safe to trigger computations. Operators themselves do not create watermarks; they only act upon the watermarks produced by the source.</p> <p><strong>Slide 77</strong></p> <p>Even though event time gives correct and deterministic results, processing time is still useful because it optimizes for speed and simplicity rather than correctness under disorder. Processing-time windows trigger based on the machine’s wall clock, so results are produced immediately as data arrives, giving the lowest possible latency with no need to wait for late events or watermarks. This makes them ideal for real-time monitoring, dashboards, and alerting, where users care more about seeing what is happening now than about perfectly accurate historical results. In addition, processing time reflects the actual arrival pattern of the stream, including delays and bursts, which can be valuable when analyzing system behavior or load rather than the underlying business events.</p> <p><strong>Slide 116-119</strong></p> <p>In Flink, state is always tied to a specific operator, and operators must register their state so that the runtime can track and manage it. There are two main types of state: operator state and keyed state. Operator state is scoped to a single operator task; all records processed by that task share the same state, but it cannot be accessed by other tasks of the same or different operators. Keyed state, on the other hand, is partitioned by a key defined in the records of the input stream. Each key has its own state instance, and all records with the same key are routed to the operator task that maintains that key’s state.</p> <p>To enable efficient state access with low latency, Flink keeps the state locally within each parallel task. How the state is stored, accessed, and maintained is handled by a state backend, such as RocksDB. The state backend is responsible for managing the local state for fast access during processing and for checkpointing the state to a remote location to enable fault tolerance and recovery. This design allows Flink to maintain consistent, high-performance state even in large-scale, distributed streaming applications.</p> <p><strong>Slide 121</strong></p> <p>Flink’s recovery mechanism relies on consistent checkpoints of the application’s state. A checkpoint is a snapshot of the state of all tasks in the streaming application at a specific point in time. For the checkpoint to be consistent, it must capture the state such that all tasks have processed exactly the same input events up to that point. This ensures that, in case of a failure, the application can be restored to a previous checkpoint and resume processing without losing data or processing events out of order.</p> <p><strong>Slide 123</strong></p> <p>Flink can provide exactly-once state consistency through its checkpointing and recovery mechanism, but only if certain conditions are met. First, all operators must correctly checkpoint and restore their state, so that after a failure, the internal state of the application reflects a precise moment in time. Second, all input streams must be reset to the exact position they were at when the checkpoint was taken, so the same events can be replayed deterministically. Whether this is possible depends on the data source: event logs like Kafka support resetting to a previous offset, allowing Flink to reprocess events reliably, whereas sources like sockets cannot be reset, since consumed data is lost. When both state restoration and input replay are possible, Flink can guarantee that each event affects the application state exactly once, even in the presence of failures.</p> <p><strong>Slide 124</strong></p> <p>Flink’s checkpointing and recovery mechanism guarantees exactly-once consistency only for the application’s internal state, not automatically for the external systems where results are written. When a failure occurs, Flink restores operator state from the latest checkpoint and replays input events, which can cause some output records to be emitted again. If the sink does not support transactions or idempotent writes (for example, a plain filesystem, database, or event log), these repeated emissions may lead to duplicate results in downstream systems. To achieve end-to-end exactly-once semantics, the sink itself must be able to handle duplicates safely or participate in Flink’s transactional mechanisms.</p> <p><strong>Slide 125</strong></p> <p>End-to-end exactly-once consistency is achieved when both Flink’s internal state and the external sink observe each event exactly once. Flink provides special exactly-once sink implementations for some storage systems (such as Kafka or transactional filesystems) that buffer output and only commit the emitted records when a checkpoint successfully completes, ensuring that partial results from failed executions are never made visible. For storage systems that do not support transactions, idempotent updates are commonly used: the sink is designed so that writing the same record multiple times produces the same final result (for example, using upserts or overwriting by a unique key). With transactional sinks or idempotent writes, replayed events during recovery do not cause incorrect or duplicated outcomes, enabling true end-to-end exactly-once behavior.</p> <p><strong>Slide 126 - 136</strong></p> <p>Flink’s checkpointing algorithm is based on the Chandy–Lamport distributed snapshot algorithm, which allows the system to take a consistent snapshot of a running distributed application without stopping the entire dataflow. Instead of pausing all processing, Flink decouples checkpointing from normal execution so that tasks can keep processing data while checkpoints are being coordinated and state is being persisted. This is crucial for low-latency stream processing.</p> <p>The core mechanism behind this is the checkpoint barrier. A checkpoint barrier is a special marker record that Flink’s source operators inject into the data stream when a checkpoint is triggered by the JobManager. Each barrier carries a checkpoint ID and flows through the same channels as normal records, but it cannot be overtaken by other records. Conceptually, the barrier splits the stream: all state updates caused by records before the barrier belong to the current checkpoint, and updates caused by records after the barrier belong to a future checkpoint.</p> <p>When a checkpoint starts, the JobManager instructs all source tasks to begin a new checkpoint. Each source temporarily pauses emitting records, snapshots its local state to the configured state backend, and then broadcasts checkpoint barriers on all its outgoing streams. Once the state snapshot is complete, the source acknowledges the checkpoint to the JobManager and resumes normal data emission.</p> <p>When an intermediate or downstream task receives a checkpoint barrier, it must ensure consistency across all its inputs. The task performs barrier alignment: it waits until it has received the same checkpoint barrier from all its input partitions. While waiting, it continues processing records from inputs that have not yet sent a barrier, but buffers records from inputs that already forwarded a barrier. This guarantees that no record belonging to the “future” checkpoint is processed too early.</p> <p>As soon as the task has received barriers from all inputs, it snapshots its own state to the state backend, forwards the checkpoint barrier to its downstream tasks, and then resumes processing by draining the buffered records. This process continues through the entire dataflow graph. Finally, when the JobManager has received checkpoint acknowledgements from all tasks, it marks the checkpoint as complete. At that point, Flink has a globally consistent snapshot of the application state, which can be used for recovery and provides the foundation for exactly-once state consistency.</p>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Personal Notes 10]]></summary></entry><entry><title type="html">Apache Kafka</title><link href="https://monishver11.github.io/blog/2025/big-data-10-kafka/" rel="alternate" type="text/html" title="Apache Kafka"/><published>2025-12-17T19:02:00+00:00</published><updated>2025-12-17T19:02:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-10-kafka</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-10-kafka/"><![CDATA[<p><strong>Data at rest versus data in motion</strong></p> <ul> <li>So far in this semester, we have been focusing on data at rest. <ul> <li>The data is stored in a storage system.</li> <li>Examples: HDFS, Hive, HBase, …</li> </ul> </li> <li>Today, we will focus on data in motion.</li> <li>The data is in route between source and destination in the network.</li> <li>We need to handle the continuous flow of data.</li> </ul> <p><strong>Why is it important?</strong></p> <ul> <li>We need to get the data from where it is created to where it can be analyzed. <ul> <li>The faster we can do this, the more agile and responsive our business can be.</li> <li>The less eﬀort we spend on moving data around, the more we can focus on the core business at hand.</li> </ul> </li> <li>This is why the pipeline is a critical component in the data-driven business.</li> <li>How we move the data becomes nearly as important as the data itself.</li> </ul> <p><strong>A critical component of data-driven applications</strong></p> <ul> <li>Publish/subscribe (pub/sub) messaging is a pattern that is characterized by the sender (publisher) of a piece of data (message) not specifically directing it to a receiver.</li> <li>Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages.</li> <li>Publish/subscribe systems often have a broker, a central point where messages are published, to facilitate this pattern.</li> </ul> <p><strong>How it starts</strong></p> <ul> <li>Many use cases for publish/subscribe start out the same way: with a simple message queue or interprocess communication channel.</li> <li>Example: Suppose you create an application that needs to send monitoring information somewhere… <ul> <li>You open a direct connection from your application to an application that displays your metrics on a dashboard.</li> <li>Then, you push metrics over that connection.</li> </ul> </li> <li>As you have more applications that are using those servers to get individual metrics and use them for various purposes, your architecture soon becomes…</li> <li>To clean up that mess, you set up a single application that receives metrics from all the applications out there, and provide a server to query those metrics for any system that needs them.</li> <li>Maybe you need to process more than metrics data…</li> <li>There is still a lot of duplication in the system!</li> <li>What you would like to have is a single centralized system that allows for publishing generic types of data, which will grow as your business grows. Kafka to the rescue!</li> </ul> <div class="row justify-content-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-1-480.webp 480w,/assets/img/kafka-1-800.webp 800w,/assets/img/kafka-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-2-480.webp 480w,/assets/img/kafka-2-800.webp 800w,/assets/img/kafka-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-3-480.webp 480w,/assets/img/kafka-3-800.webp 800w,/assets/img/kafka-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="kafka-basics"><strong>Kafka basics</strong></h4> <p><strong>Overview</strong></p> <ul> <li>Kafka was developed as a publish/subscribe messaging system designed to solve this problem.</li> <li>It is often described as a distributing streaming platform.</li> <li>Data within Kafka is stored durably, in order, and can be read deterministically.</li> <li>In addition, the data can be distributed within the system to provide additional protections against failures, as well as significant opportunities for scaling performance.</li> <li>Q: What does “distributed streaming platform” mean in Kafka?</li> <li>A: A distributed streaming platform is a system that can collect, store, process, and deliver continuous streams of data across multiple machines. In Kafka’s case, “streaming” means data is produced and consumed continuously (not as one-time batches), and “distributed” means the data and workload are spread across many brokers for scalability and fault tolerance. Kafka keeps streams durably on disk, ordered within partitions, and replicated so that failures of individual machines do not cause data loss, while allowing many producers and consumers to operate in parallel.</li> </ul> <p><strong>Messages</strong></p> <ul> <li>The unit of data within Kafka is called a message. It’s similar to a row or a record in a database.</li> <li>A message is simply an array of bytes. The data contained within it does not have a specific format or meaning to Kafka.</li> <li>A message can have an optional piece of metadata, referred to as a key. <ul> <li>The key is also a byte array and also has no specific meaning to Kafka.</li> <li>Keys are used when messages are to be written to partitions in a more controlled manner.</li> </ul> </li> <li>Q: What does it mean that a message is an “array of bytes”?</li> <li>A: When Kafka says a message (and its key) is an array of bytes, it means Kafka treats message data as raw binary data (byte[]) without interpreting its structure or semantics. Kafka does not care whether those bytes represent a string, JSON, Avro, Protobuf, an image, or anything else. It simply stores and transfers bytes efficiently. It is the responsibility of producers to serialize objects into bytes and consumers to deserialize those bytes back into meaningful data using an agreed-upon format.</li> </ul> <p><strong>Batches</strong></p> <ul> <li>For eﬃciency, messages are written into Kafka in batches.</li> <li>A batch is just a collection of messages, all of which are being produced to the same topic and partition.</li> <li>This is a trade-oﬀ between latency and throughput. The larger the batches, the more messages that can be handled per unit of time, but the longer it takes an individual message to propagate.</li> <li>Batches are typically compressed, providing more eﬃcient data transfer and storage at the cost of some processing power.</li> </ul> <p><strong>Schemas</strong></p> <ul> <li>Although Kafka itself treats messages as opaque byte arrays, in practice applications need a schema to describe the structure and meaning of the data inside those bytes. A schema defines fields, data types, and sometimes rules, making messages easier to interpret, validate, and evolve over time. Without schemas, producers and consumers must rely on implicit agreements, which easily break as systems change.</li> <li>Formats like JSON or XML are popular because they are simple and human readable, but they lack strong typing and make it hard to safely evolve data formats without breaking consumers. Apache Avro, on the other hand, provides compact binary serialization, strict data types, and built-in support for schema evolution. Avro keeps schemas separate from the message payload and does not require regenerating code when schemas change, which fits well with Kafka’s long-lived data streams.</li> <li>A consistent schema allows producers and consumers to evolve independently. If schemas are versioned and stored in a shared repository (such as a Schema Registry), producers can start writing data with a new schema version while consumers continue to read older versions or gradually adapt. This avoids tight coordination where all producers and consumers must be updated at the same time, enabling safer upgrades and more flexible, scalable Kafka-based systems.</li> <li>Conceptual Flow: In Kafka, the message itself is just a byte array (both the value and the optional key). Kafka does not understand or enforce any schema. The schema is used by the producer and consumer, not by Kafka, to serialize data into bytes when writing and deserialize bytes back into structured data when reading. Typically, the producer serializes an object according to a schema (for example, Avro) into a byte array and sends it to Kafka, and the consumer uses the same (or a compatible) schema to interpret those bytes. When a schema registry is used, the schema information is stored separately and referenced by the message, enabling versioning and compatibility without embedding the full schema in every message.</li> </ul> <p><strong>Topics and partitions</strong></p> <ul> <li>Messages in Kafka are categorized into topics. They’re similar to tables in a database or folders in a filesystem.</li> <li>Topics are additionally broken down into a number of partitions.</li> <li>Example: Suppose we use Kafka to store the commit log. <ul> <li>Then, a partition would be a single log.</li> <li>Messages are written to it in an append-only fashion and are read in order from beginning to end.</li> </ul> </li> <li>A topic typically has multiple partitions.</li> <li>Kafka guarantees message ordering within each partition, but there is no guarantee of message ordering across the entire topic.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-4-480.webp 480w,/assets/img/kafka-4-800.webp 800w,/assets/img/kafka-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Partitions are the mechanism Kafka uses to achieve scalability and fault tolerance. Because each partition of a topic can be placed on a different broker (server), Kafka can spread the load of reads and writes across many machines, allowing a single topic to scale horizontally beyond the limits of one server. To handle failures, partitions can also be replicated, meaning the same partition is stored on multiple brokers; if one broker fails, another replica can take over without data loss.</li> <li>The term stream is a logical concept used when talking about how data flows through Kafka-based systems. A stream refers to all the data in a topic as a whole, even though that topic may be physically split across many partitions. From the perspective of producers and consumers, this represents a continuous flow of records over time.</li> <li>This terminology becomes especially important in stream processing, where frameworks (such as Kafka Streams, Flink, or Spark Streaming) process data as it arrives, rather than in batches. These frameworks treat a topic as a single stream of events, abstracting away the underlying partitions while still leveraging them internally for parallelism and scalability.</li> <li>More clearly: a Kafka topic is logically one stream of messages, but physically it is split into partitions. When replication is not used, each partition holds different messages, and together all partitions make up the complete data for the topic. No two partitions contain the same records. Messages are assigned to partitions (based on key or round-robin), and once written, they exist only in that one partition. Replication, when enabled, simply creates copies of a partition’s data on other brokers for fault tolerance; it does not change the fact that partitions themselves divide the topic’s data uniquely.</li> </ul> <p><strong>Producers and consumers</strong></p> <ul> <li>In Kafka, clients are applications that interact with the Kafka cluster, and they come in two fundamental types: producers and consumers. A producer publishes (writes) messages to Kafka topics, while a consumer subscribes to topics and reads messages from them.</li> <li>On top of these basic clients, Kafka provides advanced client APIs. Kafka Connect is used for data integration, allowing Kafka to reliably move data between Kafka and external systems like databases, filesystems, or cloud services with minimal custom code. Kafka Streams is a stream-processing library that lets applications read from Kafka topics, process the data in real time (such as filtering, aggregating, or joining streams), and write the results back to Kafka.</li> <li>Internally, both Kafka Connect and Kafka Streams are built using producers and consumers, but they expose higher-level abstractions so developers can focus on integration or processing logic rather than low-level messaging details.</li> </ul> <p><strong>Producers</strong></p> <ul> <li>Producers (a.k.a., publishers or writers) create new messages.</li> <li>A message will be produced to a specific topic.</li> <li>By default, the producer will balance messages over all partitions of a topic.</li> <li>In some cases, the producer will direct messages to specific partitions. <ul> <li>This is typically done using the message key and a partitioner that will generate a hash of the key and map it to a specific partition. This ensures that all messages produced with the same key will get written to the same partition.</li> <li>The producer could also use a custom partitioner that follows other business rules for mapping messages to partitions.</li> </ul> </li> </ul> <p><strong>Consumers</strong></p> <ul> <li>Consumers (a.k.a., subscribers or readers) read messages.</li> <li>The consumer subscribes to one or more topics and reads the messages in the order in which they were produced to each partition.</li> <li>The consumer keeps track of which messages it has already consumed by keeping track of the oﬀset of messages.</li> </ul> <p><strong>Oﬀsets</strong></p> <ul> <li>Kafka tracks a consumer’s progress using offsets, which are monotonically increasing integer values that Kafka assigns to messages as they are appended to a partition.</li> <li>Each message in a partition has a unique offset, and later messages always have larger offsets (though some numbers may be skipped).</li> <li>A consumer records the next offset to read for each partition it consumes from; this offset is stored persistently (earlier in ZooKeeper, now typically in Kafka itself). Because offsets are stored outside the consumer process, a consumer can stop, crash, or restart and then resume reading from exactly where it left off, without losing or reordering data.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-5-480.webp 480w,/assets/img/kafka-5-800.webp 800w,/assets/img/kafka-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Consumer group</strong></p> <ul> <li>Consumers work as part of a consumer group, which is one or more consumers that work together to consume a topic.</li> <li>The group ensures that each partition is only consumed by one member.</li> <li>The mapping of a consumer to a partition is often called ownership of the partition by the consumer.</li> <li>In this way, consumers can horizontally scale to consume topics with a large number of messages.</li> <li>If a single consumer fails, the remaining members of the group will reassign the partitions being consumed to take over for the missing member.</li> <li>Note: <ul> <li>A consumer can consume from multiple partitions if there are fewer consumers than partitions.</li> <li>If there are more consumers than partitions, some consumers will be idle.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-6-480.webp 480w,/assets/img/kafka-6-800.webp 800w,/assets/img/kafka-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Brokers</strong></p> <ul> <li>A single Kafka server is called a broker.</li> <li>The broker receives messages from producers, assigns oﬀsets to them, and writes the messages to storage on disk.</li> <li>It also services consumers, responding to fetch requests for partitions and responding with the messages that have been published.</li> <li>A single broker can usually handle thousands of partitions and millions of messages per second.</li> </ul> <p><strong>Clusters</strong></p> <ul> <li>Kafka brokers are designed to operate as part of a cluster.</li> <li>Within a cluster of brokers, one broker will also function as the cluster controller (automatically elected from the live members of the cluster).</li> <li>The controller is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures.</li> </ul> <p><strong>Leader and followers</strong></p> <ul> <li>A partition is owned by a single broker in the cluster. That broker is called the leader of the partition.</li> <li>A replicated partition is assigned to additional brokers. They are called followers of the partition.</li> <li>Replication provides redundancy of messages in the partition. If there is a broker failure, one of the followers can take over leadership.</li> <li>All producers must connect to the leader in order to publish messages, but consumers may fetch from either the leader or one of the followers.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-7-480.webp 480w,/assets/img/kafka-7-800.webp 800w,/assets/img/kafka-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Retention</strong></p> <ul> <li>Retention is the durable storage of messages for some period of time.</li> <li>Kafka brokers are configured with a default retention setting for topics. <ul> <li>Time-based retention: retaining messages for some period of time (e.g., 7 days)</li> <li>Space-based retention: retaining until the partition reaches a certain size in bytes (e.g., 1 GB).</li> </ul> </li> <li>Once these limits are reached, messages are expired and deleted.</li> <li>Individual topics can also be configured with their own retention settings so that messages are stored for only as long as they are useful.</li> <li>Example <ul> <li>A tracking topic might be retained for several days.</li> <li>Application metrics might be retained for only a few hours.</li> </ul> </li> <li>Topics can also be configured as log compacted. <ul> <li>Kafka will retain only the last message produced with a specific key.</li> <li>Useful for changelog data, where only the last update is interesting.</li> </ul> </li> </ul> <p><strong>Multiple clusters</strong></p> <ul> <li>As Kafka deployments grow, it also supports multiple clusters.</li> <li>Why is it useful? <ul> <li>Segregation of types of data.</li> <li>Isolation for security requirements.</li> <li>Multiple data centers (disaster recovery).</li> </ul> </li> <li>When working with multiple data centers in particular, it is often required that messages be copied between them. However, the replication mechanisms within the Kafka clusters are designed only to work within a single cluster, not between multiple clusters.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-8-480.webp 480w,/assets/img/kafka-8-800.webp 800w,/assets/img/kafka-8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-8" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>MirrorMaker is a tool for replicating data to other clusters.</li> <li>At its core, MirrorMaker is simply a Kafka consumer and producer, linked together with a queue.</li> <li>Messages are consumed from one Kafka cluster and produced to another.</li> </ul> <p><strong>What makes Kafka a good choice?</strong></p> <ul> <li>Multiple producers: Ideal for aggregating data from many frontend systems and making it consistent.</li> <li>Multiple consumers: Multiple consumers can read any single stream of messages without interfering with other clients.</li> <li>Disk-based retention: If a consumer falls behind or temporarily becomes oﬄine, there is no danger of losing data.</li> <li>Scalable: Expansions can be performed while the cluster is online, without impacting whole system availability.</li> <li>High performance: Kafka provides sub-second message latency from producing a message to availability to consumers.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-9-480.webp 480w,/assets/img/kafka-9-800.webp 800w,/assets/img/kafka-9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-9" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Note:</strong></p> <ul> <li>If a topic has 2 partitions and 4 consumers in the same consumer group, Kafka assigns at most one consumer per partition, so only 2 consumers actively read data (one per partition) while the remaining 2 stay idle; adding more consumers does not increase throughput because parallelism is bounded by the number of partitions. If one of the active consumers fails, Kafka automatically triggers a rebalance and assigns the affected partition to one of the idle consumers, which then continues consuming from the last committed offset. If the 4 consumers belong to different consumer groups, each group independently consumes all messages from both partitions, meaning all 4 consumers receive the full data stream.</li> <li>Follow-up: What happens when partitions are replicated?</li> <li>When partitions are replicated, each partition has one leader replica and one or more follower replicas on different brokers. Consumers always read from the leader replica. If the broker hosting the leader fails, Kafka automatically elects one of the in-sync follower replicas as the new leader, and consumers continue reading transparently without data loss (assuming the replica was in sync). Replication therefore provides fault tolerance and high availability, but it does not increase consumer parallelism—only the number of partitions does.</li> </ul> <p><strong>Use cases</strong></p> <ul> <li>Activity tracking <ul> <li>The original use case for Kafka at LinkedIn is user activity tracking.</li> <li>A website’s users interact with frontend applications, which generate messages regarding actions the user is taking. <ul> <li>Passive information: page views, click tracking, …</li> <li>More complex information: a user adds to their profile, …</li> </ul> </li> <li>The messages are published to one or more topics, which are then consumed by applications on the backend. These applications generate reports, feed machine learning systems, update search results, or perform other operations to provide a rich user experience.</li> </ul> </li> <li>Messaging <ul> <li>Kafka is also used for messaging, where applications need to send notifications (e.g., emails) to users.</li> <li>Those applications can produce messages without needing to be concerned about formatting or how the messages will actually be sent.</li> <li>A single application can then read all the messages to be sent and handle them consistently, including: <ul> <li>Formatting the messages (a.k.a., decorating) using a common look and feel.</li> <li>Collecting multiple messages into a single notification to be sent.</li> <li>Applying a user’s preferences for how they want to receive messages.</li> </ul> </li> </ul> </li> <li>Metrics and logging <ul> <li>Kafka is also ideal for collecting application and system metrics and logs.</li> <li>Applications publish metrics on a regular basis to a Kafka topic, and those metrics can be consumed by systems for monitoring and alerting.</li> <li>They can also be used in an oﬄine system (e.g., Hadoop) to perform longer-term analysis (e.g., growth projections), or routed to dedicated log search systems (e.g., Elasticsearch, security analysis applications).</li> <li>When the destination system needs to change (e.g., it’s time to update the log storage system), there is no need to alter the frontend applications or the means of aggregation.</li> </ul> </li> <li>Commit log <ul> <li>Database changes can be published to Kafka, and applications can easily monitor this stream to receive live updates as they happen.</li> <li>This changelog stream can also be used for replicating database updates to a remote system, or for consolidating changes from multiple applications into a single database view.</li> <li>Durable retention is useful here for providing a buﬀer for the changelog, so it can be replayed in the event of a failure of the consuming applications.</li> <li>Alternately, log-compacted topics can be used to provide longer retention by only retaining a single change per key.</li> </ul> </li> <li>Stream processing <ul> <li>Stream processing refers to applications that provide similar functionality to map/reduce processing in Hadoop.</li> <li>Hadoop usually relies on aggregation of data over a long time frame, either hours or days. On the other hand, stream processing operates on data in real time, as quickly as messages are produced.</li> <li>Stream frameworks allow users to write small applications to operate on Kafka messages, performing tasks such as counting metrics, partitioning messages for eﬃcient processing by other applications, or transforming messages using data from multiple sources.</li> </ul> </li> </ul> <h4 id="kafka-internals"><strong>Kafka internals</strong></h4> <p><strong>Cluster membership</strong></p> <ul> <li>Kafka uses ZooKeeper to maintain the list of brokers that are currently members of a cluster. Recall the group membership example from ZooKeeper</li> <li>Every broker has a unique identifier. Every time a broker process starts, it registers itself with its ID in ZooKeeper by creating an ephemeral znode under the path “/brokers/ids”.</li> <li>Kafka brokers, the controller, and some of the ecosystem tools place a watch on that path so that they get notified when brokers are added or removed.</li> <li>When a broker loses connectivity to ZooKeeper, the ephemeral znode that the broker created when starting will be automatically removed from ZooKeeper.</li> <li>Kafka components that are watching the list of brokers will be notified that the broker is gone.</li> <li>Although the znode representing the broker is gone when the broker is stopped, the broker ID still exists in Kafka’s internal data structures.</li> <li>This way, if you completely lose a broker and start a brand-new broker with the ID of the old one, it will immediately join the cluster in place of the missing broker with the same partitions and topics assigned to it.</li> </ul> <p><strong>Controller</strong></p> <ul> <li>The controller is one of the Kafka brokers that, in addition to the usual broker functionality, is responsible for electing partition leaders. <ul> <li>The first broker that starts in the cluster becomes the controller by creating an ephemeral znode in ZooKeeper called “/controller”.</li> <li>When other brokers start, they also try to create this node but receive a “node already exists” exception, which causes them to “realize” that the controller node already exists and that the cluster already has a controller.</li> <li>The brokers create a ZooKeeper watch on the controller znode so they get notified of changes to this znode.</li> </ul> </li> <li>This way, we guarantee that the cluster will only have one controller at a time.</li> <li>When the controller broker is stopped or loses connectivity to ZooKeeper, the ephemeral znode will disappear.</li> <li>Other brokers in the cluster will be notified through the ZooKeeper watch that the controller is gone and will attempt to create the controller znode in ZooKeeper themselves.</li> <li>The first node to create the new controller in ZooKeeper becomes the next controller, while the other nodes will receive a “node already exists” exception and re-create the watch on the new controller znode.</li> <li>When the controller first comes up, it has to read the latest replica state map from ZooKeeper before it can start managing the cluster metadata and performing leader elections.</li> <li>When the controller notices that a broker left the cluster, it knows that all the partitions that had a leader on that broker will need a new leader.</li> <li>It goes over all the partitions that need a new leader and determines who the new leader should be (simply the next replica in the replica list of that partition).</li> <li>Then it persists the new state to ZooKeeper and sends information about the leadership change to all the brokers that contain replicas for those partitions.</li> <li>Example flow: <ul> <li>Suppose a Kafka cluster has three brokers: B1, B2, and B3. When the cluster starts, B1 starts first and creates the ephemeral znode /controller in ZooKeeper, becoming the controller. When B2 and B3 start, they try to create the same znode but get a “node already exists” exception, so they know B1 is already the controller and set a watch on /controller. Now, imagine B1 crashes or loses connectivity. Its ephemeral znode disappears, and B2 and B3 are notified via their watches. Both try to create /controller; whichever succeeds first becomes the new controller (say B2), while the other (B3) gets an exception and resets its watch on /controller.</li> <li>After B2 becomes the new controller, it first reads the latest replica state from ZooKeeper to understand the current mapping of partitions and their leaders. Suppose the cluster has a topic orders with 3 partitions: <ul> <li>Partition 0 → replicas [B1, B2, B3], leader was B1</li> <li>Partition 1 → replicas [B1, B3, B2], leader was B1</li> <li>Partition 2 → replicas [B2, B1, B3], leader was B2</li> </ul> </li> <li>Since B1 has failed, partitions 0 and 1 need new leaders. B2 examines the replica lists: <ul> <li>For Partition 0, the next replica in the list after B1 is B2, so B2 becomes the new leader.</li> <li>For Partition 1, the next replica after B1 is B3, so B3 becomes the new leader.</li> <li>Partition 2 is unaffected because its leader, B2, is still active.</li> </ul> </li> <li>The controller then updates ZooKeeper with the new leadership information for partitions 0 and 1. It also notifies all brokers that host replicas of these partitions (B2 and B3 for Partition 0, B2 and B3 for Partition 1) about the changes.</li> <li>This ensures that producers and consumers can continue sending and receiving messages without disruption. By following this process, Kafka guarantees automatic failover, consistent leadership, and uninterrupted availability of all partitions in the cluster.</li> </ul> </li> </ul> <p><strong>A Quick Review:</strong></p> <ul> <li>Broker as leader: <ul> <li>Each partition in a topic has exactly one leader at a time.</li> <li>A broker can be the leader for multiple partitions, even across multiple topics. So if a broker handles multiple topics, it can simultaneously be the leader for several partitions of different topics.</li> <li>Followers replicate the leader’s data for fault tolerance, but only the leader handles reads/writes for that partition.</li> </ul> </li> <li>Consumers and partitions: <ul> <li>A consumer in a consumer group can read from one or more partitions, but each partition can be read by only one consumer in the same group at a time.</li> <li>For a single topic with multiple partitions, multiple consumers in the same group allow parallel consumption, but each partition is still assigned to only one consumer.</li> <li>If a consumer subscribes to multiple topics, it can consume from one or more partitions from each topic, depending on how partitions are assigned to it.</li> </ul> </li> <li>Example: <ul> <li>Topic A has 3 partitions: P0, P1, P2. Topic B has 2 partitions: Q0, Q1.</li> <li>Broker B1 might be leader for P0 and Q1; B2 leader for P1 and Q0; B3 leader for P2.</li> <li>Consumer C1 in a consumer group could be assigned P0 (topic A) and Q0 (topic B), while C2 gets P1 and Q1.</li> </ul> </li> <li>So, brokers can be leaders for multiple partitions across topics, and consumers can read from multiple partitions across topics, but within a consumer group, each partition is served by only one consumer at a time.</li> </ul> <p><strong>Replication</strong></p> <ul> <li>Replication is critical because it is the way Kafka guarantees availability and durability when individual nodes inevitably fail.</li> <li>As we’ve already discussed, data in Kafka is organized by topics. <ul> <li>Each topic is partitioned.</li> <li>Each partition can have multiple replicas.</li> </ul> </li> <li>Those replicas are stored on brokers, and each broker typically stores hundreds or even thousands of replicas belonging to diﬀerent topics and partitions.</li> <li>There are two types of replicas.</li> <li>Leader replica <ul> <li>Each partition has a single replica designated as the leader.</li> <li>All produce requests go through the leader to guarantee consistency.</li> <li>Clients can consume from either the leader replica or its followers.</li> </ul> </li> <li>Follower replica <ul> <li>All replicas for a partition that are not leaders are called followers.</li> <li>If a leader replica for a partition fails, one of the follower replicas will be promoted to become the new leader for the partition.</li> </ul> </li> <li>Kafka provides the following reliability guarantees: <ul> <li>Kafka provides order guarantee of messages in a partition.</li> <li>Produced messages are considered “committed” when they were written to the partition on all its in-sync replicas (but not necessarily flushed to disk). In-sync replicas are the leader and followers that lags within 10 seconds (configurable).</li> <li>Messages that are committed will not be lost as long as at least one replica remains alive.</li> <li>Consumers can only read messages that are committed.</li> </ul> </li> <li>Q: Can Kafka consumers read from any replica of a partition, or only from the leader?</li> <li>A: In Kafka, each partition has a single leader replica and zero or more follower replicas. By default, consumers read from the leader replica, which guarantees that they see all committed messages in order and avoids inconsistencies. Followers replicate the leader’s data and can serve as read sources only if the system is explicitly configured to allow reading from followers, for example, to balance read load. However, followers that are out-of-sync may not have all committed messages or may lag behind the leader, so reading from them can result in missing or stale data. Therefore, reading from the leader is the default and safest approach, ensuring correctness, ordering, and durability, while followers primarily provide redundancy and high availability in case of leader failure.</li> </ul> <p><strong>Request processing</strong></p> <ul> <li>A Kafka broker processes requests sent to the partition leaders from clients, partition replicas, and the controller.</li> <li>Clients always initiate connections and send requests, and the broker processes the requests and responds to them.</li> <li>All requests sent to the broker from a specific client will be processed in the order in which they were received This guarantee allows Kafka to behave as a message queue and provide ordering guarantees on the messages it stores.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-10-480.webp 480w,/assets/img/kafka-10-800.webp 800w,/assets/img/kafka-10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-10" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>How do clients know where to send the requests?</li> <li>Clients know where to send requests because each partition has a designated leader, and brokers maintain metadata about which broker is the leader for which partition. When a client wants to produce or consume messages, it first fetches the cluster metadata (from ZooKeeper in older versions or from the Kafka bootstrap servers). This metadata includes the list of brokers, topics, partitions, and the leader for each partition. Using this information, the client can send requests directly to the broker that is the leader for the partition it wants to read from or write to.</li> <li>This ensures that requests always reach the correct partition leader, maintaining message ordering and consistency guarantees, and reduces unnecessary network hops through other brokers.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-11-480.webp 480w,/assets/img/kafka-11-800.webp 800w,/assets/img/kafka-11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-11" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Clients can set an upper/lower bound on the amount of data the broker can return, as well as a timeout.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-12-480.webp 480w,/assets/img/kafka-12-800.webp 800w,/assets/img/kafka-12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-12" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Consumers only see messages that were replicated to in-sync replicas.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-13-480.webp 480w,/assets/img/kafka-13-800.webp 800w,/assets/img/kafka-13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-13" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Physical storage</strong></p> <ul> <li>The basic storage unit of Kafka is a partition replica.</li> <li>Partitions cannot be split between multiple brokers, and not even between multiple disks on the same broker.</li> <li>Therefore, the size of a partition is limited by the space available on a single mount point.</li> <li>Kafka 3.9 (November 2024) supports “tiered storage”. (see KIP-405) <ul> <li>The local tier: store the log segments on the local disks. (same as current)</li> <li>The remote tier: store the completed log segments on HDFS, S3, … (new)</li> </ul> </li> <li>When you create a topic, Kafka first decides how to allocate the partitions between brokers. <ul> <li>The default replication factor is 3.</li> <li>Users can add or remove replicas even after a topic exists.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-14-480.webp 480w,/assets/img/kafka-14-800.webp 800w,/assets/img/kafka-14-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-14" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Kafka does not keep data forever. It doesn’t even wait for all consumers to read a message before deleting it.</li> <li>Instead, the Kafka administrator configures a retention period for each topic. <ul> <li>Time-based: how long to store messages before deleting them.</li> <li>Space-based: how much data to store before older messages are purged.</li> </ul> </li> <li>Each partition is split into segments. By default, each segment contains either 1 GB of data or a week of data, whichever is smaller.</li> <li>As a Kafka broker is writing to a partition, if the segment limit is reached, it closes the file and starts a new one.</li> <li>Each segment is stored in a single data file, which stores Kafka messages and their oﬀsets.</li> <li>The format of the data on the disk is identical to the format of the messages that we send from the producer to the broker and later from the broker to the consumers (i.e., the wire protocol). <ul> <li>This allows Kafka to use a zero-copy method to send messages to clients. Kafka sends messages from the file (or more likely, the Linux filesystem cache) directly to the network channel without any intermediate buﬀers.</li> <li>It also avoids decompressing and recompressing messages that the producer already compressed.</li> </ul> </li> <li> <p>Note: Zero-copy is an optimization in which Kafka sends data from disk to the network without multiple memory copies between user space and kernel space. Normally, sending data involves copying from disk to kernel buffers, then to user-space buffers, and back to kernel network buffers before transmission. With zero-copy (using OS features like Linux sendfile), Kafka can send messages directly from the on-disk segment files or the Linux page cache to the network socket. This reduces CPU and memory usage, speeds up message delivery, and works efficiently because Kafka’s on-disk segment format matches the wire protocol, so no data transformation is needed. The wire protocol is the format and rules that define how data is encoded and transmitted over the network between clients and servers.</p> </li> <li>Kafka provides flexible message retrieval and retention mechanisms. Consumers can start reading messages from any offset, and to make this efficient, each partition maintains an offset-to-segment index, which maps offsets to their exact position within segment files.</li> <li>Additionally, Kafka maintains a timestamp-to-offset index to quickly locate messages by time, which is useful for Kafka Streams and certain failover scenarios.</li> <li>For retention, topics can be configured in three ways: delete, which removes messages older than a configured time; compact, which keeps only the latest value for each key; or delete and compact, combining compaction with time-based deletion.</li> <li>Note: Both compaction and deletion operate on the messages stored on the brokers for a given topic.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-15-480.webp 480w,/assets/img/kafka-15-800.webp 800w,/assets/img/kafka-15-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-15" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="kafka-producer-flow"><strong>Kafka Producer Flow</strong></h4> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-16-480.webp 480w,/assets/img/kafka-16-800.webp 800w,/assets/img/kafka-16-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-16" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>When a producer wants to send messages to a Kafka topic, it first connects to one of the bootstrap brokers configured in its setup. It then requests metadata from the broker, which includes the list of partitions for the topic, the leader broker for each partition, and the replicas for each partition. The producer caches this metadata so that it can send records directly to the correct leader broker for the target partition.</li> <li>If the leader changes due to a failover or if the cached metadata expires, the producer automatically refreshes the metadata by querying the brokers again. This ensures that the producer always knows which broker to contact for each partition and guarantees that messages are sent to the appropriate leader.</li> </ul> <h4 id="kafka-consumer---commits-and-oﬀsets"><strong>Kafka Consumer - Commits and oﬀsets</strong></h4> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-17-480.webp 480w,/assets/img/kafka-17-800.webp 800w,/assets/img/kafka-17-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-17" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>In Kafka, consumers track which messages they have read by maintaining offsets, which represent their current position in each partition. Whenever a consumer calls poll(), it retrieves all messages from the last committed offset up to the latest.</li> <li>Unlike traditional message queues, Kafka does not track individual acknowledgments for each message. Instead, a consumer commits offsets, which updates a special internal topic called __consumer_offsets. This commit records the last successfully processed message in a partition, implicitly marking all prior messages as processed. By default, Kafka consumers commit offsets automatically every few seconds, but this can be disabled to allow explicit commits, which helps prevent message loss or duplication during rebalancing events.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 9]]></summary></entry><entry><title type="html">Apache ZooKeeper</title><link href="https://monishver11.github.io/blog/2025/big-data-9-zookeeper/" rel="alternate" type="text/html" title="Apache ZooKeeper"/><published>2025-12-17T02:58:00+00:00</published><updated>2025-12-17T02:58:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-9-zookeeper</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-9-zookeeper/"><![CDATA[<h4 id="introduction"><strong>Introduction</strong></h4> <ul> <li>So far in this semester, we have been studying large-scale distributed data processing systems.</li> <li>Today is diﬀerent. We will study how to build general distributed applications using Hadoop’s distributed coordination service, ZooKeeper.</li> </ul> <h4 id="distributed-coordination"><strong>Distributed coordination</strong></h4> <ul> <li>What should an autonomic service provide? <ul> <li>Self-configuring: All configuration should happen with little or no intervention by the user.</li> <li>Self-healing: When a process, service, or node stops working, it should repair itself.</li> <li>Self-protecting: The service should continually assess its own health. Redundant, cooperating health checkers can health-check each other.</li> <li>Self-optimizing: The service should continually assess its own performance. Perform load-balancing operations or request resources to maintain a desired level of performance.</li> </ul> </li> <li>Why is building distributed applications hard? <ul> <li>The main reason is partial failure.</li> <li>When a message is sent across the network between two nodes and the network fails, the sender does not know whether the receiver got the message. <ul> <li>It may have gotten through before the network failed.</li> <li>Or it may not have gotten through at all.</li> <li>Or perhaps the network is just too slow, and the message is just delayed?</li> </ul> </li> <li>The only way that the sender can find out what happened is to reconnect to the receiver and ask it.</li> <li>This is partial failure: when we don’t even know if an operation failed.</li> <li>Distributed programming is fundamentally diﬀerent from local programming.</li> <li>Without fresh, accurate, and timely information, it’s very diﬃcult to identify the root cause of the failure in realtime. <ul> <li>Network failure.</li> <li>Process failure.</li> <li>Shared resource failure (e.g., data store).</li> <li>Physical machine failure.</li> <li>Virtual machine failure.</li> <li>Configuration errors.</li> </ul> </li> </ul> </li> <li>How can ZooKeeper help? <ul> <li>ZooKeeper can’t make partial failures go away: Partial failures are intrinsic to distributed systems.</li> <li>ZooKeeper does not hide partial failures, either: What ZooKeeper does is give you a set of tools to build distributed applications that can safely handle partial failures.</li> </ul> </li> </ul> <h4 id="zookeeper"><strong>ZooKeeper</strong></h4> <p><strong>Characteristics</strong></p> <ul> <li>ZooKeeper is simple: At its core, ZooKeeper is a stripped-down filesystem that exposes a few simple operations.</li> <li>ZooKeeper is expressive: The ZooKeeper primitives are a rich set of building blocks that can be used to build many systems.</li> <li>ZooKeeper is highly available: ZooKeeper runs on a collection of machines with no single point of failure.</li> <li>ZooKeeper facilitates loosely coupled interactions: Participants do not need to know about one another or exist simultaneously.</li> <li>ZooKeeper is a library: ZooKeeper has a vibrant open-source community and rich documentation.</li> </ul> <p><strong>Data model</strong></p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-1-480.webp 480w,/assets/img/zk-1-800.webp 800w,/assets/img/zk-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Znodes <ul> <li>ZooKeeper maintains a hierarchical tree of nodes called znodes.</li> <li>A znode stores data and has an associated access-control list and version number.</li> <li>The default size limit of the stored data is 1MB. This is because ZooKeeper is designed for coordination (which typically uses small datafiles), not high-volume data storage.</li> <li>Accessing the data associated with a znode is atomic. <ul> <li>A client reading the data will either receive the entire data or fail.</li> <li>A client writing the data will replace all the data with a znode or fail.</li> </ul> </li> <li>There is no such thing as a partial read/write.</li> <li>ZooKeeper does not support an append operation. These characteristics contrast with HDFS. HDFS is designed for high-volume data storage with streaming data access and provides an append operation.</li> <li>In ZooKeeper, the statement “no append operation” means that you cannot add data incrementally to a znode. When writing to a znode, the entire content must be replaced in a single operation; partial writes or incremental appends are not supported.</li> <li>Znodes are referenced by paths.</li> <li>A path is a slash-delimited Unicode string, similar to a filesystem path.</li> <li>Paths must be absolute, i.e., begin with a slash.</li> <li>All paths are canonical, i.e., each path has a single representation. Paths do not undergo resolution. You cannot use “.” or “..” as in a filesystem path.</li> <li>The path “/zookeeper” is reserved to store management information.</li> <li>There are two types of znodes. Ephemeral znode: It will be deleted by the ZooKeeper service when the client that created it disconnects, either explicitly or because the client terminates for whatever reason. And Persistent znode: It will not be deleted when the client disconnects. The type of a znode is set at creation time and may not be changed later.</li> <li>Ephemeral znodes <ul> <li>An ephemeral znode is automatically deleted by ZooKeeper when the creating client’s session ends.</li> <li>An ephemeral znode may not have children, not even ephemeral ones. If ephemeral nodes were allowed to have children, it would create complications when the parent is deleted, what happens to the children? To avoid this problem, ZooKeeper enforces that ephemeral znodes are always leaf nodes.</li> <li>Even though ephemeral nodes are tied to a client session, they are visible to all clients (subject to their access-control list policies).</li> <li>Ephemeral znodes are ideal for building applications that need to know when certain distributed resources are available.</li> </ul> </li> <li>Persistent znodes <ul> <li>A persistent znode is not tied to the client’s session.</li> <li>It is deleted only when explicitly deleted by a client. Not necessarily by the client that created it.</li> <li>A persistent node can have children, similar to a directory in a filesystem.</li> </ul> </li> </ul> </li> <li>Sequence numbers <ul> <li>A sequential znode is given a sequence number by ZooKeeper as a part of its name.</li> <li>If a znode is created with the sequential flag set, then the value of a monotonically increasing counter is appended to its name. The counter is maintained by its parent znode.</li> <li>Sequence numbers can be used to impose a global ordering on events in a distributed system and may be used by the client to infer the ordering.</li> <li>Example: if the client asks to create a sequential znode named “/a/b-”, the service may create a znode named “/a/b-3” and later another znode “/a/b-5”.</li> </ul> </li> <li>Watches <ul> <li>Watches allow clients to get notifications when a znode changes in some way.</li> <li>The client can set a watch with read operations on the ZooKeeper service. Watches are triggered by write operations on the ZooKeeper service.</li> <li>Watches are triggered only once. To receive multiple notifications, a client needs to reregister the watch.</li> <li>For example, a client can call exists on a znode with a watch; if the znode does not exist, the call returns false, but later, when another client creates that znode, the watch fires and the first client is notified.</li> <li>Watches are set on specific znodes, not on the entire ZooKeeper service. <ul> <li>You attach a watch to a znode when performing a read operation (exists, getData, or getChildren).</li> <li>The watch is only triggered by changes to that particular znode (or its children, in the case of getChildren).</li> </ul> </li> <li>A client can have multiple watches simultaneously, each on different znodes or different aspects of the same znode. <ul> <li>Each watch is one-time: once triggered, it must be reregistered if the client wants future notifications.</li> <li>So at any given time, a client can be watching many znodes, receiving multiple notifications as changes occur—but each individual watch only fires once.</li> </ul> </li> </ul> </li> </ul> <p><strong>Operations</strong></p> <ul> <li>Overview <ul> <li><code class="language-plaintext highlighter-rouge">create</code>: Create a znode (the parent znode must already exist).</li> <li><code class="language-plaintext highlighter-rouge">delete</code>: Delete a znode (the znode must not have any children).</li> <li><code class="language-plaintext highlighter-rouge">exists</code>: Tests whether a znode exists and retrieves its metadata.</li> <li><code class="language-plaintext highlighter-rouge">getACL, setACL</code>: Get/set the access-control list of a znode.</li> <li><code class="language-plaintext highlighter-rouge">getChildren</code>: Get a list of the children of a znode.</li> <li><code class="language-plaintext highlighter-rouge">getData, setData</code>: Get/set the data associated with a znode.</li> <li><code class="language-plaintext highlighter-rouge">sync</code>: Synchronize a client’s view of a znode with ZooKeeper. <ul> <li>It ensures that any read operation after sync reflects the most up-to-date data for that znode, even in a distributed system with multiple clients and servers.</li> <li>Essentially, it forces the client to catch up with all recent updates that may have been applied by other clients or nodes, providing a consistent and current view of the znode’s data.</li> <li><code class="language-plaintext highlighter-rouge">sync</code> is session-wide, not per-znode, but it’s usually used to ensure freshness before reading a specific znode.</li> </ul> </li> </ul> </li> <li>Version number <ul> <li>The exists operation returns the version number in the znode’s metadata.</li> <li>Update operations (delete, setData) in ZooKeeper are conditional and nonblocking. <ul> <li>The client has to specify the version number of the znode that is being updated.</li> <li>If the version number does not match, the update will fail. It means that another process updated the znode in the meantime.</li> <li>This is an optimistic concurrency mechanism that allows clients to detect conflicts over znode modification without locking.</li> <li>The version check can be bypassed by using a version number of -1.</li> </ul> </li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-2-480.webp 480w,/assets/img/zk-2-800.webp 800w,/assets/img/zk-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Diﬀerence from filesystems <ul> <li>Although ZooKeeper can be viewed as a filesystem, it does away with some filesystem primitives for simplicity.</li> <li>In ZooKeeper, files are small and are written and read in their entirety. <ul> <li>There is no need to provide open, close, or seek operations.</li> <li>Therefore, ZooKeeper does not use handles to access znodes.</li> <li>Each operation includes the full path of the znode being operated on.</li> </ul> </li> <li>The sync operation is also diﬀerent from fsync in a filesystem.</li> <li>Writes in ZooKeeper are atomic and does not need to be synced.</li> <li>The sync operation is there to allow a client to catch up with the latest state.</li> <li>Q: What does it mean when ZooKeeper says “writes are atomic and do not need to be synced,” even though it runs on many machines?</li> <li>A: In ZooKeeper, a write operation (such as create, delete, or setData) is atomic, meaning it either fully succeeds or fully fails—there is no partial update that clients can observe. Although ZooKeeper is a distributed system running on multiple machines, it uses a consensus protocol (ZAB) to ensure that every write is agreed upon, replicated to a majority of servers, and committed in the same order before the client receives a success response. Because durability and ordering are already guaranteed internally by this protocol, the client does not need to call a filesystem-style fsync to force data to disk. ZooKeeper’s sync operation is therefore not about making writes durable; instead, it allows a client to catch up and ensure its read view reflects the latest committed state.</li> </ul> </li> <li>Multiupdate <ul> <li>ZooKeeper also provides the multi operation that batches together multiple primitive operations into a single unit that either succeeds or fails in its entirety.</li> <li>Multiupdate is useful for building structures that maintain some global invariant.</li> <li>Example: an undirected graph. <ul> <li>Each vertex in the graph is represented as a znode.</li> <li>We need to update two znodes to add or remove an edge.</li> <li>Batching the updates on the two znodes into one multi operation ensures that the update is atomic.</li> </ul> </li> </ul> </li> <li>Synchronous and asynchronous API <ul> <li>ZooKeeper provides both synchronous and asynchronous APIs for all operations to suit different programming needs. In the synchronous API, a call (such as exists) blocks until the operation completes and directly returns a result, typically a Stat object or null, or throws an exception on failure. In contrast, the asynchronous API is non-blocking: the client issues the request and continues execution, and ZooKeeper later delivers the result via a callback method (processResult), which includes the return code, path, context, and result data. This allows applications to handle ZooKeeper operations efficiently without blocking threads, which is especially useful in high-concurrency or event-driven systems.</li> <li>A callback is a function (or method) that you provide to an API so it can be invoked later when an asynchronous operation completes. Instead of blocking and waiting for a result, the program continues running, and when the operation finishes, the system “calls back” your function with the result (success, failure, or data). In ZooKeeper, callbacks are used in asynchronous APIs to deliver operation results—such as status codes and metadata—once the server has processed the request.</li> </ul> </li> <li>Watch triggers <ul> <li>Read operations (exists, getChildren, getData) may have watches set on them.</li> <li>The watches are triggered by write operations (create, delete, setData).</li> <li>Operations on the access-control list (getACL, setACL) do not participate in watches.</li> <li>When a watch is triggered, a watch event is generated, which includes the path of the znode that wa. s involved in the event. <ul> <li>The watch event does not provide the changed data itself.</li> <li>To discover the data, the client needs to reissue a read operation.</li> </ul> </li> <li>The watch event’s type depends both on the watch and the operation that triggered it. <ul> <li>A watch set on an exists operation will be triggered when the znode being watched is created, deleted, or has its data updated.</li> <li>A watch set on a getData operation will be triggered when the znode being watched is deleted or has its data updated.</li> <li>A watch set on a getChildren operation will be triggered when a child of the znode being watched is created or deleted, or when the znode itself is deleted.</li> </ul> </li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-3-480.webp 480w,/assets/img/zk-3-800.webp 800w,/assets/img/zk-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Access-control lists <ul> <li>A znode can be created with an access-control list, which determines who can perform certain operations on it.</li> <li>The client identifies itself to ZooKeeper using an authentication scheme.</li> <li>Employing authentication and access-control lists is optional.</li> </ul> </li> </ul> <p><strong>Implementation</strong></p> <ul> <li>Ensemble <ul> <li>ZooKeeper runs on a cluster of machines called an ensemble.</li> <li>ZooKeeper achieves high availability through replication, and can provide a service as long as a majority of the machines in the ensemble are live.</li> <li>Example: a 5-node ensemble can tolerate at most 2 node failures. It is usual to have an odd number of machines in an ensemble.</li> <li>Note: it is critical that ZooKeeper can perform its functions in a timely manner. Therefore, ZooKeeper should run on machines that are dedicated to ZooKeeper alone. Having other applications contend for resources can cause ZooKeeper’s performance to degrade significantly.</li> <li>Conceptually, ZooKeeper is very simple.</li> <li>All it has to do is ensure that every modification to the tree of znodes is replicated to a majority of the ensemble. <ul> <li>If a minority of the machines fail, then at least one machine will survive with the latest state.</li> <li>The other remaining replicas will eventually catch up with this state.</li> </ul> </li> </ul> </li> <li>Zab <ul> <li>ZooKeeper uses a protocol called Zab that runs in two phases, which may be repeated indefinitely.</li> <li>Phase 1: Leader election: <ul> <li>The machines in an ensemble go through a process of electing a distinguished member called the leader.</li> <li>The other machines are called followers.</li> <li>This phase is finished once a majority of followers (called “quorum”) have synchronized their state with the leader.</li> </ul> </li> <li>Phase 2: Atomic broadcast: <ul> <li>All write requests are forwarded to the leader.</li> <li>The leader then broadcasts the update to the followers.</li> <li>When a majority have persisted the change, the leader commits the update, and the client gets a response saying the update succeeded.</li> <li>The protocol for achieving consensus is designed to be atomic, so a change either succeeds or fails. It resembles a two-phase commit protocol (2PC).</li> </ul> </li> </ul> </li> <li>What if the leader fails? <ul> <li>If the leader fails, the remaining machines hold another leader election and continue as before with the new leader.</li> <li>If the old leader later recovers, it then starts as a follower.</li> <li>Leader election is very fast (~200ms), so performance does not noticeably degrade during an election.</li> </ul> </li> <li>Replicated database <ul> <li>Each zookeeper replica maintains an in-memory database containing the entire znode tree.</li> <li>Writes: All machines in the ensemble first write updates to disk, and then update their in-memory copies of the znode tree.</li> <li>Reads: Read requests may be serviced from any machine. Read requests are very fast because they involve only a lookup from memory.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-4-480.webp 480w,/assets/img/zk-4-800.webp 800w,/assets/img/zk-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Consistency</strong></p> <ul> <li>Understanding consistency <ul> <li>In ZooKeeper, a follower may lag the leader by a number of updates.</li> <li>This is because only a majority and not all members of the ensemble need to have persisted a change before it is committed.</li> <li>A client has no control whether it is connected to a leader or a follower, and cannot even know this.</li> </ul> </li> <li>ZooKeeper transaction ID <ul> <li>Every update made to the znode tree is given a globally unique identifier, called a zxid (i.e., ZooKeeper transaction ID).</li> <li>In ZooKeeper, updates are totally ordered. If zxid is less than , then happened before according to ZooKeeper.</li> </ul> </li> <li>Consistency guarantees <ul> <li>Guarantee 1: Sequential consistency <ul> <li>Updates from any particular client are applied in the order that they are sent. If a client updates the znode <code class="language-plaintext highlighter-rouge">z</code> to the value <code class="language-plaintext highlighter-rouge">a</code>, and in a later operation, it <code class="language-plaintext highlighter-rouge">z</code> updates to the value <code class="language-plaintext highlighter-rouge">b</code>, then no client will ever see <code class="language-plaintext highlighter-rouge">z</code> with value <code class="language-plaintext highlighter-rouge">a</code> after it has seen it with value b (if no other updates are made to <code class="language-plaintext highlighter-rouge">z</code>).</li> </ul> </li> <li>Guarantee 2: Atomicity <ul> <li>Updates either succeed or fail. If an update fails, no client will ever see it.</li> </ul> </li> <li>Guarantee 3: Single system image <ul> <li>A client will see the same view of the system, regardless of the server it connects to.</li> <li>If a client connects to a new server during the same session, it will not see an older state of the system than the one it saw with the previous server.</li> <li>When a server fails and a client tries to connect to another in the ensemble, a server that is behind the one that failed will not accept connections from the client until it has caught up with the failed server.</li> </ul> </li> <li>Guarantee 4: Durability <ul> <li>Once an update has succeeded, it will persist and will not be undone.</li> <li>Update will survive server failures.</li> </ul> </li> <li>Guarantee 5: Timeliness <ul> <li>The lag in any client’s view of the system is bounded, so it will not be out of date by more than some multiple of tens of seconds.</li> <li>Rather than allow a client to see data that is very stale, a server will shut down, forcing the client to switch to a more up-to-date server.</li> </ul> </li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-5-480.webp 480w,/assets/img/zk-5-800.webp 800w,/assets/img/zk-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>(In-)consistency across clients <ul> <li>For performance reasons, reads are satisfied from a ZooKeeper server’s memory and do not participate in the global ordering of writes.</li> <li>ZooKeeper does not provide simultaneously consistent cross-client views. <ul> <li>It is possible for two clients to observe updates at diﬀerent times.</li> <li>If two clients communicate outside ZooKeeper (“hidden channel”), the diﬀerence becomes apparent.</li> </ul> </li> <li>If a client need to catch up with the latest state, the sync operation forces the ZooKeeper server to which the client is connected to catch up with the leader.</li> <li>Gist: This means that ZooKeeper optimizes reads by serving them locally, without coordinating them through the global consensus protocol that orders writes. Concretely, when a client issues a read, the ZooKeeper server it is connected to simply returns the value from its in-memory state, instead of synchronizing with the leader or other servers. Because of this, reads are not totally ordered with respect to writes across the entire ensemble. As a result, two clients connected to different servers may temporarily see different versions of the same znode, even though all writes themselves are globally ordered and atomic. This design trades strict read consistency for low latency and high throughput, and clients that need the most up-to-date view can explicitly call sync to force their server to catch up with the leader before reading.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-6-480.webp 480w,/assets/img/zk-6-800.webp 800w,/assets/img/zk-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Sessions</strong></p> <ul> <li>Client startup <ul> <li>A ZooKeeper client is configured with the list of servers in the ensemble.</li> <li>On startup, it tries to connect to one of the servers in the list.</li> <li>If the connection fails, it tries another server in the list, and so on, until it either successfully connects to one of them or fails because all ZooKeeper servers are unavailable.</li> <li>Once a connection has been made with a ZooKeeper server, the server creates a new session for the client.</li> <li>Q: Can a ZooKeeper server handle multiple client sessions simultaneously?</li> <li>A: Yes, a ZooKeeper server can manage multiple client sessions at the same time. Each connected client is assigned a unique session that tracks ephemeral znodes, watches, and session timeouts. The server maintains the session state for all clients in memory, allowing it to serve many clients concurrently. If a client disconnects temporarily, its session can remain active for the configured timeout period, enabling the client to reconnect without losing ephemeral nodes or watches. Thus, a single ZooKeeper server is designed to handle multiple client sessions efficiently, while the ensemble as a whole provides scalability and fault tolerance.</li> </ul> </li> <li>Session timeout <ul> <li>In ZooKeeper, the client specifies the session timeout when connecting to the ensemble by sending a requested timeout value during the handshake. The server may adjust this value based on its configuration and load, but generally tries to honor the client’s request within allowed limits. This timeout determines how long the server will keep the session active without receiving any requests or heartbeats from the client. Once the timeout expires, the session is terminated, it may not be reopened and any ephemeral znodes created under it are automatically deleted.</li> </ul> </li> <li>Heartbeats <ul> <li>In ZooKeeper, each client session has a timeout. To prevent the server from thinking the client is dead, the client periodically sends heartbeats—small messages that indicate it is still alive—even when no other requests are being made. The interval between heartbeats is set short enough so that: <ul> <li>If a server fails, the client detects it quickly (through a read timeout).</li> <li>The client can reconnect to another server in the ensemble before the session timeout expires, keeping the session active.</li> </ul> </li> <li>Without heartbeats, the server might expire the session, deleting any ephemeral znodes tied to it. So heartbeats are essential to maintain session liveness during idle periods.</li> <li>And although the session is tracked by the server, ZooKeeper’s design allows the session state to be transferred to another server: when the client connects to a new server, the ensemble ensures that the new server resumes the same session with all associated ephemeral znodes and watches intact. This mechanism allows the client to maintain a consistent session across server failures or reboots, keeping ephemeral nodes alive as long as the session timeout is not exceeded.</li> </ul> </li> <li>Failover <ul> <li>Failover to another ZooKeeper server is handled automatically by the ZooKeeper client.</li> <li>Sessions (and associated ephemeral znodes) are still valid after another server takes over from the failed one.</li> <li>During failover, the application will receive notifications of disconnections and connections to the service. <ul> <li>Watch notifications will not be delivered while the client is disconnected, but they will be delivered when the client successfully reconnects.</li> <li>If the application tries to perform an operation while the client is reconnecting to another server, the operation will fail.</li> </ul> </li> </ul> </li> <li>Time <ul> <li>There are several time parameters in ZooKeeper.</li> <li>The tick time is the fundamental period of time in ZooKeeper and is used by servers in the ensemble to define the schedule on which their interactions run. Other settings are defined in terms of tick time or constrained by it.</li> <li>A common tick time setting is 2 seconds.</li> <li>Example: the session timeout can only be configured between 2~20 ticks (i.e., 4~40 seconds).</li> <li>The tick time in ZooKeeper is configurable, not fixed. Administrators can tune tick time based on the deployment’s latency and performance requirements.</li> </ul> </li> <li>States <ul> <li>In ZooKeeper, a client’s ZooKeeper object goes through different states during its lifecycle, such as connecting, connected, disconnected, or expired. You can query the current state at any time using the getState() method.</li> <li>Additionally, clients can register a Watcher to receive notifications whenever the state changes, allowing the application to respond to events like connection loss, reconnection, or session expiration.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-7-480.webp 480w,/assets/img/zk-7-800.webp 800w,/assets/img/zk-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="building-applications-with-zookeeper"><strong>Building applications with ZooKeeper</strong></h4> <p><strong>Group membership</strong></p> <ul> <li>ZooKeeper can be used to implement group membership for distributed services, allowing clients to locate active servers reliably. To maintain the membership list without a single point of failure, ZooKeeper uses znodes: a parent znode represents the group, and child znodes represent individual members. To create a group, a znode (e.g., /zoo) is created for the group itself. Servers join the group by creating ephemeral child znodes, which automatically disappear if the server fails or exits, ensuring the membership list reflects the current active members. Clients can list all members using the getChildren() method on the parent znode. Deleting a group requires first removing all child znodes, since ZooKeeper does not support recursive deletion; the delete() method is used, and specifying -1 for the version bypasses version checks. This approach provides an active, fault-tolerant membership list that dynamically updates as servers join or leave.</li> </ul> <p><strong>Configuration service</strong></p> <ul> <li>ZooKeeper can serve as a highly available configuration service for distributed applications, enabling machines in a cluster to share common configuration data. At a basic level, it stores configuration as key-value pairs, where the keys are represented by znode paths and the values are strings. Clients can retrieve or update this data, and using watches, interested clients can be notified automatically when configuration changes occur, creating an active configuration service. This model often assumes a single client performs updates at a time—for example, a master node like the HDFS NameNode updating information that worker nodes need—ensuring consistent and up-to-date configuration across the cluster.</li> </ul> <p><strong>Lock service</strong></p> <ul> <li>ZooKeeper provides a robust distributed lock service, which is essential for coordinating access to shared resources in a distributed system. A distributed lock ensures mutual exclusion, meaning that at any given time, only one client can hold the lock. To implement this, a lock znode is designated, for example /leader, representing the lock on a resource. Clients that wish to acquire the lock create ephemeral sequential child znodes under this parent znode. ZooKeeper assigns a unique, monotonically increasing sequence number to each child znode, providing a total ordering among clients. The client whose znode has the lowest sequence number is considered the current lock holder. Ephemeral znodes ensure automatic cleanup in case a client crashes, releasing the lock for the next contender without manual intervention.</li> <li>When multiple clients contend for the lock, efficient notification is crucial to avoid overloading the ZooKeeper ensemble. In a naïve approach, all clients watch the lock znode for changes in its children. However, this leads to the herd effect, where thousands of clients are notified of changes even though only one can acquire the lock at a time, creating traffic spikes and performance bottlenecks. ZooKeeper avoids this by having each waiting client set a watch only on the znode immediately preceding its own in sequence (i.e., the znode with the previous sequence number). For example, if the lock znodes are /leader/lock-1, /leader/lock-2, and /leader/lock-3, the client for lock-3 watches only lock-2. When lock-2 is deleted, the client for lock-3 is notified and can acquire the lock. This selective notification ensures that only the next contender is alerted, reducing unnecessary traffic and preventing the herd effect while maintaining correct lock ordering.</li> <li>The lock acquisition process works as follows: a client creates its ephemeral sequential znode, retrieves the list of children under the lock znode, and checks if its znode has the lowest sequence number. If it does, the lock is acquired. If not, the client sets a watch on the znode immediately preceding its own and waits. When that znode is deleted, the client is notified, repeats the check, and acquires the lock if it now has the lowest sequence number. This process guarantees deterministic ordering among clients while maintaining fairness in lock acquisition.</li> <li>Lock release and failover are automatic and reliable due to ephemeral znodes. When the lock-holding client deletes its znode—or if the client crashes, causing ZooKeeper to remove the ephemeral znode—the next client in sequence is notified and acquires the lock. This mechanism ensures automatic failover without manual intervention. The combination of sequential numbering and ephemeral znodes allows the system to handle client crashes gracefully while maintaining strict mutual exclusion.</li> <li>Overall, ZooKeeper’s distributed lock service leverages ephemeral sequential znodes, watches on immediate predecessors, and deterministic ordering to provide a scalable, fault-tolerant, and efficient locking mechanism. It avoids unnecessary notifications, supports automatic failover, and guarantees that locks are acquired fairly among contending clients. This design makes it ideal for building higher-level coordination services, such as leader election, configuration management, and other distributed synchronization tasks in large-scale systems.</li> </ul> <hr/> <p><strong>Doubts:</strong></p> <ul> <li>Q: How is a leader elected in ZooKeeper?</li> <li> <p>A: ZooKeeper elects a leader to coordinate write operations and maintain consistency across the ensemble. When servers start, each enters the LOOKING state and proposes a leader candidate, typically based on the highest transaction ID (zxid) or server ID. Servers exchange votes, and a candidate becomes leader once it receives a majority of votes. The elected leader transitions to the LEADING state, while others become FOLLOWERS. If the leader fails, the remaining servers automatically re-enter the election process. This election uses the Zab (ZooKeeper Atomic Broadcast) protocol to ensure the chosen leader is consistent and preserves all committed transactions.</p> </li> <li>Q: Why is there no full consensus needed in ZooKeeper’s ephemeral sequential znode leader election?</li> <li>A: In ephemeral sequential znode leader election, a full consensus protocol is not needed because ZooKeeper itself guarantees atomic and ordered creation of sequential znodes. Each server creates a znode with a unique increasing sequence number, and the leader is deterministically chosen as the server with the smallest sequence number. Since all servers can independently see the same sequence order, there are no conflicting proposals and no ambiguity in leader selection. Consensus protocols like Zab are still required for replicating write transactions, but leader election leverages ZooKeeper’s built-in ordering guarantees, making the process coordination-free and deterministic.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 8]]></summary></entry><entry><title type="html">Apache HBase</title><link href="https://monishver11.github.io/blog/2025/big-data-8-hbase/" rel="alternate" type="text/html" title="Apache HBase"/><published>2025-12-16T20:36:00+00:00</published><updated>2025-12-16T20:36:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-8-hbase</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-8-hbase/"><![CDATA[<h4 id="oltp-online-transactional-processing"><strong>OLTP (online transactional processing)</strong></h4> <ul> <li>OLTP enables the real-time execution of large numbers of database transactions by large numbers of people, typically over the internet.</li> <li>A database transaction is a change, insertion, deletion, or query of data in a database.</li> <li>In OLTP, the common, defining characteristic of any database transaction is its atomicity—a transaction either succeeds as a whole or fails (or is canceled). It cannot remain in a pending or intermediate state.</li> <li>Characteristics of OLTP systems <ul> <li>Process a large number of relatively simple transactions.</li> <li>Enable multi-user access to the same data, while ensuring data integrity.</li> <li>Emphasize very rapid processing, with response times measured in milliseconds.</li> <li>Provide indexed datasets.</li> <li>Are available 24/7/365.</li> </ul> </li> <li>OLTP versus OLAP</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbase-1-480.webp 480w,/assets/img/hbase-1-800.webp 800w,/assets/img/hbase-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hbase-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hbase-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <h4 id="nosql-not-only-sql-databases"><strong>NoSQL (Not only SQL) databases</strong></h4> <ul> <li>NoSQL refers to Not only SQL databases.</li> <li>They are non-relational databases for storing huge datasets eﬀectively.</li> <li>They are good for: <ul> <li>Indexing huge amount of documents.</li> <li>Serving pages on high-traﬃc websites.</li> <li>Delivering streaming media.</li> </ul> </li> <li>Consistency is less important. ACID properties are traded for performance.</li> <li>NoSQL refers to a range of databases that are not relational databases. <ul> <li>They do not support SQL.</li> <li>They often do not guarantee ACID properties.</li> </ul> </li> <li>Many NoSQL databases are descendants of Google’s Bigtable and Amazon’s Dynamo. <ul> <li>They are designed to be distributed across many nodes.</li> <li>They usually provide eventual consistency.</li> <li>They have very flexible schema.</li> </ul> </li> <li>NoSQL databases do not use SQL for data manipulation. <ul> <li>The database is optimized for retrieval and append operations.</li> <li>They oﬀer a key-value store. Some are built on GFS/HDFS.</li> </ul> </li> <li>NoSQL databases are designed for scalability and performance. <ul> <li>They are useful for big data in applications where a relational model is not needed.</li> <li>They can easily scale up by adding inexpensive commodity servers. Much easier than with relational databases.</li> </ul> </li> <li>NoSQL database systems are developed to manage large volumes of data that do not necessarily follow a fixed schema. <ul> <li>Data is sharded and stored across many servers.</li> <li>The architecture is distributed and fault-tolerant.</li> <li>They are useful for managing large amounts of data where satisfying realtime constraints is the priority. The goal is near-realtime or soft realtime, i.e., fast enough for a web service.</li> </ul> </li> <li>Q: Do NoSQL databases relax consistency but keep atomicity?</li> <li>A: In most NoSQL databases, atomicity is preserved but only at a limited scope, such as a single row, document, or key. Consistency across multiple records is often relaxed, and isolation guarantees are weaker, while durability is usually maintained. This trade-off is intentional, allowing NoSQL systems to achieve high scalability and performance in distributed environments.</li> <li>Google’s solution for NoSQL <ul> <li>Bigtable: a distributed storage system for structured data</li> <li>Best Paper award at OSDI’06 (one of the two most prestigious systems conferences held once every two years).</li> <li>As a side note, Google also published Chubby in the same conference.</li> <li>Today, Bigtable is still one of the most widely-used NoSQL databases. Google oﬀers Cloud Bigtable as part of its cloud computing services.</li> </ul> </li> <li>Hadoop’s solution for NoSQL <ul> <li>HBase is modeled after Google’s Bigtable.</li> <li>Bigtable is closed-source; HBase is open-source.</li> <li>It is suitable for extremely large databases. Billions of rows, millions of columns.</li> <li>It is distributed across thousands of nodes.</li> <li>Facebook used HBase for its messaging system from 2010 to 2018.</li> </ul> </li> </ul> <h4 id="hbase"><strong>HBase</strong></h4> <ul> <li><strong>Use cases</strong> <ul> <li>Facebook messages: At the high end: over one million HBase cluster operations per second. Processing thousands of records per second per node.</li> <li>Large amount of stored data: Queries only require small amount of rows in response.</li> <li>Use HBase for random reading or writing, or both: When your data is in the TB range.</li> </ul> </li> <li><strong>NoJOIN?</strong> <ul> <li>Traditional JOIN operations are not supported in NoSQL.</li> <li>Implementing JOIN is impractical. <ul> <li>In HBase, data is sharded across many servers.</li> <li>HBase wants to provide fast response.</li> </ul> </li> <li>But in HBase, the capability exists for very, very large rows (millions of columns). <ul> <li>JOINs are not needed.</li> <li>The recommendation is that data should be de-normalized.</li> </ul> </li> <li>Conceptual Flow: <ul> <li>HBase does not support traditional SQL JOIN operations because implementing joins in a distributed NoSQL system is impractical and expensive. In HBase, data is sharded across many servers, so performing a join would require large amounts of network communication and coordination, which would significantly increase latency. Since HBase is designed for fast, low-latency access, joins would violate its performance goals. Instead, HBase supports very wide rows with potentially millions of columns, allowing related data to be stored together in a single row. Because of this design, joins are usually unnecessary, and the recommended approach is to denormalize the data, storing all frequently accessed related data together to enable fast reads.</li> <li>In HBase, data is sharded (partitioned) by row key, not by columns. Each row can be extremely wide (even millions of columns), and all columns for a given row are stored together. HBase uses the row key to determine which region (and therefore which server/node) holds that row.</li> <li>So when a request comes in, HBase routes it directly to the node responsible for that row key, allowing fast reads and writes without needing joins. This row-based sharding, combined with wide rows and denormalized data, is what enables HBase to scale horizontally while maintaining low-latency access.</li> </ul> </li> </ul> </li> <li><strong>Sparse rows</strong> <ul> <li>HBase is a wide-column store.</li> <li>HBase can handle sparse records. <ul> <li>Sparse records means that some columns are not filled in.</li> </ul> </li> <li>In HBase, there is no penalty for sparse data because no space is allocated. This is in contrast to relational databases in which an unpopulated field is also allocated space.</li> </ul> </li> <li><strong>Designing tables</strong> <ul> <li>The RDBMS approach to design is relationship-centric.</li> <li>However, HBase requires an access-centric approach to design.</li> <li>We cannot take an RDBMS and model it directly in HBase. <ul> <li>RDBMS will have normalized data. However, HBase will perform better with de-normalized data.</li> <li>HBase columns must be grouped into column families based on expected access patterns.</li> <li>The columns of a column family are stored close together on disk for fast access.</li> </ul> </li> <li>We’ll see more on this in HBase’s Data Model (see below).</li> </ul> </li> <li><strong>NoACID?</strong> <ul> <li>HBase is not a fully ACID-compliant database. <ul> <li>HBase does not provide strong consistency.</li> <li>It does not provide atomicity across rows.</li> <li>However, it does provide atomic operations at the row level.</li> </ul> </li> </ul> </li> <li><strong>Storage platform</strong> <ul> <li>HBase uses HDFS underneath for storage.</li> <li>HBase does not convert commands to MapReduce jobs.</li> <li>HBase supports random reads and writes. <ul> <li>Writes are implemented through versioning of cells.</li> <li>The user can define the max number of versions to maintain.</li> <li>The user can also define the time-to-live. After that time, the row is automatically marked for deletion by HBase.</li> </ul> </li> </ul> </li> </ul> <h4 id="hbase-architecture"><strong>HBase Architecture</strong></h4> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbase-2-480.webp 480w,/assets/img/hbase-2-800.webp 800w,/assets/img/hbase-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hbase-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hbase-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>An HBase cluster is comprised of master and worker nodes.</li> <li>Similar master-worker architecture as ween with… <ul> <li>HDFS: NameNode and DataNodes.</li> <li>YARN: ResourceManager and NodeManagers.</li> <li>MapReduce: ApplicationMaster and tasks.</li> <li>Trino: Coordinator and workers.</li> </ul> </li> <li><strong>Master node</strong> <ul> <li>The master node manages a cluster of Regionservers.</li> <li>It bootstraps the initial install.</li> <li>It assigns regions to registered Regionservers.</li> <li>It recovers Regionserver failures.</li> </ul> </li> <li>The master node is lightly loaded. <ul> <li>Client data does not move through the master node.</li> <li>Clients do not rely on the master node for region location information.</li> </ul> </li> <li><strong>Regionservers</strong> <ul> <li>Each Regionserver carries zero or more regions. <ul> <li>A region is a subset of a table’s rows.</li> <li>Row updates are atomic.</li> </ul> </li> <li>They handle read/write requests.</li> <li>They perform region splits, which they communicate to the master node. <ul> <li>Writes cause regions to grow. Eventually, they must be split.</li> </ul> </li> </ul> </li> <li><strong>ZooKeeper cluster</strong> <ul> <li>ZooKeeper is a distributed coordination service (e.g., configuration, synchronization, naming registry).</li> <li>HBase uses ZooKeeper to host vitals such as: <ul> <li>The location of the hbase:meta catalog table.</li> <li>The address of the current cluster master.</li> </ul> </li> <li>HBase also uses ZooKeeper to host the transaction state of region assignments. This is to support fast recovery.</li> <li>More clearly: In HBase, the transaction state of region assignments stored in ZooKeeper represents the current and in-progress states of assigning regions to RegionServers, such as unassigned, assigning, or assigned. ZooKeeper maintains this shared state so the HBase master and RegionServers have a consistent view of region ownership. If a master or RegionServer fails during an assignment, the new master can read this state from ZooKeeper and safely resume or recover the operation, ensuring that each region is assigned to exactly one RegionServer at a time and enabling fast, reliable cluster recovery.</li> <li>Fresh clients connect to the ZooKeeper to learn the location of hbase:meta. The result is cached at the clients until there is a fault.</li> </ul> </li> <li><strong>The hbase:meta catalog table</strong> <ul> <li>It maintains the current list, state, and locations of all user-space regions afloat on the cluster.</li> <li>Entries in hbase:meta are keyed by region name, which is made up of: <ul> <li>The table name.</li> <li>The start row key.</li> <li>The creation time.</li> <li>A checksum.</li> </ul> </li> <li>Conceptual Flow: <ul> <li>The hbase:meta catalog table is a special system table in HBase that acts as the central directory for all user-space regions in the cluster. It keeps track of the current list of regions, their states (e.g., online, offline, splitting), and the RegionServers that host them. Each entry in hbase:meta is keyed by a region name, which is a combination of the table name, the region’s start row key, the creation timestamp, and a checksum to ensure uniqueness.</li> <li>This catalog table is essential for HBase’s operation: when a client wants to read or write data, it queries hbase:meta (directly or via ZooKeeper) to determine which RegionServer holds the desired row. Similarly, the HBase master uses it to manage region assignments, splits, and recovery. By maintaining an up-to-date map of the cluster’s regions, hbase:meta enables fast lookups, load balancing, and fault-tolerant access to data in a distributed environment.</li> </ul> </li> </ul> </li> <li><strong>Cluster expansion</strong> <ul> <li>Expanding an HBase cluster is much easier than scaling a traditional relational database.</li> <li>In RDBMS systems, expansion is often complex, error-prone, and difficult to maintain, because horizontal scaling was not part of their original design. Features like joins, complex queries, and strict ACID guarantees make distributing data across multiple nodes challenging.</li> <li>In contrast, HBase is designed for horizontal scalability, allowing nodes to be added with minimal disruption. Its row-based sharding, denormalized data models, and simplified consistency guarantees make it straightforward to expand the cluster and handle growing datasets efficiently.</li> </ul> </li> </ul> <h4 id="hbase-data-model"><strong>HBase Data model</strong></h4> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbase-3-480.webp 480w,/assets/img/hbase-3-800.webp 800w,/assets/img/hbase-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hbase-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hbase-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li><strong>Column family</strong> <ul> <li>HBase is a distributed column-family-oriented database built on top of HDFS.</li> <li>A column family is a grouping of columns. <ul> <li>Columns that belong to a given column family are physically stored together.</li> <li>A column is referenced as columnFamilyName:columnName. The columnName is sometimes referred to as the qualifier.</li> <li>In HBase, tuning is performed on a column family bases.</li> </ul> </li> <li>HBase column families are defined at table definition time, but columns can be defined dynamically.</li> <li>Conceptual Flow: <ul> <li>In HBase, a column family is a grouping of columns whose data is physically stored together in HDFS files called HFiles, allowing efficient access to all columns in the family.</li> <li>Columns are referenced as columnFamily:columnName, where columnName is also called the qualifier. Column families are defined at table creation, but individual columns within a family can be added dynamically.</li> <li>All regions of a table share the same column family structure, even though many rows may be sparse and lack values for some columns; empty cells are simply not stored.</li> <li>HFiles are stored on HDFS and subject to the HDFS block size (typically 128 MB); if a column family’s data exceeds a block, it is automatically split across multiple blocks distributed across different data nodes. HBase manages reading and writing from these HFiles, including handling multiple blocks, region splits, and region-to-node assignments, using metadata and indexing to ensure fast, atomic access at the row level.</li> <li>This design supports very wide tables, sparse and denormalized data, and scalable, high-throughput access while keeping related columns physically close for efficient reads.</li> </ul> </li> </ul> </li> <li><strong>Cells</strong> <ul> <li>Values are stored in HBase cells.</li> <li>A cell lies at the intersection of a row and a column, at a particular version.</li> <li>By default, a cell version is a timestamp. The timestamp is automatically assigned by HBase.</li> <li>To reference a value in a cell, use: row key + columnFamily:columnName + timestamp.</li> <li>A cell’s content is an uninterpreted array of bytes.</li> </ul> </li> <li>Regions <ul> <li>A region (“tablet” in Google’s jargon) is a subset of an HBase table’s rows.</li> <li>It is defined by a start row key (inclusive) and an end row key (exclusive). Every table’s rows belong to some HBase region.</li> <li>Initially, by default, a table is comprised of just one region. As the size of the region grows, it splits.</li> <li>A region splits at a row boundary into two new regions of about equal size. The threshold at which a region will be split is configurable.</li> <li>In HBase, when a region splits at a row boundary, it means the split happens between two rows, not in the middle of a row’s data. Each row is the atomic unit for storage and updates, so HBase ensures that a row’s contents are never divided across two regions.</li> <li>Regions are the units that get distributed throughout an HBase cluster. This is how a table can grow very large without the constraints that hamper RDBMS table grows.</li> <li>A table’s total content is the full set of the regions that hold its rows.</li> </ul> </li> <li>Row keys <ul> <li>In HBase, an operation on a given row is atomic. So, the locking model is simple.</li> <li>HBase provides just one index: the row key.</li> <li>Row keys are stored in sorted order. It is fast and easy to locate a particular row via lookup. Each row has a row key. The row key is also an array of bytes.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbase-4-480.webp 480w,/assets/img/hbase-4-800.webp 800w,/assets/img/hbase-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hbase-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hbase-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li><strong>Write operation</strong> <ul> <li>On the Regionserver, writes are assisted by the Write Ahead Log (WAL).</li> <li>All writes are first written to the WAL on HDFS, and then to the memstore (Google calls it the memtable) for quick lookup.</li> <li>When the amount of data in the memstore reaches a threshold (configurable), the memstore is flushed to HDFS.</li> <li>Each time data is flushed from the memstore, it is stored on disk in an HFile (Google uses SSTable).</li> </ul> </li> <li>Read operation <ul> <li>The region’s memstore is first consulted. If more versions are needed, flush files are consulted from newest to oldest.</li> <li>Reads are assisted by the block cache: The block cache is in the Regionserver. It uses the Least Recently Used (LRU) algorithm.</li> <li>The cache is configurable: It can model multi-level cache. It can choose where data is cached.</li> <li>So while the cache is local to each RegionServer, you can tune it independently on different servers to optimize read performance based on workload and memory availability.</li> </ul> </li> <li>Delete operation <ul> <li>When you delete a version, it means to delete all cells where the version is less than or equal to this version.</li> <li>Since HBase never modifies data in place, it will not immediately delete (or mark as deleted) the entries in the HFiles.</li> <li>Instead, it writes a tombstone marker, which will mask the deleted values.</li> </ul> </li> <li><strong>Compactions</strong> <ul> <li>Minor compaction is a process that compacts a (configurable) number of adjacent small HFiles into a large HFile. It does not drop deletes or expired versions.</li> <li>Major compaction is a heavyweight process that: Rewrites all files within a column family for a region into a single new file. Removes tombstone markers any dead entries. Deletes any expired data (Based on TTL).</li> </ul> </li> <li><strong>Note:</strong> <ul> <li>Q: What is the relationship between HFiles and HDFS blocks in HBase?</li> <li>A: HFiles are logical files managed by HBase that store the data for a column family. They are physically stored on HDFS, which splits files into blocks (typically 128 MB). If an HFile is smaller than the HDFS block size, it may share a block with other small files, but each HFile is usually treated as a separate file. When an HFile grows larger than the block size, HDFS splits it across multiple blocks stored on different data nodes. HBase manages these HFiles independently, using indexing and metadata for efficient reads, while HDFS handles their block-level storage and distribution.</li> <li>So, HFiles are a logical abstraction created by HBase to manage data storage on HDFS. They enable HBase to efficiently store column-family data, maintain indexes, metadata, and Bloom filters for fast reads, and handle memstore flushes, compactions, and region splits. Physically, HFiles are just normal HDFS files, but the abstraction allows HBase to manage wide rows, sparse data, multiple versions, and efficient retrieval without exposing the underlying HDFS complexities to users.</li> <li>Conceptual flow of how HBase routes read and write requests to the correct RegionServer: When a client wants to read or write a row, it first contacts ZooKeeper to find the location of the hbase:meta catalog table. The client then queries hbase:meta to determine which RegionServer hosts the region containing the target row key. With this information, the client communicates directly with the appropriate RegionServer, which manages the region’s memstore and HFiles to handle the read or write. The client caches the region-to-RegionServer mapping for future requests to avoid repeated lookups. If a region moves due to splits or server failures, the cache is updated using ZooKeeper and hbase:meta. Throughout this process, the HBase master is not in the path of reads or writes; it only coordinates region assignments, splits, and recovery, while RegionServers perform the actual data storage and access.</li> </ul> </li> </ul> <h4 id="hbase-usage"><strong>HBase Usage</strong></h4> <ul> <li><strong>Programming</strong> <ul> <li>Aside from the HBase shell, you can use compiled languages with the HBase API.</li> <li>Java is natively supported.</li> <li>Python is also supported with the Thrift server. Python will be slower than Java due to the Thrift server overhead (additional interface).</li> <li>Multiple Thrift servers may be needed for improved performance when datasets are large.</li> </ul> </li> <li><strong>MapReduce</strong> <ul> <li>HBase can be used as a source and/or sink in MapReduce jobs.</li> <li>The TableInputFormat class makes splits on region boundaries so maps are handed a single region to work on.</li> <li>The TableOutputFormat class will write the result of the reduce into HBase.</li> </ul> </li> </ul> <h4 id="summary"><strong>Summary</strong></h4> <ul> <li>HBase is a distributed column-oriented database built on top of HDFS.</li> <li>No real indexes: rows and columns are stored sequentially.</li> <li>Automatic partitioning: Regions are split and distributed across the cluster.</li> <li>Scale linearly with new nodes: Regions automatically rebalance.</li> <li>Commodity hardware: much less I/O hungry than RDBMSs.</li> <li>Fault tolerance: no need to worry about individual node downtime.</li> <li>Batch processing: support distributed MapReduce jobs with locality awareness.</li> </ul> <hr/> <p><strong>Doubts:</strong></p> <ul> <li>In Webtable example(Slide 31), What is the reason for having the row-key as “com:cnn.www”? Understand it clearly.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 7]]></summary></entry><entry><title type="html">Hive &amp;amp; Trino</title><link href="https://monishver11.github.io/blog/2025/big-data-7-hive/" rel="alternate" type="text/html" title="Hive &amp;amp; Trino"/><published>2025-12-16T03:40:00+00:00</published><updated>2025-12-16T03:40:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-7-hive</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-7-hive/"><![CDATA[<h4 id="online-analytical-processing-olap"><strong>Online Analytical Processing (OLAP)</strong></h4> <ul> <li>OLAP is software for performing multidimensional analysis at high speeds on large volumes of data from a data warehouse, data mart, or some other unified, centralized data store.</li> <li>Most business data have multiple dimensions—multiple categories into which the data are broken down for presentation, tracking, or analysis.</li> <li>But in a data warehouse, data sets are stored in tables, each of which can organize data into just two of these dimensions at a time. OLAP extracts data from multiple relational data sets and reorganizes it into a multidimensional format that enables very fast processing and very insightful analysis.</li> <li>Example: Sales figures might have several dimensions related to Location (region, country, state/province, store), Time (year, month, week, day), Product (clothing, men/women/children, brand, type);</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hive-1-480.webp 480w,/assets/img/hive-1-800.webp 800w,/assets/img/hive-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hive-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hive-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="hive"><strong>Hive</strong></h4> <ul> <li>Hive is a data warehousing framework developed by Facebook.</li> <li>Hive is built on top of HDFS (for storage) and MapReduce (for processing).</li> <li>Hive supports HiveQL: <ul> <li>HiveQL is a dialect of SQL.</li> <li>HiveQL is heavily influenced by MySQL.</li> </ul> </li> <li> <p>User data are organized into tables, which are stored as files on HDFS.</p> </li> <li><strong>Metastore</strong> <ul> <li>It’s the central repository of Hive metadata such as table schemas.</li> <li>It’s a relational database automatically created by Hive.</li> </ul> </li> <li>Q: What is stored in the Hive Metastore?</li> <li>A: The Hive Metastore stores metadata about Hive objects, not the actual data. This includes database and table definitions, column names and data types, partition information and locations, storage formats and SerDe details, table properties, statistics used by the query optimizer, and access privileges. The actual table data itself is stored separately in HDFS.</li> <li>Q: Why does Hive use an RDBMS for the Metastore?</li> <li> <p>A: Hive uses an RDBMS for the Metastore because metadata is highly structured and relational, requiring fast, indexed lookups and strong consistency. An RDBMS provides ACID guarantees and supports concurrent access from multiple clients, ensuring metadata remains consistent and durable. It is also optimized for frequent small reads and writes, which is inefficient on HDFS, making an RDBMS the right choice for storing Hive metadata.</p> </li> <li>Hive converts your query into a set of MapReduce jobs. Therefore: <ul> <li>It’s batch-oriented.</li> <li>Its response time is relatively long (tens of seconds to minutes).</li> </ul> </li> <li>Hive provides a shell for interactively issuing commands.</li> <li>There are a number of Hive services in addition to the Hive shell.</li> <li>For example, the hiveserver2 service exposes a Thrift service <ul> <li>It enables access by clients written in diﬀerent languages.</li> <li>It supports applications using Thrift, JDBC, ODBC connectors.</li> </ul> </li> <li>RPC (Remote Procedure Call) is a communication mechanism that allows a program to execute a function or procedure on another machine or process as if it were a local function call. The client sends a request with the function name and arguments over the network, the server executes the function, and the result is sent back to the client. RPC abstracts away network details like sockets and message passing, making distributed systems easier to build.</li> <li>Thrift lets a service provide a set of functions (APIs) that can be called remotely, and programs written in different programming languages can use those functions without worrying about the details of how the network communication works. For example, HiveServer2 can run in Java, but a Python or C++ client can still send queries to it using Thrift. Essentially, Thrift handles the translation between languages and the network communication for you.</li> <li> <p>HiveServer2 also supports standard connectivity through JDBC (Java Database Connectivity) for Java applications and ODBC (Open Database Connectivity) for applications in various other languages, enabling external tools and BI applications to interact with Hive without using the Hive shell directly.</p> </li> <li><strong>Architecture</strong></li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hive-2-480.webp 480w,/assets/img/hive-2-800.webp 800w,/assets/img/hive-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hive-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hive-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>Schema on write versus schema on read</strong> <ul> <li>Schema on write <ul> <li>Traditional databases uses “schema on write”.</li> <li>Used by relational databases like MySQL.</li> <li>Schema is enforced at load time.</li> <li>If the data being loaded does not conform to the schema, the load fails.</li> <li>Queries are faster because columns can be indexed and data are compressed at load time. However, loading takes a long time.</li> </ul> </li> <li>Schema on read <ul> <li>Hive uses “schema on read”.</li> <li>Table’s schema is not enforced until a query is issued.</li> <li>Makes for much faster loading.</li> <li>Schema on read is more flexible.</li> <li>Multiple schemas can be supported simultaneously.</li> <li>A Hive table is essentially an HDFS directory containing one or more files that comprises the table.</li> <li>Users may define multiple Hive table schemas for a given Hive table.</li> </ul> </li> </ul> </li> <li><strong>Features</strong> <ul> <li>Older versions of Hive did not support updates due to limitations of HDFS.</li> <li>Throughout the years, Hive has been supporting more and more usages.</li> <li>Now, Hive supports INSERT INTO for adding rows to existing tables.</li> <li>Hive also supports: <ul> <li>Indexes (as of version 0.7.0).</li> <li>Primitive types as found in Java.</li> <li>Java complex types ARRAY and MAP.</li> <li>STRUCT type, which is a record type.</li> </ul> </li> </ul> </li> <li><strong>Tables</strong> <ul> <li>A Hive table is logically made up of the data being stored and the associated metadata describing the layout of the data in the table.</li> <li>The data typically resides in HDFS. The metadata is stored in the metastore, which is a relational database.</li> <li>Hive supports managed tables and external tables.</li> <li>Managed tables <ul> <li>Hive moves the data into its warehouse directory.</li> <li>When you DROP a managed table, its metadata and its data are deleted.</li> </ul> </li> <li>External tables <ul> <li>You control the creation and deletion of the data.</li> <li>Hive refers to the data at an existing location outside the warehouse directory.</li> <li>When you DROP an external table, only the metadata is deleted, and the data is untouched.</li> </ul> </li> <li>In Hive, the warehouse directory is the default location on HDFS where Hive stores the data files for managed tables. By default, this is usually something like /user/hive/warehouse/. When you create a managed table, Hive moves or stores the table’s data in this directory, and it manages the lifecycle of both the metadata and the data. For external tables, the data stays outside this directory, and Hive only keeps metadata pointing to its location.</li> </ul> </li> <li><strong>Partitions</strong> <ul> <li>Hive allows partitions to be defined.</li> <li>It divides a table based on the value of a partition column (e.g., date).</li> <li>Using partitions can make it faster to do queries on slices of the data.</li> <li>Partitions are nested subdirectories of the table directory on HDFS.</li> <li>If a table has a nested directory structure in HDFS but no partition column is defined in Hive, Hive will not treat those directories as partitions. The table will be considered unpartitioned, and all the data files under the table directory (including subdirectories) will be read as a single logical table. Queries won’t automatically skip directories, they’ll scan all data files, which can make queries slower.</li> <li>While Creating Hive partitions, note that while the partitions are referenced in queries just as if they were fields, no data has been added to the table contents.</li> <li>Ex: hive&gt; CREATE TABLE logs (timestamp BIGINT, line STRING) PARTITIONED BY (theDate STRING, campus STRING);</li> <li>The LOAD DATA command is used to populate a table with data. The source of the data is specified after the INPATH keyword.</li> <li>If multiple files are found in the same directory, they are all part of the table.</li> <li>You can issue SQL queries on the Hive table.</li> <li>In the query, the partition column is treated just like a column internal to the table, even though values are not contained in the data files.</li> <li>Gist: A Hive table is a logical abstraction over data stored in HDFS and can be either managed, where Hive owns and stores the data in its warehouse directory, or external, where the data lives outside Hive and only metadata is managed. A partition in a Hive table means a logical subdivision of the table’s rows based on the values of one or more partition columns (for example, date or campus). Physically, each partition maps to a subdirectory in HDFS, named using the partition column values (e.g., date=2025-12-15/campus=NYC/). All files within a partition directory belong to that partition, and multiple files are treated as one logical chunk of the table. Partition column values are not stored inside the data files; Hive infers them from the directory structure. Data is loaded into tables and partitions using LOAD DATA or external tools, and Hive automatically associates files with partitions based on their HDFS paths. When querying, partition columns behave like normal table columns, but Hive can use them for partition pruning, reading only the relevant directories instead of scanning the entire table, which significantly improves query performance.</li> </ul> </li> <li><strong>Joins</strong> <ul> <li>Hive supports conventional SQL joins: Inner joins, Left/right/full outer joins, and Left/right semi joins.</li> <li>Remember replicated joins (map-side joins or broadcast join)? They are automatic in Hive.</li> </ul> </li> <li><strong>Extensibility</strong> <ul> <li>Hive is extensible via: <ul> <li>UDFs: user-defined functions -&gt; Input one row, output one row.</li> <li>UDAFs: user-defined aggregate functions -&gt; Input multiple rows, output one row.</li> <li>UDTFs: user-defined table-generating functions -&gt; Input one row, output multiple rows — a table.</li> </ul> </li> </ul> </li> <li><strong>Presto &amp; Trino</strong> <ul> <li>Since 2008, Hive became widely used within Facebook for running analytics against data in HDFS on its very large Hadoop cluster.</li> <li>Hive was not suitable for interactive queries at Facebook’s Scale.</li> <li>By 2012, Facebook’s Hive data warehouse was 250 PB in size and needed to handle</li> <li>hundreds of users issuing 10k+ queries each day.</li> <li>Hive could not query other data sources. This means Hive was largely limited to querying data stored in HDFS (and Hive-managed tables) and was not designed to easily query multiple, heterogeneous data sources in a single query.</li> <li>Hive either could not access these sources directly or required complex ingestion pipelines to first copy the data into HDFS. This made interactive analytics slow and inflexible.</li> <li>In 2012, Facebook started to develop Presto from scratch to address the performance, scalability, and extensibility needs for analytics at Facebook.</li> <li>In 2018, its creators left Facebook with their project and later renamed it Trino.</li> </ul> </li> </ul> <h4 id="trino"><strong>Trino</strong></h4> <p>TODO: Add Notes for Trino;</p>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 6]]></summary></entry><entry><title type="html">MapReduce Design Patterns</title><link href="https://monishver11.github.io/blog/2025/big-data-5-mr-dp/" rel="alternate" type="text/html" title="MapReduce Design Patterns"/><published>2025-12-15T22:40:00+00:00</published><updated>2025-12-15T22:40:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-5-mr-dp</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-5-mr-dp/"><![CDATA[<h4 id="serialization"><strong>Serialization</strong></h4> <ul> <li>Serialization is the process of turning structured objects into a byte stream for transmission over a network or for writing to persistent storage.</li> <li>Deserialization is the reverse process of turning a byte stream back into a series of structured objects.</li> <li>A good serialization format should be compact, fast, extensible and interoperable.</li> <li>Hadoop uses its own serialization format: Writable.</li> <li>Hadoop comes with a large selection of Writable classes, which are available in the org.apache.hadoop.io package.</li> <li>There are Writable wrappers for all the Java primitive types except char (which can be stored in an IntWritable).</li> <li>All Writable wrappers have a get() and set() method for retrieving and storing the wrapped value. Example: IntWritable count = new IntWritable(42).</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mr-dp1-480.webp 480w,/assets/img/mr-dp1-800.webp 800w,/assets/img/mr-dp1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mr-dp1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mr-dp-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Text is the Writable wrapper for mutable UTF-8 strings. Ex: Text word = new Text(“Hadoop”);</li> <li>BytesWritable is the Writable wrapper for byte[].</li> <li>NullWritable is a special type of Writable, which has zero-length serialization. No bytes are written to or read from the stream. It is used as a placeholder.</li> <li>For example, in MapReduce, a key or a value can be declared as a NullWritable when you don’t need to use that position, eﬀectively storing a constant empty value. It is an immutable singleton, and the instance can be retrieved by calling NullWritable.get(). Ex: NullWritable nullKey = NullWritable.get();</li> </ul> <h4 id="counters"><strong>Counters</strong></h4> <ul> <li>Counters are a useful channel for gathering statistics about the job: For quality control (Example: what’s the percentage of records that are invalid?), For application-level statistics (Example: how many users in the dataset are between the ages of 18—64?)</li> <li>MapReduce allows user code to define a set of counters, which are then incremented as desired in the mapper or reducer.</li> <li>Counters are defined by a Java enum, which serves to group related counters.</li> <li>A job may define any number of enums, each with any number of fields. The name of the enum is the group name. The enum’s fields are the counter names.</li> <li>Counters are global: the MapReduce framework aggregates them across all mappers and reducers to produce a grand total at the end of the job.</li> <li>Ex: enum Temperature { MISSING, MALFORMED}</li> <li>context.getCounter(Temperature.MALFORMED).increment(1);</li> <li>context.getCounter(Temperature.MISSING).increment(1);</li> <li>context.getCounter(“TemperatureQuality”, parser.getQuality()).increment(1); //dynamic counter, here “TemperatureQuality” is a manual group name (not an enum).</li> <li>Note: In Hadoop, counters are defined and incremented by the Mapper or Reducer, tracked locally by the NodeManager, aggregated by the Application Master, and finally reported to the client by the Resource Manager at job completion.</li> <li>Note: Hadoop also provides built-in counter groups such as FileSystemCounters (bytes read/written), TaskCounters (records processed, spilled data), and JobCounters (launched or failed tasks).</li> <li>Ex: Counter hdfsRead = context.getCounter(“FileSystemCounters”, “HDFS_BYTES_READ”); //access built-in FileSystem counter</li> </ul> <h4 id="mapreduce-design-patterns"><strong>MapReduce design patterns</strong></h4> <ul> <li>Summarization patterns <ul> <li>Numerical summarizations</li> <li>Inverted index summarizations</li> <li>Counting with counters</li> </ul> </li> <li>Filtering patterns <ul> <li>Filtering</li> <li>Bloom filtering</li> <li>Top ten</li> <li>Distinct</li> </ul> </li> <li>Data organization patterns <ul> <li>Structured to hierarchical</li> <li>Partitioning</li> <li>Binning</li> <li>Total order sorting</li> <li>Shuﬄing</li> </ul> </li> <li>Join patterns <ul> <li>Reduce-side join</li> <li>Replicated join</li> <li>Cartesian product</li> </ul> </li> <li>Metapatterns <ul> <li>Job chaining</li> <li>Chain folding</li> <li>Job merging</li> </ul> </li> <li>Input and output patterns</li> </ul> <p><strong>TODO:</strong> Add details, explanation and images to MapReduce design patterns;</p>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 5]]></summary></entry><entry><title type="html">Big Data Processing Concepts &amp;amp; MapReduce</title><link href="https://monishver11.github.io/blog/2025/big-data-4-mapreduce/" rel="alternate" type="text/html" title="Big Data Processing Concepts &amp;amp; MapReduce"/><published>2025-12-15T22:39:00+00:00</published><updated>2025-12-15T22:39:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-4-mapreduce</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-4-mapreduce/"><![CDATA[<h4 id="big-data-processing-concepts"><strong>Big data processing concepts</strong></h4> <ul> <li>Parallel data processing <ul> <li>Parallel data processing reduces the execution time by dividing a single large job into multiple smaller tasks that run concurrently. Ex: a single machine with multiple processors or cores.</li> </ul> </li> <li>Distributed data processing <ul> <li>Distributed data processing is achieved through physically separate machines that are networked together as a cluster.</li> </ul> </li> <li>Hadoop <ul> <li>Hadoop is an open-source framework for large-scale data storage and data processing that is compatible with commodity hardware.</li> <li>It can be used as an analytics engine for processing large amounts of structured, semi-structured and unstructured data.</li> <li>It implements the MapReduce processing framework.</li> </ul> </li> <li>Processing workloads <ul> <li>A processing workload in Big Data is defined as the amount and nature of data that is processed within a certain amount of time.</li> <li>Workloads are usually divided into two types: Batch processing and transactional processing.</li> <li>Batch processing: <ul> <li>(A.k.a. offline processing) involves processing data in batches and usually imposes delays, which results in high-latency responses.</li> <li>Batch workloads typically involve large quantities of data with sequential read/writes and comprise of groups of read or write queries.</li> <li>Queries can be complex and involve multiple joins.</li> <li>OLAP (online analytical processing) systems commonly process workloads in batches.</li> <li>Gist: In batch processing, data is read and written in large, continuous chunks on disk (sequentially), which is faster than random access. Instead of many small operations, the system groups multiple read/write queries—such as aggregations, joins, or filters—into a single large job (like MapReduce), processing them together for high throughput but with higher latency.</li> </ul> </li> <li>Transactional processing: <ul> <li>(A.k.a online processing) follows an approach whereby data is processed interactively without delay, resulting in low-latency responses.</li> <li>Transaction workloads involve small amounts of data with random reads and writes and fewer joins.</li> <li>OLTP (online transaction processing) system fall within this category.</li> </ul> </li> </ul> </li> </ul> <h4 id="mapreduce"><strong>MapReduce</strong></h4> <ul> <li>MapReduce is a widely used implementation of a batch processing framework.</li> <li>It is highly scalable and reliable and is based on the principle of divide-and-conquer, which provides built-in fault tolerance and redundancy.</li> <li>MapReduce does not require that the input data conform to any particular data model. Therefore, it can be used to process schema-less datasets.</li> <li>A dataset is broken down into multiple smaller parts, and operations are performed on each part independently and in parallel.</li> <li>The results from all operations are then summarized to arrive at the answer.</li> <li>Traditionally, data processing requires moving data from the storage node to the processing node that runs the data processing algorithm. This approach works fine for smaller datasets. However, with large datasets, moving data can incur more overhead than the actual processing of the data.</li> <li>With MapReduce, the data processing algorithm is instead moved to the nodes that store the data. The data processing algorithm executes in parallel on these nodes, thereby eliminating the need to move the data first. It saves network bandwidth and reduces processing time for large datasets.</li> <li>Terminology: <ul> <li>A MapReduce job is a unit of work that the client wants to be performed.</li> <li>Hadoop runs the job by dividing it into tasks: map tasks and reduce tasks.</li> <li>Hadoop divides the input to a MapReduce job into fixed-size splits.</li> <li>Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-1-480.webp 480w,/assets/img/mapreduce-1-800.webp 800w,/assets/img/mapreduce-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Map: <ul> <li>The dataset is divided into multiple smaller splits. Each split contains multiple key-value pairs.</li> <li>The map function (“mapper”) executes user-defined logic on each split: (K1, V1) -&gt; list(K2, V2)</li> <li>A mapper may generate zero(filtering) or more than one(demultiplexing) key-value pairs.</li> </ul> </li> <li>Combine: <ul> <li>With larger datasets, moving data between map and reduce stages is more expensive than the actual processing.</li> <li>The optional combiner function summarizes a mapper’s output before it gets processed by the reducer: (K2, list(V2)) -&gt; list(K2, V2)</li> </ul> </li> <li>Partition: <ul> <li>If more than one reducer is involved, a partitioning function (“partitioner”) divides the output from the mapper or combiner (if used) into partitions between reducer instances.</li> <li>Although each partition contains multiple key-value pairs, all records for a particular key are assigned to the same partition.</li> </ul> </li> <li>Shuffle &amp; Sort: <ul> <li>Output from all partitioners is copied across the network to the nodes running the reduce task.</li> <li>The key-value pairs are grouped and sorted according to the keys, list(K2, V2) -&gt; (K2, list(V2))</li> </ul> </li> <li>Reduce: <ul> <li>The reduce function (“reducer”) further summarizes its input: (K2, list(V2)) -&gt; list(K3, V3).</li> <li>The output of the reducer is then written as a separate file. One file per reducer.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-2-480.webp 480w,/assets/img/mapreduce-2-800.webp 800w,/assets/img/mapreduce-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>MapReduce in action (Ex: NCDC weather dataset) <ul> <li>Note: In Hadoop’s TextInputFormat, each line of a file is given a key representing its byte offset from the start of the entire file, not just the split. Offsets are used instead of line numbers because files are split and processed in parallel across multiple mappers, and byte offsets allow each mapper to locate its data efficiently without reading previous lines, ensuring scalability and consistency.</li> <li>Note: Line numbers can’t be used in Hadoop because the input files are split and processed in parallel by multiple mappers. Each mapper starts reading from a different byte position in the file, so it has no way to know how many lines came before its split without scanning the entire file sequentially. Byte offsets, on the other hand, can be determined directly from the file’s position on disk, making them independent, efficient, and uniquely identifiable across splits — perfect for distributed processing.</li> <li>The map function extracts the year and the air temperature, and emits them as its output.</li> <li>The output from the map function is processed by the MapReduce framework before being sent to the reduce function. This processing sorts and groups the key-value pairs by key.</li> <li>For each input, the reduce function iterates through the list and picks up the maximum reading ￼</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-3-480.webp 480w,/assets/img/mapreduce-3-800.webp 800w,/assets/img/mapreduce-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-4-480.webp 480w,/assets/img/mapreduce-4-800.webp 800w,/assets/img/mapreduce-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ ￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-5-480.webp 480w,/assets/img/mapreduce-5-800.webp 800w,/assets/img/mapreduce-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-6-480.webp 480w,/assets/img/mapreduce-6-800.webp 800w,/assets/img/mapreduce-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ ￼</p> <ul> <li>Data flow <ul> <li>Where to run the map task?</li> <li>Data locality: run the map task on a node where the input data resides in HDFS, because it doesn’t use valuable cluster bandwidth.</li> <li>If not possible, the job scheduler will look for a free map slot on a node in the same rack as one of the blocks, and the required data block is transferred over the rack’s local network.</li> <li>If still not possible, an oﬀ-rack node is used, which results in an inter-rack network transfer.</li> <li>So, what’s transferred is the actual HDFS block data required by the Map task to perform its computation. ￼</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-7-480.webp 480w,/assets/img/mapreduce-7-800.webp 800w,/assets/img/mapreduce-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ </p> <ul> <li>How large is an input split? <ul> <li>Map tasks process input splits in parallel. So the processing is better load balanced when the splits are small.</li> <li>However, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time.</li> <li>For most jobs, a good split size tends to be the size of an HDFS block, which is 128 MB by default. It is the largest size of input that can be guaranteed to be stored on a single node.</li> </ul> </li> <li>Where to store the map output? <ul> <li>Map tasks write their output to the local disk, not to HDFS. Why? Map output is intermediate output. It’s processed by reduce tasks to produce the final output. Once the job is complete, the map output can be thrown away.</li> <li>Storing it in HDFS with replication would be overkill.</li> <li>What if the node running the map task fails before the map output has been consumed by the reduce task? If a node fails before the reduce phase reads its map output, Hadoop simply re-runs the failed map task on another node. Since the map output is intermediate and deterministic, it can be regenerated from the original input data stored safely in HDFS.</li> </ul> </li> <li>Why doesn’t Hadoop move the map task to another node that holds the same HDFS block instead of transferring the data? Why move data instead of computation? <ul> <li>A: Hadoop’s scheduler tries first to move computation (the map task) to where the data already resides — this is called data locality, and it’s the preferred option. However, if all nodes holding that block are busy (no free map slots), the scheduler can’t wait indefinitely because it would delay the job. In that case, it assigns the task to another available node and transfers the required data block over the network. So, Hadoop only moves data as a fallback when moving computation isn’t immediately possible, balancing performance and cluster utilization.</li> </ul> </li> <li>Do reduce tasks enjoy data locality?</li> <li>The input to each reduce task is normally the output from all mappers. ￼</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-8-480.webp 480w,/assets/img/mapreduce-8-800.webp 800w,/assets/img/mapreduce-8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-8" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Minimizing the data transferred between map and reduce tasks <ul> <li>The combiner function is an optimization that runs on the map output to reduce the amount of data transferred to the reducers by performing local aggregation. Hadoop does not provide a guarantee of how many times it will call it for a particular map output record. Calling the combiner function zero, one, or many times should produce the same output from the reducer.</li> <li>It works best for associative and commutative operations, where partial results can be safely combined.</li> <li>Which of the following data processing can benefit from a combiner function? <ul> <li>Count the number of occurrences. YES</li> <li>Find the maximum value. YES</li> <li>Find the average value. NO, since the combiner may process subsets differently, leading to incorrect results unless additional logic (like tracking sums and counts separately) is used.</li> <li>Filter values based on a predicate. YES</li> </ul> </li> </ul> </li> </ul> <h4 id="architecture"><strong>Architecture</strong></h4> <p><strong>YARN (Yet Another Resource Negotiator)</strong></p> <ul> <li>YARN is Hadoop’s cluster resource management system.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-9-480.webp 480w,/assets/img/mapreduce-9-800.webp 800w,/assets/img/mapreduce-9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-9" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ ￼</p> <ul> <li>YARN provides its core services via two types of long-running daemons.</li> <li>Resource manager (only one in the cluster): manage the use of resources across the cluster.</li> <li>Node managers (on each node): launch and monitor containers. A container executes an application-specific process with a constrained set of resources (memory, CPU, …). ￼</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-10-480.webp 480w,/assets/img/mapreduce-10-800.webp 800w,/assets/img/mapreduce-10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-10" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Running an application: A client contacts the resource manager and asks it to run an application master process. The resource manager finds a node manager that can launch the application master in a container. The application master may request more containers from the resource manager. The application master use them to run a distributed computation (e.g., MapReduce).</li> <li>Types of YARN scheduler: (FIFO, Capacity, FAIR) scheduler.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-11-480.webp 480w,/assets/img/mapreduce-11-800.webp 800w,/assets/img/mapreduce-11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-11" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <p><strong>Hadoop</strong></p> <ul> <li>Running a MapReduce job:</li> <li>Client: submit the MapReduce job.</li> <li>YARN resource manager: coordinate the allocation of compute resources in cluster.</li> <li>YARN node managers: launch and monitor the containers on machines in the cluster.</li> <li>MapReduce application master: coordinate the tasks running the MR job. The application master and the MapReduce tasks run in containers scheduled by the resource manager and managed by the node managers.</li> <li>HDFS: share job files between the other entities.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-12-480.webp 480w,/assets/img/mapreduce-12-800.webp 800w,/assets/img/mapreduce-12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-12" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <ul> <li>Progress &amp; status updates:</li> <li>A job and each of its tasks have a status.</li> <li>The state of the job or task (e.g., running, successfully completed, failed).</li> <li>The progress of maps and reduces.</li> <li>The values of the job’s counters.</li> <li>A status message or description.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-13-480.webp 480w,/assets/img/mapreduce-13-800.webp 800w,/assets/img/mapreduce-13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-13" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <ul> <li>Q: Does each node hold HDFS blocks and containers for task execution, managed by the NodeManager?</li> <li>A: Yes. Each node stores blocks as part of HDFS and can also run containers, which execute tasks (Map or Reduce) scheduled by YARN. The NodeManager on each node handles launching, monitoring, and reporting on these containers, while the ResourceManager coordinates cluster-wide resource allocation.</li> <li>Q: What does the YARN scheduler do? Does it schedule Map and Reduce tasks, and how is memory/CPU utilization handled?</li> <li>A: The YARN scheduler (FIFO, Capacity, or FAIR) manages the allocation of resources—CPU, memory, and containers—across the cluster. It decides which nodes get containers for tasks. While it schedules containers for Map and Reduce tasks indirectly via the ApplicationMaster, the actual memory and CPU usage is constrained per container on individual nodes as specified by the scheduler.</li> <li>Q: Does the ResourceManager use HDFS block information to improve data locality and distributed efficiency?</li> <li>A: Yes. The ResourceManager, through the ApplicationMaster, tries to schedule tasks on nodes that already hold the input HDFS blocks to exploit data locality, reducing network transfer and improving performance. If local nodes aren’t available, it may schedule tasks on the same rack or another node as a fallback.</li> <li>Q: What are job counters in Hadoop, and what do they mean?</li> <li>A: Counters are metrics collected during job execution. They track things like the number of bytes read/written, number of records processed, map/reduce task attempts, and custom user-defined counters. Counters provide insight into job progress, efficiency, and can help debug or optimize jobs.</li> <li>Q: What is a status message, and how is it used?</li> <li>A: A status message is a short description of the current state of a job or task (e.g., “Reading input”, “Merging outputs”, “Task failed”). It helps users and administrators monitor progress, understand failures, and debug issues during job execution.</li> <li>Gist: In Hadoop with YARN, each node stores HDFS blocks and also runs containers for executing tasks, with the NodeManager handling container lifecycle and reporting. The ResourceManager coordinates cluster-wide resource allocation, while the YARN scheduler (FIFO, Capacity, or FAIR) decides which nodes get containers, controlling CPU and memory usage per container. To maximize efficiency, tasks are ideally scheduled on nodes holding the relevant HDFS blocks, leveraging data locality; if unavailable, tasks may run on the same rack or another node. During execution, job counters track metrics like bytes read/written, records processed, and task attempts, providing insight for monitoring and optimization. Status messages report the current state of jobs or tasks, helping users and administrators monitor progress and debug issues.</li> </ul> <p><strong>Resilience</strong></p> <ul> <li>Where can a failure happen? It can happen in 4 places: A MapReduce task, the MapReduce application master, YARN node manager or the YARN resource manager.</li> <li>Task failure: <ul> <li>Possible causes: Mapper/reducer bug, JVM bug, Hanging tasks: timeout (default: 10 min) exceeded without a progress update.</li> <li>The application master will reschedule the task on another node manager.</li> <li>If a task fails too many times (default: 4), it will not be retried again, and the whole job will fail.</li> </ul> </li> <li>Application master failure: <ul> <li>An application master sends periodic heartbeats to the resource manager.</li> <li>In the event of application master failure, the resource manager will detect the failure and start a new instance of the master running in a new container (managed by a node manager).</li> <li>In the case of the MapReduce application master, it’ll use the job history to recover the state of the tasks that were already run by the (failed) application, so they don’t have to be rerun.</li> <li>If a MapReduce application master fails too many times (default: 2), it will not be retried again, and the whole job will fail.</li> <li>Q: Where is the MapReduce job history stored and retrieved if the application master fails?</li> <li>A: The job history is persisted in HDFS, not in the application master’s memory. When a new application master is started after a failure, it reads the job history from HDFS to recover the state of already completed or partially completed tasks, so they don’t have to be rerun.</li> </ul> </li> <li>Node manager failure: <ul> <li>The resource manager will notice a node manager that has stopped sending heartbeats (default: 10 min) and remove it from its pool of nodes to schedule containers on.</li> <li>Q: If a NodeManager fails, do we need to rerun completed Map tasks?</li> <li>Yes, because the intermediate results are only stored in the node’s disk and not in hdfs. So, we’ve to rerun, so as to collect all the intermediate results from all mappers before the reducer part.</li> <li>Q: If a NodeManager fails, do we need to rerun completed Reduce tasks?</li> <li>A: No. Completed Reduce tasks write their output to HDFS, which is replicated and durable. Only in-progress Reduce tasks on the failed node need to be rescheduled on another node.</li> </ul> </li> <li>Resource manager failure: <ul> <li>Failure of the resource manager is serious, because without it, neither jobs nor task containers can be launched.</li> <li>In the default configuration, the resource manager is a single point of failure, since in the (unlikely) event of machine failure, all running jobs fails, and can’t be recovered.</li> <li>For high availability (HA), we need to configure a standby resource manager.</li> <li>Q: In ResourceManager failure, do we need to store information about all running applications?A: Yes. To recover after a ResourceManager failure, information about all running applications—including job metadata and application master states—needs to be persisted(not in HDFS). In HA setups, a standby ResourceManager keeps this information in ZooKeeper or shared storage(NFS) to resume operations without losing job information.</li> <li>Q: In ResourceManager failure, do we need to store NodeManager information?A: No. It can be reconstructed back again from the NodeManager’s heartbeats.</li> <li>Q: In ResourceManager failure, what about task information?A: Task information (status, progress, container allocation) is primarily tracked by the ApplicationMaster. The ResourceManager coordinates resources but doesn’t store detailed task outputs. In HA setups, the standby ResourceManager works with running ApplicationMasters to continue scheduling containers and managing tasks without losing track of progress.</li> </ul> </li> <li>Shuffle and sort (Most important part) <ul> <li>MapReduce guarantees that the input to every reducer is sorted by key.</li> <li>Flow: In MapReduce, each map task processes an input split, which contains multiple records. The map function runs on each record and generates intermediate key-value pairs, which are initially buffered in memory rather than written to disk immediately. This in-memory buffering allows the system to perform local partitioning, which determines which reducer each key belongs to, as well as optional combining, which reduces the number of intermediate records by aggregating data locally before transfer. Additionally, the intermediate data is sorted in memory by key to facilitate efficient merging later. Once the buffer reaches a configured threshold (default 100 MB), it is spilled to disk as a temporary file. Large map outputs may generate multiple spill files, which are later merged using a merge sort into a single sorted file per partition. During the shuffle phase, reducers fetch the intermediate data from all mappers, ensuring that each reducer receives all data corresponding to its assigned partition. Because the map output is already sorted locally, reducers perform a multi-way merge rather than a full sort, which improves efficiency. The final merged data is then streamed directly to the reduce function without writing it back to disk, minimizing I/O. If the intermediate data is too large to fit in memory, MapReduce applies external sorting, reading chunks of data from disk, merging them in memory, and writing back to disk. After merging, the reducer processes the records using the reduce function and writes the final output to HDFS. Throughout this process, combiners may optionally reduce the volume of data transferred, merges are optimized with multi-way strategies, and copying of map outputs can begin even before all map tasks finish, though the reduce function only runs after all intermediate data for that partition has been fetched.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-14-480.webp 480w,/assets/img/mapreduce-14-800.webp 800w,/assets/img/mapreduce-14-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-14" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <p><strong>Speculative execution</strong></p> <ul> <li>The job execution time is sensitive to slow-running tasks (“stragglers”).</li> <li>Hadoop doesn’t try to diagnose and fix slow-running tasks; instead, it tries to detect when a task is running slower than expected and launches another equivalent task as a backup.</li> <li>The scheduler tracks the progress of all tasks of the same type (map and reduce) in a job, and only launches speculative duplicates for the small portion that are running significantly slower than the average.</li> <li>When a task completes successfully, any duplicate tasks that are running are killed since they are no longer needed.</li> <li>Q: Which part of Hadoop handles speculative execution?</li> <li>A: Speculative execution is managed by the Hadoop MapReduce framework itself, specifically by the ApplicationMaster in YARN (Hadoop 2+).</li> <li>Q: Is the YARN scheduler part of the ApplicationMaster, and how do they interact?</li> <li>A: No, the YARN scheduler is part of the ResourceManager and operates cluster-wide, deciding which nodes get containers based on available resources and scheduling policies (FIFO, Capacity, FAIR). It does not manage task-level details like progress or stragglers. The ApplicationMaster, on the other hand, is job-specific. It requests containers from the ResourceManager (which allocates them via the scheduler), monitors task progress, handles failures, and manages speculative execution. Essentially, the ApplicationMaster depends on the scheduler for container allocation, and once allocated, it schedules and manages tasks within those containers, coordinating retries, merges, and data flow. The scheduler handles resource allocation, while the ApplicationMaster handles job and task management.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4]]></summary></entry></feed>