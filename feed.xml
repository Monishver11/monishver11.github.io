<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-15T04:08:25+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A space for journaling my learnings, career, thoughts, and experiences in this amazing ride of life. </subtitle><entry><title type="html">MLP Standard Derivatives Derivation</title><link href="https://monishver11.github.io/blog/2025/mlp-derivatives/" rel="alternate" type="text/html" title="MLP Standard Derivatives Derivation"/><published>2025-12-13T13:14:00+00:00</published><updated>2025-12-13T13:14:00+00:00</updated><id>https://monishver11.github.io/blog/2025/mlp-derivatives</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/mlp-derivatives/"><![CDATA[<p><strong>Softmax Definition</strong></p> \[y_i = \mathrm{softmax}(a)_i = \frac{e^{a_i}}{\sum_k e^{a_k}} \qquad\text{let } S = \sum_k e^{a_k}.\] <p><strong>Softmax Jacobian:</strong> \(\frac{\partial y_i}{\partial a_j}\)</p> <p>Start from:</p> \[y_i = \frac{e^{a_i}}{S}.\] <p>Differentiate w.r.t. \(a_j\) using the quotient rule:</p> \[\frac{\partial y_i}{\partial a_j} = \frac{S\cdot\frac{\partial}{\partial a_j}e^{a_i} \;-\; e^{a_i}\cdot\frac{\partial S}{\partial a_j}}{S^2}.\] <p>Compute derivatives:</p> <ul> <li> \[\frac{\partial}{\partial a_j} e^{a_i} = e^{a_i}\delta_{ij}\] </li> <li> \[\frac{\partial S}{\partial a_j} = e^{a_j}\] </li> </ul> <p>Substitute:</p> \[\frac{\partial y_i}{\partial a_j} = \frac{S(e^{a_i}\delta_{ij}) - e^{a_i}e^{a_j}}{S^2} = \frac{e^{a_i}}{S^2}(S\delta_{ij} - e^{a_j}).\] <p>Use softmax definitions:</p> \[y_i = \frac{e^{a_i}}{S}, \qquad y_j = \frac{e^{a_j}}{S}.\] <p>Final form:</p> \[\frac{\partial y_i}{\partial a_j} = y_i\delta_{ij} - y_i y_j = y_i (\delta_{ij} - y_j).\] <p><strong>Cross-Entropy Loss</strong></p> <p>For one-hot target \(t\):</p> \[L = -\sum_i t_i \log y_i.\] <p>Differentiate w.r.t. \(a_j\):</p> \[\frac{\partial L}{\partial a_j} = \sum_i \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial a_j} = \sum_i \left( -\frac{t_i}{y_i} \right) y_i(\delta_{ij} - y_j).\] <p>Simplify:</p> \[= \sum_i -t_i(\delta_{ij} - y_j) = -\sum_i t_i\delta_{ij} + \sum_i t_i y_j.\] <p>Use:</p> <ul> <li> \[\sum_i t_i\delta_{ij} = t_j\] </li> <li>\(\sum_i t_i = 1\) for one-hot \(t\)</li> </ul> <p>Thus:</p> \[\frac{\partial L}{\partial a_j} = -t_j + y_j = y_j - t_j.\] \[\boxed{ \frac{\partial L}{\partial a_j} = y_j - t_j }\] <p><strong>Note:</strong><br/> \(\delta_{ij}\) is the <strong>Kronecker delta</strong>, defined as</p> \[\delta_{ij} = \begin{cases} 1, &amp; i = j \\ 0, &amp; i \ne j \end{cases}\] <p>It acts as an index selector in summations.<br/> For example:</p> \[\sum_i a_i \delta_{ij} = a_j.\] <p>In the softmax Jacobian derivation, \(\delta_{ij}\) appears because</p> \[\frac{\partial e^{a_i}}{\partial a_j} = e^{a_i} \delta_{ij},\] <p>meaning only the term with matching indices contributes to the derivative.</p> <p><strong>Sigmoid Derivative (Element-wise)</strong></p> <p>The sigmoid function is defined as:</p> \[\sigma(x) = \frac{1}{1 + e^{-x}}.\] <p>Let the activation be applied element-wise: \(y_i = \sigma(a_i).\)</p> <p><strong>Derivative of Sigmoid</strong></p> <p>Differentiate w.r.t. \(a_i\):</p> \[\frac{d\sigma(x)}{dx} = \frac{d}{dx}\left( \frac{1}{1 + e^{-x}} \right) = \frac{e^{-x}}{(1 + e^{-x})^2}.\] <p>Rewrite in terms of \(\sigma(x)\):</p> \[\sigma(x) = \frac{1}{1 + e^{-x}}, \qquad 1 - \sigma(x) = \frac{e^{-x}}{1 + e^{-x}}.\] <p>Thus:</p> \[\frac{d\sigma(x)}{dx} = \sigma(x)\bigl(1 - \sigma(x)\bigr).\] <p><strong>Jacobian Form</strong></p> <p>Since sigmoid acts independently on each component,</p> \[\frac{\partial y_i}{\partial a_j} = \sigma(a_i)\bigl(1 - \sigma(a_i)\bigr)\delta_{ij}.\] <p>The Jacobian is diagonal.</p> <p><strong>Derivative of Matrix Multiplication</strong></p> <p>Let:</p> \[Y = A X\] <p>where:</p> <ul> <li> \[A \in \mathbb{R}^{m \times n}\] </li> <li> \[X \in \mathbb{R}^{n \times p}\] </li> <li> \[Y \in \mathbb{R}^{m \times p}\] </li> </ul> <p><strong>Gradient with Respect to \(A\)</strong></p> \[\frac{\partial L}{\partial A} = \frac{\partial L}{\partial Y} \, X^\top\] <p><strong>Gradient with Respect to \(X\)</strong></p> \[\frac{\partial L}{\partial X} = A^\top \frac{\partial L}{\partial Y}\] <p><strong>Matrix Multiplication Gradients in <code class="language-plaintext highlighter-rouge">einsum</code> Form</strong></p> <p>Let:</p> \[Y = A X\] <p>with components:</p> \[Y_{ij} = \sum_k A_{ik} X_{kj}.\] <p>Let the upstream gradient be:</p> \[G_{ij} = \frac{\partial L}{\partial Y_{ij}}.\] <p><strong>Gradient with Respect to (A)</strong></p> <p>Using the chain rule:</p> \[\frac{\partial L}{\partial A_{ik}} = \sum_j \frac{\partial L}{\partial Y_{ij}} \frac{\partial Y_{ij}}{\partial A_{ik}} = \sum_j G_{ij} X_{kj}.\] <p>In Einstein summation notation:</p> \[\frac{\partial L}{\partial A_{ik}} = G_{ij} X_{kj}.\] <p><strong><code class="language-plaintext highlighter-rouge">einsum</code> implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">ik,kj-&gt;ij</span><span class="sh">"</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>


<span class="n">dLdX</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">ik,ij-&gt;kj</span><span class="sh">"</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">G</span><span class="p">)</span>
<span class="n">dLdA</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">ij,kj-&gt;ik</span><span class="sh">"</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[MLP Standard Derivatives Derivation]]></summary></entry><entry><title type="html">Simple MLP - Forward and Backward Pass (With Einsum) Derivation</title><link href="https://monishver11.github.io/blog/2025/mlp-fw-bwd/" rel="alternate" type="text/html" title="Simple MLP - Forward and Backward Pass (With Einsum) Derivation"/><published>2025-12-13T12:37:00+00:00</published><updated>2025-12-13T12:37:00+00:00</updated><id>https://monishver11.github.io/blog/2025/mlp-fw-bwd</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/mlp-fw-bwd/"><![CDATA[<h4 id="neural-network-forward-and-backward-pass"><strong>Neural Network Forward and Backward Pass</strong></h4> <p><strong>Notation</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">h = Sigmoid(wx + b)</code></li> <li><code class="language-plaintext highlighter-rouge">w = Nh × Ni</code></li> <li><code class="language-plaintext highlighter-rouge">V = No × Nh</code></li> <li><code class="language-plaintext highlighter-rouge">y' = Softmax(Vh + c)</code></li> <li><code class="language-plaintext highlighter-rouge">b = Nh</code></li> <li><code class="language-plaintext highlighter-rouge">c = No</code></li> <li><code class="language-plaintext highlighter-rouge">x = Ni × 1</code> (1 training sample)</li> </ul> <p><strong>Forward Pass</strong></p> \[\begin{aligned} h_a &amp;= W @ x + b \quad &amp;&amp; (Nh \times Ni) @ (Ni \times 1) + (Nh \times 1) \\ h &amp;= \text{Sigmoid}(h_a) \quad &amp;&amp; (Nh \times 1) \\ y_a &amp;= V @ h + c \quad &amp;&amp; (No \times Nh) \times (Nh \times 1) + (No \times 1) \\ y &amp;= \text{Softmax}(y_a) \quad &amp;&amp; (No \times 1) \end{aligned}\] <p>Cross-entropy loss: \(L = -\sum_{i=0}^{n} t \log y_i \quad \text{(scalar)} \rightarrow -t \log y \text{(vector)} \quad (No \times 1)\)</p> <p><strong>Backward Pass</strong></p> <p>Gradients w.r.t. loss:</p> \[\frac{\partial L}{\partial L} = 1 ; \quad \frac{\partial L}{\partial y} = -\frac{t}{y}\] \[\frac{\partial L}{\partial y_a} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial y_a} = y - t \quad (No \times 1)\] <p><strong>Note:</strong> Output shape must match the variable shape w.r.t. which derivative is taken.</p> <p>Derivatives:</p> \[\frac{\partial L}{\partial V} = \frac{\partial L}{\partial y_a} \cdot \frac{\partial y_a}{\partial V} = (y - t) \cdot h^\top \quad \Rightarrow \quad (No \times 1) @ (1 \times Nh) = (No \times Nh)\] \[\frac{\partial L}{\partial h} = \frac{\partial L}{\partial y_a} \cdot \frac{\partial y_a}{\partial h} = V^\top \cdot (y - t) \quad \Rightarrow \quad (Nh \times No) @ (No \times 1) = (Nh \times 1)\] \[\frac{\partial L}{\partial c} = \frac{\partial L}{\partial y_a} \cdot \frac{\partial y_a}{\partial c} = (y - t) \cdot 1 \quad \Rightarrow \quad (No \times 1)\] \[\frac{\partial L}{\partial h_a} = \frac{\partial L}{\partial h} \cdot \frac{\partial h}{\partial h_a} = \frac{\partial L}{\partial h} \left[ \sigma(h_a) \cdot (1 - \sigma(h_a)) \right] \quad \Rightarrow \quad (Nh \times 1) \cdot (Nh \times 1) = (Nh \times 1)\] \[\frac{\partial L}{\partial w} = \frac{\partial L}{\partial h_a} \cdot \frac{\partial h_a}{\partial w} = \frac{\partial L}{\partial h_a} \cdot x^\top = (Nh \times 1) @ (1 \times Ni) = (Nh \times Ni)\] \[\frac{\partial L}{\partial b} = \frac{\partial L}{\partial h_a} \cdot \frac{\partial h_a}{\partial b} = \frac{\partial L}{\partial h_a} \cdot 1 = (Nh \times 1)\] <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mlp-fw-bwd-480.webp 480w,/assets/img/mlp-fw-bwd-800.webp 800w,/assets/img/mlp-fw-bwd-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mlp-fw-bwd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bd-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="forward-and-backward-pass-using-einsum-single-sample-mlp"><strong>Forward and Backward Pass Using <code class="language-plaintext highlighter-rouge">einsum</code> (Single-Sample MLP)</strong></h4> <p><strong>Model</strong></p> <ul> <li>Hidden layer: \(h = \sigma(Wx + b)\)</li> <li>Output layer: \(y = \text{softmax}(Vh + c)\)</li> </ul> <p><strong>Index Convention</strong></p> <ul> <li>\(i\): input dimension index, \(i = 1 \dots N_i\)</li> <li>\(h\): hidden dimension index, \(h = 1 \dots N_h\)</li> <li>\(o\): output dimension index, \(o = 1 \dots N_o\)</li> </ul> <h5 id="forward-pass"><strong>Forward Pass</strong></h5> <p><strong>Hidden pre-activation</strong></p> \[h_{a,h} = \sum_i W_{h i} x_i + b_h\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">hi,i-&gt;h</span><span class="sh">"</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div></div> <p><strong>Hidden activation (sigmoid)</strong></p> \[h_h = \sigma(h_{a,h})\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">h</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">ha</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output pre-activation</strong></p> \[y_{a,o} = \sum_h V_{o h} h_h + c_o\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ya</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">oh,h-&gt;o</span><span class="sh">"</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>
</code></pre></div></div> <p><strong>Output activation (softmax)</strong></p> \[y_o = \frac{e^{y_{a,o}}}{\sum_{o'} e^{y_{a,o'}}}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">ya</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Loss (Cross-Entropy)</strong></p> \[L = -\sum_o t_o \log y_o\] <h5 id="backward-pass"><strong>Backward Pass</strong></h5> <p><strong>Gradient w.r.t. output pre-activation</strong></p> \[\frac{\partial L}{\partial y_{a,o}} = y_o - t_o\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdya</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">t</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. output weights \(V\)</strong></p> \[\frac{\partial L}{\partial V_{o h}} = (y_o - t_o) h_h\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdV</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">o,h-&gt;oh</span><span class="sh">"</span><span class="p">,</span> <span class="n">dLdya</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. output bias \(c\)</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdc</span> <span class="o">=</span> <span class="n">dLdya</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. hidden activations</strong></p> \[\frac{\partial L}{\partial h_h} = \sum_o V_{o h} (y_o - t_o)\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdh</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">oh,o-&gt;h</span><span class="sh">"</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dLdya</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. hidden pre-activations</strong></p> \[\frac{\partial L}{\partial h_{a,h}} = \frac{\partial L}{\partial h_h} \cdot \sigma(h_{a,h})(1 - \sigma(h_{a,h}))\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdha</span> <span class="o">=</span> <span class="n">dLdh</span> <span class="o">*</span> <span class="nf">sigmoidgrad</span><span class="p">(</span><span class="n">ha</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. input weights \(W\)</strong></p> \[\frac{\partial L}{\partial W_{h i}} = \frac{\partial L}{\partial h_{a,h}} x_i\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">h,i-&gt;hi</span><span class="sh">"</span><span class="p">,</span> <span class="n">dLdha</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. hidden bias \(b\)</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdb</span> <span class="o">=</span> <span class="n">dLdha</span>
</code></pre></div></div> <h5 id="summary"><strong>Summary;</strong></h5> <ul> <li>All operations are tensor contractions.</li> <li><code class="language-plaintext highlighter-rouge">einsum</code> makes index flow explicit.</li> <li>Backprop through linear layers reduces to transposes and outer products.</li> </ul> <h4 id="code"><strong>Code:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Dimensions
</span><span class="n">Ni</span> <span class="o">=</span> <span class="mi">784</span>  <span class="c1"># Input dimension (e.g., MNIST)
</span><span class="n">Nh</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># Hidden dimension
</span><span class="n">No</span> <span class="o">=</span> <span class="mi">10</span>   <span class="c1"># Output dimension (e.g., 10 classes)
</span>
<span class="c1"># Initialize parameters
</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">Nh</span><span class="p">,</span> <span class="n">Ni</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">Nh</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">No</span><span class="p">,</span> <span class="n">Nh</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">No</span><span class="p">)</span>

<span class="c1"># Single training example
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">Ni</span><span class="p">)</span>    <span class="c1"># Input vector
</span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">No</span><span class="p">)</span>           <span class="c1"># One-hot target
</span><span class="n">t</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>                   <span class="c1"># Example: class 3
</span>
<span class="c1"># --- Forward Pass ---
</span><span class="n">ha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">hi,i-&gt;h</span><span class="sh">"</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>        <span class="c1"># (Nh,)
</span><span class="n">h</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">ha</span><span class="p">))</span>                  <span class="c1"># (Nh,)
</span><span class="n">ya</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">oh,h-&gt;o</span><span class="sh">"</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>        <span class="c1"># (No,)
# Softmax with numerical stability
</span><span class="n">ya_max</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">ya</span><span class="p">)</span>
<span class="n">exp_ya</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ya</span> <span class="o">-</span> <span class="n">ya_max</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">exp_ya</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_ya</span><span class="p">)</span>                <span class="c1"># (No,)
</span>
<span class="c1"># Loss
</span><span class="n">L</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>

<span class="c1"># --- Backward Pass ---
</span><span class="n">dL_dya</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">t</span>                             <span class="c1"># (No,)
</span><span class="n">dL_dV</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">o,h-&gt;oh</span><span class="sh">"</span><span class="p">,</span> <span class="n">dL_dya</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>    <span class="c1"># (No, Nh)
</span><span class="n">dL_dc</span> <span class="o">=</span> <span class="n">dL_dya</span>                             <span class="c1"># (No,)
</span><span class="n">dL_dh</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">oh,o-&gt;h</span><span class="sh">"</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dL_dya</span><span class="p">)</span>    <span class="c1"># (Nh,)
</span><span class="n">sigmoid_grad</span> <span class="o">=</span> <span class="n">h</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span>                 <span class="c1"># (Nh,)
</span><span class="n">dL_dha</span> <span class="o">=</span> <span class="n">dL_dh</span> <span class="o">*</span> <span class="n">sigmoid_grad</span>              <span class="c1"># (Nh,)
</span><span class="n">dL_dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">h,i-&gt;hi</span><span class="sh">"</span><span class="p">,</span> <span class="n">dL_dha</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>    <span class="c1"># (Nh, Ni)
</span><span class="n">dL_db</span> <span class="o">=</span> <span class="n">dL_dha</span>                             <span class="c1"># (Nh,)
</span>
<span class="c1"># --- Update parameters (gradient descent) ---
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">W</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dL_dW</span>
<span class="n">b</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dL_db</span>
<span class="n">V</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dL_dV</span>
<span class="n">c</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dL_dc</span>
</code></pre></div></div> <p><strong>Question</strong></p> <p>When computing<br/> \(\frac{\partial L}{\partial h} = V^\top \frac{\partial L}{\partial y_a},\) using <code class="language-plaintext highlighter-rouge">einsum</code>, does <code class="language-plaintext highlighter-rouge">einsum</code> implicitly transpose tensors?<br/> Should I change the index order (e.g. use <code class="language-plaintext highlighter-rouge">"ho"</code> instead of <code class="language-plaintext highlighter-rouge">"oh"</code>) to represent the transpose?</p> <p>**Answer **</p> <p>No. <strong><code class="language-plaintext highlighter-rouge">einsum</code> never performs implicit transposes.</strong><br/> All transposes must be expressed <strong>explicitly through index labels</strong>, not by rearranging axes incorrectly.</p> <p>In this example, the weight matrix is defined as:</p> \[V \in \mathbb{R}^{N_o \times N_h}, \quad \text{i.e. } V_{o h}.\] <p>The correct derivative is:</p> \[\frac{\partial L}{\partial h_h} = \sum_o V_{o h} \frac{\partial L}{\partial y_{a,o}}.\] <p>This maps <strong>directly</strong> to the following <code class="language-plaintext highlighter-rouge">einsum</code>:</p> <p>```python dL_dh = np.einsum(“oh, o-&gt;h”, V, dL_dya)</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[Simple MLP - Forward and Backward Pass Derivation]]></summary></entry><entry><title type="html">GPU Notes</title><link href="https://monishver11.github.io/blog/2025/gpu-notes/" rel="alternate" type="text/html" title="GPU Notes"/><published>2025-12-11T19:58:00+00:00</published><updated>2025-12-11T19:58:00+00:00</updated><id>https://monishver11.github.io/blog/2025/gpu-notes</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/gpu-notes/"><![CDATA[<h4 id="concepts"><strong>Concepts:</strong></h4> <ul> <li>Shared memory is divided into banks (usually 32 banks), so warp-level accesses can be parallelized if threads don’t conflict. If warp threads access different banks → all 32 loads happen in parallel. If multiple threads hit the same bank → bank conflict, serialized access, slower. That’s why memory access patterns matter (e.g. row-major vs col-major).</li> <li>There is an upper bound on the # threads that can within a block. Similarly, there is an upper bound on the # blocks per SM. This is because of the hardware as it needs to schedule the warps based on them and the hardware is fixed. SOTA is like 2048 threads per block for now. But, this upper bound is good enough and even higher for most applications.</li> <li>Compute capability: a property of the hardware</li> <li>Pay for branch divergence, instead of resolving idle SPs or inactive threads in a warp, as the former is less expensive.</li> <li>If you need a global/kernel level sync, then it’s a good point to cut to form a new kernel starting from that point.</li> <li>Don’t use __syncthreads in an if-else block, it leads to undefined behavior.</li> <li>CUDA only assigns a block to a SM, if all of the block resources are available beforehand. This is needed for zero context switch overhead for GPUs.</li> <li>Thread scheduling is totally outside our control. So, don’t make assumption on which warps finishes before other</li> <li>Maximize threads per SM to the upper bound, instead of # blocks per SM -&gt; latency hiding (how?)</li> <li>Warps are the unit of scheduling inside a SM and execution of a warp is the definition of SIMD.</li> <li>Branch prediction - applicable in every multi-core systems. In GPU, we try to avoid this if-else conditions, as it causes warp divergence.</li> <li>Latency hiding/ Keeping the gpu busy by switching between warps while other warps wait for data. This is also called latency tolerance.</li> <li>No. of warps and problems size are two key factors that play a important role in deciding the threads/block.</li> <li>Rule of thumb - use cuda get device properties and maximize threads/SM for max parallelism and best performance. Because this causes highest latency tolerance and as there will be some warps to execute.</li> <li>Maxing block/SM is not in your control, that scheduling happens at the chip level.</li> <li>Never make your code dependable on a hardware feature, like warp size.</li> <li>CGMA ratio - Compute to global memory access. Also called arithmetic intensity.</li> <li>Global memory persists for an application and not just the kernel execution</li> <li>Shared memory of a SM is virtualized, meaning if two blocks are assigned to a SM, then each block sees and accesses its own shared memory block only.</li> <li>Constant memory is written only by the host and its read only for the device. It’s much faster accessible than global memory. It’s accessed by everyone and in our direct control. The size of this memory is comparable to that of shared memory.</li> <li>Registers, shared memory, constant memory and global memory are in our direct control. Cache is not in our direct control/in-direct control.</li> <li>The life-time of a register is related to the thread that holds it.</li> <li>Each access to registers involves fewer machine instructions than global memory.</li> <li>Register files means group of registers inside a SM.</li> <li>Address space is the set of addresses your program can access.</li> <li>If in a kernel, I’ve int x, then every thread has that variable in its set of registers.</li> <li>If its <strong>shared</strong> int y, then every block in a SM, has their own y variable.</li> <li><strong>device</strong> int z, is in global memory, so everyone in the grid can access it.</li> <li>Automatic array variables local to a thread reside in local memory. This local memory doesn’t physically exist. It’s an abstractions to the local scope of a thread and put in global memory by the compiler. Again, since this is in global memory, its takes 100x more instructions to access this even though it appear local. So, try to make the variable put in registers as much as possible. Reduce this lmem(local memory) for performance improvements.</li> <li>Global memory access is a performance bottleneck. So, we try to reduce this. A common strategy is tiling, in which we partition the data we want to use into subset called tiles, such that each tile fits into the shared memory.</li> <li>In GPU, we want the performance/watt to be high and they’re quite good given the right problem.</li> <li> <p>Global synchronization across blocks of a grid isn’t possible and not expected. This is due to the possibility of deadlock and the fact that there can be many blocks waiting to be scheduled on the SM. In this case, blocks executing in a SM, will finish and reach the barrier, they wait for the others that are waiting to be scheduled in the SM, but can’t because the finished ones are waiting too and this is a deadlock.</p> </li> <li>Registers are faster, but those values are fetched from global memory. So, if I have a variable x in a thread execution, then each of x value for each thread will need to fetched by global memory. So, putting x in shared memory will only fetch once from the global memory. So, in this case putting in shared memory is best.</li> <li>A kernel that is memory bound is an indication that it must be optimized for performance. Always make sure the kernel is compute bound and most of its work is in this part. Also, to determine whether some kernel is compute/memory bound, we need both SW and HW details.</li> <li>In thread divergence, you lose 50%, even if 31 falls in if case and 1 falls in else case and 16 in if case and 16 in else case. The power consumption is double, as both cases are executed.</li> <li>System to device memory bandwidth link is a bottleneck. The GPU(SM &amp; shared memory) to device memory is better as its handled by NVIDIA an optimized for GPU usage.</li> <li>Sending large amount of data in one transfer is better than sending several small transfers. The reason is we avoid the overheads of the each individual transfer.</li> <li>Coalescing of the memory access is done at a per warp level, as the threads in the warp execute the same instructions at once.</li> <li>Accessing col wise in a 2D matrix of its memory locations is better than accessing the memory locations row wise. Draw a diagram to understand. So, in a matmul, accessing both col wise of its input is better. And there is a way to do, check about it. This is at a warp level and not at each individual thread level.</li> <li>Less global memory instructions -&gt; more opportunities to coalesce. Meaning, if there are less trips to the memory, we can get a coalesced block of data that we require in less trips.</li> <li>Making the memory accessed to be contiguous as much as possible. There is an alignment that is done by the OS, but as a programmer, you also need to make sure it’s aligned to get better performance. It reduces the no. of memory trips to the memory, there by increasing coalescing. Also, the cache picks a block of data from memory, so if the data you want is already contiguoos, you avoid a lot of cache misses.</li> <li>The # registers used by the thread/warp is directly based on the code we write, the variables and the structure determine it.</li> <li>Latency hiding - something must be executing while you’re retrieving the memory.</li> <li>If you’ve many instructions between the (instruction to memory access) and the actual usage, then for those many instructions you can schedule the computations, thus making it compute bound a little more.</li> <li> <p>Loop unrolling reduces the # instructions that are executed by the GPU assembly. Do this, once your code is already optimized for memory, resources etc. Follow the order of optimizations as we learned in class slides.</p> </li> <li>Reduce global memory trips using tiling, shared memory etc.</li> <li>Data prefetching becomes less beneficial as thread granularity increases. The reason is the shared memory is already used by the threads in this case to the max.</li> <li>What if we don’t know the shared memory size beforehand? Dynamic shared memory. Even here you need to know the size before kernel launch, but after the program launch. And you can’t have more than one dynamic allocation like this explicitly. But, we can play with pointers, but that’s not used much as the shared memory is small for this sophistication.</li> <li>If you’ve two files (c/c++) with int x; Then the linker, picks one at random, if we want to run both files. This can be a source of bug. To avoid this, we use extern keyword to tell the linker where to check for.</li> <li>The memory is cut into pieces to avoid it being big, power-hungry and slow. So, we’ve many pieces called banks. How about the address spaces for each?</li> <li>Shared memory accesses that span b distinct banks yield an effective bandwidth that is b times as high as the bandwidth of when accesses map to the same bank. Shared memory is now 32 banks with fixed width of four bytes. Consecutive four-byte data map to consecutive banks. So, if the threads is a warp access contiguous memory locations, it’ll take the memory from the 32 banks, which are independent and results b times higher than accessing the same bank. So, memory coalescing is important in this aspect too.</li> <li>Shared memory bank conflicts - since the width of the bank is minimal, only one at a time is possible for transfer, so it takes more time than if the accesses are from different banks which can be accessed in parallel.</li> <li>IEEE floating point standard (single-precision): (–1)^sign x (1+mantissa) x 2^(exponent – 127)</li> <li>With just this encoding, we can’t define 0 in this format. So, we’ve 3 encoding schemes based on exp.</li> <li>In tensor cores, the FMA is between 3 matrices, multiply two and add with third. The multiply is single precision and the additions is half precision.</li> <li>In multiplying two vectors, parallel compute approach results in more accurate than serial approach, but FMA is the most accurate when compared to the exact value. The GPU always uses FMA and the CPU is serial, and that’s the reason why GPU is more favorable in-terms of accuracy of floating point operations than CPU. Also, SP’s are optimized to do FMA in a single cycle.</li> <li>Asynchronous = returns to host right-away and doesn’t wait for device. (Non-blocking).</li> <li>Some CUDA API calls and all kernel launches are asynchronous with respect to the host code. This means error-reporting is also asynchronous.</li> <li>If the kernel launch is non-blocking and if I have a mem copy to host from device is next statement after the kernel launch statement, then it might lead to error right, because the kernel isn’t done yet, but we’re trying to load the result from device to host. So, what happens under the hood is that, there is a queue(streams) maintained in the device, and this queue holds the kernel launch first and then the mem copy, and the queue’s function will be executed sequentially, this is the reason, the prior works.</li> <li>You can have many queues, called streams and with CUDA, you’ve control to put functions in different streams. Until now, we’ve dealt with only 1 stream, so all operations are executed sequentially in that queue of the device.</li> <li>Operations in different streams can be interleaved and, when possible, they can even run concurrently. We can have several open streams to the same device at once. Need GPUs with concurrent transfer/execution capability.</li> <li>All operations to non-default streams are non-blocking wrt the host code.</li> <li>Some times for correctness reasons, you need to synchronize the host code with operations in a stream.</li> <li>Three options: cudaDeviceSynchronize() → blocks host for all streams, cudaStreamSynchronize(stream) → blocks host for this particular stream and cudaStreamQuery(stream) → does not block host for this stream.</li> <li>Streams are good way to overlap execution and transfer, hardware permits.</li> <li>Accessing host memory from device without explicit copy is called zero-copy mechanism. If it’s a small data access, then zero-copy is better, as it avoids the overhead of transferring the data to global memory and then accessing it within the device.</li> <li>As we program GPUs we need to pay attention to several performance bottlenecks: <ul> <li>Branch diversion</li> <li>Global memory latency</li> <li>Global memory bandwidth</li> <li>Shared memory bank conflicts</li> <li>Communication</li> <li>Limited resources</li> </ul> </li> <li>We have several techniques in our arsenal to enhance performance <ul> <li>Try to make threads in the same warp follow the same control flow</li> <li>Tiling</li> <li>Coalescing</li> <li>Loop unrolling</li> <li>Increase thread granularity</li> <li>Trade one resource for another</li> <li>Memory access pattern</li> <li>Streams</li> </ul> </li> <li>_syncthreads() - within one block - inside a kernel - waits for all threads in the same block to reach that point before continuing.</li> <li>cudaDeviceSynchronize() - across entire device - one host side - waits until all previously launched kernels on the GPU are complete.</li> <li> <p>cudaEvent_t - calculates the actual GPU execution time (when GPU was busy running your kernels). Its measures pure GPU time.</p> </li> <li>General rule of thumb: as the problem size increases, increasing the # threads must be done and that needs to increase performance, and not amount of work done by each thread. This applies to parallel computing in general.</li> <li>Order of cost: communication &amp; memory access &gt; computation</li> <li>In a CUDA program, if we suspect an error has occurred during a kernel launch, then we must explicitly check for it after the kernel has executed.</li> <li>If debugging, compile with: nvcc -DDEBUG code.cu -o code. This invokes the cuda error handling implicitly.</li> <li> <p>MPI, OPENMP and CUDA - Multicore + MultiGPU communication setup. MPI is for Multi-node GPU communication.</p> </li> <li>Zero-copy: nothing is copied, instead accessed directly. Between host to device or device to device. Transfer happens between the other memory to the current device’s L2 cache directly.</li> <li>Unified virtual address: puts all the host, device memory of all devices into a single address space.</li> <li>Unified memory: creates a pool of managed memory that is shared between the CPU and GPU. Under the hood, the data(pages) automatically migrates from CPU to GPU and among GPU’s for which ever needs that data.</li> <li>All this, is for ease of use and not for performance reasons. So, sometimes manual control is better/best as in the prior you might not exactly know the actual place where the data resides.</li> <li>UM is built on top of UVA, and UM has this extra capability of data(page) movements.</li> <li>All this, is only with respect to the GPU’s global memory. The shared memory, tiling, L1 cache are still within and unique for each devices.</li> <li>Zero-Copy: use it when you have a small piece of data for reading a few times. Manual: complex pattern between host and device. If it’s structured regular thing, use unified memory.</li> <li>Within the UVA, when we need data, we use the copy… option, but it actually transfers the pages. Without, this the copy can easily fill up the space and its redundant.</li> <li>UM is performant than UVA.</li> <li>Coherence: will not allow writes to the same page at the same time, even when it leads to low performance.</li> <li>Dynamic parallelism: something related to nested code/recursion. In GPUs, it means, the kernel can start new kernel. Streams can spawn new streams. It permits dynamic run time decisions.</li> <li>Until the child kernel finishes, the parent kernel isn’t done, subject to the availability of the resources. If there’s less resources, then there is a possibility of a deadlock.</li> <li>To speed-up, start a dummy kernel first. Because, the stage setting takes time with the CUDA runtime, so the first kernel launch takes more time than the subsequent ones.</li> <li>CudaThreadSynchronize() - the device waits for the work to be done on another device, for the work sent to the child kernel by the parent kernel. Even without this, the parent waits till the child kernel finishes. But does the parent kernel block, or it does some work itself? Check??</li> <li>Alignment with Cuda Pitch for 2D arrays/kernels. CUDA ensures that each row starts at an address aligned to 64 B or 128 B multiples. Aligned memory → fewer memory transactions → higher bandwidth utilization.</li> <li>Cuda Compilation: CUDA file(.cu)-&gt; PTX(Intermediate representation) -&gt; SASS(or other assemblies) -&gt; CuBit(Cuda binary) -&gt; execute</li> <li>-arch: for virtual compute architecture for generation of PTX code.</li> <li>-code: specifies the actual device that will be targeted by SASS ad the cuBin binary.</li> <li>Without -code, the final form is PTX, so every time for the GPU code generation, it uses a JIT compiler.</li> <li>Fat binary: an executable or object file that contains multiple versions of GPU code. One or more machine-specific binaries (SASS), compiled for concrete GPU architectures like sm_30, sm_35, etc. Optionally, one or more PTX versions, virtual assembly for JIT-compilation on future GPUs.</li> <li> <p>When a CUDA kernel is launched, the driver checks the GPU’s SM version and looks in the fatbinary for a matching compiled binary (cubin). If it finds one, it runs it directly for maximum speed; otherwise, it falls back to the embedded PTX, JIT-compiles it into a cubin, and caches it (in ~/.nv/ComputeCache) for future use. This allows the driver to automatically choose the optimal version at runtime with no code changes needed.</p> </li> <li>In the nvcc cmd, Arch gets you the PTX for that architecture and code gives you the assembly and binary for your specific device.</li> <li>(-arch=compute_Xi works with -code=sm_Xj, where i&lt;=j) - Check</li> <li>With just -arch and without -code, it’ll give you the ptx only. And at runtime, it uses the JIT compiler to compile this into an assembly and binary as executable. So, it takes a bit more time.</li> <li>Write code, that’s wrap friendly(that reduces thread divergence) and cache friendly for good memory access. This can be cultivated with experience and system design thinking.</li> <li><strong>shfl_sync</strong>: fastest way to share data between threads within a warp, instead of going into shared memory or cache. This goes into low-level hardware and used by Nvidia libraries.</li> <li>Thread block cluster - a group of thread blocks. We introduce this a layer in between the blocks and grids. The next layer is grid with cluster. For a thread block cluster, all blocks within must be assigned to each of the SMs for the single thread block to be executed. The shared memory of all the blocks within a thread block cluster are accessible and called distributed shared memory. This needs C++, Cuda Blackwell architecture. One advantage is that this now can give us block level synchronization. - Check last point.</li> <li>How to reduce the performance dip because of AtomicAdd, but still maintaining the atomicity? Solution: Privatization. This is always good when we’ve severe collision. But, it’s costly. There’s overhead for creating and initializing private copies for each thread block and the overhead for accumulating the contents of private copies into the final copy. The benefit is much less contention and serialization in accessing both the private copies and the final copy. The overall performance can often be improved more than 10x. These private copies are stored in shared memory. Even in AtomicAdd, now we’ve variations based on the level of atomicity we want. For block level, we can use AtomicAdd_Block with privatization for best performance.</li> <li>What if the copy is too large to privatize? Sometimes one can partially privatize an output copy and use range testing to go to either global memory or shared memory.</li> </ul> <h4 id="lecture-doubts"><strong>Lecture doubts:</strong></h4> <ul> <li>If the kernel call API is non-blocking, then the after steps like bring the results back to host will start immediately, how is it possible? Or why kernel call API is non-blocking? - A: CUDA streams.</li> <li>What is codaMalloc(void<strong>) -&gt; what’s this void</strong> means? cudaMalloc() allocates memory on the GPU and writes the GPU address into your device pointer variable. To let CUDA modify that pointer, you must pass its address (i.e., a pointer to your pointer). Since cudaMalloc() expects a void<strong>, we cast our variable’s address — e.g., (void</strong>)&amp;d_curr — to match its signature. This cast simply tells CUDA, “here’s the address of my pointer; fill it with the device memory location.”</li> <li>As a programmer, do we have access to coalesce the memory access before the actual accessing from the memory?</li> <li>What is data prefetch? technique used to hide memory access latency by loading data into faster memory (like shared memory or registers) before it is actually needed for computation.</li> <li>What is presorting overhead in floating point operations?</li> <li>For mem copy from host to device, will it be done via pinned pages or the usage of pinned pages in in programmers control? If so, what are the specific api’s that allows us to do this?</li> <li>Nvidia doesn’t support backward compatibility??</li> <li>Access vs Transfer in GPU peer-to-peer ??. Access is unto L2 cache and transfer goes till the GPU global memory. Check more??</li> <li>What is zero copy and how its different from/related to peer-to-peer copy??</li> <li>cudaHostAlloc() ??</li> <li>By default, grids launched within a thread block are executed sequentially. • This happens even if grids are launched by different threads within the block. • To deal with this drawback → streams • streams created on the host cannot be used on the device. • Streams created in a block can be used by all threads in that block. ??</li> <li><strong>shfl_sync</strong> and its variations. How its used and what happens due to this under the hood and what evens without this?</li> <li>Q: If threads in a warp should access contiguous memory for performance, but shared memory accesses must avoid bank conflicts to be parallel, isn’t this contradictory?</li> <li>A: No, because these rules apply to two different memory systems. Contiguous access refers to global memory, where consecutive addresses allow requests from a warp to be coalesced into fewer DRAM transactions, maximizing bandwidth. Bank conflicts apply to shared memory, which is divided into banks; here, threads must access different banks to avoid serialization. In practice, an optimal kernel loads data from global memory using coalesced (contiguous) accesses, then rearranges it in shared memory into a layout that avoids bank conflicts for computation. Both conditions are required: coalescing ensures efficient data movement into the SM, while conflict-free shared memory ensures fast, parallel use of that data once it is on-chip.</li> </ul> <h4 id="c--cuda"><strong>C++ &amp; CUDA:</strong></h4> <ul> <li>std::shared_ptr<T> → smart pointer, auto memory cleanup.</T></li> <li>Custom deleter = control how memory is freed (e.g. cudaFree).</li> <li>Constructor initializer list (: h(h_), w(w_)) → compact way to set members.</li> <li>Templates (template <typename T="">) → write generic code for any type.</typename></li> <li>Macros (#define Index(…)) → text substitution, quick shorthand.</li> <li>Exceptions (throw std::runtime_error(“msg”)) → safe error reporting.</li> <li>Header files: <ul> <li>#include = “paste the file here.”</li> <li>&lt;…&gt; = system/standard headers</li> <li>“…” = local project headers</li> <li>#pragma once -&gt; Ensures header file is included only once and prevents duplicate definitions.</li> </ul> </li> <li>Files: <ul> <li>.hh / .hpp → C++ headers (project-specific)</li> <li>.cuh → CUDA headers (contain CUDA-related declarations)</li> <li>.cpp / .cu → source files (where you usually put bigger function definitions, kernels, training loops)</li> </ul> </li> <li>operator() makes an object callable like a function</li> <li>functors (function objects)</li> <li>kernels can’t take references (&amp;) — arguments must be passed by value</li> </ul>]]></content><author><name></name></author><category term="GPU-NYU"/><category term="GPU"/><summary type="html"><![CDATA[GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes]]></summary></entry><entry><title type="html">Big Data Storage</title><link href="https://monishver11.github.io/blog/2025/big-data-2-storage/" rel="alternate" type="text/html" title="Big Data Storage"/><published>2025-10-22T15:25:00+00:00</published><updated>2025-10-22T15:25:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-2-storage</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-2-storage/"><![CDATA[<ul> <li>“Big Data” is not new. Oil companies, telecommunications companies, and other data-centric industries have had huge datasets for a long time.</li> <li>As storage capacity continues to expand, today’s “big” is tomorrow’s “small.”</li> <li>“Big Data” is when the size of the data itself becomes part of the problem.</li> <li>Why is Big Data a problem? <ul> <li>Where can I store my company’s ever-growing data?</li> <li>How much is that going to cost?</li> <li>How am I going to manage all the hardware and software?</li> <li>Users are asking bigger questions - how can I provide compute power?</li> <li>And, I/O speed is a problem too.</li> </ul> </li> <li>One 10 TB hard disk drive (HDD) vs 10 TB of storage with 100 HDDs, which is better?</li> <li>10 TB of storage with 100 HDDs has an advantage of having one read/write head per drive. So, the whole disk read/write time reduces overall when compared to the whole disk read/write time of the single 10 TB HDD.</li> <li>In practice, multiple disk drives are installed in one server, sometimes as many as 12 or more. Each drive is 2 TB ~ 6 TB in size.</li> <li>It is important to match the speed of the drives to the processing power of the server, because the CPU can become the bottleneck.</li> <li>New problems: <ul> <li>How can I read the parts of my file simultaneously from multiple drives on multiple servers?</li> <li>How do I even know where the pieces of my file are, if they’re stored on multiple servers?</li> </ul> </li> <li>Before answering these questions, we need to know some of the big data storage concepts that are used and involved.</li> <li>Big Data analytics uses highly scalable distributed technologies and frameworks to analyze large volumes of data from diﬀerent sources.</li> <li>To store Big Data datasets, often in multiple copies, innovative storage strategies and technologies have been created to achieve cost-eﬀective and highly scalable storage solutions.</li> <li>We’ll introduce the following concepts: Clusters, distributed file systems, relational database management systems, NoSQL, sharding, replication, CAP theorem, ACID, and BASE.</li> <li>A cluster is a tightly coupled collection of servers (“nodes”).</li> <li>These servers usually have the same hardware specifications and are connected together via a network to work as a single unit.</li> <li>Each node in the cluster has its own dedicated resources, such as memory, a processor, and a hard drive.</li> <li>A cluster can execute a job by splitting it into small pieces (“tasks”) and distributing their execution onto diﬀerent computers that belong to the cluster.</li> <li> <p>Clusters -&gt; Racks -&gt; Nodes(servers)</p> </li> <li>Next is Distributed file systems (DFS).</li> <li>A file is the most basic unit of storage to store data.</li> <li>A file system (FS) is the method of organizing files on a storage device.</li> <li>A DFS is a file system that can store large files spread across the nodes of a cluster. E.g., Google File System (GFS), Hadoop Distributed File System (HDFS).</li> <li>Next is Relational database management systems (RDBMS).</li> <li>A RDBMS is a product that presents a view of data as a collection of rows and columns.</li> <li>SQL (structured query language) is used for querying and maintaining the database.</li> <li>A transaction symbolizes a unit of work performed against a database, and treated in a coherent and reliable way independent of other transactions.</li> <li>Next is NoSQL.</li> <li>A Not-only SQL (NoSQL) database is a non-relational database that is highly scalable, fault-tolerant and specifically designed to house semi-structured and unstructured data.</li> <li> <p>E.g., Key-value store: Redis, Dynamo. Document store: MongoDB, CouchDB. Wide column store: Bigtable, HBase, Cassandra. Graph store: Pregel, Giraph.</p> </li> <li>Next is Sharding.</li> <li>Sharding is the process of horizontally partitioning a large dataset into a collection of smaller, more manageable datasets (“shards”).</li> <li>Each shard is stored on a separate node, and is responsible for only the data stored on it.</li> <li>All shards share the same schema. They collectively represent the complete dataset.</li> <li>How does sharding work in practice? <ul> <li>Each shard can independently service reads and writes for the specific subset of data that it is responsible for.</li> <li>Depending on the query, data may need to be fetched from both shards.</li> </ul> </li> <li>Benefits of sharding: <ul> <li>Sharding allows the distribution of processing loads across multiple nodes to achieve horizontal scalability.</li> <li>Sharding provides partial tolerance toward failures. In case of a node failure, only data stored on that node is aﬀected.</li> </ul> </li> <li>Concerns with sharding: <ul> <li>Queries requiring data from multiple shards will impose performance penalties.</li> <li>To mitigate such performance issues, data locality keeps commonly accessed data co-located on a single shard. This idea leads to the concept of replication.</li> </ul> </li> <li>Replication stores multiple copies of a dataset (“replicas”) on multiple nodes.</li> <li> <p>There are two methods of replication: Master-slave replication and Peer-to-peer replication.</p> </li> <li><strong>Master-slave replication</strong> <ul> <li>All data is written to a master node.</li> <li>Once saved, the data is replicated over to multiple slave nodes.</li> <li>Write requests, including insert, update and delete, occur on the master node.</li> <li>Read requests can be fulfilled by any slave node.</li> <li>This is ideal for read intensive loads. Growing read demands can be managed by horizontal scaling to add more slave nodes.</li> <li>The writes are consistent. All writes are coordinated by the master node. However, write performance will suffer as the amount of writes increases.</li> <li>If the master node fails, reads are still possible via any of the slave nodes. But, writes are not supported until a master node is reestablished.</li> <li>For recovery, we resurrect the master node from a backup or choose a new master node from the slave nodes.</li> <li>There is a concern of read inconsistency.</li> <li>Ex: User A updates data. The data is copied over to Slave A by the Master. Before the data is copied over to Slave B, User B tries to read the data from Slave B, which results in an inconsistent read. The data will eventually become consistent when Slave B is updated by the Master.</li> <li>There are other solutions as well, but we’ll see those as we go and later.</li> </ul> </li> <li><strong>Peer-to-peer replication</strong> <ul> <li>All nodes (“peers”) operate at the same level.</li> <li>Each peer is equally capable of handling reads and writes.</li> <li>Each write is copied to all peers.</li> <li>In this replication strategy, we might face both read and write inconsistency.</li> <li>Read inconsistency: User A updates data. The data is copied over to Peer A and Peer B. Before the data is copied over to Peer C, User B tries to read the data from Peer C, resulting in an inconsistent read. The data will eventually be updated on Peer C, and the database will once again become consistent.</li> <li>Write inconsistency: A simultaneous update of the same data may happen across multiple peers.</li> <li>Strategies to resolve these: <ul> <li>Pessimistic concurrency is a proactive strategy. It uses locking to ensure that only one update to a record can occur at a time. However, this is detrimental to availability since the database record being updated remains unavailable until all locks are released.</li> <li>Based on what is held by the lock and who holds the lock, there are many different ways of achieving this strategy. There can be write locks (exclusive locks) and read locks(shared locks), and the unit of locking depends on the system and its users, it could be fine-grained like at a record level, or coarse-grained at a table level. And since its peer-to-peer, the lock can be managed by centralized lock manager (as a service by zookeeper) or distributed lock manager (using a distributed consensus protocol like Paxos, Raft) or other application-level ownership.</li> <li>Optimistic concurrency is a reactive strategy that does not use locking. Instead, it allows inconsistency to occur with knowledge that eventually consistency will be achieved after all updates have propagated.</li> </ul> </li> </ul> </li> <li>Sharding vs replication <ul> <li>Actually, both sharding and replication can be used together.</li> <li>We can combine, sharding and master-slave replication, sharding and peer-to-peer replication or any other commendations.</li> </ul> </li> <li><strong>Sharding and master-slave replication</strong></li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharding_ms-480.webp 480w,/assets/img/sharding_ms-800.webp 800w,/assets/img/sharding_ms-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/sharding_ms.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="sharding_ms" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>Sharding and peer-to-peer replication</strong></li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharding_pp-480.webp 480w,/assets/img/sharding_pp-800.webp 800w,/assets/img/sharding_pp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/sharding_pp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="sharding_pp" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>CAP theorem</strong> <ul> <li>A distributed database system may wish to provide three guarantees.</li> <li>Consistency: a read request from any node results in the same, most recently written data across multiple nodes.</li> <li>Availability: a read/write request will always be acknowledged in the form of a success or a failure.</li> <li>Partition tolerance: the database system can tolerate communication outages that split the cluster into multiple silos and can still service read/write requests.</li> <li>CAP theorem states that a distributed database system can only provide two of the three properties. C+A+P is not possible.</li> <li>Although communication outages are rare and temporary, partition tolerance (P) must be supported by a distributed database.</li> <li>Therefore, CAP is generally a choice between choosing C+P or A+P.</li> <li>Below explains how things can go wrong in each CAP combinations.</li> <li>C + A (no Partition Tolerance): When a network partition happens, nodes can’t communicate to stay consistent. Since the system isn’t partition-tolerant, it must shut down or reject requests, leading to unavailability.</li> <li>C + P (no Availability): During a partition, nodes stop serving requests until they can synchronize again to maintain consistency. This causes temporary unavailability, as the system pauses updates to avoid conflicts.</li> <li>A + P (no Consistency): When a partition occurs, all nodes continue serving requests independently. Because they can’t coordinate, updates may diverge, producing inconsistent or stale data until the partition heals and replicas reconcile.</li> <li>In summary: Choosing C + A means all nodes must stay in sync, but the system fails if a partition occurs. Choosing C + P ensures data consistency across partitions, but some nodes may become unavailable. Choosing A + P keeps the system running despite partitions, but nodes may serve inconsistent data until synchronization occurs.</li> </ul> </li> <li><strong>ACID</strong> <ul> <li>ACID is a traditional database design principle on transaction management.</li> <li>It stands for Atomicity, Consistency, Isolation and Durability.</li> <li>Traditional databases leverages pessimistic concurrency controls (i.e., locking) to provide ACID guarantees.</li> <li>Atomicity ensures that all transactions will always succeed or fail completely. In other words, there are no partial transactions.</li> <li>Ex: A user attempts to update three records as a part of a transaction. Two records are successfully updated before the occurrence of an error. As a result, the database rolls back any partial eﬀects of the transaction and puts the system back to its prior state.</li> <li>Consistency ensures that only data that conforms to the constraints of the database schema can be written to the database.</li> <li>Ex: A user attempts to update the “amount” column of the table that is of type “float” with a value of type “varchar.” The database rejects this update because the value violates the constraint checks for the “amount” column.</li> <li>Note: consistency here is different from the consistency in a distributed database system.</li> <li>Isolation ensures that the results of a transaction are not visible to other operations until it is complete.</li> <li>Ex: User A attempts to update two records as part of a transaction. The database successfully updates the first record. However, before it can update the second record, User B attempts to update the same record. The database does not permit User B’s update until User A’s update succeeds or fails completely. This occurs because the record with id = 3 is locked by the database until the transaction is complete.</li> <li>Durability ensures that the results of a transaction are permanent, regardless of any system failure.</li> <li>Ex: A user updates a record as part of a transaction. The database successfully updates the record. Right after this update, a power failure occurs. The database maintains its state while there is no power. The power is resumed. The database serves the record as per last update when requested by the user.</li> <li>This ACID property relates to the CAP theorem in a way that the database systems providing traditional ACID guarantees choose consistency over availability. So it ensures C+P.</li> <li>Ex: User A attempts to update a record as part of a transaction. The database validates the value and the update is successfully applied. After the successful completion of the transaction, when Users B and C request the same record, the database provides the updated value to both the users.</li> </ul> </li> <li><strong>BASE</strong> <ul> <li>BASE (pun intended) is a database design principle leveraged by many distributed database systems.</li> <li>It stands for Basically Available, Soft state and Eventual consistency.</li> <li>When a database supports BASE, it favors availability over consistency. So, it ensures A+P.</li> <li>BASE leverages optimistic concurrency by relaxing the strong consistency constraints mandated by the ACID properties.</li> <li>Basically available means that the database will always acknowledge a client’s request.</li> <li>This database is basically available, even though it has been partitioned as a result of a network failure. It can just return a failure response for the user request in this case.</li> <li>Soft state means that a database may be in an inconsistent state when data is read.</li> <li>The results may change if the same data is requested again.</li> <li>Ex: User A updates a record on Peer A. Before the other peers are updated, User B requests the same record from Peer C. The database is now in a soft state, and stale data is returned to User B.</li> <li>Eventual consistency means that the database only attains consistency once the changes have been propagated to all nodes.</li> <li>Ex: User A updates a record. The record only gets updated at Peer A, but before the other peers can be updated, User B requests the same record. The database is now in a soft state. Stale data is returned to User B from Peer C. However, the consistency is eventually attained, and User C gets the correct value.</li> </ul> </li> <li><strong>ACID vs BASE</strong> <ul> <li>ACID ensures immediate consistency at the expense of availability due to the record locking.</li> <li>BASE emphasizes availability over immediate consistency.</li> <li>This soft approach toward consistency allows BASE-compliant databases to serve multiple clients without any latency though serving inconsistent results.</li> <li>However, BASE-compliant databases are not useful for transactional systems where lack of consistency is a concern and needed.</li> <li>A distributed database system may choose to provide some ACID properties.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><category term="RBDA"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 2]]></summary></entry><entry><title type="html">Introduction to Realtime and Big Data Analytics</title><link href="https://monishver11.github.io/blog/2025/big-data-1-intro/" rel="alternate" type="text/html" title="Introduction to Realtime and Big Data Analytics"/><published>2025-10-22T15:23:00+00:00</published><updated>2025-10-22T15:23:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-1-intro</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-1-intro/"><![CDATA[<ul> <li>Big Data is a field dedicated to the analysis, processing, and storage of large collections of data that frequently originate from disparate sources.</li> <li>Big Data addresses distinct requirements: I) Combine multiple unrelated datasets. II) Process large amounts of unstructured data. III) Harvest hidden information in a time-sensitive manner.</li> <li>A dataset is a collection or group of related data.</li> <li>Each dataset member (“datum”) shares the same set of attributes or properties as others in the same dataset.</li> <li>Ex: An extract of rows from a database table stored in a CSV formatted file.</li> <li>Data analysis is the process of examining data to find facts, relationships, patterns, insights and/or trends.</li> <li>The overall goal of data analysis is to support better decision-making.</li> <li>Data analytics is a discipline that includes the management of the complete data lifecycle, which encompasses collecting, cleaning, organizing, storing, analyzing and governing data.</li> <li>Ex: In business-oriented environments, data analytics results can lower operational costs and facilitate strategic decision-making. In the scientific domain, data analytics can help identify the cause of a phenomenon to improve the accuracy of predictions. In service-based environments, data analytics can help strengthen the focus on delivering high-quality services by driving down costs.</li> <li>There are four general categories of analytics that are distinguished by the results they produce: descriptive, diagnostic, predictive and prescriptive analysis.</li> <li>Descriptive analytics aim to answer questions about events that have already occurred. Descriptive analytics contextualizes data to generate information.</li> <li>The operational systems (e.g., OLTP, CRM, ERP) are queried via descriptive analytics tools to generate static reports or dashboards.</li> <li>Diagnostic analysts aim to determine the cause of a phenomenon that occurred in the past using questions that focus on the reason behind the event.</li> <li>The goal is to determine what information is related to the phenomenon in order to answer questions that seek to determine why something has occurred.</li> <li>Diagnostic analytics usually collect data from multiple sources and store it in a structure (e.g., OLAP) so that users can perform interactive drill-down and roll-up analysis.</li> <li>Predictive analytics aim to determine the outcome of an event that might occur in the future.</li> <li>Information is associated to build models that are used to generate future predictions based upon past events.</li> <li>Predictive analytics use large datasets of internal and external data and various data analysis techniques to provide user-friendly front-end interfaces.</li> <li>Prescriptive analytics build upon the results of predictive analytics by prescribing actions that should be taken.</li> <li>The focus is not only on what prescribed option is best to follow, but why.</li> <li>Prescriptive analytics use business rules and large amounts of internal and external data to simulate outcomes and prescribe the best course of action.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bd-1-480.webp 480w,/assets/img/bd-1-800.webp 800w,/assets/img/bd-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/bd-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bd-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Big data characteristics, the five V’s: Volume, Velocity, Variety, Veracity and Value.</li> <li>Veracity refers to the quality of data. Data with a high signal-to-noise ratio has more veracity.</li> <li>Value is defined as the usefulness of data for an enterprise.</li> <li>Value is also impacted by data lifecycle-related concerns, like how well has the data been stored? Were valuable attributes of data removed during data cleansing? Are the right types of questions being asked during data analysis? Are the results of the analysis being accurately communicated to the appropriate decision-makers?</li> <li>There are different types of data. Based on source, we’ve Human-generated data and Machine-generated data. Based on format, we’ve structured, unstructured, semi-structured data and metadata.</li> <li>Structured data conforms to a data model or schema. It is used to capture relationships between different entities and is therefore most often stored in relational database. Ex: banking transactions, invoices, customer records, etc.</li> <li>Unstructured data doesn’t conform to a data model or schema. It is either textual or binary and often conveyed via files that are self-contained and non-relational. The majority of data is unstructured. Ex: textual data, video, image files, audio, etc.</li> <li>Semi-structured data has a defined level of structure and consistency, but is not relational in nature. Instead, it is hierarchical or graph based. This kind of data is commonly stored in text-based files. Ex: XML data, JSON data, sensor data(CSV), etc.</li> <li>Metadata provides information about a dataset’s characteristics and structure. It is mostly machine-generated and can be appended to data. Ex: XML tags providing the author and creation date of a document, Attributes providing the file size and resolution of a digital photograph, etc.</li> <li>Big data solutions need to support multiple formats and types of data.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><category term="RBDA"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 1]]></summary></entry><entry><title type="html">GPU Essentials - A Concise Technical Guide</title><link href="https://monishver11.github.io/blog/2025/GPU-Intro/" rel="alternate" type="text/html" title="GPU Essentials - A Concise Technical Guide"/><published>2025-09-23T04:46:00+00:00</published><updated>2025-09-23T04:46:00+00:00</updated><id>https://monishver11.github.io/blog/2025/GPU-Intro</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/GPU-Intro/"><![CDATA[<p>This is a concise GPU introduction I found helpful. With it, you can start CUDA programming and understand the basic terms you’ll encounter. These notes are adapted from <a href="https://pages.cs.wisc.edu/~markhill/restricted/ieeemicro10_gpu.pdf">this article</a>, which was itself inspired by Jen-Hsun Huang’s keynote at Hot Chips 21 in 2009. Although the article was published in 2010—15 years ago—the GPU architecture concepts and terminology haven’t changed much. Modern GPUs include additional features for improved performance, but the fundamentals remain largely the same.</p> <hr/> <p>With the GPU’s rapid evolution from a configurable graphics processor to a programmable parallel processor, the ubiquitous GPU in every PC, laptop, desktop, and workstation is a many-core multi-threaded multiprocessor that excels at both graphics and computing applications.</p> <h4 id="gpu-computings-evolution"><strong>GPU computing’s evolution</strong></h4> <ul> <li>Rendering high-definition graphics scenes is a problem with tremendous inherent parallelism. A graphics programmer writes a single-thread program that draws one pixel, and the GPU runs multiple instances of this thread in parallel—drawing multiple pixels in parallel.</li> <li>Also, GPU computing programs—written in C or C++ with the CUDA parallel computing model, or using a parallel computing API inspired by CUDA such as Direct- Compute or OpenCL — scale transparently over a wide range of parallelism. Software scalability, too, has enabled GPUs to rapidly increase their parallelism and performance with increasing transistor density.</li> <li>Evolving to modern GPUs involved adding programmability incrementally—from fixed function pipelines to microcoded processors, configurable processors, programmable processors, and scalable parallel processors.</li> <li>GPUs first used floating-point arithmetic to calculate 3D geometry and vertices, then applied it to pixel lighting and color values to handle high-dynamic-range scenes and to simplify programming.</li> <li>The GeForce 6800 scalable processor core architecture facilitated multiple GPU implementations with different numbers of processor cores.</li> <li>Early GPGPU computing programs achieved high performance, but were difficult to write because programmers had to express non-graphics computations with a graphics API such as OpenGL.</li> <li>The GeForce 8800 introduced in 2006 featured the first unified graphics and computing GPU architecture7,8 programmable in C with the CUDA parallel computing model, in addition to using DX10 and OpenGL.</li> <li>Its unified streaming processor cores executed vertex, geometry, and pixel shader threads for DX10 graphics programs, and also executed computing threads for CUDA C programs.</li> <li>Hardware multithread- ing enabled the GeForce 8800 to efficiently execute up to 12,288 threads concurrently in 128 processor cores.</li> <li>NVIDIA deployed the scalable architecture in a family of GeForce GPUs with different numbers of processor cores for each market segment.</li> <li>The GeForce 8800 was the first GPU to use scalar thread processors rather than vector processors, matching standard scalar languages like C, and eliminating the need to manage vector registers and program vector operations.</li> <li>(# Note: Scalar processors execute one operation per thread on a single data element, while vector processors execute the same operation on multiple data elements at once, requiring explicit vector instructions #)</li> <li>It added instructions to support C and other general-purpose languages, including integer arithmetic, IEEE 754 floating-point arithmetic, and load/store memory access instructions with byte addressing.</li> <li>It provided hardware and instructions to support parallel computation, communication, and synchronization—including thread arrays, shared memory, and fast barrier synchronization.</li> <li>(# Note: Fast barrier synchronization is a mechanism that quickly pauses threads in a block until all have reached the same point, ensuring they proceed together without race conditions #)</li> <li>NVIDIA introduced the third-generation Fermi GPU computing architecture in 2009.</li> <li>Fermi implemented IEEE 754-2008 and significantly increased double-precision performance. It added error-correcting code (ECC) memory protection for large-scale GPU computing, 64-bit unified addressing, cached memory hierarchy, and instructions for C, C++, Fortran, OpenCL, and DirectCompute.</li> <li>The GPU computing ecosystem is expanding rapidly, enabled by the deployment of more than 180 million CUDA-capable GPUs.</li> <li>NVIDIA developed the parallel Nsight GPU development environment, debugger, and analyzer integrated with Microsoft Visual Studio.</li> </ul> <h4 id="cuda-scalable-parallel-architecture"><strong>CUDA scalable parallel architecture</strong></h4> <ul> <li>CUDA is a hardware and software coprocessing architecture for parallel computing that enables NVIDIA GPUs to execute programs written with C, C++, Fortran, OpenCL, DirectCompute, and other languages.</li> <li>Because most languages were designed for one sequential thread, CUDA preserves this model and extends it with a minimalist set of abstractions for expressing parallelism. This lets the programmer focus on the important issues of parallelism—how to design efficient parallel algorithms—using a familiar language.</li> <li>By design, CUDA enables the development of highly scalable parallel programs that can run across tens of thousands of concurrent threads and hundreds of processor cores.</li> <li>A compiled CUDA program executes on any size GPU, automatically using more parallelism on GPUs with more processor cores and threads.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-1-480.webp 480w,/assets/img/gpu-1-800.webp 800w,/assets/img/gpu-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>A CUDA program is organized into a host program, consisting of one or more sequential threads running on a host CPU, and one or more parallel kernels suitable for execution on a parallel computing GPU. A kernel executes a sequential program on a set of lightweight parallel threads. As Figure 1 shows, the programmer or compiler organizes these threads into a grid of thread blocks. The threads comprising a thread block can synchronize with each other via barriers and communicate via a high-speed, per-block shared memory.</li> <li>Threads from different blocks in the same grid can coordinate via atomic operations in global memory space shared by all threads. Sequentially dependent kernel grids can synchronize via global barriers and coordinate via global shared memory.</li> <li>CUDA requires that thread blocks be independent, which provides scalability to GPUs with different numbers of processor cores and threads.</li> <li><mark style="background: #FFF3A3A6;">Thread blocks implement coarse-grained scalable data parallelism, while the light-weight threads comprising each thread block provide fine-grained data parallelism. Thread blocks executing different kernels implement coarse-grained task parallelism. Threads executing different paths implement fine-grained thread-level parallelism.</mark></li> <li>(# Note: Imagine a restaurant kitchen — multiple kitchens (thread blocks) each cook the same dish in parallel = coarse-grained data parallelism; within one kitchen, many chefs (threads) chop ingredients simultaneously = fine-grained data parallelism; if different kitchens prepare entirely different dishes = coarse-grained task parallelism; if chefs in the same kitchen follow slightly different recipes = fine-grained thread-level parallelism #)</li> <li>(# Note: Think of a university — each class (thread block) works on the same assignment = coarse-grained data parallelism; within a class, each student (thread) solves a small part of the assignment = fine-grained data parallelism; if different classes work on different subjects = coarse-grained task parallelism; if students in the same class take different approaches to solving a problem = fine-grained thread-level parallelism #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-2-480.webp 480w,/assets/img/gpu-2-800.webp 800w,/assets/img/gpu-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Figure 2 shows some basic features of parallel programming with CUDA. It contains sequential and parallel implementations of the SAXPY routine defined by the basic linear algebra subroutines (BLAS) library.</li> <li>The serial implementation is a simple loop that computes one element of y per iteration. The parallel kernel executes each of these independent iterations in parallel, assigning a separate thread to compute each element of y.</li> <li>The __ global __ modifier indicates that the procedure is a kernel entry point, and the extended function-call syntax saxpy«&lt;B, T»&gt;(. . .) launches the kernel saxpy() in parallel across B blocks of T threads each.</li> <li>Each thread determines which element it should process from its integer thread block index blockIdx.x, its thread index within its block threadIdx.x, and the total number of threads per block blockDim.x.</li> <li>This example demonstrates a common parallelization pattern, where we can transform a serial loop with independent iterations to execute in parallel across many threads.</li> <li>In the CUDA paradigm, the programmer writes a scalar program—the parallel saxpy() kernel—that specifies the behavior of a single thread of the kernel. This lets CUDA leverage standard C language with only a few small additions, such as built-in thread and block index variables.</li> </ul> <h4 id="gpu-computing-architecture"><strong>GPU computing architecture</strong></h4> <ul> <li>To address different market segments, GPU architectures scale the number of processor cores and memories to implement different products for each segment while using the same scalable architecture and software.</li> <li>NVIDIA’s scalable GPU computing architecture varies the number of streaming multi-processors to scale computing performance, and varies the number of DRAM memories to scale memory bandwidth and capacity.</li> <li><mark style="background: #FFF3A3A6;">Each multithreaded streaming multiprocessor provides sufficient threads, processor cores, and shared memory to execute one or more CUDA thread blocks. The parallel processor cores within a streaming multi-processor execute instructions for parallel threads.</mark></li> <li>(# Note: Picture a library — the building is a streaming multiprocessor (SM), the reading tables inside are processor cores, and the shared bookshelf is shared memory. A group of students (a thread block) comes in; they can sit across tables, use the shared books, and study in parallel. Multiple groups can use the same library if resources allow #)</li> <li><mark style="background: #FFF3A3A6;">Multiple streaming multiprocessors provide coarse-grained scalable data and task parallelism to execute multiple coarse-grained thread blocks (possibly running different kernels) in parallel.</mark></li> <li>(# Note: Imagine a city with many libraries (multiple SMs). Each library can host different study groups (thread blocks). Some groups may study the same subject = data parallelism, while others study different subjects = task parallelism. Because there are many libraries, multiple groups can work in parallel at a larger scale #)</li> <li><mark style="background: #FFF3A3A6;">Multithreading and parallel-pipelined processor cores within each streaming multiprocessor implement fine-grained data and thread-level parallelism to execute hundreds of fine-grained threads in parallel.</mark></li> <li>(# Note: Think of an assembly line in a factory — each worker (core) handles a specific step, and many items (threads) move through simultaneously. Because there are many workers and multiple lines, hundreds of small tasks get done in parallel with no idle time = fine-grained parallelism #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-3-480.webp 480w,/assets/img/gpu-3-800.webp 800w,/assets/img/gpu-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>To illustrate GPU computing architecture, Figure 3 shows the third-generation Fermi computing architecture configured with 16 streaming multiprocessors, each with 32 CUDA processor cores, for a total of 512 cores.</li> <li><mark style="background: #FFF3A3A6;">The GigaThread work scheduler distributes CUDA thread blocks to streaming multiprocessors with available capacity, dynamically balancing the computing workload across the GPU, and running multiple kernel tasks in parallel when appropriate.</mark></li> <li>The multi-threaded streaming multiprocessors schedule and execute CUDA thread blocks and individual threads.</li> <li>Each streaming multiprocessor executes up to 1,536 concurrent threads to help cover long latency loads from DRAM memory. As each thread block completes executing its kernel program and releases its streaming multiprocessor resources, the work scheduler assigns a new thread block to that streaming multiprocessor.</li> <li>The PCIe host interface connects the GPU and its DRAM memory with the host CPU and system memory.</li> <li>The streaming multiprocessor threads access system memory via the PCIe interface, and CPU threads access GPU DRAM memory via PCIe.</li> <li><mark style="background: #FFF3A3A6;">The GPU architecture balances its parallel computing power with parallel DRAM memory controllers designed for high memory bandwidth.</mark></li> <li>Fermi introduces a parallel cached memory hierarchy for load, store, and atomic memory accesses by general applications.</li> <li>Each streaming multiprocessor has a first-level (L1) data cache, and the streaming multi- processors share a common 768-Kbyte unified second-level (L2) cache.</li> <li>The L2 cache connects with six 64-bit DRAM interfaces and the PCIe interface, which connects with the host CPU, system memory, and PCIe devices.</li> <li>It caches DRAM memory locations and system memory pages accessed via the PCIe interface.</li> <li>The unified L2 cache services load, store, atomic, and texture instruction requests from the streaming multiprocessors and requests from their L1 caches, and fills the streaming multiprocessor instruction caches and uniform data caches.</li> <li>(# Note: In GPU graphics, a texture is an image or data map applied to a 3D object’s surface. Texture instructions fetch and manipulate this data efficiently; in computing, “texture memory” can also be used as a read-only cached memory space optimized for certain access patterns #)</li> <li>Fermi implements a 40-bit physical address space that accesses GPU DRAM, CPU system memory, and PCIe device addresses. It provides a 40-bit virtual address space to each application context and maps it to the physical address space with translation lookaside buffers and page tables.</li> <li>Fermi ECC corrects single-bit errors and detects double-bit errors in the DRAM memory, GPU L2 cache, L1 caches, and streaming multiprocessor registers.</li> <li>The ECC lets us integrate thousands of GPUs in a system while maintaining a high mean time between failures (MTBF) for high-performance computing and super-computing systems.</li> <li>(# Note: Mean Time Between Failures (MTBF) is the average time a system or component operates before a failure occurs. Higher MTBF indicates more reliable hardware, crucial when thousands of GPUs work together in HPC systems #)</li> </ul> <h4 id="streaming-multiprocessor"><strong>Streaming multiprocessor</strong></h4> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-4-480.webp 480w,/assets/img/gpu-4-800.webp 800w,/assets/img/gpu-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The streaming multiprocessor implements zero-overhead multithreading and thread scheduling for up to 1,536 concurrent threads.</li> <li>(# Note: The SM supports zero-overhead multithreading by maintaining hardware context for each thread—registers, program counter, and state—so switching between 1,536 threads happens instantly without saving/restoring state. For example, if an SM has 32 cores and each warp has 32 threads, it can manage 48 warps concurrently: 32 × 48 = 1,536 threads #)</li> <li><mark style="background: #FFF3A3A6;">To efficiently manage and execute this many individual threads, the multiprocessor employs the single-instruction multiple-thread (SIMT) architecture introduced in the first unified computing GPU.</mark></li> <li>The SIMT instruction logic creates, manages, schedules, and executes concurrent threads in groups of 32 parallel threads called warps.</li> <li><mark style="background: #FFF3A3A6;">A CUDA thread block comprises one or more warps. Each Fermi streaming multiprocessor has two warp schedulers and two dispatch units that each select a warp and issue an instruction from the warp to 16 CUDA cores, 16 load/store units, or four SFUs.</mark></li> <li>(# Note: Imagine a classroom divided into smaller study groups (warps). The teacher (warp scheduler) chooses one group at a time and gives them an instruction. The students in that group then split into roles: some write (cores), some fetch books (load/store units), and some handle special tricky problems (SFUs). This shows how warps are managed and distributed to different execution resources #)</li> <li><mark style="background: #FFF3A3A6;">Because warps execute independently, the streaming multiprocessor can issue two warp instructions to appropriate sets of CUDA cores, load/store units, and SFUs.</mark></li> <li>To support C, C++, and standard single-thread programming languages, each streaming multiprocessor thread is independent, having its own private registers, condition codes and predicates, private per-thread memory and stack frame, instruction address, and thread execution state.</li> <li>The SIMT instructions control the execution of an individual thread, including arithmetic, memory access, and branching and control flow instructions.</li> <li><mark style="background: #FFF3A3A6;">For efficiency, the SIMT multiprocessor issues an instruction to a warp of 32 independent parallel threads.</mark></li> <li><mark style="background: #FFF3A3A6;">The streaming multiprocessor realizes full efficiency and performance when all threads of a warp take the same execution path.</mark></li> <li>If threads of a warp diverge at a data-dependent conditional branch, execution serializes for each branch path taken, and when all paths complete, the threads converge to the same execution path.</li> <li>Parallel thread execution (PTX) instructions describe the execution of a single thread in a parallel CUDA program.</li> <li>The PTX instructions focus on scalar (rather than vector) operations to match standard scalar programming languages.</li> <li><mark style="background: #FFF3A3A6;">Each pipelined CUDA core executes a scalar floating point or integer instruction per clock for a thread. With 32 cores, the streaming multiprocessor can execute up to 32 arithmetic thread instructions per clock.</mark></li> <li>The integer unit implements 32-bit precision for scalar integer operations, including 32-bit multiply and multiply-add operations, and efficiently supports 64-bit integer operations.</li> <li>The Fermi CUDA core floating-point unit implements the IEEE 754-2008 floating-point arithmetic standard for 32-bit single-including fused multiply-add (FMA) instructions.</li> <li>FMA computes D = A * B + C with no loss of precision by retaining full precision in the intermediate product and addition, then rounding the final sum to form the result.</li> <li>Using FMA enables fast division and square-root operations with exactly rounded results.</li> <li>Fermi raises the throughput of 64-bit double-precision operations to half that of single precision operations, a dramatic improvement over the T10 GPU.</li> <li>The SFUs execute 32-bit floating-point instructions for fast approximations of reciprocal, reciprocal square root, sin, cos, exp, and log functions.</li> <li>The streaming multiprocessor load/store units execute load, store, and atomic memory access instructions.</li> <li><mark style="background: #FFF3A3A6;">A warp of 32 active threads presents 32 individual byte addresses, and the instruction accesses each memory address. The load/store units coalesce 32 individual thread accesses into a minimal number of memory block accesses.</mark></li> <li>(# Note: Picture 32 people each ordering one item from a store. Instead of processing 32 separate trips, the store groups the orders into as few bulk deliveries as possible. This is how memory coalescing works — combining many small requests into fewer large, efficient ones #)</li> <li>(# Note: Think of 32 friends each mailing a letter to the same neighborhood. Instead of sending 32 separate mail trucks, the post office bundles the letters and sends them together in one truck. That’s memory coalescing — merging many nearby requests into one efficient transfer #)</li> <li>Fermi implements a unified thread address space that accesses the three separate parallel memory spaces of Figure 1: per-thread local, per-block shared, and global memory spaces.</li> <li><mark style="background: #FFF3A3A6;">A unified load/store instruction can access any of the three memory spaces, steering the access to the correct memory, which enables general C and C++ pointer access anywhere.</mark></li> <li>Fermi provides a terabyte 40-bit unified byte address space, and the load/store ISA supports 64-bit byte addressing for future growth. The ISA also provides 32-bit addressing instructions when the program can limit its accesses to the lower 4 Gbytes of address space.</li> <li>On-chip shared memory provides low-latency, high-bandwidth access to data shared by cooperating threads in the same CUDA thread block.</li> <li>Fast shared memory significantly boosts the performance of many applications having predictable regular addressing patterns, while reducing DRAM memory traffic.</li> <li>Fermi introduces a configurable-capacity L1 cache to aid unpredictable or irregular memory accesses, along with a configurable-capacity shared memory.</li> <li>Each streaming multiprocessor has 64 Kbytes of on-chip memory, configurable as 48 Kbytes of shared memory and 16 Kbytes of L1 cache, or as 16 Kbytes of shared memory and 48 Kbytes of L1 cache.</li> </ul> <h4 id="cpugpu-co-processing"><strong>CPU+GPU co-processing</strong></h4> <ul> <li>Heterogeneous CPU+GPU co-processing systems evolved because the CPU and GPU have complementary attributes that allow applications to perform best using both types of processors.</li> <li>CUDA programs are coprocessing programs—serial portions execute on the CPU, while parallel portions execute on the GPU. Coprocessing optimizes total application performance.</li> <li><mark style="background: #FFF3A3A6;">With coprocessing, we use the right core for the right job. We use a CPU core (optimized for low latency on a single thread) for a code’s serial portions, and we use GPU cores (optimized for aggregate throughput on a code’s parallel portions) for parallel portions of code.</mark></li> <li>This approach gives more performance per unit area or power than either CPU or GPU cores alone.</li> <li>The comparison in Table 2 illustrates the advantage of CPU+GPU coprocessing using Amdahl’s law.</li> <li>(# Note: Amdahl’s Law predicts the maximum speedup of a program using multiple processors, based on the fraction of the code that must run sequentially. Even if 90% of a program is parallelizable, the remaining 10% limits total speedup, showing why a CPU+GPU combination can outperform a pure GPU for mixed workloads #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-5-480.webp 480w,/assets/img/gpu-5-800.webp 800w,/assets/img/gpu-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The table compares the performance of four configurations: <ul> <li>a system containing one latency-optimized (CPU) core,</li> <li>a system containing 500 throughput-optimized (GPU) cores,</li> <li>a system containing 10 CPU cores, and</li> <li>a coprocessing system that contains a single CPU core and 450 GPU cores.</li> </ul> </li> <li>Table 2 assumes that a CPU core is 5 faster and 50 the area of a GPU core—numbers consistent with contemporary CPUs and GPUs. The coprocessing system devotes 10 percent of its area to the single CPU core and 90 percent of its area to the 450 GPU cores.</li> <li>The coprocessing architecture is the fastest on both programs.</li> <li><mark style="background: #FFF3A3A6;">On the parallel-intensive program, the coprocessing architecture is slightly slower on the parallel portion than the pure GPU configuration (0.44 seconds versus 0.40 seconds) but more than makes up for this by running the tiny serial portion 5 faster (1 second versus5 seconds). The heterogeneous architecture has an advantage over the pure throughput- optimized configuration here because serial performance is important even for mostly parallel codes.</mark></li> <li>Even on mostly sequential codes, it’s more efficient to run the code’s parallel portion on a throughput-optimized architecture.</li> <li>The coprocessing architecture provides the best performance across a wide range of the serial fraction because it uses the right core for each task.</li> <li>By using a latency- optimized CPU to run the code’s serial fraction, it gives the best possible performance on the serial fraction—which is important even for mostly parallel codes.</li> <li>By using throughput-optimized cores to run the code’s parallel portion, it gives near- optimal performance on the parallel fraction as well—which becomes increasingly important as codes become more parallel.</li> <li>It’s wasteful to use large, inefficient latency-optimized cores to run parallel code segments.</li> </ul> <h4 id="application-performance"><strong>Application performance</strong></h4> <ul> <li>Many applications consist of a mixture of fundamentally serial control logic and inherently parallel computations.</li> <li><mark style="background: #FFF3A3A6;">Furthermore, these parallel computations are frequently data-parallel in nature. This directly matches the CUDA coprocessing programming model, namely a sequential control thread capable of launching a series of parallel kernels.</mark></li> <li><mark style="background: #FFF3A3A6;">The use of parallel kernels launched from a sequential program also makes it relatively easy to parallelize an application’s individual components rather than rewrite the entire application.</mark></li> <li>(# Note: Imagine renovating a house — instead of rebuilding the whole house from scratch, you can work on individual rooms in parallel (kitchen, bathroom, bedroom) while the overall house structure stays the same. Similarly, parallel kernels let you speed up parts of a program without rewriting the entire application #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-6-480.webp 480w,/assets/img/gpu-6-800.webp 800w,/assets/img/gpu-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Table 3 lists some representative applications along with the runtime speedups obtained for the whole application using CPU+GPU coprocessing over CPU alone, as measured by application developers.</li> <li>The speedups using GeForce 8800, Tesla T8, GeForce GTX 280, Tesla T10, and GeForce GTX 285 range from 9 to more than 130 , with the higher speedups reflecting applications where more of the work ran in parallel on the GPU.</li> <li>The lower speedups—while still quite attractive—represent applications that are limited by the code’s CPU portion, coprocessing overhead, or by divergence in the code’s GPU fraction.</li> <li>The speedups achieved on this diverse set of applications validate the programmability of GPUs—in addition to their performance.</li> <li>Applications with dense matrices, sparse matrices, and arbitrary pointer structures have all been successfully implemented in CUDA with impressive speedups. Similarly, applications with diverse control structures and significant data-dependent control, such as ray tracing, have achieved good performance in CUDA.</li> <li>Many real-world applications (such as interactive ray tracing) are composed of many different algorithms, each with varying degrees of parallelism.</li> <li>(# Note: Ray tracing is a graphics technique that simulates the path of light rays to produce realistic images with reflections, shadows, and refractions. Each ray’s calculation is independent, making it highly parallelizable on GPUs #)</li> <li>OptiX, our interactive ray-tracing software developer’s kit built in the CUDA architecture, provides a mechanism to control and schedule a wide variety of tasks on both the CPU and GPU.</li> <li>Some tasks are primarily serial and execute on the CPU, such as compilation, data structure management, and coordination with the operating system and user interaction.</li> <li>Other tasks, such as building an acceleration structure or updating animations, may run either on the CPU or the GPU depending on the choice of algorithms and the performance required.</li> <li>The net result is that CPUþGPU coprocessing enables fast, interactive ray tracing of complex scenes while you watch, which is an application that researchers previously considered too irregular for a GPU.</li> <li>GPU computing is at the tipping point. <mark style="background: #FFF3A3A6;">Single-threaded processor performance is no longer scaling at historic rates.</mark></li> <li>Thus, we must use parallelism for the increased performance required to deliver more value to users.</li> <li><mark style="background: #FFF3A3A6;">A GPU that’s optimized for throughput delivers parallel performance much more efficiently than a CPU that’s optimized for latency.</mark></li> <li>A heterogeneous coprocessing architecture that combines a single latency-optimized core (a CPU) with many throughput-optimized cores (a GPU) performs better than either alternative alone.</li> <li>This is because it uses the right processor for the right job—the CPU for serial sections and critical paths and the GPU for the parallel sections.</li> <li>In high-performance computing, technical computing, and consumer media processing, CPU+GPU coprocessing has become the architecture of choice.</li> <li>GPU architecture will evolve to further increase the span of applications that it can efficiently address.</li> <li>GPU cores will not become CPUs—they will continue to be optimized for throughput, rather than latency.</li> <li><mark style="background: #FFF3A3A6;">However, they will evolve to become more agile and better able to handle arbitrary control and data access patterns.</mark></li> </ul> <hr/> <h4 id="minor-details-that-are-frequently-misunderstood"><strong>Minor details that are frequently misunderstood:</strong></h4> <ul> <li>A thread block always executes on one SM. Multiple smaller thread blocks may be present on one SM. There are more threads than execution units (“cuda cores”) on an SM which means not every thread gets to schedule a new instruction each clock cycle. That’s okay because threads often wait for memory or floating point operations that take multiple clock cycles to finish – <a href="https://stackoverflow.com/users/17167312/homer512" title="14,961 reputation">Homer512</a></li> <li><a href="https://stackoverflow.com/questions/64624793/warp-and-block-scheduling-in-cuda-what-exactly-happens-and-questions-about-el">Warp and block scheduling in CUDA - what exactly happens, and questions about eligible warps</a></li> <li><a href="https://stackoverflow.com/questions/62147624/how-many-cuda-cores-is-used-to-process-a-cuda-warp">How many CUDA cores is used to process a CUDA warp?</a></li> <li><a href="https://stackoverflow.com/questions/76678083/confusion-around-no-of-cuda-cores-and-the-number-of-parallel-threads">Confusion around no of CUDA Cores and the number of parallel threads</a></li> </ul> <h4 id="references"><strong>References:</strong></h4> <ul> <li><a href="https://modal.com/gpu-glossary/readme">Modal - GPU Glossary</a> - Read</li> <li><a href="https://www.modular.com/blog/democratizing-compute-part-2-what-exactly-is-cuda">What exactly is “CUDA”? (Democratizing AI Compute, Part 2)</a> - Read</li> </ul>]]></content><author><name></name></author><category term="GPU-NYU"/><category term="GPU"/><summary type="html"><![CDATA[A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.]]></summary></entry><entry><title type="html">Wrapping Up Our ML Foundations Journey</title><link href="https://monishver11.github.io/blog/2025/wrapping-ml-basics/" rel="alternate" type="text/html" title="Wrapping Up Our ML Foundations Journey"/><published>2025-05-17T19:45:00+00:00</published><updated>2025-05-17T19:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/wrapping-ml-basics</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/wrapping-ml-basics/"><![CDATA[<p>After completing 51 comprehensive blog posts on machine learning fundamentals, it’s time to wrap up this series. This journey has been both challenging and immensely rewarding.</p> <p><strong>There were several key motivations behind creating this content:</strong></p> <ol> <li>To provide a <strong>structured approach</strong> that methodically builds intuition for a solid foundational understanding of machine learning</li> <li>To explain complex concepts in an <strong>accessible way</strong> that’s easy to remember and articulate</li> <li>To ensure <strong>mathematical rigor</strong> is maintained while keeping explanations clear and thorough</li> <li>To create a resource I wish I had when first learning these concepts</li> </ol> <p>I believe I’ve accomplished these goals, and I’m genuinely satisfied with the outcome.</p> <p><strong>Looking back at our path:</strong></p> <ul> <li>We began with essential <strong>mathematical prerequisites</strong> (multivariate calculus, linear algebra, probability theory) to build a strong foundation</li> <li>Progressed to the <strong>fundamentals of machine learning</strong> with supervised learning and empirical risk minimization</li> <li>Explored <strong>optimization techniques</strong> through gradient descent and stochastic gradient descent</li> <li>Examined various <strong>loss functions and regularization approaches</strong> to understand model development</li> <li>Delved into <strong>linear models</strong> and their extensions with SVMs and margin classifiers</li> <li>Advanced to <strong>nonlinear feature maps and kernels</strong> to tackle more complex problems</li> <li>Investigated <strong>probabilistic modeling and Bayesian approaches</strong> for a different perspective on machine learning</li> <li>Addressed <strong>multiclass classification</strong> methods and structured prediction</li> <li>Introduced <strong>decision trees</strong> as our first truly non-linear classifiers</li> <li>Concluded with <strong>ensemble methods</strong> from bagging and random forests to various boosting algorithms, culminating with gradient boosting</li> </ul> <p>This progression represents a logical flow from fundamentals to advanced concepts, carefully designed to build upon each previous lesson.</p> <p><strong>I have several plans for the future:</strong></p> <ol> <li> <p>I intend to cover the <strong>foundational aspects of deep learning</strong> in a similar structured fashion, though not immediately. I will ensure this content aligns well with the machine learning material we’ve already covered.</p> </li> <li> <p>Following advice from a friend, I recognize the importance of focusing on <strong>depth in ML and practical applications</strong>. I’ll be dedicating time to projects that enhance my practical experience and will share insights as they develop.</p> </li> <li> <p>I welcome suggestions for <strong>specific topics</strong> you’d like to see broken down in this style, or if you have interesting ML ideas for collaboration. Feel free to DM me.</p> </li> <li> <p>With summer break approaching, I’ll be taking some time to <strong>rest and recharge</strong> after this challenging but highly educational semester.</p> </li> </ol> <p>I’d like to express my sincere gratitude to:</p> <ul> <li>My professors for their excellent teaching of these complex subjects, especially Professor <a href="https://mengyeren.com/">Mengye Ren</a>, who taught me this course.</li> <li>Everyone who supported me throughout this journey</li> <li>You, the readers, for your engagement and feedback</li> </ul> <h5 id="final-thoughts"><strong>Final Thoughts</strong></h5> <p>This project began with the aim of clarifying machine learning concepts for myself and others. As we progressed from basic mathematical foundations all the way to advanced ensemble methods like gradient boosting, I hope these explanations have helped demystify machine learning and provided you with both theoretical understanding and practical insights.</p> <p>The complete list of topics is available at: <a href="https://monishver11.github.io/blog/category/ml-nyu/">ML-NYU Category</a> or <a href="https://drive.google.com/file/d/1t1r7w_0gSJIcaEATQlAn1gyMU8yM582v/view?usp=sharing">This list</a></p> <p>Until we meet again in future learning adventures, keep exploring, stay curious, and never stop learning!</p> <hr/>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.]]></summary></entry><entry><title type="html">Gradient Boosting in Practice</title><link href="https://monishver11.github.io/blog/2025/gb-in-practice/" rel="alternate" type="text/html" title="Gradient Boosting in Practice"/><published>2025-05-15T01:10:00+00:00</published><updated>2025-05-15T01:10:00+00:00</updated><id>https://monishver11.github.io/blog/2025/gb-in-practice</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/gb-in-practice/"><![CDATA[<p>In the previous post, we introduced Gradient Boosting with logistic loss, discussing how weak learners can be sequentially combined to minimize differentiable loss functions using functional gradient descent. Now, we turn our attention to how this elegant theory translates into robust, high-performing models in practice, particularly focusing on regularization techniques that help control overfitting and improve generalization.</p> <hr/> <h5 id="preventing-overfitting-in-gradient-boosting"><strong>Preventing Overfitting in Gradient Boosting</strong></h5> <p>While <strong>gradient boosting</strong> is often surprisingly resistant to overfitting compared to other ensemble methods like bagging, it is not immune. Its resilience stems from several key characteristics of the boosting process:</p> <ul> <li> <p><strong>Implicit Feature Selection</strong>:<br/> One of the reasons gradient boosting is relatively resistant to overfitting is that it performs <strong>feature selection implicitly</strong> during training. At each boosting iteration, the algorithm fits a weak learner often a decision tree to the current pseudo-residuals (the gradients of the loss).</p> <p>Decision trees naturally perform feature selection: they evaluate all available features and choose the one that offers the best split (i.e., the greatest reduction in loss). This means that only the most predictive features are chosen at each step. Over multiple rounds, this leads to an ensemble that selectively and adaptively focuses on the most informative parts of the input space.</p> <p>As a result, even in high-dimensional settings or with noisy features, gradient boosting can often avoid overfitting by not relying too heavily on irrelevant or redundant features. It prioritizes features that consistently contribute to reducing the loss, acting like a built-in greedy feature selection mechanism.</p> </li> <li> <p><strong>Localized Additive Updates</strong>:<br/> In gradient boosting, each weak learner (such as a shallow decision tree) is trained to correct the mistakes of the current model. Because these learners are typically low-capacity (e.g., stumps or small trees), their predictions only affect specific regions of the input space - they’re “localized” in that sense.</p> <p>As boosting progresses, the model becomes more accurate, and the residuals (errors) it tries to fix get smaller and more focused. Consequently, the subsequent learners make increasingly smaller and more targeted updates. This means that instead of making large, sweeping changes to the model, later learners adjust predictions only in regions where the model is still wrong.</p> <p>This gradual, fine-grained updating process helps avoid overfitting, as the model doesn’t overreact to noise or outliers - it incrementally improves where it matters most.</p> </li> </ul> <p>Together, these mechanisms help gradient boosting maintain a balance between flexibility and generalization. However, it can still overfit when models are overly complex or trained for too many iterations - which is why regularization techniques such as shrinkage, subsampling, and tree size constraints are essential in practice.</p> <ul> <li> <p><strong>Shrinkage</strong>: Use a small learning rate (step size) \(\lambda\) to scale the contribution of each weak learner:</p> \[f_m(x) = f_{m-1}(x) + \lambda \cdot v_m h_m(x)\] <p>Smaller \(\lambda\) slows down the learning process, often resulting in better generalization.</p> </li> <li><strong>Stochastic Gradient Boosting</strong>: Instead of using the full dataset at each boosting round, randomly sample a subset of training examples.</li> <li><strong>Feature (Column) Subsampling</strong>: Randomly select a subset of input features for each boosting iteration.</li> </ul> <p>These methods inject randomness into the learning process and reduce variance, both of which help mitigate overfitting.</p> <h5 id="step-size-as-regularization"><strong>Step Size as Regularization</strong></h5> <p>One of the simplest and most effective ways to regularize gradient boosting is by adjusting the <strong>step size</strong> or learning rate. This directly controls how far the model moves in the direction of the negative gradient at each step. Smaller step sizes lead to slower learning, but allow the model to build more nuanced approximations of the target function.</p> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-5-480.webp 480w,/assets/img/gb-5-800.webp 800w,/assets/img/gb-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Left is training set &amp; right is validation set </div> <p>This effect is clearly observed in tasks like <strong>Sinc function regression</strong>(which we saw in the last post - <a href="https://monishver11.github.io/blog/2025/binomial-boost/">BinomialBoost</a>), where the number of boosting rounds and the choice of shrinkage rate dramatically affect both training and validation performance. Lower learning rates typically result in better generalization when combined with more boosting rounds.</p> <h5 id="stochastic-gradient-boosting"><strong>Stochastic Gradient Boosting</strong></h5> <p>To improve efficiency and reduce overfitting, we can employ <strong>stochastic gradient boosting</strong> - a strategy analogous to minibatch gradient descent in optimization. So in minibatch, for each boosting round:</p> <ul> <li>Randomly sample a subset of the training data.</li> <li>Fit the base learner using this subset.</li> <li>Use it to approximate the gradient and update the model.</li> </ul> <p>Benefits of this approach include:</p> <ul> <li><strong>Regularization</strong>: Injecting randomness into the training process helps prevent overfitting.</li> <li><strong>Efficiency</strong>: Training is faster per iteration since we work with fewer data points.</li> <li><strong>Improved performance</strong>: Empirically, this often leads to better results for the same computational budget.</li> </ul> <h5 id="column-feature-subsampling"><strong>Column (Feature) Subsampling</strong></h5> <p>Another effective technique borrowed from Random Forests is <strong>column or feature subsampling</strong>. For each boosting round:</p> <ul> <li>Randomly select a subset of input features.</li> <li>Learn a weak learner using only this subset.</li> </ul> <p>This further reduces correlation between individual learners and acts as a strong regularizer. In fact, the <strong>XGBoost</strong> paper emphasizes that column subsampling can reduce overfitting <strong>even more</strong> effectively than row subsampling.</p> <p>Additionally, limiting the number of features also improves training speed, especially for high-dimensional datasets.</p> <hr/> <h5 id="summary-gradient-boosting-as-a-versatile-framework"><strong>Summary: Gradient Boosting as a Versatile Framework</strong></h5> <p>Let’s step back and summarize the key takeaways:</p> <ul> <li><strong>Motivation</strong>: Combine many weak learners (e.g., shallow trees) to build a strong predictor.</li> <li><strong>Statistical view</strong>: Fit an additive model greedily, one function at a time.</li> <li><strong>Optimization view</strong>: Boosting makes local improvement iteratively and perform gradient descent in function space.</li> </ul> <p>Gradient Boosting is a <strong>flexible and powerful meta-algorithm</strong>:</p> <ul> <li>Supports <strong>any differentiable loss function</strong></li> <li>Applicable to <strong>classification, regression, ranking, multiclass tasks</strong>, and more</li> <li>Highly <strong>scalable</strong> with implementations like <strong>XGBoost</strong>, <strong>LightGBM</strong>, and <strong>CatBoost</strong></li> </ul> <p>With proper regularization, including shrinkage, stochastic updates, and feature subsampling - Gradient Boosting becomes one of the most effective tools in the machine learning toolkit.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.]]></summary></entry><entry><title type="html">BinomialBoost</title><link href="https://monishver11.github.io/blog/2025/binomial-boost/" rel="alternate" type="text/html" title="BinomialBoost"/><published>2025-05-10T00:58:00+00:00</published><updated>2025-05-10T00:58:00+00:00</updated><id>https://monishver11.github.io/blog/2025/binomial-boost</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/binomial-boost/"><![CDATA[<p>In the previous post, we introduced the <strong>Gradient Boosting framework</strong> as functional gradient descent, where we minimize a loss function by iteratively adding base learners that approximate the negative gradient (pseudo-residuals) of the loss. We demonstrated this with the squared loss, where residuals had a direct and intuitive interpretation. In this post, we extend that idea to <strong>logistic loss</strong>, which is more appropriate for binary classification tasks. This special case is often referred to as <strong>BinomialBoost</strong>.</p> <hr/> <h5 id="logistic-loss-and-pseudo-residuals"><strong>Logistic Loss and Pseudo-Residuals</strong></h5> <p>For binary classification with labels \(Y = \{-1, 1\}\), the <strong>logistic loss</strong> is given by:</p> \[\ell(y, f(x)) = \log(1 + e^{-y f(x)})\] <p>At each boosting iteration, we need to compute the <strong>pseudo-residuals</strong>, which are the negative gradients of the loss with respect to the model’s prediction. For the \(i\)-th training example:</p> \[\begin{aligned} r_i &amp;= - \frac{\partial}{\partial f(x_i)} \ell(y_i, f(x_i)) \\ &amp;= - \frac{\partial}{\partial f(x_i)} \log(1 + e^{-y_i f(x_i)}) \\ &amp;= y_i \cdot \frac{e^{-y_i f(x_i)}}{1 + e^{-y_i f(x_i)}} \\ &amp;= \frac{y_i}{1 + e^{y_i f(x_i)}} \end{aligned}\] <p>These pseudo-residuals guide the model by indicating how each example’s prediction should be adjusted to reduce the classification loss.</p> <h5 id="step-direction-and-boosting-update"><strong>Step Direction and Boosting Update</strong></h5> <p>Once we compute the pseudo-residuals \(r_i\), the next base learner \(h_m \in \mathcal{H}\) is fit to match them in a least squares sense:</p> \[h_m = \arg\min_{h \in \mathcal{H}} \sum_{i=1}^n \left( \frac{y_i}{1 + e^{y_i f_{m-1}(x_i)}} - h(x_i) \right)^2\] <p>The model is then updated as:</p> \[f_m(x) = f_{m-1}(x) + \nu h_m(x)\] <p>where \(\nu \in (0, 1]\) is the learning rate or shrinkage parameter and \(f_{m-1}(x)\) is prediction after \(m−1\) rounds.</p> <h5 id="gradient-tree-boosting"><strong>Gradient Tree Boosting</strong></h5> <p>A particularly effective version of gradient boosting uses <strong>regression trees</strong> as base learners. The hypothesis space is:</p> \[\mathcal{H} = \{ \text{regression trees with } S \text{ terminal nodes} \}\] <ul> <li>\(S = 2\) corresponds to decision stumps, a very simple weak learner that makes a prediction based on a single feature threshold.</li> <li>Larger values of \(S\) allow more expressive trees, capable of capturing more complex interactions among features.</li> <li>Common choices for tree size: \(4 \leq S \leq 8\)</li> </ul> <p>Gradient Tree Boosting combines the predictive power of decision trees with the optimization capabilities of functional gradient descent. Each tree fits the pseudo-residuals (i.e., the gradient of the loss), and the overall model evolves by sequentially adding these trees with appropriate scaling (via step size or shrinkage).</p> <p>This approach is widely known as <strong>Gradient Tree Boosting</strong> and is implemented in various software packages:</p> <ul> <li><strong>R</strong>: <code class="language-plaintext highlighter-rouge">gbm</code></li> <li><strong>scikit-learn</strong>: <code class="language-plaintext highlighter-rouge">GradientBoostingClassifier</code>, <code class="language-plaintext highlighter-rouge">GradientBoostingRegressor</code></li> <li><strong>XGBoost</strong>, <strong>LightGBM</strong>: state-of-the-art libraries for scalable, high-performance boosting</li> </ul> <h5 id="visual-example"><strong>Visual Example;</strong></h5> <div class="row justify-content-center"> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-3-480.webp 480w,/assets/img/gb-3-800.webp 800w,/assets/img/gb-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As a simple regression example, we can use an ensemble of decision stumps to fit a noisy version of the sinc function using squared loss. Even shallow learners (depth-1 trees) become powerful when combined via boosting. Here’s what the model looks like after different boosting rounds:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-4-480.webp 480w,/assets/img/gb-4-800.webp 800w,/assets/img/gb-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Decision stumps with 1,10,50, and 100 steps </div> <p>The shrinkage parameter \(\lambda = 1\) is used in this example to simplify learning, though smaller values are typically preferred in practice to prevent overfitting.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>With all the pieces in place, we can summarize what’s needed to implement Gradient Boosting (also known as <strong>AnyBoost</strong>):</p> <ul> <li>A differentiable loss function: e.g., squared loss for regression, logistic loss for classification</li> <li>A base hypothesis space: e.g., regression trees of fixed depth</li> <li>A gradient descent procedure in function space</li> <li>Hyperparameters: step size \(\nu\), number of boosting rounds \(M\), tree size \(S\)</li> </ul> <p>This general and flexible framework can adapt to a wide variety of tasks, making Gradient Boosting one of the most versatile and powerful tools in modern machine learning.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[See how the gradient boosting framework naturally extends to binary classification using the logistic loss.]]></summary></entry><entry><title type="html">Gradient Boosting / “Anyboost”</title><link href="https://monishver11.github.io/blog/2025/gradient-boosting/" rel="alternate" type="text/html" title="Gradient Boosting / “Anyboost”"/><published>2025-05-08T16:56:00+00:00</published><updated>2025-05-08T16:56:00+00:00</updated><id>https://monishver11.github.io/blog/2025/gradient-boosting</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/gradient-boosting/"><![CDATA[<p>In our previous post, we explored how <strong>Forward Stagewise Additive Modeling (FSAM)</strong> with exponential loss recovers the AdaBoost algorithm. But FSAM is not limited to exponential loss — it can be extended to <strong>any differentiable loss</strong>, leading to the powerful and flexible framework of <strong>Gradient Boosting Machines (GBMs)</strong>.</p> <p>This post walks through the derivation of gradient boosting, starting with the squared loss, and builds toward a general functional gradient descent interpretation of boosting.</p> <hr/> <h5 id="fsam-with-squared-loss"><strong>FSAM with Squared Loss</strong></h5> <p>Let’s begin with FSAM using squared loss. At the \(m\)-th boosting round, we wish to find a new function \(v h(x)\) to add to the model \(f_{m-1}(x)\). The objective becomes:</p> \[J(v, h) = \frac{1}{n} \sum_{i=1}^n \left( y_i - \underbrace{[f_{m-1}(x_i) + v h(x_i)]}_{\text{new model}} \right)^2\] <p>If the hypothesis space \(\mathcal{H}\) is closed under rescaling (i.e., if \(h \in \mathcal{H}\) implies \(v h \in \mathcal{H}\) for all \(v \in \mathbb{R}\)), we can drop \(v\) and simplify the objective:</p> \[J(h) = \frac{1}{n} \sum_{i=1}^n \left( \underbrace{[y_i - f_{m-1}(x_i)]}_{\text{residual}} - h(x_i) \right)^2\] <p>This is just <strong>least squares regression on the residuals</strong> — fit \(h(x)\) to approximate the residuals \([y_i - f_{m-1}(x_i)]\).</p> <h5 id="interpreting-the-residuals"><strong>Interpreting the Residuals</strong></h5> <p>Let’s take a closer look at how <strong>residuals relate to gradients</strong> in the context of boosting with squared loss.</p> <p>The objective for squared loss is:</p> \[J(f) = \frac{1}{n} \sum_{i=1}^n (y_i - f(x_i))^2\] <p>This measures how far the model predictions \(f(x_i)\) are from the true labels \(y_i\) across the dataset.</p> <p>To minimize this objective, we can perform gradient descent. So we ask: <strong>what is the gradient of \(J(f)\) with respect to \(f(x_i)\)?</strong></p> <p>Let’s compute it:</p> \[\frac{\partial}{\partial f(x_i)} J(f) = \frac{\partial}{\partial f(x_i)} \left[ \frac{1}{n} \sum_{j=1}^n (y_j - f(x_j))^2 \right]\] <p>Because only the \(i\)-th term depends on \(f(x_i)\), this simplifies to:</p> \[\frac{\partial}{\partial f(x_i)} J(f) = \frac{1}{n} \cdot (-2)(y_i - f(x_i)) = -\frac{2}{n}(y_i - f(x_i))\] <p>The term \(y_i - f(x_i)\) is the <strong>residual</strong> at point \(x_i\). So we see:</p> <blockquote> <p>The residual is proportional to the <strong>negative gradient</strong> of the squared loss.</p> </blockquote> <p><strong>Why This Matters for Boosting</strong></p> <p>Gradient descent updates a parameter in the opposite direction of the gradient. Similarly, in <strong>gradient boosting</strong>, we update our model \(f\) in the direction of a function that tries to approximate the <strong>negative gradient</strong> at every data point.</p> <p>In the case of squared loss, this just means:</p> <ul> <li>Compute the residuals \(r_i = y_i - f(x_i)\).</li> <li>Fit a new base learner \(h_m\) to those residuals.</li> <li>Add \(h_m\) to the model: \(f \leftarrow f + v h_m\).</li> </ul> <p>This process mimics the behavior of <strong>gradient descent in function space</strong>.</p> <p>We can now draw an analogy:</p> <ul> <li><strong>Boosting update:</strong>  \(f \leftarrow f + \textcolor{red}{v} \textcolor{green}{h}\)</li> <li><strong>Gradient descent:</strong> \(f \leftarrow f - \textcolor{red}{\alpha} \textcolor{green}{\nabla_f J(f)}\)</li> </ul> <p><strong>Note: Observe the variables highlighted in red and green.</strong></p> <p>Where:</p> <ul> <li>\(h\) approximates the direction of steepest descent (the gradient),</li> <li>\(v\) is the step size (akin to learning rate \(\alpha\)),</li> <li>and the update improves the model’s predictions to reduce the loss.</li> </ul> <p>This perspective generalizes easily to other loss functions, which we explore in the next sections on <strong>functional gradient descent</strong>.</p> <hr/> <h5 id="functional-gradient-descent-intuition-and-setup"><strong>Functional Gradient Descent: Intuition and Setup</strong></h5> <p>To generalize FSAM to arbitrary (differentiable) loss functions, we adopt the <strong>functional gradient descent</strong> perspective.</p> <p>Suppose we have a loss function that depends on predictions \(f(x_i)\) at \(n\) training examples:</p> \[J(f) = \sum_{i=1}^n \ell(y_i, f(x_i))\] <p>Note that \(f\) is a function, but this loss only depends on \(f\) through its values on the training points. So, we can treat:</p> \[f = (f(x_1), f(x_2), \dots, f(x_n))^\top\] <p>as a vector in \(\mathbb{R}^n\), and write:</p> \[J(f) = \sum_{i=1}^n \ell(y_i, f_i)\] <p>where \(f_i := f(x_i)\). We want to minimize \(J(f)\) by updating our predictions \(f_i\) in the <strong>steepest descent direction</strong>.</p> <h5 id="unconstrained-step-direction-pseudo-residuals"><strong>Unconstrained Step Direction (Pseudo-Residuals)</strong></h5> <p>We compute the gradient of the loss with respect to each prediction:</p> \[g = \nabla_f J(f) = \left( \frac{\partial \ell(y_1, f_1)}{\partial f_1}, \dots, \frac{\partial \ell(y_n, f_n)}{\partial f_n} \right)\] <p>The <strong>negative gradient direction</strong> is:</p> \[-g = -\nabla_f J(f) = \left( -\frac{\partial \ell(y_1, f_1)}{\partial f_1}, \dots, -\frac{\partial \ell(y_n, f_n)}{\partial f_n} \right)\] <p>This tells us how to change each \(f(x_i)\) to decrease the loss — it’s the direction of steepest descent in \(\mathbb{R}^n\).</p> <p>We call this vector the <strong>pseudo-residuals</strong>. In the case of squared loss:</p> \[\ell(y_i, f_i) = (y_i - f_i)^2 \quad \Rightarrow \quad -g_i = y_i - f_i\] <p>So, pseudo-residuals coincide with actual residuals for squared loss.</p> <h5 id="projection-step-fitting-the-pseudo-residuals"><strong>Projection Step: Fitting the Pseudo-Residuals</strong></h5> <p>We now want to update the function \(f\) by stepping in direction \(-g\). However, we can’t directly take a step in \(\mathbb{R}^n\) — we must stay within our base hypothesis space \(\mathcal{H}\).</p> <p>So we find the <strong>function \(h \in \mathcal{H}\)</strong> that best fits the negative gradient at the training points. This is a projection of \(-g\) onto the function class:</p> \[h = \arg\min_{h \in \mathcal{H}} \sum_{i=1}^n \left( -g_i - h(x_i) \right)^2\] <p>This is just <strong>least squares regression</strong> of pseudo-residuals.</p> <p>Once we have \(h\), we take a step:</p> \[f \leftarrow f + v h\] <p>where \(v\) is a step size, which can either be fixed (e.g., shrinkage factor \(\lambda\)) or found via line search.</p> <p><strong>All Together;</strong></p> <ul> <li> <p><strong>Objective</strong>: \(J(f) = \sum_{i=1}^n \ell(y_i, f(x_i))\)</p> </li> <li> <p><strong>Unconstrained gradient</strong>: \(g = \nabla_f J(f) = \left( \frac{\partial \ell(y_1, f_1)}{\partial f_1}, \dots, \frac{\partial \ell(y_n, f_n)}{\partial f_n} \right) \tag{25}\)</p> </li> <li> <p><strong>Projection</strong>: \(h = \arg\min_{h \in \mathcal{H}} \sum_{i=1}^n (-g_i - h(x_i))^2 \tag{26}\)</p> </li> <li> <p><strong>Boosting update</strong>: \(f \leftarrow f + v h\)</p> </li> </ul> <p>This gives a general recipe for boosting: <strong>approximate the negative gradient with a weak learner and take a small step in that direction</strong> — hence the name <strong>gradient boosting</strong>.</p> <h5 id="functional-gradient-descent-hyperparameters-and-regularization"><strong>Functional Gradient Descent: Hyperparameters and Regularization</strong></h5> <p>Once we have our base learner \(h_m\) for iteration \(m\), we need to decide how far to move in that direction.</p> <p>We can either:</p> <ul> <li> <p><strong>Choose a step size \(v_m\) by line search</strong>: \(v_m = \arg\min_v \sum_{i=1}^n \ell\left(y_i, f_{m-1}(x_i) + v h_m(x_i)\right)\)</p> </li> <li> <p><strong>Or</strong> use a fixed step size \(v\) as a hyperparameter. Line search is not strictly necessary but can improve performance.</p> </li> </ul> <p>To <strong>regularize</strong> and control overfitting, we scale the step size with a <strong>shrinkage factor</strong> \(\lambda \in (0, 1]\):</p> \[f_m(x) = f_{m-1}(x) + \lambda v_m h_m(x)\] <p>This shrinkage slows down learning and typically improves generalization. A common choice is \(\lambda = 0.1\).</p> <p>We also need to decide:</p> <ul> <li>The <strong>number of steps \(M\)</strong> (i.e., number of boosting rounds). <ul> <li>This is typically chosen via early stopping or tuning on a validation set.</li> </ul> </li> </ul> <hr/> <h5 id="gradient-boosting-algorithm"><strong>Gradient Boosting Algorithm</strong></h5> <p>Putting everything together, the gradient boosting algorithm proceeds as follows:</p> <ol> <li> <p><strong>Initialize the model</strong> with a constant function: \(f_0(x) = \arg\min_\gamma \sum_{i=1}^n \ell(y_i, \gamma)\)</p> </li> <li> <p><strong>For</strong> \(m = 1\) to \(M\):</p> <ol> <li> <p><strong>Compute the pseudo-residuals</strong> (i.e., the negative gradients):</p> \[r_{im} = -\left[ \frac{\partial}{\partial f(x_i)} \ell(y_i, f(x_i)) \right]_{f(x_i) = f_{m-1}(x_i)}\] </li> <li> <p><strong>Fit a base learner</strong> \(h_m\) (e.g., regression tree) to the dataset \(\{(x_i, r_{im})\}_{i=1}^n\) using squared error.</p> </li> <li> <p><em>(Optional)</em> <strong>Line search</strong> to find best step size: \(v_m = \arg\min_v \sum_{i=1}^n \ell(y_i, f_{m-1}(x_i) + v h_m(x_i))\)</p> </li> <li> <p><strong>Update the model</strong>: \(f_m(x) = f_{m-1}(x) + \lambda v_m h_m(x)\)</p> </li> </ol> </li> <li> <p><strong>Return</strong> the final model: \(f_M(x)\)</p> </li> </ol> <hr/> <h5 id="conclusion-the-gradient-boosting-machine-ingredients"><strong>Conclusion: The Gradient Boosting Machine Ingredients</strong></h5> <p>To implement gradient boosting, you need:</p> <ul> <li>A loss function \(\ell(y, f(x))\) that is differentiable w.r.t. \(f(x)\)</li> <li>A base hypothesis space \(\mathcal{H}\) (e.g., decision trees) for regression</li> <li>A method to choose step size: fixed, or via line search</li> <li>A stopping criterion: number of iterations \(M\), or early stopping</li> <li>Regularization through <strong>shrinkage</strong> (\(\lambda\))</li> </ul> <p>Once these ingredients are in place, you’re ready to build powerful models with <strong>Gradient Boosting Machines (GBMs)</strong>!</p> <p>In the next post, we’ll explore specific loss functions like <strong>logistic loss</strong> for classification and how gradient boosting works in this setting.</p> <p>Take care!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.]]></summary></entry></feed>