<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-30T22:04:28+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A space for journaling my learnings, career, thoughts, and experiences in this amazing ride of life. </subtitle><entry><title type="html">Big Data Storage</title><link href="https://monishver11.github.io/blog/2025/big-data-2-storage/" rel="alternate" type="text/html" title="Big Data Storage"/><published>2025-10-22T15:25:00+00:00</published><updated>2025-10-22T15:25:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-2-storage</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-2-storage/"><![CDATA[<ul> <li>“Big Data” is not new. Oil companies, telecommunications companies, and other data-centric industries have had huge datasets for a long time.</li> <li>As storage capacity continues to expand, today’s “big” is tomorrow’s “small.”</li> <li>“Big Data” is when the size of the data itself becomes part of the problem.</li> <li>Why is Big Data a problem? <ul> <li>Where can I store my company’s ever-growing data?</li> <li>How much is that going to cost?</li> <li>How am I going to manage all the hardware and software?</li> <li>Users are asking bigger questions - how can I provide compute power?</li> <li>And, I/O speed is a problem too.</li> </ul> </li> <li>One 10 TB hard disk drive (HDD) vs 10 TB of storage with 100 HDDs, which is better?</li> <li>10 TB of storage with 100 HDDs has an advantage of having one read/write head per drive. So, the whole disk read/write time reduces overall when compared to the whole disk read/write time of the single 10 TB HDD.</li> <li>In practice, multiple disk drives are installed in one server, sometimes as many as 12 or more. Each drive is 2 TB ~ 6 TB in size.</li> <li>It is important to match the speed of the drives to the processing power of the server, because the CPU can become the bottleneck.</li> <li>New problems: <ul> <li>How can I read the parts of my file simultaneously from multiple drives on multiple servers?</li> <li>How do I even know where the pieces of my file are, if they’re stored on multiple servers?</li> </ul> </li> <li>Before answering these questions, we need to know some of the big data storage concepts that are used and involved.</li> <li>Big Data analytics uses highly scalable distributed technologies and frameworks to analyze large volumes of data from diﬀerent sources.</li> <li>To store Big Data datasets, often in multiple copies, innovative storage strategies and technologies have been created to achieve cost-eﬀective and highly scalable storage solutions.</li> <li>We’ll introduce the following concepts: Clusters, distributed file systems, relational database management systems, NoSQL, sharding, replication, CAP theorem, ACID, and BASE.</li> <li>A cluster is a tightly coupled collection of servers (“nodes”).</li> <li>These servers usually have the same hardware specifications and are connected together via a network to work as a single unit.</li> <li>Each node in the cluster has its own dedicated resources, such as memory, a processor, and a hard drive.</li> <li>A cluster can execute a job by splitting it into small pieces (“tasks”) and distributing their execution onto diﬀerent computers that belong to the cluster.</li> <li>Clusters -&gt; Racks -&gt; Nodes(servers)</li> <li>Next is Distributed file systems (DFS).</li> <li>A file is the most basic unit of storage to store data.</li> <li>A file system (FS) is the method of organizing files on a storage device.</li> <li>A DFS is a file system that can store large files spread across the nodes of a cluster. E.g., Google File System (GFS), Hadoop Distributed File System (HDFS).</li> <li>Next is Relational database management systems (RDBMS).</li> <li>A RDBMS is a product that presents a view of data as a collection of rows and columns.</li> <li>SQL (structured query language) is used for querying and maintaining the database.</li> <li>A transaction symbolizes a unit of work performed against a database, and treated in a coherent and reliable way independent of other transactions.</li> <li>Next is NoSQL.</li> <li>A Not-only SQL (NoSQL) database is a non-relational database that is highly scalable, fault-tolerant and specifically designed to house semi-structured and unstructured data.</li> <li>E.g., Key-value store: Redis, Dynamo. Document store: MongoDB, CouchDB. Wide column store: Bigtable, HBase, Cassandra. Graph store: Pregel, Giraph.</li> <li>Next is Sharding.</li> <li>Sharding is the process of horizontally partitioning a large dataset into a collection of smaller, more manageable datasets (“shards”).</li> <li>Each shard is stored on a separate node, and is responsible for only the data stored on it.</li> <li>All shards share the same schema. They collectively represent the complete dataset.</li> <li>How does sharding work in practice? <ul> <li>Each shard can independently service reads and writes for the specific subset of data that it is responsible for.</li> <li>Depending on the query, data may need to be fetched from both shards.</li> </ul> </li> <li>Benefits of sharding: <ul> <li>Sharding allows the distribution of processing loads across multiple nodes to achieve horizontal scalability.</li> <li>Sharding provides partial tolerance toward failures. In case of a node failure, only data stored on that node is aﬀected.</li> </ul> </li> <li>Concerns with sharding: <ul> <li>Queries requiring data from multiple shards will impose performance penalties.</li> <li>To mitigate such performance issues, data locality keeps commonly accessed data co-located on a single shard. This idea leads to the concept of replication.</li> </ul> </li> <li>Replication stores multiple copies of a dataset (“replicas”) on multiple nodes.</li> <li>There are two methods of replication: Master-slave replication and Peer-to-peer replication.</li> <li>Master-slave replication <ul> <li>All data is written to a master node.</li> <li>Once saved, the data is replicated over to multiple slave nodes.</li> <li>Write requests, including insert, update and delete, occur on the master node.</li> <li>Read requests can be fulfilled by any slave node.</li> <li>This is ideal for read intensive loads. Growing read demands can be managed by horizontal scaling to add more slave nodes.</li> <li>The writes are consistent. All writes are coordinated by the master node. However, write performance will suffer as the amount of writes increases.</li> <li>If the master node fails, reads are still possible via any of the slave nodes. But, writes are not supported until a master node is reestablished.</li> <li>For recovery, we resurrect the master node from a backup or choose a new master node from the slave nodes.</li> <li>There is a concern of read inconsistency.</li> <li>Ex: User A updates data. The data is copied over to Slave A by the Master. Before the data is copied over to Slave B, User B tries to read the data from Slave B, which results in an inconsistent read. The data will eventually become consistent when Slave B is updated by the Master.</li> <li>There are other solutions as well, but we’ll see those as we go and later.</li> </ul> </li> <li>Peer-to-peer replication <ul> <li>All nodes (“peers”) operate at the same level.</li> <li>Each peer is equally capable of handling reads and writes.</li> <li>Each write is copied to all peers.</li> <li>In this replication strategy, we might face both read and write inconsistency.</li> <li>Read inconsistency: User A updates data. The data is copied over to Peer A and Peer B. Before the data is copied over to Peer C, User B tries to read the data from Peer C, resulting in an inconsistent read. The data will eventually be updated on Peer C, and the database will once again become consistent.</li> <li>Write inconsistency: A simultaneous update of the same data may happen across multiple peers.</li> <li>Strategies to resolve these: <ul> <li>Pessimistic concurrency is a proactive strategy. It uses locking to ensure that only one update to a record can occur at a time. However, this is detrimental to availability since the database record being updated remains unavailable until all locks are released.</li> <li>Based on what is held by the lock and who holds the lock, there are many different ways of achieving this strategy. There can be write locks (exclusive locks) and read locks(shared locks), and the unit of locking depends on the system and its users, it could be fine-grained like at a record level, or coarse-grained at a table level. And since its peer-to-peer, the lock can be managed by centralized lock manager (as a service by zookeeper) or distributed lock manager (using a distributed consensus protocol like Paxos, Raft) or other application-level ownership.</li> <li>Optimistic concurrency is a reactive strategy that does not use locking. Instead, it allows inconsistency to occur with knowledge that eventually consistency will be achieved after all updates have propagated.</li> </ul> </li> </ul> </li> <li>Sharding vs replication <ul> <li>Actually, both sharding and replication can be used together.</li> <li>We can combine, sharding and master-slave replication, sharding and peer-to-peer replication or any other commendations.</li> <li>Sharding and master-slave replication</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharding_ms-480.webp 480w,/assets/img/sharding_ms-800.webp 800w,/assets/img/sharding_ms-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/sharding_ms.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="sharding_ms" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Sharding and peer-to-peer replication
</code></pre></div></div> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharding_pp-480.webp 480w,/assets/img/sharding_pp-800.webp 800w,/assets/img/sharding_pp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/sharding_pp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="sharding_pp" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>CAP theorem <ul> <li>A distributed database system may wish to provide three guarantees.</li> <li>Consistency: a read request from any node results in the same, most recently written data across multiple nodes.</li> <li>Availability: a read/write request will always be acknowledged in the form of a success or a failure.</li> <li>Partition tolerance: the database system can tolerate communication outages that split the cluster into multiple silos and can still service read/write requests.</li> <li>CAP theorem states that a distributed database system can only provide two of the three properties. C+A+P is not possible.</li> <li>Although communication outages are rare and temporary, partition tolerance (P) must be supported by a distributed database.</li> <li>Therefore, CAP is generally a choice between choosing C+P or A+P.</li> <li>Below explains how things can go wrong in each CAP combinations.</li> <li>C + A (no Partition Tolerance): When a network partition happens, nodes can’t communicate to stay consistent. Since the system isn’t partition-tolerant, it must shut down or reject requests, leading to unavailability.</li> <li>C + P (no Availability): During a partition, nodes stop serving requests until they can synchronize again to maintain consistency. This causes temporary unavailability, as the system pauses updates to avoid conflicts.</li> <li>A + P (no Consistency): When a partition occurs, all nodes continue serving requests independently. Because they can’t coordinate, updates may diverge, producing inconsistent or stale data until the partition heals and replicas reconcile.</li> <li>In summary: Choosing C + A means all nodes must stay in sync, but the system fails if a partition occurs. Choosing C + P ensures data consistency across partitions, but some nodes may become unavailable. Choosing A + P keeps the system running despite partitions, but nodes may serve inconsistent data until synchronization occurs.</li> </ul> </li> <li>ACID <ul> <li>ACID is a traditional database design principle on transaction management.</li> <li>It stands for Atomicity, Consistency, Isolation and Durability.</li> <li>Traditional databases leverages pessimistic concurrency controls (i.e., locking) to provide ACID guarantees.</li> <li>Atomicity ensures that all transactions will always succeed or fail completely. In other words, there are no partial transactions.</li> <li>Ex: A user attempts to update three records as a part of a transaction. Two records are successfully updated before the occurrence of an error. As a result, the database rolls back any partial eﬀects of the transaction and puts the system back to its prior state.</li> <li>Consistency ensures that only data that conforms to the constraints of the database schema can be written to the database.</li> <li>Ex: A user attempts to update the “amount” column of the table that is of type “float” with a value of type “varchar.” The database rejects this update because the value violates the constraint checks for the “amount” column.</li> <li>Note: consistency here is different from the consistency in a distributed database system.</li> <li>Isolation ensures that the results of a transaction are not visible to other operations until it is complete.</li> <li>Ex: User A attempts to update two records as part of a transaction. The database successfully updates the first record. However, before it can update the second record, User B attempts to update the same record. The database does not permit User B’s update until User A’s update succeeds or fails completely. This occurs because the record with id = 3 is locked by the database until the transaction is complete.</li> <li>Durability ensures that the results of a transaction are permanent, regardless of any system failure.</li> <li>Ex: A user updates a record as part of a transaction. The database successfully updates the record. Right after this update, a power failure occurs. The database maintains its state while there is no power. The power is resumed. The database serves the record as per last update when requested by the user.</li> <li>This ACID property relates to the CAP theorem in a way that the database systems providing traditional ACID guarantees choose consistency over availability. So it ensures C+P.</li> <li>Ex: User A attempts to update a record as part of a transaction. The database validates the value and the update is successfully applied. After the successful completion of the transaction, when Users B and C request the same record, the database provides the updated value to both the users.</li> </ul> </li> <li>BASE <ul> <li>BASE (pun intended) is a database design principle leveraged by many distributed database systems.</li> <li>It stands for Basically Available, Soft state and Eventual consistency.</li> <li>When a database supports BASE, it favors availability over consistency. So, it ensures A+P.</li> <li>BASE leverages optimistic concurrency by relaxing the strong consistency constraints mandated by the ACID properties.</li> <li>Basically available means that the database will always acknowledge a client’s request.</li> <li>This database is basically available, even though it has been partitioned as a result of a network failure. It can just return a failure response for the user request in this case.</li> <li>Soft state means that a database may be in an inconsistent state when data is read.</li> <li>The results may change if the same data is requested again.</li> <li>Ex: User A updates a record on Peer A. Before the other peers are updated, User B requests the same record from Peer C. The database is now in a soft state, and stale data is returned to User B.</li> <li>Eventual consistency means that the database only attains consistency once the changes have been propagated to all nodes.</li> <li>Ex: User A updates a record. The record only gets updated at Peer A, but before the other peers can be updated, User B requests the same record. The database is now in a soft state. Stale data is returned to User B from Peer C. However, the consistency is eventually attained, and User C gets the correct value.</li> </ul> </li> <li>ACID vs BASE <ul> <li>ACID ensures immediate consistency at the expense of availability due to the record locking.</li> <li>BASE emphasizes availability over immediate consistency.</li> <li>This soft approach toward consistency allows BASE-compliant databases to serve multiple clients without any latency though serving inconsistent results.</li> <li>However, BASE-compliant databases are not useful for transactional systems where lack of consistency is a concern and needed.</li> <li>A distributed database system may choose to provide some ACID properties.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><category term="RBDA"/><summary type="html"><![CDATA[Notes on Big Data Storage - Concepts and Architecture]]></summary></entry><entry><title type="html">Introduction to Realtime and Big Data Analytics</title><link href="https://monishver11.github.io/blog/2025/big-data-1-intro/" rel="alternate" type="text/html" title="Introduction to Realtime and Big Data Analytics"/><published>2025-10-22T15:23:00+00:00</published><updated>2025-10-22T15:23:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-1-intro</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-1-intro/"><![CDATA[<ul> <li>Big Data is a field dedicated to the analysis, processing, and storage of large collections of data that frequently originate from disparate sources.</li> <li>Big Data addresses distinct requirements: I) Combine multiple unrelated datasets. II) Process large amounts of unstructured data. III) Harvest hidden information in a time-sensitive manner.</li> <li>A dataset is a collection or group of related data.</li> <li>Each dataset member (“datum”) shares the same set of attributes or properties as others in the same dataset.</li> <li>Ex: An extract of rows from a database table stored in a CSV formatted file.</li> <li>Data analysis is the process of examining data to find facts, relationships, patterns, insights and/or trends.</li> <li>The overall goal of data analysis is to support better decision-making.</li> <li>Data analytics is a discipline that includes the management of the complete data lifecycle, which encompasses collecting, cleaning, organizing, storing, analyzing and governing data.</li> <li>Ex: In business-oriented environments, data analytics results can lower operational costs and facilitate strategic decision-making. In the scientific domain, data analytics can help identify the cause of a phenomenon to improve the accuracy of predictions. In service-based environments, data analytics can help strengthen the focus on delivering high-quality services by driving down costs.</li> <li>There are four general categories of analytics that are distinguished by the results they produce: descriptive, diagnostic, predictive and prescriptive analysis.</li> <li>Descriptive analytics aim to answer questions about events that have already occurred. Descriptive analytics contextualizes data to generate information.</li> <li>The operational systems (e.g., OLTP, CRM, ERP) are queried via descriptive analytics tools to generate static reports or dashboards.</li> <li>Diagnostic analysts aim to determine the cause of a phenomenon that occurred in the past using questions that focus on the reason behind the event.</li> <li>The goal is to determine what information is related to the phenomenon in order to answer questions that seek to determine why something has occurred.</li> <li>Diagnostic analytics usually collect data from multiple sources and store it in a structure (e.g., OLAP) so that users can perform interactive drill-down and roll-up analysis.</li> <li>Predictive analytics aim to determine the outcome of an event that might occur in the future.</li> <li>Information is associated to build models that are used to generate future predictions based upon past events.</li> <li>Predictive analytics use large datasets of internal and external data and various data analysis techniques to provide user-friendly front-end interfaces.</li> <li>Prescriptive analytics build upon the results of predictive analytics by prescribing actions that should be taken.</li> <li>The focus is not only on what prescribed option is best to follow, but why.</li> <li>Prescriptive analytics use business rules and large amounts of internal and external data to simulate outcomes and prescribe the best course of action.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bd-1-480.webp 480w,/assets/img/bd-1-800.webp 800w,/assets/img/bd-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/bd-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bd-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Big data characteristics, the five V’s: Volume, Velocity, Variety, Veracity and Value.</li> <li>Veracity refers to the quality of data. Data with a high signal-to-noise ratio has more veracity.</li> <li>Value is defined as the usefulness of data for an enterprise.</li> <li>Value is also impacted by data lifecycle-related concerns, like how well has the data been stored? Were valuable attributes of data removed during data cleansing? Are the right types of questions being asked during data analysis? Are the results of the analysis being accurately communicated to the appropriate decision-makers?</li> <li>There are different types of data. Based on source, we’ve Human-generated data and Machine-generated data. Based on format, we’ve structured, unstructured, semi-structured data and metadata.</li> <li>Structured data conforms to a data model or schema. It is used to capture relationships between different entities and is therefore most often stored in relational database. Ex: banking transactions, invoices, customer records, etc.</li> <li>Unstructured data doesn’t conform to a data model or schema. It is either textual or binary and often conveyed via files that are self-contained and non-relational. The majority of data is unstructured. Ex: textual data, video, image files, audio, etc.</li> <li>Semi-structured data has a defined level of structure and consistency, but is not relational in nature. Instead, it is hierarchical or graph based. This kind of data is commonly stored in text-based files. Ex: XML data, JSON data, sensor data(CSV), etc.</li> <li>Metadata provides information about a dataset’s characteristics and structure. It is mostly machine-generated and can be appended to data. Ex: XML tags providing the author and creation date of a document, Attributes providing the file size and resolution of a digital photograph, etc.</li> <li>Big data solutions need to support multiple formats and types of data.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><category term="RBDA"/><summary type="html"><![CDATA[Notes on Big Data (the 5 V's & data types) and the Four Categories of Data Analytics (Descriptive, Diagnostic, Predictive, and Prescriptive).]]></summary></entry><entry><title type="html">GPU Essentials - A Concise Technical Guide</title><link href="https://monishver11.github.io/blog/2025/GPU-Intro/" rel="alternate" type="text/html" title="GPU Essentials - A Concise Technical Guide"/><published>2025-09-23T04:46:00+00:00</published><updated>2025-09-23T04:46:00+00:00</updated><id>https://monishver11.github.io/blog/2025/GPU-Intro</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/GPU-Intro/"><![CDATA[<p>This is a concise GPU introduction I found helpful. With it, you can start CUDA programming and understand the basic terms you’ll encounter. These notes are adapted from <a href="https://pages.cs.wisc.edu/~markhill/restricted/ieeemicro10_gpu.pdf">this article</a>, which was itself inspired by Jen-Hsun Huang’s keynote at Hot Chips 21 in 2009. Although the article was published in 2010—15 years ago—the GPU architecture concepts and terminology haven’t changed much. Modern GPUs include additional features for improved performance, but the fundamentals remain largely the same.</p> <hr/> <p>With the GPU’s rapid evolution from a configurable graphics processor to a programmable parallel processor, the ubiquitous GPU in every PC, laptop, desktop, and workstation is a many-core multi-threaded multiprocessor that excels at both graphics and computing applications.</p> <h4 id="gpu-computings-evolution"><strong>GPU computing’s evolution</strong></h4> <ul> <li>Rendering high-definition graphics scenes is a problem with tremendous inherent parallelism. A graphics programmer writes a single-thread program that draws one pixel, and the GPU runs multiple instances of this thread in parallel—drawing multiple pixels in parallel.</li> <li>Also, GPU computing programs—written in C or C++ with the CUDA parallel computing model, or using a parallel computing API inspired by CUDA such as Direct- Compute or OpenCL — scale transparently over a wide range of parallelism. Software scalability, too, has enabled GPUs to rapidly increase their parallelism and performance with increasing transistor density.</li> <li>Evolving to modern GPUs involved adding programmability incrementally—from fixed function pipelines to microcoded processors, configurable processors, programmable processors, and scalable parallel processors.</li> <li>GPUs first used floating-point arithmetic to calculate 3D geometry and vertices, then applied it to pixel lighting and color values to handle high-dynamic-range scenes and to simplify programming.</li> <li>The GeForce 6800 scalable processor core architecture facilitated multiple GPU implementations with different numbers of processor cores.</li> <li>Early GPGPU computing programs achieved high performance, but were difficult to write because programmers had to express non-graphics computations with a graphics API such as OpenGL.</li> <li>The GeForce 8800 introduced in 2006 featured the first unified graphics and computing GPU architecture7,8 programmable in C with the CUDA parallel computing model, in addition to using DX10 and OpenGL.</li> <li>Its unified streaming processor cores executed vertex, geometry, and pixel shader threads for DX10 graphics programs, and also executed computing threads for CUDA C programs.</li> <li>Hardware multithread- ing enabled the GeForce 8800 to efficiently execute up to 12,288 threads concurrently in 128 processor cores.</li> <li>NVIDIA deployed the scalable architecture in a family of GeForce GPUs with different numbers of processor cores for each market segment.</li> <li>The GeForce 8800 was the first GPU to use scalar thread processors rather than vector processors, matching standard scalar languages like C, and eliminating the need to manage vector registers and program vector operations.</li> <li>(# Note: Scalar processors execute one operation per thread on a single data element, while vector processors execute the same operation on multiple data elements at once, requiring explicit vector instructions #)</li> <li>It added instructions to support C and other general-purpose languages, including integer arithmetic, IEEE 754 floating-point arithmetic, and load/store memory access instructions with byte addressing.</li> <li>It provided hardware and instructions to support parallel computation, communication, and synchronization—including thread arrays, shared memory, and fast barrier synchronization.</li> <li>(# Note: Fast barrier synchronization is a mechanism that quickly pauses threads in a block until all have reached the same point, ensuring they proceed together without race conditions #)</li> <li>NVIDIA introduced the third-generation Fermi GPU computing architecture in 2009.</li> <li>Fermi implemented IEEE 754-2008 and significantly increased double-precision performance. It added error-correcting code (ECC) memory protection for large-scale GPU computing, 64-bit unified addressing, cached memory hierarchy, and instructions for C, C++, Fortran, OpenCL, and DirectCompute.</li> <li>The GPU computing ecosystem is expanding rapidly, enabled by the deployment of more than 180 million CUDA-capable GPUs.</li> <li>NVIDIA developed the parallel Nsight GPU development environment, debugger, and analyzer integrated with Microsoft Visual Studio.</li> </ul> <h4 id="cuda-scalable-parallel-architecture"><strong>CUDA scalable parallel architecture</strong></h4> <ul> <li>CUDA is a hardware and software coprocessing architecture for parallel computing that enables NVIDIA GPUs to execute programs written with C, C++, Fortran, OpenCL, DirectCompute, and other languages.</li> <li>Because most languages were designed for one sequential thread, CUDA preserves this model and extends it with a minimalist set of abstractions for expressing parallelism. This lets the programmer focus on the important issues of parallelism—how to design efficient parallel algorithms—using a familiar language.</li> <li>By design, CUDA enables the development of highly scalable parallel programs that can run across tens of thousands of concurrent threads and hundreds of processor cores.</li> <li>A compiled CUDA program executes on any size GPU, automatically using more parallelism on GPUs with more processor cores and threads.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-1-480.webp 480w,/assets/img/gpu-1-800.webp 800w,/assets/img/gpu-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>A CUDA program is organized into a host program, consisting of one or more sequential threads running on a host CPU, and one or more parallel kernels suitable for execution on a parallel computing GPU. A kernel executes a sequential program on a set of lightweight parallel threads. As Figure 1 shows, the programmer or compiler organizes these threads into a grid of thread blocks. The threads comprising a thread block can synchronize with each other via barriers and communicate via a high-speed, per-block shared memory.</li> <li>Threads from different blocks in the same grid can coordinate via atomic operations in global memory space shared by all threads. Sequentially dependent kernel grids can synchronize via global barriers and coordinate via global shared memory.</li> <li>CUDA requires that thread blocks be independent, which provides scalability to GPUs with different numbers of processor cores and threads.</li> <li><mark style="background: #FFF3A3A6;">Thread blocks implement coarse-grained scalable data parallelism, while the light-weight threads comprising each thread block provide fine-grained data parallelism. Thread blocks executing different kernels implement coarse-grained task parallelism. Threads executing different paths implement fine-grained thread-level parallelism.</mark></li> <li>(# Note: Imagine a restaurant kitchen — multiple kitchens (thread blocks) each cook the same dish in parallel = coarse-grained data parallelism; within one kitchen, many chefs (threads) chop ingredients simultaneously = fine-grained data parallelism; if different kitchens prepare entirely different dishes = coarse-grained task parallelism; if chefs in the same kitchen follow slightly different recipes = fine-grained thread-level parallelism #)</li> <li>(# Note: Think of a university — each class (thread block) works on the same assignment = coarse-grained data parallelism; within a class, each student (thread) solves a small part of the assignment = fine-grained data parallelism; if different classes work on different subjects = coarse-grained task parallelism; if students in the same class take different approaches to solving a problem = fine-grained thread-level parallelism #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-2-480.webp 480w,/assets/img/gpu-2-800.webp 800w,/assets/img/gpu-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Figure 2 shows some basic features of parallel programming with CUDA. It contains sequential and parallel implementations of the SAXPY routine defined by the basic linear algebra subroutines (BLAS) library.</li> <li>The serial implementation is a simple loop that computes one element of y per iteration. The parallel kernel executes each of these independent iterations in parallel, assigning a separate thread to compute each element of y.</li> <li>The __ global __ modifier indicates that the procedure is a kernel entry point, and the extended function-call syntax saxpy«&lt;B, T»&gt;(. . .) launches the kernel saxpy() in parallel across B blocks of T threads each.</li> <li>Each thread determines which element it should process from its integer thread block index blockIdx.x, its thread index within its block threadIdx.x, and the total number of threads per block blockDim.x.</li> <li>This example demonstrates a common parallelization pattern, where we can transform a serial loop with independent iterations to execute in parallel across many threads.</li> <li>In the CUDA paradigm, the programmer writes a scalar program—the parallel saxpy() kernel—that specifies the behavior of a single thread of the kernel. This lets CUDA leverage standard C language with only a few small additions, such as built-in thread and block index variables.</li> </ul> <h4 id="gpu-computing-architecture"><strong>GPU computing architecture</strong></h4> <ul> <li>To address different market segments, GPU architectures scale the number of processor cores and memories to implement different products for each segment while using the same scalable architecture and software.</li> <li>NVIDIA’s scalable GPU computing architecture varies the number of streaming multi-processors to scale computing performance, and varies the number of DRAM memories to scale memory bandwidth and capacity.</li> <li><mark style="background: #FFF3A3A6;">Each multithreaded streaming multiprocessor provides sufficient threads, processor cores, and shared memory to execute one or more CUDA thread blocks. The parallel processor cores within a streaming multi-processor execute instructions for parallel threads.</mark></li> <li>(# Note: Picture a library — the building is a streaming multiprocessor (SM), the reading tables inside are processor cores, and the shared bookshelf is shared memory. A group of students (a thread block) comes in; they can sit across tables, use the shared books, and study in parallel. Multiple groups can use the same library if resources allow #)</li> <li><mark style="background: #FFF3A3A6;">Multiple streaming multiprocessors provide coarse-grained scalable data and task parallelism to execute multiple coarse-grained thread blocks (possibly running different kernels) in parallel.</mark></li> <li>(# Note: Imagine a city with many libraries (multiple SMs). Each library can host different study groups (thread blocks). Some groups may study the same subject = data parallelism, while others study different subjects = task parallelism. Because there are many libraries, multiple groups can work in parallel at a larger scale #)</li> <li><mark style="background: #FFF3A3A6;">Multithreading and parallel-pipelined processor cores within each streaming multiprocessor implement fine-grained data and thread-level parallelism to execute hundreds of fine-grained threads in parallel.</mark></li> <li>(# Note: Think of an assembly line in a factory — each worker (core) handles a specific step, and many items (threads) move through simultaneously. Because there are many workers and multiple lines, hundreds of small tasks get done in parallel with no idle time = fine-grained parallelism #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-3-480.webp 480w,/assets/img/gpu-3-800.webp 800w,/assets/img/gpu-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>To illustrate GPU computing architecture, Figure 3 shows the third-generation Fermi computing architecture configured with 16 streaming multiprocessors, each with 32 CUDA processor cores, for a total of 512 cores.</li> <li><mark style="background: #FFF3A3A6;">The GigaThread work scheduler distributes CUDA thread blocks to streaming multiprocessors with available capacity, dynamically balancing the computing workload across the GPU, and running multiple kernel tasks in parallel when appropriate.</mark></li> <li>The multi-threaded streaming multiprocessors schedule and execute CUDA thread blocks and individual threads.</li> <li>Each streaming multiprocessor executes up to 1,536 concurrent threads to help cover long latency loads from DRAM memory. As each thread block completes executing its kernel program and releases its streaming multiprocessor resources, the work scheduler assigns a new thread block to that streaming multiprocessor.</li> <li>The PCIe host interface connects the GPU and its DRAM memory with the host CPU and system memory.</li> <li>The streaming multiprocessor threads access system memory via the PCIe interface, and CPU threads access GPU DRAM memory via PCIe.</li> <li><mark style="background: #FFF3A3A6;">The GPU architecture balances its parallel computing power with parallel DRAM memory controllers designed for high memory bandwidth.</mark></li> <li>Fermi introduces a parallel cached memory hierarchy for load, store, and atomic memory accesses by general applications.</li> <li>Each streaming multiprocessor has a first-level (L1) data cache, and the streaming multi- processors share a common 768-Kbyte unified second-level (L2) cache.</li> <li>The L2 cache connects with six 64-bit DRAM interfaces and the PCIe interface, which connects with the host CPU, system memory, and PCIe devices.</li> <li>It caches DRAM memory locations and system memory pages accessed via the PCIe interface.</li> <li>The unified L2 cache services load, store, atomic, and texture instruction requests from the streaming multiprocessors and requests from their L1 caches, and fills the streaming multiprocessor instruction caches and uniform data caches.</li> <li>(# Note: In GPU graphics, a texture is an image or data map applied to a 3D object’s surface. Texture instructions fetch and manipulate this data efficiently; in computing, “texture memory” can also be used as a read-only cached memory space optimized for certain access patterns #)</li> <li>Fermi implements a 40-bit physical address space that accesses GPU DRAM, CPU system memory, and PCIe device addresses. It provides a 40-bit virtual address space to each application context and maps it to the physical address space with translation lookaside buffers and page tables.</li> <li>Fermi ECC corrects single-bit errors and detects double-bit errors in the DRAM memory, GPU L2 cache, L1 caches, and streaming multiprocessor registers.</li> <li>The ECC lets us integrate thousands of GPUs in a system while maintaining a high mean time between failures (MTBF) for high-performance computing and super-computing systems.</li> <li>(# Note: Mean Time Between Failures (MTBF) is the average time a system or component operates before a failure occurs. Higher MTBF indicates more reliable hardware, crucial when thousands of GPUs work together in HPC systems #)</li> </ul> <h4 id="streaming-multiprocessor"><strong>Streaming multiprocessor</strong></h4> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-4-480.webp 480w,/assets/img/gpu-4-800.webp 800w,/assets/img/gpu-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The streaming multiprocessor implements zero-overhead multithreading and thread scheduling for up to 1,536 concurrent threads.</li> <li>(# Note: The SM supports zero-overhead multithreading by maintaining hardware context for each thread—registers, program counter, and state—so switching between 1,536 threads happens instantly without saving/restoring state. For example, if an SM has 32 cores and each warp has 32 threads, it can manage 48 warps concurrently: 32 × 48 = 1,536 threads #)</li> <li><mark style="background: #FFF3A3A6;">To efficiently manage and execute this many individual threads, the multiprocessor employs the single-instruction multiple-thread (SIMT) architecture introduced in the first unified computing GPU.</mark></li> <li>The SIMT instruction logic creates, manages, schedules, and executes concurrent threads in groups of 32 parallel threads called warps.</li> <li><mark style="background: #FFF3A3A6;">A CUDA thread block comprises one or more warps. Each Fermi streaming multiprocessor has two warp schedulers and two dispatch units that each select a warp and issue an instruction from the warp to 16 CUDA cores, 16 load/store units, or four SFUs.</mark></li> <li>(# Note: Imagine a classroom divided into smaller study groups (warps). The teacher (warp scheduler) chooses one group at a time and gives them an instruction. The students in that group then split into roles: some write (cores), some fetch books (load/store units), and some handle special tricky problems (SFUs). This shows how warps are managed and distributed to different execution resources #)</li> <li><mark style="background: #FFF3A3A6;">Because warps execute independently, the streaming multiprocessor can issue two warp instructions to appropriate sets of CUDA cores, load/store units, and SFUs.</mark></li> <li>To support C, C++, and standard single-thread programming languages, each streaming multiprocessor thread is independent, having its own private registers, condition codes and predicates, private per-thread memory and stack frame, instruction address, and thread execution state.</li> <li>The SIMT instructions control the execution of an individual thread, including arithmetic, memory access, and branching and control flow instructions.</li> <li><mark style="background: #FFF3A3A6;">For efficiency, the SIMT multiprocessor issues an instruction to a warp of 32 independent parallel threads.</mark></li> <li><mark style="background: #FFF3A3A6;">The streaming multiprocessor realizes full efficiency and performance when all threads of a warp take the same execution path.</mark></li> <li>If threads of a warp diverge at a data-dependent conditional branch, execution serializes for each branch path taken, and when all paths complete, the threads converge to the same execution path.</li> <li>Parallel thread execution (PTX) instructions describe the execution of a single thread in a parallel CUDA program.</li> <li>The PTX instructions focus on scalar (rather than vector) operations to match standard scalar programming languages.</li> <li><mark style="background: #FFF3A3A6;">Each pipelined CUDA core executes a scalar floating point or integer instruction per clock for a thread. With 32 cores, the streaming multiprocessor can execute up to 32 arithmetic thread instructions per clock.</mark></li> <li>The integer unit implements 32-bit precision for scalar integer operations, including 32-bit multiply and multiply-add operations, and efficiently supports 64-bit integer operations.</li> <li>The Fermi CUDA core floating-point unit implements the IEEE 754-2008 floating-point arithmetic standard for 32-bit single-including fused multiply-add (FMA) instructions.</li> <li>FMA computes D = A * B + C with no loss of precision by retaining full precision in the intermediate product and addition, then rounding the final sum to form the result.</li> <li>Using FMA enables fast division and square-root operations with exactly rounded results.</li> <li>Fermi raises the throughput of 64-bit double-precision operations to half that of single precision operations, a dramatic improvement over the T10 GPU.</li> <li>The SFUs execute 32-bit floating-point instructions for fast approximations of reciprocal, reciprocal square root, sin, cos, exp, and log functions.</li> <li>The streaming multiprocessor load/store units execute load, store, and atomic memory access instructions.</li> <li><mark style="background: #FFF3A3A6;">A warp of 32 active threads presents 32 individual byte addresses, and the instruction accesses each memory address. The load/store units coalesce 32 individual thread accesses into a minimal number of memory block accesses.</mark></li> <li>(# Note: Picture 32 people each ordering one item from a store. Instead of processing 32 separate trips, the store groups the orders into as few bulk deliveries as possible. This is how memory coalescing works — combining many small requests into fewer large, efficient ones #)</li> <li>(# Note: Think of 32 friends each mailing a letter to the same neighborhood. Instead of sending 32 separate mail trucks, the post office bundles the letters and sends them together in one truck. That’s memory coalescing — merging many nearby requests into one efficient transfer #)</li> <li>Fermi implements a unified thread address space that accesses the three separate parallel memory spaces of Figure 1: per-thread local, per-block shared, and global memory spaces.</li> <li><mark style="background: #FFF3A3A6;">A unified load/store instruction can access any of the three memory spaces, steering the access to the correct memory, which enables general C and C++ pointer access anywhere.</mark></li> <li>Fermi provides a terabyte 40-bit unified byte address space, and the load/store ISA supports 64-bit byte addressing for future growth. The ISA also provides 32-bit addressing instructions when the program can limit its accesses to the lower 4 Gbytes of address space.</li> <li>On-chip shared memory provides low-latency, high-bandwidth access to data shared by cooperating threads in the same CUDA thread block.</li> <li>Fast shared memory significantly boosts the performance of many applications having predictable regular addressing patterns, while reducing DRAM memory traffic.</li> <li>Fermi introduces a configurable-capacity L1 cache to aid unpredictable or irregular memory accesses, along with a configurable-capacity shared memory.</li> <li>Each streaming multiprocessor has 64 Kbytes of on-chip memory, configurable as 48 Kbytes of shared memory and 16 Kbytes of L1 cache, or as 16 Kbytes of shared memory and 48 Kbytes of L1 cache.</li> </ul> <h4 id="cpugpu-co-processing"><strong>CPU+GPU co-processing</strong></h4> <ul> <li>Heterogeneous CPU+GPU co-processing systems evolved because the CPU and GPU have complementary attributes that allow applications to perform best using both types of processors.</li> <li>CUDA programs are coprocessing programs—serial portions execute on the CPU, while parallel portions execute on the GPU. Coprocessing optimizes total application performance.</li> <li><mark style="background: #FFF3A3A6;">With coprocessing, we use the right core for the right job. We use a CPU core (optimized for low latency on a single thread) for a code’s serial portions, and we use GPU cores (optimized for aggregate throughput on a code’s parallel portions) for parallel portions of code.</mark></li> <li>This approach gives more performance per unit area or power than either CPU or GPU cores alone.</li> <li>The comparison in Table 2 illustrates the advantage of CPU+GPU coprocessing using Amdahl’s law.</li> <li>(# Note: Amdahl’s Law predicts the maximum speedup of a program using multiple processors, based on the fraction of the code that must run sequentially. Even if 90% of a program is parallelizable, the remaining 10% limits total speedup, showing why a CPU+GPU combination can outperform a pure GPU for mixed workloads #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-5-480.webp 480w,/assets/img/gpu-5-800.webp 800w,/assets/img/gpu-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The table compares the performance of four configurations: <ul> <li>a system containing one latency-optimized (CPU) core,</li> <li>a system containing 500 throughput-optimized (GPU) cores,</li> <li>a system containing 10 CPU cores, and</li> <li>a coprocessing system that contains a single CPU core and 450 GPU cores.</li> </ul> </li> <li>Table 2 assumes that a CPU core is 5 faster and 50 the area of a GPU core—numbers consistent with contemporary CPUs and GPUs. The coprocessing system devotes 10 percent of its area to the single CPU core and 90 percent of its area to the 450 GPU cores.</li> <li>The coprocessing architecture is the fastest on both programs.</li> <li><mark style="background: #FFF3A3A6;">On the parallel-intensive program, the coprocessing architecture is slightly slower on the parallel portion than the pure GPU configuration (0.44 seconds versus 0.40 seconds) but more than makes up for this by running the tiny serial portion 5 faster (1 second versus5 seconds). The heterogeneous architecture has an advantage over the pure throughput- optimized configuration here because serial performance is important even for mostly parallel codes.</mark></li> <li>Even on mostly sequential codes, it’s more efficient to run the code’s parallel portion on a throughput-optimized architecture.</li> <li>The coprocessing architecture provides the best performance across a wide range of the serial fraction because it uses the right core for each task.</li> <li>By using a latency- optimized CPU to run the code’s serial fraction, it gives the best possible performance on the serial fraction—which is important even for mostly parallel codes.</li> <li>By using throughput-optimized cores to run the code’s parallel portion, it gives near- optimal performance on the parallel fraction as well—which becomes increasingly important as codes become more parallel.</li> <li>It’s wasteful to use large, inefficient latency-optimized cores to run parallel code segments.</li> </ul> <h4 id="application-performance"><strong>Application performance</strong></h4> <ul> <li>Many applications consist of a mixture of fundamentally serial control logic and inherently parallel computations.</li> <li><mark style="background: #FFF3A3A6;">Furthermore, these parallel computations are frequently data-parallel in nature. This directly matches the CUDA coprocessing programming model, namely a sequential control thread capable of launching a series of parallel kernels.</mark></li> <li><mark style="background: #FFF3A3A6;">The use of parallel kernels launched from a sequential program also makes it relatively easy to parallelize an application’s individual components rather than rewrite the entire application.</mark></li> <li>(# Note: Imagine renovating a house — instead of rebuilding the whole house from scratch, you can work on individual rooms in parallel (kitchen, bathroom, bedroom) while the overall house structure stays the same. Similarly, parallel kernels let you speed up parts of a program without rewriting the entire application #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-6-480.webp 480w,/assets/img/gpu-6-800.webp 800w,/assets/img/gpu-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Table 3 lists some representative applications along with the runtime speedups obtained for the whole application using CPU+GPU coprocessing over CPU alone, as measured by application developers.</li> <li>The speedups using GeForce 8800, Tesla T8, GeForce GTX 280, Tesla T10, and GeForce GTX 285 range from 9 to more than 130 , with the higher speedups reflecting applications where more of the work ran in parallel on the GPU.</li> <li>The lower speedups—while still quite attractive—represent applications that are limited by the code’s CPU portion, coprocessing overhead, or by divergence in the code’s GPU fraction.</li> <li>The speedups achieved on this diverse set of applications validate the programmability of GPUs—in addition to their performance.</li> <li>Applications with dense matrices, sparse matrices, and arbitrary pointer structures have all been successfully implemented in CUDA with impressive speedups. Similarly, applications with diverse control structures and significant data-dependent control, such as ray tracing, have achieved good performance in CUDA.</li> <li>Many real-world applications (such as interactive ray tracing) are composed of many different algorithms, each with varying degrees of parallelism.</li> <li>(# Note: Ray tracing is a graphics technique that simulates the path of light rays to produce realistic images with reflections, shadows, and refractions. Each ray’s calculation is independent, making it highly parallelizable on GPUs #)</li> <li>OptiX, our interactive ray-tracing software developer’s kit built in the CUDA architecture, provides a mechanism to control and schedule a wide variety of tasks on both the CPU and GPU.</li> <li>Some tasks are primarily serial and execute on the CPU, such as compilation, data structure management, and coordination with the operating system and user interaction.</li> <li>Other tasks, such as building an acceleration structure or updating animations, may run either on the CPU or the GPU depending on the choice of algorithms and the performance required.</li> <li>The net result is that CPUþGPU coprocessing enables fast, interactive ray tracing of complex scenes while you watch, which is an application that researchers previously considered too irregular for a GPU.</li> <li>GPU computing is at the tipping point. <mark style="background: #FFF3A3A6;">Single-threaded processor performance is no longer scaling at historic rates.</mark></li> <li>Thus, we must use parallelism for the increased performance required to deliver more value to users.</li> <li><mark style="background: #FFF3A3A6;">A GPU that’s optimized for throughput delivers parallel performance much more efficiently than a CPU that’s optimized for latency.</mark></li> <li>A heterogeneous coprocessing architecture that combines a single latency-optimized core (a CPU) with many throughput-optimized cores (a GPU) performs better than either alternative alone.</li> <li>This is because it uses the right processor for the right job—the CPU for serial sections and critical paths and the GPU for the parallel sections.</li> <li>In high-performance computing, technical computing, and consumer media processing, CPU+GPU coprocessing has become the architecture of choice.</li> <li>GPU architecture will evolve to further increase the span of applications that it can efficiently address.</li> <li>GPU cores will not become CPUs—they will continue to be optimized for throughput, rather than latency.</li> <li><mark style="background: #FFF3A3A6;">However, they will evolve to become more agile and better able to handle arbitrary control and data access patterns.</mark></li> </ul> <hr/> <h4 id="minor-details-that-are-frequently-misunderstood"><strong>Minor details that are frequently misunderstood:</strong></h4> <ul> <li>A thread block always executes on one SM. Multiple smaller thread blocks may be present on one SM. There are more threads than execution units (“cuda cores”) on an SM which means not every thread gets to schedule a new instruction each clock cycle. That’s okay because threads often wait for memory or floating point operations that take multiple clock cycles to finish – <a href="https://stackoverflow.com/users/17167312/homer512" title="14,961 reputation">Homer512</a></li> <li><a href="https://stackoverflow.com/questions/64624793/warp-and-block-scheduling-in-cuda-what-exactly-happens-and-questions-about-el">Warp and block scheduling in CUDA - what exactly happens, and questions about eligible warps</a></li> <li><a href="https://stackoverflow.com/questions/62147624/how-many-cuda-cores-is-used-to-process-a-cuda-warp">How many CUDA cores is used to process a CUDA warp?</a></li> <li><a href="https://stackoverflow.com/questions/76678083/confusion-around-no-of-cuda-cores-and-the-number-of-parallel-threads">Confusion around no of CUDA Cores and the number of parallel threads</a></li> </ul> <h4 id="references"><strong>References:</strong></h4> <ul> <li><a href="https://modal.com/gpu-glossary/readme">Modal - GPU Glossary</a> - Read</li> <li><a href="https://www.modular.com/blog/democratizing-compute-part-2-what-exactly-is-cuda">What exactly is “CUDA”? (Democratizing AI Compute, Part 2)</a> - Read</li> </ul>]]></content><author><name></name></author><category term="GPU-NYU"/><category term="GPU"/><summary type="html"><![CDATA[A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.]]></summary></entry><entry><title type="html">Wrapping Up Our ML Foundations Journey</title><link href="https://monishver11.github.io/blog/2025/wrapping-ml-basics/" rel="alternate" type="text/html" title="Wrapping Up Our ML Foundations Journey"/><published>2025-05-17T19:45:00+00:00</published><updated>2025-05-17T19:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/wrapping-ml-basics</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/wrapping-ml-basics/"><![CDATA[<p>After completing 51 comprehensive blog posts on machine learning fundamentals, it’s time to wrap up this series. This journey has been both challenging and immensely rewarding.</p> <p><strong>There were several key motivations behind creating this content:</strong></p> <ol> <li>To provide a <strong>structured approach</strong> that methodically builds intuition for a solid foundational understanding of machine learning</li> <li>To explain complex concepts in an <strong>accessible way</strong> that’s easy to remember and articulate</li> <li>To ensure <strong>mathematical rigor</strong> is maintained while keeping explanations clear and thorough</li> <li>To create a resource I wish I had when first learning these concepts</li> </ol> <p>I believe I’ve accomplished these goals, and I’m genuinely satisfied with the outcome.</p> <p><strong>Looking back at our path:</strong></p> <ul> <li>We began with essential <strong>mathematical prerequisites</strong> (multivariate calculus, linear algebra, probability theory) to build a strong foundation</li> <li>Progressed to the <strong>fundamentals of machine learning</strong> with supervised learning and empirical risk minimization</li> <li>Explored <strong>optimization techniques</strong> through gradient descent and stochastic gradient descent</li> <li>Examined various <strong>loss functions and regularization approaches</strong> to understand model development</li> <li>Delved into <strong>linear models</strong> and their extensions with SVMs and margin classifiers</li> <li>Advanced to <strong>nonlinear feature maps and kernels</strong> to tackle more complex problems</li> <li>Investigated <strong>probabilistic modeling and Bayesian approaches</strong> for a different perspective on machine learning</li> <li>Addressed <strong>multiclass classification</strong> methods and structured prediction</li> <li>Introduced <strong>decision trees</strong> as our first truly non-linear classifiers</li> <li>Concluded with <strong>ensemble methods</strong> from bagging and random forests to various boosting algorithms, culminating with gradient boosting</li> </ul> <p>This progression represents a logical flow from fundamentals to advanced concepts, carefully designed to build upon each previous lesson.</p> <p><strong>I have several plans for the future:</strong></p> <ol> <li> <p>I intend to cover the <strong>foundational aspects of deep learning</strong> in a similar structured fashion, though not immediately. I will ensure this content aligns well with the machine learning material we’ve already covered.</p> </li> <li> <p>Following advice from a friend, I recognize the importance of focusing on <strong>depth in ML and practical applications</strong>. I’ll be dedicating time to projects that enhance my practical experience and will share insights as they develop.</p> </li> <li> <p>I welcome suggestions for <strong>specific topics</strong> you’d like to see broken down in this style, or if you have interesting ML ideas for collaboration. Feel free to DM me.</p> </li> <li> <p>With summer break approaching, I’ll be taking some time to <strong>rest and recharge</strong> after this challenging but highly educational semester.</p> </li> </ol> <p>I’d like to express my sincere gratitude to:</p> <ul> <li>My professors for their excellent teaching of these complex subjects, especially Professor <a href="https://mengyeren.com/">Mengye Ren</a>, who taught me this course.</li> <li>Everyone who supported me throughout this journey</li> <li>You, the readers, for your engagement and feedback</li> </ul> <h5 id="final-thoughts"><strong>Final Thoughts</strong></h5> <p>This project began with the aim of clarifying machine learning concepts for myself and others. As we progressed from basic mathematical foundations all the way to advanced ensemble methods like gradient boosting, I hope these explanations have helped demystify machine learning and provided you with both theoretical understanding and practical insights.</p> <p>The complete list of topics is available at: <a href="https://monishver11.github.io/blog/category/ml-nyu/">ML-NYU Category</a> or <a href="https://drive.google.com/file/d/1t1r7w_0gSJIcaEATQlAn1gyMU8yM582v/view?usp=sharing">This list</a></p> <p>Until we meet again in future learning adventures, keep exploring, stay curious, and never stop learning!</p> <hr/>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.]]></summary></entry><entry><title type="html">Gradient Boosting in Practice</title><link href="https://monishver11.github.io/blog/2025/gb-in-practice/" rel="alternate" type="text/html" title="Gradient Boosting in Practice"/><published>2025-05-15T01:10:00+00:00</published><updated>2025-05-15T01:10:00+00:00</updated><id>https://monishver11.github.io/blog/2025/gb-in-practice</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/gb-in-practice/"><![CDATA[<p>In the previous post, we introduced Gradient Boosting with logistic loss, discussing how weak learners can be sequentially combined to minimize differentiable loss functions using functional gradient descent. Now, we turn our attention to how this elegant theory translates into robust, high-performing models in practice, particularly focusing on regularization techniques that help control overfitting and improve generalization.</p> <hr/> <h5 id="preventing-overfitting-in-gradient-boosting"><strong>Preventing Overfitting in Gradient Boosting</strong></h5> <p>While <strong>gradient boosting</strong> is often surprisingly resistant to overfitting compared to other ensemble methods like bagging, it is not immune. Its resilience stems from several key characteristics of the boosting process:</p> <ul> <li> <p><strong>Implicit Feature Selection</strong>:<br/> One of the reasons gradient boosting is relatively resistant to overfitting is that it performs <strong>feature selection implicitly</strong> during training. At each boosting iteration, the algorithm fits a weak learner often a decision tree to the current pseudo-residuals (the gradients of the loss).</p> <p>Decision trees naturally perform feature selection: they evaluate all available features and choose the one that offers the best split (i.e., the greatest reduction in loss). This means that only the most predictive features are chosen at each step. Over multiple rounds, this leads to an ensemble that selectively and adaptively focuses on the most informative parts of the input space.</p> <p>As a result, even in high-dimensional settings or with noisy features, gradient boosting can often avoid overfitting by not relying too heavily on irrelevant or redundant features. It prioritizes features that consistently contribute to reducing the loss, acting like a built-in greedy feature selection mechanism.</p> </li> <li> <p><strong>Localized Additive Updates</strong>:<br/> In gradient boosting, each weak learner (such as a shallow decision tree) is trained to correct the mistakes of the current model. Because these learners are typically low-capacity (e.g., stumps or small trees), their predictions only affect specific regions of the input space - they’re “localized” in that sense.</p> <p>As boosting progresses, the model becomes more accurate, and the residuals (errors) it tries to fix get smaller and more focused. Consequently, the subsequent learners make increasingly smaller and more targeted updates. This means that instead of making large, sweeping changes to the model, later learners adjust predictions only in regions where the model is still wrong.</p> <p>This gradual, fine-grained updating process helps avoid overfitting, as the model doesn’t overreact to noise or outliers - it incrementally improves where it matters most.</p> </li> </ul> <p>Together, these mechanisms help gradient boosting maintain a balance between flexibility and generalization. However, it can still overfit when models are overly complex or trained for too many iterations - which is why regularization techniques such as shrinkage, subsampling, and tree size constraints are essential in practice.</p> <ul> <li> <p><strong>Shrinkage</strong>: Use a small learning rate (step size) \(\lambda\) to scale the contribution of each weak learner:</p> \[f_m(x) = f_{m-1}(x) + \lambda \cdot v_m h_m(x)\] <p>Smaller \(\lambda\) slows down the learning process, often resulting in better generalization.</p> </li> <li><strong>Stochastic Gradient Boosting</strong>: Instead of using the full dataset at each boosting round, randomly sample a subset of training examples.</li> <li><strong>Feature (Column) Subsampling</strong>: Randomly select a subset of input features for each boosting iteration.</li> </ul> <p>These methods inject randomness into the learning process and reduce variance, both of which help mitigate overfitting.</p> <h5 id="step-size-as-regularization"><strong>Step Size as Regularization</strong></h5> <p>One of the simplest and most effective ways to regularize gradient boosting is by adjusting the <strong>step size</strong> or learning rate. This directly controls how far the model moves in the direction of the negative gradient at each step. Smaller step sizes lead to slower learning, but allow the model to build more nuanced approximations of the target function.</p> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-5-480.webp 480w,/assets/img/gb-5-800.webp 800w,/assets/img/gb-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Left is training set &amp; right is validation set </div> <p>This effect is clearly observed in tasks like <strong>Sinc function regression</strong>(which we saw in the last post - <a href="https://monishver11.github.io/blog/2025/binomial-boost/">BinomialBoost</a>), where the number of boosting rounds and the choice of shrinkage rate dramatically affect both training and validation performance. Lower learning rates typically result in better generalization when combined with more boosting rounds.</p> <h5 id="stochastic-gradient-boosting"><strong>Stochastic Gradient Boosting</strong></h5> <p>To improve efficiency and reduce overfitting, we can employ <strong>stochastic gradient boosting</strong> - a strategy analogous to minibatch gradient descent in optimization. So in minibatch, for each boosting round:</p> <ul> <li>Randomly sample a subset of the training data.</li> <li>Fit the base learner using this subset.</li> <li>Use it to approximate the gradient and update the model.</li> </ul> <p>Benefits of this approach include:</p> <ul> <li><strong>Regularization</strong>: Injecting randomness into the training process helps prevent overfitting.</li> <li><strong>Efficiency</strong>: Training is faster per iteration since we work with fewer data points.</li> <li><strong>Improved performance</strong>: Empirically, this often leads to better results for the same computational budget.</li> </ul> <h5 id="column-feature-subsampling"><strong>Column (Feature) Subsampling</strong></h5> <p>Another effective technique borrowed from Random Forests is <strong>column or feature subsampling</strong>. For each boosting round:</p> <ul> <li>Randomly select a subset of input features.</li> <li>Learn a weak learner using only this subset.</li> </ul> <p>This further reduces correlation between individual learners and acts as a strong regularizer. In fact, the <strong>XGBoost</strong> paper emphasizes that column subsampling can reduce overfitting <strong>even more</strong> effectively than row subsampling.</p> <p>Additionally, limiting the number of features also improves training speed, especially for high-dimensional datasets.</p> <hr/> <h5 id="summary-gradient-boosting-as-a-versatile-framework"><strong>Summary: Gradient Boosting as a Versatile Framework</strong></h5> <p>Let’s step back and summarize the key takeaways:</p> <ul> <li><strong>Motivation</strong>: Combine many weak learners (e.g., shallow trees) to build a strong predictor.</li> <li><strong>Statistical view</strong>: Fit an additive model greedily, one function at a time.</li> <li><strong>Optimization view</strong>: Boosting makes local improvement iteratively and perform gradient descent in function space.</li> </ul> <p>Gradient Boosting is a <strong>flexible and powerful meta-algorithm</strong>:</p> <ul> <li>Supports <strong>any differentiable loss function</strong></li> <li>Applicable to <strong>classification, regression, ranking, multiclass tasks</strong>, and more</li> <li>Highly <strong>scalable</strong> with implementations like <strong>XGBoost</strong>, <strong>LightGBM</strong>, and <strong>CatBoost</strong></li> </ul> <p>With proper regularization, including shrinkage, stochastic updates, and feature subsampling - Gradient Boosting becomes one of the most effective tools in the machine learning toolkit.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.]]></summary></entry><entry><title type="html">BinomialBoost</title><link href="https://monishver11.github.io/blog/2025/binomial-boost/" rel="alternate" type="text/html" title="BinomialBoost"/><published>2025-05-10T00:58:00+00:00</published><updated>2025-05-10T00:58:00+00:00</updated><id>https://monishver11.github.io/blog/2025/binomial-boost</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/binomial-boost/"><![CDATA[<p>In the previous post, we introduced the <strong>Gradient Boosting framework</strong> as functional gradient descent, where we minimize a loss function by iteratively adding base learners that approximate the negative gradient (pseudo-residuals) of the loss. We demonstrated this with the squared loss, where residuals had a direct and intuitive interpretation. In this post, we extend that idea to <strong>logistic loss</strong>, which is more appropriate for binary classification tasks. This special case is often referred to as <strong>BinomialBoost</strong>.</p> <hr/> <h5 id="logistic-loss-and-pseudo-residuals"><strong>Logistic Loss and Pseudo-Residuals</strong></h5> <p>For binary classification with labels \(Y = \{-1, 1\}\), the <strong>logistic loss</strong> is given by:</p> \[\ell(y, f(x)) = \log(1 + e^{-y f(x)})\] <p>At each boosting iteration, we need to compute the <strong>pseudo-residuals</strong>, which are the negative gradients of the loss with respect to the model’s prediction. For the \(i\)-th training example:</p> \[\begin{aligned} r_i &amp;= - \frac{\partial}{\partial f(x_i)} \ell(y_i, f(x_i)) \\ &amp;= - \frac{\partial}{\partial f(x_i)} \log(1 + e^{-y_i f(x_i)}) \\ &amp;= y_i \cdot \frac{e^{-y_i f(x_i)}}{1 + e^{-y_i f(x_i)}} \\ &amp;= \frac{y_i}{1 + e^{y_i f(x_i)}} \end{aligned}\] <p>These pseudo-residuals guide the model by indicating how each example’s prediction should be adjusted to reduce the classification loss.</p> <h5 id="step-direction-and-boosting-update"><strong>Step Direction and Boosting Update</strong></h5> <p>Once we compute the pseudo-residuals \(r_i\), the next base learner \(h_m \in \mathcal{H}\) is fit to match them in a least squares sense:</p> \[h_m = \arg\min_{h \in \mathcal{H}} \sum_{i=1}^n \left( \frac{y_i}{1 + e^{y_i f_{m-1}(x_i)}} - h(x_i) \right)^2\] <p>The model is then updated as:</p> \[f_m(x) = f_{m-1}(x) + \nu h_m(x)\] <p>where \(\nu \in (0, 1]\) is the learning rate or shrinkage parameter and \(f_{m-1}(x)\) is prediction after \(m−1\) rounds.</p> <h5 id="gradient-tree-boosting"><strong>Gradient Tree Boosting</strong></h5> <p>A particularly effective version of gradient boosting uses <strong>regression trees</strong> as base learners. The hypothesis space is:</p> \[\mathcal{H} = \{ \text{regression trees with } S \text{ terminal nodes} \}\] <ul> <li>\(S = 2\) corresponds to decision stumps, a very simple weak learner that makes a prediction based on a single feature threshold.</li> <li>Larger values of \(S\) allow more expressive trees, capable of capturing more complex interactions among features.</li> <li>Common choices for tree size: \(4 \leq S \leq 8\)</li> </ul> <p>Gradient Tree Boosting combines the predictive power of decision trees with the optimization capabilities of functional gradient descent. Each tree fits the pseudo-residuals (i.e., the gradient of the loss), and the overall model evolves by sequentially adding these trees with appropriate scaling (via step size or shrinkage).</p> <p>This approach is widely known as <strong>Gradient Tree Boosting</strong> and is implemented in various software packages:</p> <ul> <li><strong>R</strong>: <code class="language-plaintext highlighter-rouge">gbm</code></li> <li><strong>scikit-learn</strong>: <code class="language-plaintext highlighter-rouge">GradientBoostingClassifier</code>, <code class="language-plaintext highlighter-rouge">GradientBoostingRegressor</code></li> <li><strong>XGBoost</strong>, <strong>LightGBM</strong>: state-of-the-art libraries for scalable, high-performance boosting</li> </ul> <h5 id="visual-example"><strong>Visual Example;</strong></h5> <div class="row justify-content-center"> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-3-480.webp 480w,/assets/img/gb-3-800.webp 800w,/assets/img/gb-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As a simple regression example, we can use an ensemble of decision stumps to fit a noisy version of the sinc function using squared loss. Even shallow learners (depth-1 trees) become powerful when combined via boosting. Here’s what the model looks like after different boosting rounds:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-4-480.webp 480w,/assets/img/gb-4-800.webp 800w,/assets/img/gb-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Decision stumps with 1,10,50, and 100 steps </div> <p>The shrinkage parameter \(\lambda = 1\) is used in this example to simplify learning, though smaller values are typically preferred in practice to prevent overfitting.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>With all the pieces in place, we can summarize what’s needed to implement Gradient Boosting (also known as <strong>AnyBoost</strong>):</p> <ul> <li>A differentiable loss function: e.g., squared loss for regression, logistic loss for classification</li> <li>A base hypothesis space: e.g., regression trees of fixed depth</li> <li>A gradient descent procedure in function space</li> <li>Hyperparameters: step size \(\nu\), number of boosting rounds \(M\), tree size \(S\)</li> </ul> <p>This general and flexible framework can adapt to a wide variety of tasks, making Gradient Boosting one of the most versatile and powerful tools in modern machine learning.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[See how the gradient boosting framework naturally extends to binary classification using the logistic loss.]]></summary></entry><entry><title type="html">Gradient Boosting / “Anyboost”</title><link href="https://monishver11.github.io/blog/2025/gradient-boosting/" rel="alternate" type="text/html" title="Gradient Boosting / “Anyboost”"/><published>2025-05-08T16:56:00+00:00</published><updated>2025-05-08T16:56:00+00:00</updated><id>https://monishver11.github.io/blog/2025/gradient-boosting</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/gradient-boosting/"><![CDATA[<p>In our previous post, we explored how <strong>Forward Stagewise Additive Modeling (FSAM)</strong> with exponential loss recovers the AdaBoost algorithm. But FSAM is not limited to exponential loss — it can be extended to <strong>any differentiable loss</strong>, leading to the powerful and flexible framework of <strong>Gradient Boosting Machines (GBMs)</strong>.</p> <p>This post walks through the derivation of gradient boosting, starting with the squared loss, and builds toward a general functional gradient descent interpretation of boosting.</p> <hr/> <h5 id="fsam-with-squared-loss"><strong>FSAM with Squared Loss</strong></h5> <p>Let’s begin with FSAM using squared loss. At the \(m\)-th boosting round, we wish to find a new function \(v h(x)\) to add to the model \(f_{m-1}(x)\). The objective becomes:</p> \[J(v, h) = \frac{1}{n} \sum_{i=1}^n \left( y_i - \underbrace{[f_{m-1}(x_i) + v h(x_i)]}_{\text{new model}} \right)^2\] <p>If the hypothesis space \(\mathcal{H}\) is closed under rescaling (i.e., if \(h \in \mathcal{H}\) implies \(v h \in \mathcal{H}\) for all \(v \in \mathbb{R}\)), we can drop \(v\) and simplify the objective:</p> \[J(h) = \frac{1}{n} \sum_{i=1}^n \left( \underbrace{[y_i - f_{m-1}(x_i)]}_{\text{residual}} - h(x_i) \right)^2\] <p>This is just <strong>least squares regression on the residuals</strong> — fit \(h(x)\) to approximate the residuals \([y_i - f_{m-1}(x_i)]\).</p> <h5 id="interpreting-the-residuals"><strong>Interpreting the Residuals</strong></h5> <p>Let’s take a closer look at how <strong>residuals relate to gradients</strong> in the context of boosting with squared loss.</p> <p>The objective for squared loss is:</p> \[J(f) = \frac{1}{n} \sum_{i=1}^n (y_i - f(x_i))^2\] <p>This measures how far the model predictions \(f(x_i)\) are from the true labels \(y_i\) across the dataset.</p> <p>To minimize this objective, we can perform gradient descent. So we ask: <strong>what is the gradient of \(J(f)\) with respect to \(f(x_i)\)?</strong></p> <p>Let’s compute it:</p> \[\frac{\partial}{\partial f(x_i)} J(f) = \frac{\partial}{\partial f(x_i)} \left[ \frac{1}{n} \sum_{j=1}^n (y_j - f(x_j))^2 \right]\] <p>Because only the \(i\)-th term depends on \(f(x_i)\), this simplifies to:</p> \[\frac{\partial}{\partial f(x_i)} J(f) = \frac{1}{n} \cdot (-2)(y_i - f(x_i)) = -\frac{2}{n}(y_i - f(x_i))\] <p>The term \(y_i - f(x_i)\) is the <strong>residual</strong> at point \(x_i\). So we see:</p> <blockquote> <p>The residual is proportional to the <strong>negative gradient</strong> of the squared loss.</p> </blockquote> <p><strong>Why This Matters for Boosting</strong></p> <p>Gradient descent updates a parameter in the opposite direction of the gradient. Similarly, in <strong>gradient boosting</strong>, we update our model \(f\) in the direction of a function that tries to approximate the <strong>negative gradient</strong> at every data point.</p> <p>In the case of squared loss, this just means:</p> <ul> <li>Compute the residuals \(r_i = y_i - f(x_i)\).</li> <li>Fit a new base learner \(h_m\) to those residuals.</li> <li>Add \(h_m\) to the model: \(f \leftarrow f + v h_m\).</li> </ul> <p>This process mimics the behavior of <strong>gradient descent in function space</strong>.</p> <p>We can now draw an analogy:</p> <ul> <li><strong>Boosting update:</strong>  \(f \leftarrow f + \textcolor{red}{v} \textcolor{green}{h}\)</li> <li><strong>Gradient descent:</strong> \(f \leftarrow f - \textcolor{red}{\alpha} \textcolor{green}{\nabla_f J(f)}\)</li> </ul> <p><strong>Note: Observe the variables highlighted in red and green.</strong></p> <p>Where:</p> <ul> <li>\(h\) approximates the direction of steepest descent (the gradient),</li> <li>\(v\) is the step size (akin to learning rate \(\alpha\)),</li> <li>and the update improves the model’s predictions to reduce the loss.</li> </ul> <p>This perspective generalizes easily to other loss functions, which we explore in the next sections on <strong>functional gradient descent</strong>.</p> <hr/> <h5 id="functional-gradient-descent-intuition-and-setup"><strong>Functional Gradient Descent: Intuition and Setup</strong></h5> <p>To generalize FSAM to arbitrary (differentiable) loss functions, we adopt the <strong>functional gradient descent</strong> perspective.</p> <p>Suppose we have a loss function that depends on predictions \(f(x_i)\) at \(n\) training examples:</p> \[J(f) = \sum_{i=1}^n \ell(y_i, f(x_i))\] <p>Note that \(f\) is a function, but this loss only depends on \(f\) through its values on the training points. So, we can treat:</p> \[f = (f(x_1), f(x_2), \dots, f(x_n))^\top\] <p>as a vector in \(\mathbb{R}^n\), and write:</p> \[J(f) = \sum_{i=1}^n \ell(y_i, f_i)\] <p>where \(f_i := f(x_i)\). We want to minimize \(J(f)\) by updating our predictions \(f_i\) in the <strong>steepest descent direction</strong>.</p> <h5 id="unconstrained-step-direction-pseudo-residuals"><strong>Unconstrained Step Direction (Pseudo-Residuals)</strong></h5> <p>We compute the gradient of the loss with respect to each prediction:</p> \[g = \nabla_f J(f) = \left( \frac{\partial \ell(y_1, f_1)}{\partial f_1}, \dots, \frac{\partial \ell(y_n, f_n)}{\partial f_n} \right)\] <p>The <strong>negative gradient direction</strong> is:</p> \[-g = -\nabla_f J(f) = \left( -\frac{\partial \ell(y_1, f_1)}{\partial f_1}, \dots, -\frac{\partial \ell(y_n, f_n)}{\partial f_n} \right)\] <p>This tells us how to change each \(f(x_i)\) to decrease the loss — it’s the direction of steepest descent in \(\mathbb{R}^n\).</p> <p>We call this vector the <strong>pseudo-residuals</strong>. In the case of squared loss:</p> \[\ell(y_i, f_i) = (y_i - f_i)^2 \quad \Rightarrow \quad -g_i = y_i - f_i\] <p>So, pseudo-residuals coincide with actual residuals for squared loss.</p> <h5 id="projection-step-fitting-the-pseudo-residuals"><strong>Projection Step: Fitting the Pseudo-Residuals</strong></h5> <p>We now want to update the function \(f\) by stepping in direction \(-g\). However, we can’t directly take a step in \(\mathbb{R}^n\) — we must stay within our base hypothesis space \(\mathcal{H}\).</p> <p>So we find the <strong>function \(h \in \mathcal{H}\)</strong> that best fits the negative gradient at the training points. This is a projection of \(-g\) onto the function class:</p> \[h = \arg\min_{h \in \mathcal{H}} \sum_{i=1}^n \left( -g_i - h(x_i) \right)^2\] <p>This is just <strong>least squares regression</strong> of pseudo-residuals.</p> <p>Once we have \(h\), we take a step:</p> \[f \leftarrow f + v h\] <p>where \(v\) is a step size, which can either be fixed (e.g., shrinkage factor \(\lambda\)) or found via line search.</p> <p><strong>All Together;</strong></p> <ul> <li> <p><strong>Objective</strong>: \(J(f) = \sum_{i=1}^n \ell(y_i, f(x_i))\)</p> </li> <li> <p><strong>Unconstrained gradient</strong>: \(g = \nabla_f J(f) = \left( \frac{\partial \ell(y_1, f_1)}{\partial f_1}, \dots, \frac{\partial \ell(y_n, f_n)}{\partial f_n} \right) \tag{25}\)</p> </li> <li> <p><strong>Projection</strong>: \(h = \arg\min_{h \in \mathcal{H}} \sum_{i=1}^n (-g_i - h(x_i))^2 \tag{26}\)</p> </li> <li> <p><strong>Boosting update</strong>: \(f \leftarrow f + v h\)</p> </li> </ul> <p>This gives a general recipe for boosting: <strong>approximate the negative gradient with a weak learner and take a small step in that direction</strong> — hence the name <strong>gradient boosting</strong>.</p> <h5 id="functional-gradient-descent-hyperparameters-and-regularization"><strong>Functional Gradient Descent: Hyperparameters and Regularization</strong></h5> <p>Once we have our base learner \(h_m\) for iteration \(m\), we need to decide how far to move in that direction.</p> <p>We can either:</p> <ul> <li> <p><strong>Choose a step size \(v_m\) by line search</strong>: \(v_m = \arg\min_v \sum_{i=1}^n \ell\left(y_i, f_{m-1}(x_i) + v h_m(x_i)\right)\)</p> </li> <li> <p><strong>Or</strong> use a fixed step size \(v\) as a hyperparameter. Line search is not strictly necessary but can improve performance.</p> </li> </ul> <p>To <strong>regularize</strong> and control overfitting, we scale the step size with a <strong>shrinkage factor</strong> \(\lambda \in (0, 1]\):</p> \[f_m(x) = f_{m-1}(x) + \lambda v_m h_m(x)\] <p>This shrinkage slows down learning and typically improves generalization. A common choice is \(\lambda = 0.1\).</p> <p>We also need to decide:</p> <ul> <li>The <strong>number of steps \(M\)</strong> (i.e., number of boosting rounds). <ul> <li>This is typically chosen via early stopping or tuning on a validation set.</li> </ul> </li> </ul> <hr/> <h5 id="gradient-boosting-algorithm"><strong>Gradient Boosting Algorithm</strong></h5> <p>Putting everything together, the gradient boosting algorithm proceeds as follows:</p> <ol> <li> <p><strong>Initialize the model</strong> with a constant function: \(f_0(x) = \arg\min_\gamma \sum_{i=1}^n \ell(y_i, \gamma)\)</p> </li> <li> <p><strong>For</strong> \(m = 1\) to \(M\):</p> <ol> <li> <p><strong>Compute the pseudo-residuals</strong> (i.e., the negative gradients):</p> \[r_{im} = -\left[ \frac{\partial}{\partial f(x_i)} \ell(y_i, f(x_i)) \right]_{f(x_i) = f_{m-1}(x_i)}\] </li> <li> <p><strong>Fit a base learner</strong> \(h_m\) (e.g., regression tree) to the dataset \(\{(x_i, r_{im})\}_{i=1}^n\) using squared error.</p> </li> <li> <p><em>(Optional)</em> <strong>Line search</strong> to find best step size: \(v_m = \arg\min_v \sum_{i=1}^n \ell(y_i, f_{m-1}(x_i) + v h_m(x_i))\)</p> </li> <li> <p><strong>Update the model</strong>: \(f_m(x) = f_{m-1}(x) + \lambda v_m h_m(x)\)</p> </li> </ol> </li> <li> <p><strong>Return</strong> the final model: \(f_M(x)\)</p> </li> </ol> <hr/> <h5 id="conclusion-the-gradient-boosting-machine-ingredients"><strong>Conclusion: The Gradient Boosting Machine Ingredients</strong></h5> <p>To implement gradient boosting, you need:</p> <ul> <li>A loss function \(\ell(y, f(x))\) that is differentiable w.r.t. \(f(x)\)</li> <li>A base hypothesis space \(\mathcal{H}\) (e.g., decision trees) for regression</li> <li>A method to choose step size: fixed, or via line search</li> <li>A stopping criterion: number of iterations \(M\), or early stopping</li> <li>Regularization through <strong>shrinkage</strong> (\(\lambda\))</li> </ul> <p>Once these ingredients are in place, you’re ready to build powerful models with <strong>Gradient Boosting Machines (GBMs)</strong>!</p> <p>In the next post, we’ll explore specific loss functions like <strong>logistic loss</strong> for classification and how gradient boosting works in this setting.</p> <p>Take care!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.]]></summary></entry><entry><title type="html">Forward Stagewise Additive Modeling</title><link href="https://monishver11.github.io/blog/2025/FSAM/" rel="alternate" type="text/html" title="Forward Stagewise Additive Modeling"/><published>2025-05-04T23:04:00+00:00</published><updated>2025-05-04T23:04:00+00:00</updated><id>https://monishver11.github.io/blog/2025/FSAM</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/FSAM/"><![CDATA[<p>In the previous post, we saw how learning both the weights and basis functions gives rise to adaptive models — and how neural networks and decision trees fit into this framework. Now, we dive into <strong>Forward Stagewise Additive Modeling (FSAM)</strong>, a greedy algorithm that forms the foundation of <strong>Gradient Boosting</strong>.</p> <hr/> <h5 id="what-is-fsam"><strong>What is FSAM?</strong></h5> <p>Our goal is to fit a model of the form:</p> \[f(x) = \sum_{m=1}^M v_m h_m(x)\] <p>where each \(h_m \in \mathcal{H}\) is a basis function, and \(v_m\) is its weight. The key idea is to <strong>greedily</strong> fit one function at a time without changing previously fitted ones. That’s why it’s called <strong>forward stagewise</strong>.</p> <p>After \(m - 1\) steps, we have:</p> \[f_{m-1}(x) = \sum_{i=1}^{m-1} v_i h_i(x)\] <p>At step \(m\), we select a new basis function \(h_m \in \mathcal{H}\) and weight \(v_m &gt; 0\) to form:</p> \[f_m(x) = \underbrace{ f_{m-1}(x) }_{\text{fixed}} + v_m h_m(x)\] <p>We choose \((v_m, h_m)\) to minimize the loss as much as possible.</p> <h5 id="fsam-for-empirical-risk-minimization-erm"><strong>FSAM for Empirical Risk Minimization (ERM)</strong></h5> <p>Let’s apply FSAM to an ERM objective. We proceed as follows:</p> <ol> <li> <p><strong>Initialize:</strong></p> \[f_0(x) = 0\] </li> <li><strong>For</strong> \(m = 1\) to \(M\): <ul> <li> <p>Compute:</p> \[(v_m, h_m) = \arg\min_{v \in \mathbb{R}, h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n \ell\left(y_i, f_{m-1}(x_i) + v h(x_i)\right)\] </li> <li> <p>Update:</p> \[f_m(x) = f_{m-1}(x) + v_m h_m(x)\] </li> </ul> </li> <li><strong>Return</strong> \(f_M\)</li> </ol> <hr/> <h5 id="exponential-loss"><strong>Exponential Loss</strong></h5> <p>Let’s use the <strong>exponential loss</strong>:</p> \[\ell(y, f(x)) = \exp\left( -y f(x) \right)\] <p>This loss function is margin-based, it penalizes examples based on how confidently they are classified.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-2-480.webp 480w,/assets/img/gb-2-800.webp 800w,/assets/img/gb-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="fsam-with-exponential-loss"><strong>FSAM with Exponential Loss</strong></h5> <p>Apply the FSAM steps using exponential loss:</p> <ol> <li> <p><strong>Initialize:</strong></p> \[f_0(x) = 0\] </li> <li><strong>For</strong> \(m = 1\) to \(M\): <ul> <li> <p>Compute:</p> \[(v_m, h_m) = \arg\min_{v \in \mathbb{R}, h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n \exp\left(-y_i \left( f_{m-1}(x_i) + \overbrace{v h(x_i)}^{\text{new piece}} \right)\right)\] </li> <li> <p>Update:</p> \[f_m(x) = f_{m-1}(x) + v_m h_m(x)\] </li> </ul> </li> <li><strong>Return</strong> \(f_M\)</li> </ol> <hr/> <h5 id="fsam-with-exponential-loss-basis-function"><strong>FSAM with Exponential Loss: Basis Function</strong></h5> <p>We assume the base hypothesis space is:</p> \[\mathcal{H} = \left\{ h : \mathcal{X} \to \{-1, 1\} \right\}\] <p>At the \(m^\text{th}\) round, our goal is to choose \(v\) and \(h\) to minimize the following objective:</p> \[J(v, h) = \sum_{i=1}^n \exp\left[ -y_i \left( f_{m-1}(x_i) + v h(x_i) \right) \right]\] <p>We define:</p> \[w_i^m \triangleq \exp\left( -y_i f_{m-1}(x_i) \right)\] <p>This lets us re-express the objective as:</p> \[J(v, h) = \sum_{i=1}^n w_i^m \exp\left( -y_i v h(x_i) \right)\] <p>Now, because \(h(x_i) \in \{-1, 1\}\), we can split the expression into cases:</p> \[J(v, h) = \sum_{i=1}^n w_i^m \left[ \mathbb{I}(y_i = h(x_i)) e^{-v} + \mathbb{I}(y_i \ne h(x_i)) e^{v} \right]\] <p>Recall that:</p> \[\mathbb{I}(y_i = h(x_i)) = 1 - \mathbb{I}(y_i \ne h(x_i))\] <p>Using this identity, we can further simplify:</p> \[J(v, h) = \sum_{i=1}^n w_i^m \left[ (e^v - e^{-v}) \mathbb{I}(y_i \ne h(x_i)) + e^{-v} \right]\] <p>At this point, we’re ready to decide how to pick the best \(h\).</p> <p>Note that the second term of the objective function, \(e^{-v}\) is constant with respect to \(h\), and if \(v &gt; 0\), the term \(e^v - e^{-v}\) is positive. Therefore, minimizing \(J(v, h)\) is equivalent to minimizing:</p> \[\arg \min_{h \in \mathcal{H}} \sum_{i=1}^n w_i^m \mathbb{I}(y_i \ne h(x_i))\] <p>This leads to:</p> \[h_m = \arg \min_{h \in \mathcal{H}} \sum_{i=1}^n w_i^m \mathbb{I}(y_i \ne h(x_i))\] <p>We can also write this as a weighted classification error:</p> \[h_m = \arg \min_{h \in \mathcal{H}} \frac{1}{\sum_{i=1}^n w_i^m} \sum_{i=1}^n w_i^m \mathbb{I}(y_i \ne h(x_i))\] <p>In other words, \(h_m\) is the classifier that minimizes the <strong>weighted zero-one loss</strong>.</p> <hr/> <h5 id="fsam-with-exponential-loss-classifier-weights"><strong>FSAM with Exponential Loss: Classifier Weights</strong></h5> <p>Now that we’ve selected the best basis function \(h_m\), let’s figure out how to choose the best weight \(v_m\) for it.</p> <p>We define the <strong>weighted zero-one error</strong> at round \(m\) as:</p> \[\text{err}_m = \frac{\sum_{i=1}^n w_i^m \mathbb{I}(y_i \ne h(x_i))}{\sum_{i=1}^n w_i^m}\] <p>This error measures how poorly the current hypothesis \(h\) is doing, taking into account the importance (weights) of each example.</p> <p>Then, it can be shown that the optimal value for \(v_m\) — the coefficient for the current basis function is:</p> \[v_m = \frac{1}{2} \log \frac{1 - \text{err}_m}{\text{err}_m} \tag{14}\] <p>This is exactly the same form as the weight update in AdaBoost (just scaled differently). Note that if \(\text{err}_m &lt; 0.5\) — that is, our current classifier is better than random guessing — then \(v_m &gt; 0\), meaning it contributes positively.</p> <hr/> <p><strong>Derivation of the expression for the optimal classifier weights</strong></p> <p>To justify this expression for the optimal coefficient \(v_m\), recall that our goal at each round is to minimize the exponential loss objective:</p> \[J(v, h) = \sum_{i=1}^n w_i^m \left[ \exp(-v) \cdot \mathbb{I}(y_i = h(x_i)) + \exp(v) \cdot \mathbb{I}(y_i \ne h(x_i)) \right]\] <p>We can rewrite this as:</p> \[J(v, h) = \exp(-v) \sum_{i: y_i = h(x_i)} w_i^m + \exp(v) \sum_{i: y_i \ne h(x_i)} w_i^m\] <p>Define the <strong>weighted error</strong> of hypothesis \(h\) as:</p> \[\text{err}_m = \frac{\sum_{i=1}^n w_i^m \mathbb{I}(y_i \ne h(x_i))}{\sum_{i=1}^n w_i^m}\] <p>Let:</p> <ul> <li>\(W^{+} = \sum_{i: y_i = h(x_i)} w_i^m\) (correctly classified)</li> <li>\(W^{-} = \sum_{i: y_i \ne h(x_i)} w_i^m\) (misclassified)</li> </ul> <p>So the objective becomes:</p> \[J(v, h) = W^+ \exp(-v) + W^- \exp(v)\] <p>To find the optimal \(v = v_m\) for a fixed \(h = h_m\), we differentiate with respect to \(v\) and set to zero:</p> \[\frac{dJ}{dv} = -W^+ \exp(-v) + W^- \exp(v) = 0\] <p>Solving:</p> \[W^- \exp(v) = W^+ \exp(-v) \\ \Rightarrow \frac{W^-}{W^+} = \exp(-2v) \\ \Rightarrow -2v = \log\left( \frac{W^-}{W^+} \right) \\ \Rightarrow v = \frac{1}{2} \log\left( \frac{W^+}{W^-} \right)\] <p>Since:</p> \[\text{err}_m = \frac{W^-}{W^- + W^+} \quad \text{and} \quad 1 - \text{err}_m = \frac{W^+}{W^- + W^+}\] <p>We get:</p> \[\frac{W^+}{W^-} = \frac{1 - \text{err}_m}{\text{err}_m}\] <p>Therefore, the optimal weight for classifier \(h_m\) is:</p> \[v_m = \frac{1}{2} \log\left( \frac{1 - \text{err}_m}{\text{err}_m} \right) \tag{14}\] <p>This result balances the classifier’s confidence based on how well it performs: higher confidence (larger \(v_m\)) when error is low, and lower confidence when error is close to 0.5.</p> <hr/> <h5 id="fsam-with-exponential-loss-updating-example-weights"><strong>FSAM with Exponential Loss: Updating Example Weights</strong></h5> <p>After choosing the best classifier \(h_m\) and its corresponding weight \(v_m\), we now update the example weights for the next round.</p> <p>The weights at round \(m+1\) are defined as:</p> \[w_i^{m+1} \overset{\text{def}}{=} \exp\left( -y_i f_m(x_i) \right) \tag{15}\] <p>Recall that the updated model at round \(m\) is:</p> \[f_m(x_i) = f_{m-1}(x_i) + v_m h_m(x_i) \tag{16}\] <p>Substituting this into the definition of the new weights:</p> \[w_i^{m+1} = \exp\left( -y_i (f_{m-1}(x_i) + v_m h_m(x_i)) \right) \\ = \exp\left( -y_i f_{m-1}(x_i) \right) \cdot \exp\left( -y_i v_m h_m(x_i) \right) \\ = w_i^m \cdot \exp\left( -y_i v_m h_m(x_i) \right) \tag{17}\] <p>This form shows how the current round’s prediction (\(h_m(x_i)\)) affects the weight of example \(i\) moving forward.</p> <p>Let’s interpret this based on classification correctness:</p> <ul> <li> <p>If \(y_i = h_m(x_i)\) (i.e., correctly classified), then:</p> \[-y_i v_m h_m(x_i) = -v_m \cdot 1 = -v_m\] </li> <li> <p>If \(y_i \ne h_m(x_i)\) (i.e., misclassified), since both \(y_i\) and \(h_m(x_i)\) are in \(\{-1, 1\}\):</p> \[-y_i v_m h_m(x_i) = -v_m \cdot (-1) = +v_m\] </li> </ul> <p>We can now re-express the update in terms of the indicator function:</p> \[w_i^{m+1} = w_i^m \cdot \exp\left( 2v_m \mathbb{I}(y_i \ne h_m(x_i)) \right) \cdot \underbrace{\exp(-v_m)}_{\text{constant scaler}} \tag{18}\] <p>This is because:</p> <ul> <li>When \(y_i = h_m(x_i)\), \(\mathbb{I}(y_i \ne h_m(x_i)) = 0\), so the exponent is \(0\) and we just get \(w_i^m \cdot \exp(-v_m)\).</li> <li>When \(y_i \ne h_m(x_i)\), \(\mathbb{I}(y_i \ne h_m(x_i)) = 1\), so the exponent is \(2v_m\), and the weight becomes \(w_i^m \cdot \exp(v_m)\).</li> </ul> <p><strong>Interpretation</strong></p> <ul> <li><strong>Correct classification</strong>: the weight gets multiplied by \(\exp(-v_m)\) → the example becomes <strong>less important</strong>.</li> <li><strong>Misclassification</strong>: the weight gets multiplied by \(\exp(v_m)\) → the example becomes <strong>more important</strong>.</li> </ul> <p>This mechanism focuses the learner on harder examples in future rounds, by increasing their influence.</p> <blockquote> <p>The constant factor \(\exp(-v_m)\) appears in all weights and <strong>cancels out during normalization</strong>. So only the relative importance matters.</p> </blockquote> <p><strong>Connection to AdaBoost</strong></p> <p>Observe that:</p> \[2v_m = \alpha_m\] <p>This matches the AdaBoost formulation, where \(\alpha_m\) is the weight assigned to the classifier at round \(m\).</p> <p>Hence, <strong>FSAM with exponential loss recovers AdaBoost’s update rule</strong>, making it a principled derivation from a loss minimization perspective.</p> <hr/> <h5 id="why-use-exponential-loss"><strong>Why Use Exponential Loss?</strong></h5> <p>The exponential loss function is given by:</p> \[\ell_{\text{exp}}(y, f(x)) = \exp(-y f(x))\] <p>This loss has an elegant statistical interpretation. Specifically, it turns out that minimizing the expected exponential loss leads to a prediction function that estimates the <strong>log-odds</strong> of the label being positive:</p> \[f^*(x) = \frac{1}{2} \log \left( \frac{p(y = 1 \mid x)}{p(y = -1 \mid x)} \right)\] <p>This result aligns with the principle behind <strong>logistic regression</strong>, where the model estimates the log-odds of class membership. Here’s how we can justify it:</p> <p><strong>Derivation Sketch</strong></p> <p>We seek the function \(f^*(x)\) that minimizes the expected exponential loss:</p> \[\mathbb{E}_{y \sim p(y \mid x)}\left[ \exp(-y f(x)) \right] = p(y = 1 \mid x) \exp(-f(x)) + p(y = -1 \mid x) \exp(f(x))\] <p>Define:</p> <ul> <li> \[p_+ = p(y = 1 \mid x)\] </li> <li> \[p_- = p(y = -1 \mid x) = 1 - p_+\] </li> </ul> <p>So, the expected loss becomes:</p> \[L(f) = p_+ \exp(-f) + p_- \exp(f)\] <p>Take the derivative w.r.t. \(f\) and set to zero:</p> \[\frac{dL}{df} = -p_+ \exp(-f) + p_- \exp(f) = 0 \\ \Rightarrow p_- \exp(f) = p_+ \exp(-f) \\ \Rightarrow \frac{p_-}{p_+} = \exp(-2f) \\ \Rightarrow f^*(x) = \frac{1}{2} \log \left( \frac{p_+}{p_-} \right)\] <p>Thus,</p> \[f^*(x) = \frac{1}{2} \log \left( \frac{p(y = 1 \mid x)}{p(y = -1 \mid x)} \right)\] <p><strong>Interpretation</strong></p> <p>The exponential loss encourages predictions that align with the <strong>log-odds ratio</strong>. This makes it a natural choice for binary classification problems where confidence is important, and it helps explain why AdaBoost, which minimizes exponential loss, is such a powerful classifier.</p> <hr/> <h5 id="adaboost-and-exponential-loss-robustness-issues"><strong>AdaBoost and Exponential Loss: Robustness Issues</strong></h5> <p>While exponential loss has nice theoretical and computational properties, it comes with a key drawback — <strong>lack of robustness</strong>.</p> <p>Recall that the exponential loss is:</p> \[\ell_{\text{exp}}(y, f(x)) = \exp(-y f(x))\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-2-480.webp 480w,/assets/img/gb-2-800.webp 800w,/assets/img/gb-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This function grows <strong>exponentially</strong> as the margin \(y f(x)\) becomes more negative. So:</p> <ul> <li><strong>Misclassified examples</strong> (where \(y f(x) &lt; 0\)) incur <strong>very high penalties</strong>.</li> <li>As a result, <strong>outliers</strong> or <strong>noisy labels</strong> can dominate the loss and heavily influence the model.</li> </ul> <p><strong>Practical Consequences</strong></p> <ul> <li>AdaBoost (which minimizes exponential loss) tends to <strong>over-focus on misclassified points</strong>, even if they are noisy or mislabeled.</li> <li>This leads to <strong>degraded performance</strong> in datasets with: <ul> <li>High <strong>Bayes error rate</strong> (i.e., intrinsic label uncertainty),</li> <li>Significant <strong>label noise</strong>.</li> </ul> </li> </ul> <p>In contrast, <strong>logistic loss</strong> (or log-loss), used in <strong>Logistic Regression</strong>, penalizes mistakes more <strong>gradually</strong> and is <strong>more robust</strong> in such settings.</p> <p><strong>Why Still Use Exponential Loss?</strong></p> <p>Despite these robustness concerns, exponential loss has some <strong>computational advantages</strong>:</p> <ul> <li>It leads to <strong>simpler update rules</strong> (as seen in AdaBoost),</li> <li>The math works out cleanly in boosting settings,</li> <li>It’s easy to implement and analyze.</li> </ul> <p>So in summary:</p> <blockquote> <p><strong>Exponential loss</strong> is powerful and efficient but sensitive to outliers.<br/> <strong>Logistic loss</strong> is more robust, especially when the data is noisy or inherently uncertain.</p> </blockquote> <hr/> <h5 id="wrapping-up"><strong>Wrapping up</strong></h5> <p>In this post, we’ve unpacked how <strong>Forward Stagewise Additive Modeling (FSAM)</strong> builds models by greedily adding base learners — and how, when paired with the <strong>exponential loss</strong>, it naturally recovers the well-known <strong>AdaBoost</strong> algorithm.</p> <p>Key takeaways:</p> <ul> <li>FSAM provides a clean, iterative framework for model building.</li> <li>The <strong>exponential loss</strong> leads to a simple and elegant form of weight updates.</li> <li>AdaBoost emerges as a special case of FSAM with exponential loss, emphasizing misclassified examples via <strong>weighted classification error</strong> and <strong>adaptive reweighting</strong>.</li> </ul> <p>However, this formulation is limited to specific loss functions like the exponential loss.</p> <p><strong>What’s next?</strong><br/> In the upcoming post, we’ll generalize this framework to work with <strong>any differentiable loss function</strong>, leading to the powerful and flexible family of models known as <strong>Gradient Boosted Machines (GBMs)</strong>.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A clear walkthrough of FSAM and its role in boosting with exponential loss.]]></summary></entry><entry><title type="html">Introduction to Gradient Boosting</title><link href="https://monishver11.github.io/blog/2025/intro-gradient-boosting/" rel="alternate" type="text/html" title="Introduction to Gradient Boosting"/><published>2025-05-04T17:33:00+00:00</published><updated>2025-05-04T17:33:00+00:00</updated><id>https://monishver11.github.io/blog/2025/intro-gradient-boosting</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/intro-gradient-boosting/"><![CDATA[<p>In our previous discussions on ensemble methods, we explored <strong>boosting</strong> — a technique designed to reduce bias by combining several <strong>shallow decision trees</strong>, also known as <strong>weak learners</strong>. Each learner in the ensemble focuses on correcting the errors made by its predecessors. This iterative correction process results in a strong predictive model.</p> <p>One prominent boosting method we covered was <strong>AdaBoost</strong>, a powerful off-the-shelf classifier. We touched on how AdaBoost operates by minimizing an exponential loss and adjusting the sample weights accordingly.</p> <p>Now, let’s build on this foundation and transition into more general methods like <strong>Gradient Boosting</strong>, which extends the boosting idea to arbitrary differentiable loss functions.</p> <hr/> <h5 id="from-nonlinear-regression-to-gradient-boosting"><strong>From Nonlinear Regression to Gradient Boosting</strong></h5> <p>Let’s consider the problem of <strong>nonlinear regression</strong>. How can we model a function that fits complex, nonlinear data?</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-1-480.webp 480w,/assets/img/gb-1-800.webp 800w,/assets/img/gb-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>A common approach is to use <strong>basis function models</strong>, which transform the input into a new feature space where a linear model can be applied.</p> <h5 id="linear-models-with-basis-functions"><strong>Linear Models with Basis Functions</strong></h5> <p>We begin with a function of the form:</p> \[f(x) = \sum_{m=1}^{M} v_m h_m(x)\] <p>Here, the \(h_m(x)\) are <strong>basis functions</strong> (also called feature functions), and \(v_m\) are their corresponding coefficients.</p> <p><strong>Example</strong>: In polynomial regression, the basis functions are \(h_m(x) = x^m\).</p> <p>This method works well with standard linear model fitting techniques like <strong>least squares</strong>, <strong>lasso</strong>, or <strong>ridge regression</strong>. However, there’s a limitation: the basis functions \(h_1, \dots, h_M\) are <strong>fixed in advance</strong>.</p> <p>What if we could <strong>learn these basis functions</strong> instead of fixing them?</p> <h5 id="adaptive-basis-function-models"><strong>Adaptive Basis Function Models</strong></h5> <p>This idea leads us to <strong>adaptive basis function models</strong>, where we aim to learn both the coefficients and the basis functions:</p> \[f(x) = \sum_{m=1}^{M} v_m h_m(x), \quad \text{where } h_m \in \mathcal{H}\] <p>Here, \(\mathcal{H}\) is a <strong>base hypothesis space</strong> of functions \(h : \mathcal{X} \to \mathbb{R}\). The combined hypothesis space is:</p> \[\mathcal{F}_M = \left\{ f(x) = \sum_{m=1}^{M} v_m h_m(x) \,\middle|\, v_m \in \mathbb{R},\, h_m \in \mathcal{H} \right\}\] <p>The learnable parameters now include both the weights \(v_m\) and the functions \(h_m\) — this is what makes the model <em>adaptive</em>.</p> <p>Here’s a simple analogy to help build intuition for adaptive basis function models:</p> <blockquote> <p><strong>Think of building a house:</strong> In a linear model with fixed basis functions, it’s like choosing from a catalog of pre-made furniture — your design is limited by what’s available. In contrast, adaptive basis functions let you custom-build each piece of furniture to fit your space and needs perfectly. You’re not just choosing the weights (how much furniture to use), you’re also designing what kind of furniture works best in each room.</p> </blockquote> <p><strong>In ML terms:</strong> Instead of relying on predefined transformations of input features (e.g., fixed polynomials), adaptive models learn which transformations (basis functions) are most useful for capturing the data’s patterns.</p> <hr/> <h5 id="empirical-risk-minimization-erm"><strong>Empirical Risk Minimization (ERM)</strong></h5> <p>To train adaptive basis function models, we rely on the principle of <strong>Empirical Risk Minimization (ERM)</strong>.</p> <p>The idea is simple: among all possible functions in our hypothesis space \(\mathcal{F}_M\), we want to find the one that minimizes the <strong>average loss</strong> over our training data. The loss function \(\ell(y, f(x))\) measures how far our prediction \(f(x)\) is from the true label \(y\) — common choices include squared loss, logistic loss, or exponential loss depending on the task.</p> <p>Formally, our ERM objective is:</p> \[\hat{f} = \arg\min_{f \in \mathcal{F}_M} \frac{1}{n} \sum_{i=1}^n \ell(y_i, f(x_i))\] <p>Now, expanding \(f(x)\) as a sum over learnable basis functions, we write:</p> \[f(x) = \sum_{m=1}^{M} v_m h_m(x)\] <p>Plugging this into the ERM formulation, we get the following optimization objective:</p> \[J(v_1, \dots, v_M, h_1, \dots, h_M) = \frac{1}{n} \sum_{i=1}^n \ell \left( y_i, \sum_{m=1}^M v_m h_m(x_i) \right)\] <p>This is the <strong>ERM objective</strong> we aim to minimize. The challenge now is: how do we optimize this objective when both the weights \(v_m\) and the basis functions \(h_m\) are learnable?</p> <h5 id="gradient-based-methods"><strong>Gradient-Based Methods</strong></h5> <p>Let’s first consider the case where our base hypothesis space \(\mathcal{H}\) is <strong>parameterized</strong> by a vector \(\theta \in \mathbb{R}^b\). That is, each basis function \(h_m\) is written as:</p> \[h(x; \theta_m) \in \mathcal{H}\] <p>With this setup, our model becomes:</p> \[f(x) = \sum_{m=1}^M v_m h(x; \theta_m)\] <p>Substituting into our ERM objective gives:</p> \[J(v_1, \dots, v_M, \theta_1, \dots, \theta_M) = \frac{1}{n} \sum_{i=1}^n \ell \left( y_i, \sum_{m=1}^M v_m h(x_i; \theta_m) \right)\] <p>This formulation is powerful because <strong>we can now differentiate</strong> the loss \(\ell\) with respect to both:</p> <ul> <li>the weights \(v_m\) (which scale the outputs of each basis function), and</li> <li>the parameters \(\theta_m\) (which define the internal structure of each basis function).</li> </ul> <p>If the loss function \(\ell\) is differentiable, and the basis functions \(h(x; \theta)\) are differentiable with respect to \(\theta\), then we can apply <strong>gradient-based optimization</strong> methods such as <strong>Stochastic Gradient Descent (SGD)</strong>, <strong>Adam</strong>, or other first-order techniques to train the model end-to-end.</p> <h5 id="how-neural-networks-fit-in"><strong>How Neural Networks Fit In</strong></h5> <p>A classic example of this framework is a <strong>neural network</strong>:</p> <ul> <li>The network transforms the input \(x\) through multiple layers.</li> <li>The final output is a <strong>linear combination of the neurons in the last hidden layer</strong> — these neurons act as <strong>adaptive basis functions</strong> \(h_m(x; \theta_m)\).</li> <li>The weights \(v_m\) connect these neurons to the output layer.</li> </ul> <p>Since both the neuron activations and the loss functions (like cross-entropy or MSE) are differentiable, we can compute gradients with respect to both \(v_m\) and \(\theta_m\). This is why training neural networks using backpropagation is possible — it’s essentially solving the ERM objective using gradient-based methods.</p> <p><strong>Wait, Aren’t Neural Networks Still Using Fixed Functions?</strong></p> <p>This is a question I found myself asking — and maybe you’re wondering the same:</p> <blockquote> <p><em>“Aren’t neural networks still built from fixed components like linear combinations and activation functions? If so, are we really ‘learning’ basis functions — or just tuning weights inside a fixed structure?”</em></p> </blockquote> <p>That’s a great observation — and you’re absolutely right in noting that <strong>each neuron has a predefined form</strong>: it performs a linear combination of its inputs followed by a non-linear activation, like \(\sigma(w^T x + b)\). The architecture — layers, activation functions, etc. — is fixed before training.</p> <p>So what’s actually being learned?</p> <p>While we don’t change the <em>form</em> of the individual components, neural networks <strong>adaptively construct complex functions</strong> by <strong>composing these fixed units across layers</strong>. Each layer transforms the representation from the previous one, and through this deep composition, the network can approximate highly non-linear functions.</p> <p>Thus, neural networks fall under the broader umbrella of <strong>adaptive basis function models</strong>:</p> <blockquote> <p>We don’t learn new basis function types from scratch, but we do learn <strong>how to combine and configure</strong> them to fit the data.</p> </blockquote> <p>This makes neural networks extremely flexible — capable of learning complex patterns by tuning parameters inside a rich function space defined by their architecture.</p> <p>With all that said, if you’re not familiar with this part about neural networks, no worries — feel free to skip it for now. We’ll dive deeper into it in a dedicated section later on.</p> <p>For now, it’s important to understand that <strong>not all models support gradient-based optimization</strong>. So what happens when our basis functions are <strong>non-differentiable</strong>, like decision trees?</p> <hr/> <h5 id="when-gradient-based-methods-dont-apply"><strong>When Gradient-Based Methods Don’t Apply</strong></h5> <p>So far, we’ve relied on the assumption that our basis functions are differentiable with respect to their parameters — allowing us to optimize the ERM objective using gradient-based methods. But what if this assumption doesn’t hold?</p> <p>Let’s consider an important and widely-used case: when our base hypothesis space \(\mathcal{H}\) consists of <strong>decision trees</strong>.</p> <p>Decision trees are <strong>non-parametric models</strong>. That is, they’re not defined by a small set of continuous parameters like \(\theta \in \mathbb{R}^b\). Instead, they involve discrete decisions — splitting data based on feature thresholds, forming branches and leaf nodes. This poses two fundamental challenges:</p> <ol> <li> <p><strong>Non-differentiability</strong>: Even if we try to assign parameters to a decision tree (e.g., split thresholds, structure), the output of the tree does not change <strong>smoothly</strong> with respect to those parameters. Small changes in a split value can cause <strong>large, abrupt changes</strong> in the prediction.</p> </li> <li> <p><strong>Discontinuous prediction surfaces</strong>: Since decision trees partition the input space into disjoint regions, the function \(h(x)\) they represent is <strong>piecewise constant</strong>, which means it’s flat in regions and jumps discontinuously at split boundaries. Gradients simply don’t exist in such a surface.</p> </li> </ol> <p>Therefore, traditional gradient descent — which relies on computing derivatives — breaks down in this setting.</p> <h5 id="a-greedy-stage-wise-alternative"><strong>A Greedy, Stage-Wise Alternative</strong></h5> <p>Despite the lack of gradients, we can still make progress using a <strong>greedy, stage-wise</strong> approach inspired by <strong>AdaBoost</strong>.</p> <p>Recall how AdaBoost builds an ensemble by sequentially adding weak learners (like trees) to correct the errors of the previous ones. Even though it doesn’t explicitly minimize a loss function using gradients, it turns out that <strong>AdaBoost is implicitly minimizing exponential loss</strong> through a forward stage-wise additive modeling approach.</p> <p>This insight opens the door to generalizing the idea:</p> <blockquote> <p>Can we design a similar stage-wise additive method that <strong>explicitly minimizes a differentiable loss function</strong>, even when our base learners (like trees) are non-differentiable?</p> </blockquote> <p>The answer is <strong>yes</strong> — and this leads us to the powerful technique of <strong>Gradient Boosting</strong>.</p> <h5 id="gradient-boosting"><strong>Gradient Boosting</strong></h5> <p>Gradient Boosting combines the flexibility of additive models with the ability to optimize arbitrary differentiable loss functions — all while using <strong>non-differentiable base learners</strong> like decision trees.</p> <p>But how is that possible?</p> <blockquote> <p>The key idea is to <strong>perform gradient descent not in parameter space, but in function space</strong>.</p> </blockquote> <hr/> <p>In the next post, we’ll explore how Gradient Boosting interprets the optimization problem and fits a new base learner at each step by <strong>approximating the negative gradient</strong> of the loss with respect to the current model’s predictions.</p> <p>This clever trick allows us to iteratively improve the model — even when the individual learners don’t support gradients themselves.</p> <p>Stay tuned!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.]]></summary></entry><entry><title type="html">Boosting and AdaBoost</title><link href="https://monishver11.github.io/blog/2025/adaboost/" rel="alternate" type="text/html" title="Boosting and AdaBoost"/><published>2025-04-27T18:04:00+00:00</published><updated>2025-04-27T18:04:00+00:00</updated><id>https://monishver11.github.io/blog/2025/adaboost</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/adaboost/"><![CDATA[<p>Boosting is a powerful machine learning technique that focuses on reducing the error rate of high-bias estimators by combining many weak learners, typically trained sequentially. Unlike bagging, which trains classifiers in parallel to reduce variance, boosting focuses on improving performance by training classifiers sequentially on reweighted data. The core idea behind boosting is simple: rather than using a large and complex model that may overfit, we train a series of simpler models (typically decision trees) to improve accuracy gradually.</p> <p>In contrast to bagging’s emphasis on parallel training of multiple models on different data subsets, boosting systematically reweights the training examples after each classifier is added, directing the model’s attention to examples that previous classifiers struggled with.</p> <p><strong>Key Intuition:</strong></p> <ul> <li>A <strong>weak learner</strong> is a classifier that performs slightly better than random chance. <ul> <li>Example: A rule such as “If <code class="language-plaintext highlighter-rouge">&lt;keyword&gt;</code> then spam” or “From a friend” then “not spam”.</li> </ul> </li> <li>Weak learners focus on different parts of the data, which may be misclassified by previous models.</li> <li>The final model is a weighted combination of these weak learners, with each learner contributing differently based on its performance.</li> </ul> <p>We will explore a specific boosting algorithm: <strong>AdaBoost</strong> (Freund &amp; Schapire, 1997), which is commonly used with decision trees as weak learners.</p> <hr/> <h5 id="adaboost-setting"><strong>AdaBoost: Setting</strong></h5> <p>For binary classification, where the target variable \(Y = \{-1, 1\}\), AdaBoost uses a base hypothesis space \(H = \{h : X \rightarrow \{-1, 1\}\}\). Common choices for weak learners include:</p> <ul> <li><strong>Decision stumps</strong>: A tree with a single split.</li> <li><strong>Decision trees</strong> with a few terminal nodes.</li> <li><strong>Linear decision functions</strong>.</li> </ul> <p><strong>Weighted Training Set</strong></p> <p>Each weak learner in AdaBoost is trained on a <strong>weighted</strong> version of the training data. The training set \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\) has weights associated with each example: \(w_1, w_2, \dots, w_n\).</p> <p>The <strong>weighted empirical risk</strong> is defined as:</p> \[\hat{R}_n^w(f) \overset{\text{def}}{=} \frac{1}{W} \sum_{i=1}^{n} w_i \cdot \ell(f(x_i), y_i)\] <p>where \(W = \sum_{i=1}^{n} w_i\), and \(\ell\) is a loss function (typically 0-1 loss in the case of classification).</p> <p>Examples with larger weights have a more significant impact on the loss, guiding the model to focus on harder-to-classify examples.</p> <h5 id="adaboost-sketch-of-the-algorithm"><strong>AdaBoost: Sketch of the Algorithm</strong></h5> <p>AdaBoost works by combining several weak learners to create a strong classifier.<br/> Here’s the high-level process:</p> <ol> <li><strong>Start by assigning equal weights</strong> to all training examples:</li> </ol> \[w_1 = w_2 = \cdots = w_n = 1\] <ol> <li> <p><strong>For each boosting round</strong> \(m = 1, \dots, M\) (where \(M\) is the number of classifiers we want to train):</p> <ul> <li><strong>Train a base classifier</strong> \(G_m(x)\) on the <strong>current weighted</strong> training data.</li> <li><strong>Evaluate</strong> how well \(G_m(x)\) performs.</li> <li><strong>Increase the weights</strong> of examples that were <strong>misclassified</strong>, so the next classifier focuses more on those harder examples.</li> </ul> </li> <li> <p><strong>Aggregate the predictions</strong> from all classifiers, weighted by their accuracy:</p> </li> </ol> \[G(x) = \text{sign}\left( \sum_{m=1}^{M} \alpha_m G_m(x) \right)\] <p>The key idea is: <strong>the more accurate a base learner, the higher its influence in the final prediction</strong>.</p> <h5 id="adaboost-how-to-compute-classifier-weights"><strong>AdaBoost: How to Compute Classifier Weights</strong></h5> <p>In AdaBoost, each base classifier \(G_m\) contributes to the final prediction with a weight \(\alpha_m\).<br/> We want the following:</p> <ul> <li>\(\alpha_m\) should be <strong>non-negative</strong>.</li> <li>\(\alpha_m\) should be <strong>larger</strong> when \(G_m\) fits its weighted training data well.</li> </ul> <p>The <strong>weighted 0-1 error</strong> of the base classifier \(G_m(x)\) is computed as:</p> \[\text{err}_m = \frac{1}{W} \sum_{i=1}^{n} w_i \, \mathbf{1}\left[ y_i \neq G_m(x_i) \right]\] <p>where:</p> <ul> <li>\(W = \sum_{i=1}^{n} w_i\) is the total sum of weights.</li> <li>\(\mathbf{1}[\cdot]\) is an indicator function, equal to 1 if the condition is true, and 0 otherwise.</li> </ul> <p>Since the error is normalized by the total weight, we always have:</p> \[\text{err}_m \in [0, 1]\] <p>Once we know the error \(\text{err}_m\), we compute the weight of the classifier \(G_m\) as:</p> \[\alpha_m = \ln\left( \frac{1 - \text{err}_m}{\text{err}_m} \right)\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-4-480.webp 480w,/assets/img/ensemble-4-800.webp 800w,/assets/img/ensemble-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Higher weighted error ⇒ lower weight </div> <p><strong>Interpretation</strong>:</p> <ul> <li>If \(\text{err}_m\) is <strong>small</strong> (good classifier), then \(\alpha_m\) is <strong>large</strong>.</li> <li>If \(\text{err}_m\) is <strong>large</strong> (poor classifier), then \(\alpha_m\) is <strong>small</strong>.</li> </ul> <p>Thus, <strong>more accurate classifiers get higher voting power</strong> in the final decision.</p> <h5 id="adaboost-how-example-weights-are-updated"><strong>AdaBoost: How Example Weights Are Updated</strong></h5> <p>After training a base classifier, we update the weights of the examples to <strong>focus more on mistakes</strong>.</p> <p>Suppose \(w_i\) is the weight of example \(x_i\) <strong>before</strong> training \(G_m\). After training:</p> <ul> <li>If \(G_m\) <strong>correctly classifies</strong> \(x_i\), <strong>keep \(w_i\) the same</strong>.</li> <li>If \(G_m\) <strong>misclassifies</strong> \(x_i\), <strong>increase \(w_i\)</strong>:</li> </ul> \[w_i \leftarrow w_i \times e^{\alpha_m}\] <p>This adjustment ensures that:</p> <ul> <li><strong>Hard examples</strong> (previously misclassified) <strong>get more weight</strong> and are more likely to be correctly classified by future classifiers.</li> <li>If \(G_m\) is a <strong>strong classifier</strong> (large \(\alpha_m\)), the weight update for misclassified examples is <strong>more significant</strong>.</li> </ul> <p>Alternatively, you can think of it this way:</p> \[w_i \leftarrow w_i \times \left( \frac{1}{\text{err}_m} - 1 \right)\] <p>This reweighting step is what drives AdaBoost to sequentially <strong>correct</strong> the errors of the previous learners.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-5-480.webp 480w,/assets/img/ensemble-5-800.webp 800w,/assets/img/ensemble-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> AdaBoost: Schematic </div> <p><strong>Intuition Behind AdaBoost: Analogy</strong></p> <p>To better internalize AdaBoost, imagine the process as <strong>training a team of tutors</strong> to help a student (the model) pass an exam (classification task):</p> <ul> <li> <p><strong>First Tutor</strong>: The first tutor teaches the entire syllabus equally. After the first test, they realize the student struggles with some topics (mistakes/misclassifications).</p> </li> <li> <p><strong>Second Tutor</strong>: The second tutor <strong>focuses more heavily</strong> on the topics where the student made mistakes, spending extra time on them.</p> </li> <li> <p><strong>Third Tutor</strong>: The third tutor notices there are still lingering problems on certain topics, so they <strong>focus even more narrowly</strong> on the hardest concepts.</p> </li> <li> <p><strong>And so on…</strong></p> </li> </ul> <p>Each tutor is <strong>not perfect</strong>, but by <strong>combining their focused efforts</strong>, the student gets a much more complete understanding — better than what any single tutor could achieve alone.</p> <hr/> <h5 id="simple-mathematical-example"><strong>Simple Mathematical Example</strong></h5> <p>Let’s walk through a <strong>tiny AdaBoost example</strong> to see everything in action.</p> <p>Suppose we have 4 data points:</p> <hr/> <table> <thead> <tr> <th style="text-align: center">\(x_i\)</th> <th style="text-align: center">\(y_i\) (True Label)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">+1</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">+1</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">-1</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">-1</td> </tr> </tbody> </table> <hr/> <p><strong>Step 1: Initialization</strong></p> <p>All examples start with <strong>equal weight</strong>:</p> \[w_i = \frac{1}{4} = 0.25 \quad \text{for each } i\] <p><strong>Step 2: First Classifier \(G_1(x)\)</strong></p> <p>Suppose \(G_1\) predicts:</p> <hr/> <table> <thead> <tr> <th style="text-align: center">\(x_i\)</th> <th style="text-align: center">\(G_1(x_i)\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">+1</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">-1 ❌</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">-1</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">-1</td> </tr> </tbody> </table> <hr/> <p>It misclassifies \(x_2\).</p> <p>Compute weighted error:</p> \[\text{err}_1 = \frac{w_2}{\sum_{i=1}^{4} w_i} = \frac{0.25}{1} = 0.25\] <p>Classifier weight:</p> \[\alpha_1 = \ln\left( \frac{1 - 0.25}{0.25} \right) = \ln(3) \approx 1.0986\] <p><strong>Step 3: Update Weights</strong></p> <p>Increase the weight for the misclassified example:</p> <ul> <li>For correctly classified points \(w_i\) stays the same.</li> <li>For misclassified points:</li> </ul> \[w_i \leftarrow w_i \times e^{\alpha_1}\] <p>Thus:</p> <ul> <li>\(w_2\) (misclassified) becomes:</li> </ul> \[w_2' = 0.25 \times e^{1.0986} \approx 0.25 \times 3 = 0.75\] <ul> <li>\(w_1, w_3, w_4\) stay \(0.25\).</li> </ul> <p><strong>Normalization step</strong> (so weights sum to 1):</p> <p>Total weight:</p> \[W' = 0.25 + 0.75 + 0.25 + 0.25 = 1.5\] <p>New normalized weights:</p> <hr/> <table> <thead> <tr> <th style="text-align: center">\(x_i\)</th> <th style="text-align: center">New Weight</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">\(\frac{0.25}{1.5} \approx 0.167\)</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">\(\frac{0.75}{1.5} = 0.5\)</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">\(\frac{0.25}{1.5} \approx 0.167\)</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">\(\frac{0.25}{1.5} \approx 0.167\)</td> </tr> </tbody> </table> <hr/> <p><strong>Step 4: Second Classifier \(G_2(x)\)</strong></p> <p>Train next classifier \(G_2\) <strong>on the new weights</strong>.</p> <p>Now, \(x_2\) has the highest weight (0.5), so the model focuses more on predicting \(x_2\) correctly!</p> <p>(And the process repeats…)</p> <p><strong>Key Takeaways</strong></p> <ul> <li>AdaBoost <strong>punishes mistakes</strong> by increasing weights of misclassified examples.</li> <li>Future classifiers <strong>focus</strong> on the harder examples.</li> <li>Classifier weight \(\alpha\) depends on how good the classifier is (lower error → higher weight).</li> <li>Final prediction is:</li> </ul> \[G(x) = \text{sign}\left( \alpha_1 G_1(x) + \alpha_2 G_2(x) + \cdots + \alpha_M G_M(x) \right)\] <p>Thus, even if each individual classifier is weak, <strong>together they become strong</strong>!</p> <hr/> <h5 id="adaboost-algorithm"><strong>AdaBoost: Algorithm</strong></h5> <p>Given a training set:</p> \[\mathcal{D} = \{(x_1, y_1), \ldots, (x_n, y_n)\}.\] <p>The AdaBoost procedure works as follows:</p> <p><strong>Steps:</strong></p> <ol> <li> <p><strong>Initialize observation weights</strong>:</p> <p>Set:</p> \[w_i = 1, \quad \text{for all } i = 1, 2, \ldots, n.\] </li> <li> <p><strong>For \(m = 1\) to \(M\) (number of base classifiers)</strong>:</p> <ul> <li> <p><strong>Train</strong> a base learner on the weighted training data, obtaining a classifier \(G_m(x)\).</p> </li> <li> <p><strong>Compute the weighted empirical 0-1 risk</strong>:</p> </li> </ul> \[\text{err}_m = \frac{1}{W} \sum_{i=1}^{n} w_i \, \mathbf{1}[y_i \neq G_m(x_i)],\] <p>where:</p> \[W = \sum_{i=1}^{n} w_i.\] <ul> <li> <p><strong>Compute classifier weight</strong>:</p> <p>Assign a weight to the classifier based on its error:</p> \[\alpha_m = \ln\left( \frac{1 - \text{err}_m}{\text{err}_m} \right).\] </li> <li> <p><strong>Update example weights</strong>:</p> <p>Update the training example weights to emphasize misclassified examples:</p> \[w_i \leftarrow w_i \times \exp\left( \alpha_m \mathbf{1}[y_i \neq G_m(x_i)] \right).\] </li> </ul> </li> <li> <p><strong>Final classifier</strong>:</p> <p>After \(M\) rounds, return the final classifier:</p> \[G(x) = \text{sign}\left( \sum_{m=1}^{M} \alpha_m G_m(x) \right).\] </li> </ol> <p><strong>To put it shortly:</strong></p> <ul> <li><strong>Start</strong>: Treat every sample equally.</li> <li><strong>Learn</strong>: Focus the learner on samples that previous classifiers got wrong.</li> <li><strong>Combine</strong>: Build a strong final classifier by combining the weighted votes of all the base classifiers.</li> </ul> <p>Each \(\alpha_m\) ensures that <strong>better-performing classifiers get a stronger say</strong> in the final decision!</p> <hr/> <h5 id="weighted-error-vs-classifiers-true-error"><strong>Weighted Error vs. Classifier’s True Error</strong></h5> <p>In AdaBoost, the error \(\text{err}_m\) computed at each iteration is the <strong>weighted error</strong> based on the current distribution of sample weights, <strong>not</strong> the classifier’s true (unweighted) error rate over the data.</p> <p>This distinction is important: a classifier might have a low overall misclassification rate but could still have a <strong>high weighted error</strong> if it misclassifies examples that currently have large weights (i.e., harder or previously misclassified points).</p> <p>AdaBoost intentionally shifts focus toward difficult examples, so <strong>do not confuse the weighted empirical error used in boosting with the base learner’s standard classification error</strong>.</p> <h5 id="how-is-the-base-learner-optimized-at-each-iteration"><strong>How is the Base Learner Optimized at Each Iteration?</strong></h5> <p>At each iteration \(m\) of AdaBoost, the goal is to find the base classifier \(G_m(x)\) that <strong>minimizes the weighted empirical error</strong>:</p> \[\text{err}_m = \frac{1}{W} \sum_{i=1}^{n} w_i \, \mathbf{1}[y_i \neq G_m(x_i)].\] <p>Here’s the key idea:</p> <ul> <li>We <strong>don’t</strong> try to find a classifier that perfectly fits the original (unweighted) training data.</li> <li>Instead, we <strong>optimize for the current weighted dataset</strong> — meaning examples with larger weights influence the learning process more.</li> <li>The base learner is trained to focus on <strong>minimizing mistakes</strong> on the examples that have been <strong>harder to classify</strong> so far.</li> </ul> <p><strong>Typical Optimization Process</strong>:</p> <ul> <li>If using <strong>decision stumps</strong> (one-level decision trees), the learner searches for the split that minimizes the weighted classification error.</li> <li>In general, the base model uses the <strong>sample weights</strong> as importance scores to guide its fitting.</li> </ul> <p>Thus, at each step, AdaBoost adapts the learning problem to focus on what the previous classifiers struggled with, gradually building a strong ensemble.</p> <div class="row justify-content-center"> <div class="col-sm-4 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-6-480.webp 480w,/assets/img/ensemble-6-800.webp 800w,/assets/img/ensemble-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-7-480.webp 480w,/assets/img/ensemble-7-800.webp 800w,/assets/img/ensemble-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-8-480.webp 480w,/assets/img/ensemble-8-800.webp 800w,/assets/img/ensemble-8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-8" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> AdaBoost with Decision Stumps. (I)After 1 round, (II)After 3 rounds &amp; (III)After 120 rounds. Size of plus sign represents weight of example. Blackness represents preference for red class; whiteness represents preference for blue class. </div> <h5 id="does-adaboost-overfit"><strong>Does AdaBoost Overfit?</strong></h5> <p>While boosting generally performs well, it’s natural to ask: <strong>Does AdaBoost overfit with many rounds?</strong></p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-9-480.webp 480w,/assets/img/ensemble-9-800.webp 800w,/assets/img/ensemble-9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-9" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> General learning curves if we were overfitting </div> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-10-480.webp 480w,/assets/img/ensemble-10-800.webp 800w,/assets/img/ensemble-10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-10" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Learning Curves for AdaBoost </div> <p>The learning curves for AdaBoost typically show that the test error continues to decrease even after the training error reaches zero, which indicates that AdaBoost is <strong>resistant to overfitting</strong>. This is one of the reasons why AdaBoost is so powerful: it can maintain good generalization even with many weak learners.</p> <h5 id="adaboost-in-real-world-applications"><strong>AdaBoost in Real-World Applications</strong></h5> <p>A famous application of AdaBoost is <strong>face detection</strong>, as demonstrated in Viola &amp; Jones (2001). In this case, AdaBoost uses pre-defined weak classifiers and employs a smart way of doing real-time inference, even on hardware from 2001. This demonstrates the efficiency and applicability of AdaBoost in practical scenarios.</p> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>Boosting is an ensemble technique aimed at reducing bias by combining multiple weak learners. The sequential nature of boosting means that each learner focuses on errors made by previous ones, ultimately improving the model’s performance. <strong>AdaBoost</strong> is a specific and highly effective boosting algorithm that can be used with decision trees as weak learners to achieve powerful classification results.</p> <p><strong>Next Steps:</strong></p> <p>In the next section, we’ll explore the <strong>objective function</strong> of AdaBoost in more detail, along with some <strong>generalizations</strong> to other loss functions and the popular <strong>Gradient Boosting</strong> algorithm.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.]]></summary></entry></feed>