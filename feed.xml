<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-02T03:20:47+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training</title><link href="https://monishver11.github.io/blog/2025/sgd-tips/" rel="alternate" type="text/html" title="Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training"/><published>2025-01-01T19:18:00+00:00</published><updated>2025-01-01T19:18:00+00:00</updated><id>https://monishver11.github.io/blog/2025/sgd-tips</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/sgd-tips/"><![CDATA[<p>This is a continuation of the previous blog, and the content presented here consists of notes extracted from <a href="https://leon.bottou.org/">L´eon Bottou’s</a> <a href="https://leon.bottou.org/publications/pdf/tricks-2012.pdf">Stochastic Gradient Descent Tricks</a>. If you haven’t read my previous blog, I recommend taking a look, as the notations used here are introduced there. Alternatively, if you’re comfortable with the notations, you can jump straight into the content. So, let’s get started!</p> <h5 id="stochastic-gradient-descent-sgd"><strong>Stochastic Gradient Descent (SGD)</strong></h5> <p>Stochastic Gradient Descent (SGD) is a simplified version of the standard gradient descent algorithm. Rather than computing the exact gradient of the entire cost function \(E_n(f_w)\), each iteration of SGD estimates this gradient based on a <strong>single randomly chosen example</strong> \(z_t\). The update rule for the weights \(w\) at each iteration is:</p> \[w_{t+1} = w_t - \gamma_t \nabla_w Q(z_t, w_t)\] <p>where \(\gamma_t\) is the learning rate, and \(Q(z_t, w_t)\) represents the cost function evaluated at the current weights \(w_t\) for the randomly selected example \(z_t\).</p> <h6 id="key-features-of-sgd"><strong>Key Features of SGD:</strong></h6> <ul> <li> <p><strong>Randomness</strong>: The algorithm’s stochastic nature means that the updates depend on the examples randomly picked at each iteration. This randomness introduces some noise into the optimization process, but it is hoped that the algorithm behaves like its expectation despite this noise.</p> </li> <li> <p><strong>On-the-Fly Computation</strong>: Since SGD does not need to store information about the examples visited in previous iterations, it can process examples one by one. This makes it suitable for online learning or deployed systems, where data can arrive sequentially, and the model is updated in real-time.</p> </li> <li> <p><strong>Expected Risk Optimization</strong>: In a deployed system, where examples are drawn randomly from the ground truth distribution, SGD directly optimizes the expected risk, which is the expected value of the loss function over all possible examples. So, to put in simply - Each loss computed during SGD updates serves as an approximation of the expected loss over the true data distribution, and with more examples, it gradually optimizes the expected risk.</p> </li> </ul> <h6 id="convergence-of-sgd"><strong>Convergence of SGD:</strong></h6> <p>The convergence of SGD has been studied extensively in the stochastic approximation literature. Convergence typically requires that the learning rates satisfy the conditions:</p> \[\sum_{t=1}^{\infty} \gamma_t^2 &lt; \infty \quad\] \[\text{(It means the total sum of learning rates must go to infinity over time, ensuring enough updates for convergence)}\] \[\text{and} \quad \sum_{t=1}^{\infty} \gamma_t = \infty\] \[\text{(This mean the sum of squared learning rates must remain finite, ensuring the updates become smaller and smaller as the algorithm proceeds)}\] <p>These conditions help strike the balance between making large enough updates early on to explore the parameter space, but small enough updates later on to fine-tune the model and avoid overshooting the optimum.</p> <p>The <strong>Robbins-Siegmund theorem</strong> offers a formal proof that, under the right conditions—such as appropriate decreasing learning rates—<strong>SGD will converge almost surely</strong>, even when the loss function is non-smooth. This includes cases where the loss function has discontinuities or sharp gradients, making SGD a robust optimization method.</p> <h6 id="convergence-speed"><strong>Convergence Speed:</strong></h6> <p>The speed of convergence in SGD is ultimately limited by the noisy gradient approximations. Several factors impact the rate at which the algorithm converges:</p> <ul> <li><strong>Learning Rate Decay</strong>: <ul> <li>If the learning rates decrease too slowly, the variance of the parameter estimates \(w_t\) decreases at a similarly slow rate. <strong>Why?</strong> If the learning rate decreases too slowly, updates remain large for too long, causing high variance in parameter estimates and preventing the algorithm from stabilizing near the optimum.</li> <li>If the learning rates decay too quickly, the parameter estimates \(w_t\) take a long time to approach the optimum. <strong>Why?</strong> If the learning rate decreases too quickly, updates become too small early on, leading to insufficient exploration of the parameter space and slow convergence to the optimum.</li> </ul> </li> <li><strong>Optimal Convergence Speed</strong>: <ul> <li>When the <strong>Hessian matrix</strong> of the cost function at the optimum is <strong>strictly positive definite</strong>, the best convergence rate is achieved using learning rates of the form \(\gamma_t \sim t^{-1}\). In this case, the expectation of the residual error \(\rho\) decreases at the same rate, i.e., \(E(\rho) \sim t^{-1}\). This rate is commonly observed in practice.</li> </ul> </li> <li><strong>Relaxed Assumptions</strong>: <ul> <li>When these regularity assumptions(like a positive definite hessian, smoothness and strong convexity) are relaxed, the convergence rate slows down. The theoretical convergence rate in such cases is typically \(E(\rho) \sim t^{-1/2}\). However, in practice, this slower convergence tends to only manifest during the final stages of the optimization process. Often, optimization is stopped before this stage is reached, making the slower convergence less significant.</li> </ul> </li> </ul> <p>In summary, while the convergence of SGD can be slow due to its noisy nature, proper management of the learning rate and understanding of the problem’s characteristics can ensure good performance in practice.</p> <hr/> <h5 id="second-order-stochastic-gradient-descent-2sgd"><strong>Second-Order Stochastic Gradient Descent (2SGD)</strong></h5> <p>Second-Order Stochastic Gradient Descent (2SGD) extends stochastic gradient descent by incorporating curvature information through a positive definite matrix \(\Gamma_t\), which approximates the inverse of the Hessian matrix. The update rule for 2SGD is:</p> \[w_{t+1} = w_t - \gamma_t \Gamma_t \nabla_w Q(z_t, w_t),\] <p>where:</p> <ul> <li>\(w_t\): Current weights at iteration \(t\).</li> <li>\(\gamma_t\): Learning rate (step size), which may vary over iterations.</li> <li>\(\Gamma_t\): A positive definite matrix that approximates the inverse of the Hessian.</li> <li>\(\nabla_w Q(z_t, w_t)\): Gradient of the loss function \(Q\) with respect to \(w_t\) for the stochastic sample \(z_t\).</li> </ul> <h6 id="key-advantages-of-2sgd"><strong>Key Advantages of 2SGD</strong></h6> <ol> <li><strong>Curvature Awareness</strong>: <ul> <li>The inclusion of \(\Gamma_t\) enables the algorithm to account for the curvature of the loss surface.</li> <li>This adaptation improves convergence by rescaling updates to balance faster progress in flat directions and slower progress in steep directions.</li> </ul> </li> <li><strong>Improved Constants</strong>: <ul> <li>The scaling introduced by \(\Gamma_t\) can reduce the condition number of the problem. <strong>What?</strong> The condition number is the ratio of the largest to smallest eigenvalue of the Hessian, reflecting the curvature’s uniformity. A high condition number implies uneven curvature, slowing convergence.</li> <li>2SGD addresses this by scaling the parameter space to reduce the condition number, making the optimization landscape more uniform and this leads to faster convergence in terms of iteration efficiency when compared to standard SGD.</li> </ul> </li> </ol> <h6 id="challenges-in-2sgd"><strong>Challenges in 2SGD</strong></h6> <p>Despite the advantages, 2SGD has significant limitations:</p> <ol> <li><strong>Stochastic Noise</strong>: <ul> <li>The introduction of \(\Gamma_t\) does not address the stochastic noise inherent in gradient estimates.</li> <li>As a result, the variance in the weights \(w_t\) remains high, which limits its convergence benefits.</li> </ul> </li> <li><strong>Asymptotic Behavior</strong>: <ul> <li>The expected residual error decreases at a rate of \(\mathbb{E}[\rho] \sim t^{-1}\) at best.</li> <li>While constants(i.e., step size) are improved, the convergence rate remains fundamentally constrained by the stochastic nature of the gradients.</li> </ul> </li> </ol> <h6 id="comparison-with-batch-algorithms"><strong>Comparison with Batch Algorithms</strong></h6> <p><strong>Batch Algorithms</strong>:</p> <ul> <li>Batch methods utilize the full dataset to compute gradients at each iteration.</li> <li>They achieve better asymptotic performance with convergence rates that often scale as \(t^{-2}\) or better, depending on the algorithm.</li> </ul> <p><strong>2SGD</strong>:</p> <ul> <li>2SGD operates on a per-sample basis, which limits its ability to achieve higher convergence rates in expectation.</li> <li>The variance introduced by stochastic gradients limits its asymptotic efficiency compared to batch methods.</li> </ul> <h6 id="the-bigger-picture"><strong>The Bigger Picture</strong></h6> <p>Despite being asymptotically slower than batch algorithms, 2SGD remains highly relevant in modern machine learning for many reasons:</p> <ol> <li><strong>Efficiency in Large Datasets</strong>: <ul> <li>When datasets are too large to process as a batch, 2SGD provides an efficient alternative.</li> <li>It avoids the computational and memory overhead of storing and processing the entire dataset.</li> </ul> </li> <li><strong>Online Learning</strong>: <ul> <li>In online learning scenarios, where data arrives sequentially, 2SGD offers a practical approach to updating models in real time.</li> </ul> </li> </ol> <h6 id="summary-of-convergence-behavior"><strong>Summary of Convergence Behavior</strong></h6> <hr/> <table> <thead> <tr> <th><strong>Algorithm</strong></th> <th><strong>Error Decay</strong></th> <th><strong>Asymptotic Behavior</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gradient Descent (GD)</strong></td> <td>\(|w_t - w^*| \sim \rho^t\)</td> <td>Linear convergence: \(\mathcal{O}(t^{-1})\)</td> </tr> <tr> <td><strong>Stochastic Gradient Descent (SGD)</strong></td> <td>\(\mathbb{E}[|w_t - w^*|] \sim t^{-1}\)</td> <td>Asymptotic rate: \(t^{-1}\)</td> </tr> <tr> <td><strong>Second-Order Stochastic GD (2SGD)</strong></td> <td>\(\mathbb{E}[|w_t - w^*|] \sim t^{-1}\)</td> <td>Same as SGD, but with improved constants</td> </tr> </tbody> </table> <hr/> <p>Note:</p> <ul> <li> <p><strong>Linear Convergence</strong> (\(\mathcal{O}(t^{-1})\)): Implies an exponential decay of the error over time, with the error shrinking by a constant factor at each step.</p> </li> <li> <p><strong>Asymptotic Rate</strong> (\(t^{-1}\)): Describes the long-term error decay rate, indicating a polynomial decay (slower than exponential) where the error decreases inversely with time.</p> </li> </ul> <p>By incorporating second-order information through \(\Gamma_t\), 2SGD makes more informed updates. However, its performance is ultimately limited by the stochastic noise in gradient estimates. In practice, 2SGD is a compromise between computational efficiency and convergence speed, making it suitable for large-scale and online learning tasks.</p> <h4 id="when-to-use-stochastic-gradient-descent-sgd"><strong>When to Use Stochastic Gradient Descent (SGD)</strong></h4> <p>Stochastic Gradient Descent (SGD) is particularly well-suited when <strong>training time is the bottleneck</strong>. It is an effective choice in scenarios where computational efficiency and scalability are critical, such as in large-scale machine learning tasks.</p> <h6 id="key-insights-from-table"><strong>Key Insights from Table</strong></h6> <p>The table below summarizes the asymptotic behavior of four optimization algorithms:</p> <ul> <li><strong>Gradient Descent (GD)</strong>: Standard first-order method.</li> <li><strong>Second-Order Gradient Descent (2GD)</strong>: Incorporates curvature information.</li> <li><strong>Stochastic Gradient Descent (SGD)</strong>: A stochastic variant of GD.</li> <li><strong>Second-Order Stochastic Gradient Descent (2SGD)</strong>: Combines stochastic updates with curvature adaptation.</li> </ul> <hr/> <table> <thead> <tr> <th><strong>Algorithm</strong></th> <th><strong>Time per Iteration</strong></th> <th><strong>Iterations to Accuracy (\(\rho\))</strong></th> <th><strong>Time to Accuracy (\(\rho\))</strong></th> <th><strong>Time to Excess Error \(\epsilon\)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gradient Descent (GD)</strong></td> <td>\(n\)</td> <td>\(\log(1 / \rho)\)</td> <td>\(n \log(1 / \rho)\)</td> <td>\(\frac{1}{\epsilon^{1/\alpha}} \log(1 / \epsilon)\)</td> </tr> <tr> <td><strong>Second-Order Gradient Descent (2GD)</strong></td> <td>\(n\)</td> <td>\(\log \log(1 / \rho)\)</td> <td>\(n \log \log(1 / \rho)\)</td> <td>\(\frac{1}{\epsilon^{1/\alpha}} \log(1 / \epsilon) \log \log(1 / \epsilon)\)</td> </tr> <tr> <td><strong>Stochastic Gradient Descent (SGD)</strong></td> <td>\(1\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \epsilon\)</td> </tr> <tr> <td><strong>Second-Order Stochastic GD (2SGD)</strong></td> <td>\(1\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \epsilon\)</td> </tr> </tbody> </table> <hr/> <h6 id="discussion"><strong>Discussion</strong></h6> <ol> <li><strong>Per-Iteration Cost</strong>: <ul> <li><strong>GD and 2GD</strong>: Both require \(\mathcal{O}(n)\) time per iteration due to full-batch gradient computations.</li> <li><strong>SGD and 2SGD</strong>: Require \(\mathcal{O}(1)\) time per iteration, making them computationally inexpensive for large datasets.</li> </ul> </li> <li><strong>Convergence Speed</strong>: <ul> <li>GD and 2GD converge faster in terms of the number of iterations but incur higher computational costs because of full-batch updates.</li> <li>SGD and 2SGD require more iterations to converge but compensate with lower per-iteration costs.</li> </ul> </li> <li><strong>Asymptotic Performance</strong>: <ul> <li>While SGD and 2SGD have worse optimization noise, they require significantly less time to achieve a predefined expected risk \(\epsilon\) due to their reduced computational overhead.</li> <li>In large-scale settings where computation time is the limiting factor, <strong>stochastic learning algorithms are asymptotically better</strong>.</li> </ul> </li> </ol> <h6 id="key-takeaways"><strong>Key Takeaways</strong></h6> <ul> <li>Use <strong>SGD</strong> when: <ul> <li>Dataset size is large, and full-batch methods become computationally infeasible.</li> <li>Real-time or online learning scenarios require frequent updates with minimal latency.</li> <li>Memory efficiency is a concern, as SGD processes one sample at a time.</li> </ul> </li> <li>Despite higher variance in updates, <strong>SGD and 2SGD</strong> are preferred in large-scale setups due to their faster convergence to the expected risk with minimal computational resources.</li> </ul> <p>In conclusion, while SGD and 2SGD might appear less efficient in small-scale setups, their practical advantages in high-dimensional, data-intensive tasks make them highly favorable in modern machine learning applications.</p> <hr/> <h4 id="general-recommendations-for-stochastic-gradient-descent-sgd"><strong>General Recommendations for Stochastic Gradient Descent (SGD)</strong></h4> <p>The following is a series of recommendations for using stochastic gradient algorithms. Though seemingly trivial, the author’s experience highlights how easily they can be overlooked.</p> <h5 id="1-randomly-shuffle-the-training-examples"><strong>1. Randomly Shuffle the Training Examples</strong></h5> <p>Although the theory behind Stochastic Gradient Descent (SGD) calls for picking examples randomly, it is often tempting to process them sequentially through the training set. While sequentially passing through the examples may seem like an optimization, it can be problematic when the data is structured in a way that affects training performance.</p> <h6 id="key-points"><strong>Key Points:</strong></h6> <ul> <li><strong>Class Grouping and Order</strong>: If training examples are grouped by class or presented in a particular order, processing them in sequence can lead to biases in the gradient updates.</li> <li><strong>The Importance of Randomization</strong>: Randomizing the order helps break any inherent structure or patterns in the dataset that may skew the learning process. This ensures that each update is less dependent on the order of the examples, promoting better convergence.</li> </ul> <h6 id="analogy"><strong>Analogy:</strong></h6> <p>Think of SGD like a person learning to navigate a maze. If they always follow the same path (training examples in order), they may become “stuck” in a loop. However, if they randomly choose different routes (randomized examples), they are more likely to explore and discover the optimal path.</p> <h5 id="2-use-preconditioning-techniques"><strong>2. Use Preconditioning Techniques</strong></h5> <p>Stochastic Gradient Descent (SGD) is a first-order optimization algorithm, meaning it only uses the first derivatives (gradients) to guide the updates. However, this can lead to significant issues when the optimization process encounters areas where the <strong>Hessian</strong> (the matrix of second derivatives) is ill-conditioned. In such regions, the gradients may not provide efficient updates, slowing down convergence or leading to poor results.</p> <p>Fortunately, <strong>preconditioning techniques</strong> like Adagrad or Adam, adjust the learning rates based on past gradients, helping optimize in ill-conditioned regions for faster and more stable convergence.</p> <h6 id="key-points-1"><strong>Key Points:</strong></h6> <ul> <li><strong>Ill-conditioned regions</strong>: Areas where the curvature (second derivatives) of the cost function varies dramatically, making it hard for SGD to make efficient progress.</li> <li><strong>Improved convergence</strong>: Preconditioning techniques can rescale the gradients to make the learning process more stable and faster, improving convergence even in difficult regions.</li> </ul> <h6 id="analogy-1"><strong>Analogy:</strong></h6> <p>Imagine trying to push a boulder up a steep hill (representing optimization in ill-conditioned areas). Without a proper approach, the effort may be inefficient or lead you off-course. Preconditioning techniques act like a ramp, providing a smoother path and making it easier to move the boulder in the right direction.</p> <h5 id="3-monitor-both-the-training-cost-and-the-validation-error"><strong>3. Monitor Both the Training Cost and the Validation Error</strong></h5> <p>To effectively gauge the performance of your model during training, it is crucial to monitor both the <strong>training cost</strong> and the <strong>validation error</strong>. A simple yet effective approach involves repeating the following steps:</p> <h6 id="key-steps"><strong>Key Steps:</strong></h6> <ol> <li> <p><strong>Stochastic Gradient Descent (SGD) Update</strong>: Process once through the shuffled training set and perform the SGD updates. This helps adjust the model’s parameters based on the current data.</p> </li> <li> <p><strong>Compute Training Cost</strong>: After the updates, run another loop over the training set to compute the <strong>training cost</strong>. This cost represents the criterion (such as the loss function) the algorithm is optimizing. Monitoring the training cost provides insight into how well the model is minimizing the objective.</p> </li> <li> <p><strong>Compute Validation Error</strong>: With another loop, calculate the <strong>validation error</strong> using the validation set. This error is the performance measure of interest (such as classification error, accuracy, etc.). The validation error helps track how well the model generalizes to unseen data.</p> </li> </ol> <p>Although these steps require additional computational effort, including extra passes over both the training and validation datasets, they provide critical feedback and prevent training in isolation, avoiding the risk of overfitting or diverging from the optimal solution.</p> <h6 id="analogy-2"><strong>Analogy:</strong></h6> <p>Think of training a model like tuning a musical instrument. The training cost is like checking the sound of the instrument while you play (adjusting and fine-tuning as you go), while the validation error is like getting feedback from a concert audience (seeing how the performance holds up in a real-world scenario). Without both, you might end up with a well-tuned instrument that doesn’t sound good in a performance.</p> <h5 id="4-check-the-gradients-using-finite-differences"><strong>4. Check the Gradients Using Finite Differences</strong></h5> <p>When the computation of gradients is slightly incorrect, Stochastic Gradient Descent (SGD) tends to behave slowly and erratically. This often leads to the misconception that such behavior is the normal operation of the algorithm.</p> <p>Over the years, many practitioners have sought advice on how to set the learning rates \(\gamma_t\) for a rebellious SGD program. However, the best advice is often to <strong>forget about the learning rates</strong> and ensure that the gradients are being computed correctly. Once gradients are correctly computed, setting small enough learning rates becomes easy. Those who struggle with tuning learning rates often have faulty gradients in their computations.</p> <h6 id="how-to-check-gradients-using-finite-differences"><strong>How to Check Gradients Using Finite Differences:</strong></h6> <p>Rather than manually checking each line of the gradient computation code, use finite differences to verify the accuracy of the gradients.</p> <h6 id="steps"><strong>Steps:</strong></h6> <ol> <li> <p><strong>Pick an Example</strong>: Choose a training example \(z\) from the dataset.</p> </li> <li> <p><strong>Compute the Loss</strong>: Calculate the loss function \(Q(z, w)\) for the current weights \(w\).</p> </li> <li> <p><strong>Compute the Gradient</strong>: Calculate the gradient of the loss with respect to the weights: \(g = \nabla_w Q(z, w)\)</p> </li> <li><strong>Apply a Perturbation</strong>: Slightly perturb the weights by changing them. This can be done by either: <ul> <li>Changing a single weight by a small increment: \(w' = w + \delta\)</li> <li>Perturbing the weights using the gradient: \(w' = w - \gamma g\), where \(\gamma\) is small enough.</li> </ul> </li> <li> <p><strong>Compute the New Loss</strong>: After applying the perturbation, compute the new loss \(Q(z, w')\).</p> </li> <li><strong>Verify the Approximation</strong>: Ensure that the new loss approximates the original loss plus the perturbation multiplied by the gradient: \(Q(z, w') \approx Q(z, w) + \delta g\)</li> </ol> <p><strong>Example</strong>, consider the MSE loss function:</p> \[Q(z, w) = (w - z)^2\] <ol> <li><strong>Pick an Example</strong>: Let ( z = 5 ), ( w = 4 ).</li> <li><strong>Compute the Loss</strong>: \(Q(z, w) = (4 - 5)^2 = 1\)</li> <li><strong>Compute the Gradient</strong>: \(g = \nabla_w Q(z, w) = 2(w - z) = -2\)</li> <li><strong>Apply Perturbation</strong>: \(w' = w + 0.01 = 4.01\)</li> <li><strong>Compute the New Loss</strong>: \(Q(z, w') = (4.01 - 5)^2 = 0.9801\)</li> <li><strong>Verify</strong>: \(Q(z, w') \approx Q(z, w) + 0.01 \cdot (-2) = 0.98\)</li> </ol> <h6 id="automating-the-process"><strong>Automating the Process:</strong></h6> <p>This process can be automated and should be repeated for many examples \(z\), many perturbations \(\delta\), and many initial weights \(w\). Often, flaws in the gradient computation only appear under peculiar conditions, and it’s not uncommon to discover such bugs in SGD code that has been used for years without issue.</p> <h6 id="analogy-3"><strong>Analogy:</strong></h6> <p>Think of gradient checking like testing the brakes of a car. If the brakes (gradients) are faulty, the car (SGD) might not stop properly, leading to erratic behavior. Instead of repeatedly adjusting the speed (learning rate), you test the brakes by applying a small perturbation to the system (finite differences). If the brakes are working well, the car will stop smoothly at the right place (convergence).</p> <h5 id="5-experiment-with-the-learning-rates-gamma_t-using-a-small-sample-of-the-training-set"><strong>5. Experiment with the Learning Rates \(\gamma_t\) Using a Small Sample of the Training Set</strong></h5> <p>The mathematics behind Stochastic Gradient Descent (SGD) are surprisingly independent of the training set size. Specifically, the asymptotic convergence rates of SGD are not influenced by the sample size. This means that once you’ve ensured the gradients are correct, the most effective way to determine appropriate learning rates is to experiment with a <strong>small, but representative</strong> sample of the training set.</p> <h6 id="key-steps-1"><strong>Key Steps:</strong></h6> <ol> <li> <p><strong>Use a Small Sample</strong>: Select a small subset of the training data that still reflects the diversity of the full dataset. The small size allows you to test different learning rates quickly without incurring the computational cost of working with the entire dataset.</p> </li> <li> <p><strong>Traditional Optimization Methods</strong>: Since the sample is small, you can apply traditional optimization techniques (e.g., gradient descent or other optimization algorithms) to find a reference point and set the training cost target. This provides a useful benchmark for SGD.</p> </li> <li> <p><strong>Refining Learning Rates</strong>: Experiment with various learning rates on this small dataset to find a value that minimizes the training cost efficiently. Once you identify a good learning rate, it’s likely to work well on the full dataset.</p> </li> <li> <p><strong>Scale to Full Dataset</strong>: Once the learning rates are set based on the small sample, use the same rates on the full training set. Keep in mind that the performance on the validation set is expected to plateau after a number of epochs. The number of epochs required to reach this plateau should be roughly the same as what was needed on the small dataset.</p> </li> </ol> <h6 id="analogy-4"><strong>Analogy:</strong></h6> <p>Think of this like testing the settings of a new recipe. Instead of preparing a full meal, you start with a small portion of ingredients (a small sample). Once you find the perfect amount of seasoning (learning rates), you can apply it to the full dish (the entire training set). While the small sample may not capture every nuance of the full dish, it gives you a good starting point without wasting resources.</p> <hr/> <p>This concludes the key points related to Stochastic Gradient Descent (SGD). After iterating through Gradient Descent (GD) and SGD multiple times, I hope the concepts are now firmly imprinted, even if briefly. In the upcoming blog posts, we will delve into loss functions and regression, so stay tuned!</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.]]></summary></entry><entry><title type="html">Gradient Descent Convergence - Prerequisites and Detailed Derivation</title><link href="https://monishver11.github.io/blog/2024/gd-convergence/" rel="alternate" type="text/html" title="Gradient Descent Convergence - Prerequisites and Detailed Derivation"/><published>2024-12-29T01:44:00+00:00</published><updated>2024-12-29T01:44:00+00:00</updated><id>https://monishver11.github.io/blog/2024/gd-convergence</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/gd-convergence/"><![CDATA[<p>To understand the <strong>Convergence Theorem for Fixed Step Size</strong>, it is essential to grasp a few foundational concepts like <strong>Lipschitz continuity</strong> and <strong>convexity</strong>. This section introduces these concepts and establishes the necessary prerequisites.</p> <p><strong>Quick note:</strong> If you find yourself struggling with any part or step, don’t worry—just copy and paste it into ChatGPT or Perplexity for an explanation. In most cases, you’ll be able to grasp the concept and move forward. If you’re still stuck, feel free to ask for help. The key is not to let small obstacles slow you down—keep going and seek assistance when needed!</p> <hr/> <h4 id="lipschitz-continuity"><strong>Lipschitz Continuity?</strong></h4> <p>At its core, Lipschitz continuity imposes a <strong>limit on how fast a function can change</strong>. Mathematically, a function \(g : \mathbb{R}^d \to \mathbb{R}\) is said to be <strong>Lipschitz continuous</strong> if there exists a constant \(L &gt; 0\) such that:</p> \[\|g(x) - g(x')\| \leq L \|x - x'\|, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This means the function’s rate of change is bounded by \(L\). For differentiable functions, Lipschitz continuity is often applied to the gradient. If \(\nabla f(x)\) is Lipschitz continuous with constant \(L &gt; 0\), then:</p> \[\|\nabla f(x) - \nabla f(x')\| \leq L \|x - x'\|, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This ensures the gradient does not change too rapidly, which is crucial for the convergence of optimization algorithms like gradient descent.</p> <h6 id="intuition-behind-lipschitz-continuity"><strong>Intuition Behind Lipschitz Continuity</strong></h6> <ol> <li><strong>Bounding the Slope</strong>: Lipschitz continuity ensures that the slope of the function (or the steepness of the graph) is bounded by \(L\). You can think of it as saying, “No part of the function can change too steeply.”</li> <li><strong>Gradient Smoothness</strong>: For \(\nabla f(x)\), Lipschitz continuity means the gradient varies smoothly between nearby points. This avoids abrupt jumps or erratic behavior in the optimization landscape.</li> </ol> <h6 id="visual-way-to-think-about-it"><strong>Visual Way to Think About It</strong></h6> <p>Imagine walking along a path represented by the graph of \(f(x)\). Lipschitz continuity guarantees:</p> <ul> <li>No sudden steep hills or cliffs.</li> <li>A smooth path where the steepness (gradient) is capped.</li> </ul> <p>Alternatively, picture a <strong>rubber band stretched smoothly over some pegs</strong>. The tension in the rubber band ensures there are no sharp kinks, making the graph smooth and predictable.</p> <h6 id="examples-of-lipschitz-continuous-functions"><strong>Examples of Lipschitz Continuous Functions</strong></h6> <ol> <li><strong>Linear Function</strong>: \(f(x) = mx + b\) is Lipschitz continuous because the slope \(m\) is constant, and \(|f'(x)| = |m|\) is bounded.</li> <li><strong>Quadratic Function</strong>: \(f(x) = x^2\) is \(L\)-smooth with \(L = 2\). Its gradient \(f'(x) = 2x\) satisfies:</li> </ol> \[|f'(x) - f'(x')| = |2x - 2x'| = 2|x - x'|.\] <ol> <li><strong>Non-Lipschitz Example</strong>: \(f(x) = \sqrt{x}\) (for \(x &gt; 0\)) is <strong>not Lipschitz continuous</strong> at \(x = 0\) because the slope becomes infinitely steep as \(x \to 0\). (If you’re not getting this, just plot \(\sqrt{x}\) function in <a href="https://www.desmos.com/">Desmos</a> and you’ll get it.)</li> </ol> <h6 id="why-does-lipschitz-continuity-matter"><strong>Why Does Lipschitz Continuity Matter?</strong></h6> <ol> <li><strong>Predictability</strong>: Lipschitz continuity ensures that a function behaves predictably, without sudden spikes or erratic changes.</li> <li><strong>Gradient Descent</strong>: If \(\nabla f(x)\) is Lipschitz continuous, we can choose a step size \(\eta \leq \frac{1}{L}\) to ensure gradient descent converges smoothly without overshooting the minimum.</li> </ol> <p>But Why? We’ll see that in the Convergence Theorem down below. For now, lets equip ourselves with the next important concept needed.</p> <hr/> <h4 id="2-convex-functions-and-convexity-condition"><strong>2. Convex Functions and Convexity Condition</strong></h4> <p>A function \(f : \mathbb{R}^d \to \mathbb{R}\) is <strong>convex</strong> if for any \(x, x' \in \mathbb{R}^d\) and \(\alpha \in [0, 1]\):</p> \[f(\alpha x + (1 - \alpha)x') \leq \alpha f(x) + (1 - \alpha)f(x').\] <p>Intuitively, the line segment between any two points on the graph of \(f\) lies above the graph itself.</p> <h6 id="convexity-condition-using-gradients"><strong>Convexity Condition Using Gradients</strong></h6> <p>If \(f\) is differentiable, convexity is equivalent to the following condition:</p> \[f(x') \geq f(x) + \langle \nabla f(x), x' - x \rangle, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This means that the function lies above its tangent plane at any point.</p> <hr/> <h4 id="3-l-smoothness"><strong>3. \(L\)-Smoothness</strong></h4> <p>A function \(f\) is said to be \(L\)-smooth if its gradient is Lipschitz continuous. This implies the following inequality:</p> \[f(x') \leq f(x) + \langle \nabla f(x), x' - x \rangle + \frac{L}{2} \|x' - x\|^2.\] <p>This property bounds the change in the function value using the gradient and the distance between \(x\) and \(x'\).</p> <hr/> <h4 id="4-optimality-conditions-for-convex-functions"><strong>4. Optimality Conditions for Convex Functions</strong></h4> <p>For convex functions, the following is true:</p> <ul> <li>If \(x^*\) is a minimizer of \(f\), then:</li> </ul> \[\nabla f(x^*) = 0.\] <ul> <li>For any \(x\), the difference between \(f(x)\) and \(f(x^*)\) can be bounded using the gradient:</li> </ul> \[f(x) - f(x^*) \leq \langle \nabla f(x), x - x^* \rangle.\] <p>These conditions help in deriving the convergence results for gradient descent.</p> <hr/> <p><strong>To quickly summarize, before we proceed further:</strong></p> <ol> <li><strong>Lipschitz continuity</strong> ensures the gradient does not change too rapidly.</li> <li><strong>Convexity</strong> guarantees that the function behaves well, with no local minima other than the global minimum.</li> <li><strong>\(L\)-smoothness</strong> combines convexity and Lipschitz continuity to bound the function’s behavior using gradients.</li> </ol> <hr/> <p>With these concepts in place, we can now proceed to derive the <strong>Convergence Theorem for Fixed Step Size</strong>.</p> <h4 id="convergence-of-gradient-descent-with-fixed-step-size"><strong>Convergence of Gradient Descent with Fixed Step Size</strong></h4> <h5 id="theorem"><strong>Theorem:</strong></h5> <p>Suppose the function \(f : \mathbb{R}^n \to \mathbb{R}\) is convex and differentiable, and its gradient is Lipschitz continuous with constant \(L &gt; 0\), i.e.,</p> \[\|\nabla f(x) - \nabla f(y)\|_2 \leq L \|x - y\|_2 \quad \text{for any} \quad x, y.\] <p>Then, if we run gradient descent for \(k\) iterations with a fixed step size \(t \leq \frac{1}{L}\), the solution \(x^{(k)}\) satisfies:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t k},\] <p>where \(f(x^*)\) is the optimal value.</p> <h5 id="proof"><strong>Proof:</strong></h5> <h6 id="step-1-lipschitz-continuity-and-smoothness"><strong>Step 1: Lipschitz Continuity and Smoothness</strong></h6> <p>From the Lipschitz continuity of \(\nabla f\), the function \(f\) satisfies the following inequality for any \(x, y \in \mathbb{R}^n\):</p> \[f(y) \leq f(x) + \nabla f(x)^T (y - x) + \frac{L}{2} \|y - x\|_2^2.\] <p>This inequality allows us to bound how the function \(f\) changes as we move from \(x\) to \(y\), given the Lipschitz constant \(L\).</p> <h6 id="step-2-gradient-descent-update"><strong>Step 2: Gradient Descent Update</strong></h6> <p>The gradient descent update step is defined as:</p> \[x^{+} = x - t \nabla f(x),\] <p>where \(t\) is the step size. Letting \(y = x^+\) in the smoothness inequality gives:</p> \[f(x^+) \leq f(x) + \nabla f(x)^T (x^+ - x) + \frac{L}{2} \|x^+ - x\|_2^2.\] <h6 id="step-3-substituting-the-update-rule"><strong>Step 3: Substituting the Update Rule</strong></h6> <p>Substituting \(x^+ - x = -t \nabla f(x)\), we get:</p> \[f(x^+) \leq f(x) + \nabla f(x)^T (-t \nabla f(x)) + \frac{L}{2} \| -t \nabla f(x)\|_2^2.\] <p>Simplifying each term:</p> <ul> <li>The second term simplifies to:</li> </ul> \[\nabla f(x)^T (-t \nabla f(x)) = -t \|\nabla f(x)\|_2^2.\] <ul> <li>The third term simplifies to:</li> </ul> \[\frac{L}{2} \| -t \nabla f(x)\|_2^2 = \frac{L t^2}{2} \|\nabla f(x)\|_2^2.\] <p>Combining these, we have:</p> \[f(x^+) \leq f(x) - t \|\nabla f(x)\|_2^2 + \frac{L t^2}{2} \|\nabla f(x)\|_2^2.\] <p>Factoring out \(\|\nabla f(x)\|_2^2\):</p> \[f(x^+) \leq f(x) - \left( t - \frac{L t^2}{2} \right) \|\nabla f(x)\|_2^2.\] <h6 id="step-4-ensuring-decrease-in-fx"><strong>Step 4: Ensuring Decrease in \(f(x)\)</strong></h6> <p>To ensure that the function value decreases at each iteration, the coefficient \(t - \frac{L t^2}{2}\) must be non-negative. This holds when \(t \leq \frac{1}{L}\). Substituting \(t = \frac{1}{L}\), we verify:</p> \[t - \frac{L t^2}{2} = \frac{1}{L} - \frac{L}{2} \cdot \frac{1}{L^2} = \frac{1}{L} - \frac{1}{2L} = \frac{1}{2L}.\] <p>Thus, with \(t \leq \frac{1}{L}\), the function value strictly decreases:</p> \[f(x^+) \leq f(x) - \frac{t}{2} \|\nabla f(x)\|_2^2.\] <h6 id="step-5-bounding-fx---fx"><strong>Step 5: Bounding \(f(x^+) - f(x^*)\)</strong></h6> <p>From the convexity of \(f\), we know:</p> \[f(x^*) \geq f(x) + \nabla f(x)^T (x^* - x).\] <p>Rearranging: \(f(x) \leq f(x^*) + \nabla f(x)^T (x - x^*).\)</p> <p>Substituting this into the inequality for \(f(x^+)\):</p> \[f(x^+) \leq f(x^*) + \nabla f(x)^T (x - x^*) - \frac{t}{2} \|\nabla f(x)\|_2^2.\] <p>Rearranging terms:</p> \[f(x^+) - f(x^*) \leq \frac{1}{2t} \left( \|x - x^*\|_2^2 - \|x^+ - x^*\|_2^2 \right).\] <p>This shows how the objective value at \(x^+\) is related to the distance between \(x\) and the optimal solution \(x^*\).</p> <h6 id="step-6-summing-over-k-iterations"><strong>Step 6: Summing Over \(k\) Iterations</strong></h6> <p>Let \(x^{(i)}\) denote the iterate after \(i\) steps. Applying the inequality iteratively, we have:</p> \[f(x^{(i)}) - f(x^*) \leq \frac{1}{2t} \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right).\] <p>Summing over \(i = 1, 2, \dots, k\):</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \sum_{i=1}^k \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right).\] <h6 id="step-7-telescoping-sum"><strong>Step 7: Telescoping Sum</strong></h6> <p>The terms on the right-hand side form a telescoping sum:</p> \[\sum_{i=1}^k \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right) = \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2.\] <p>Thus, we have:</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Since \(f(x^{(i)})\) is decreasing with each iteration, the largest term dominates the average:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right).\] <p>But, why is the above inequality right? Let’s find out:</p> <p>The inequality</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right)\] <p>is derived based on the property that \(f(x^{(i)})\) is <strong>monotonically decreasing</strong> during gradient descent. Let’s break it down step by step.</p> <ul> <li><strong>Key Property: Monotonic Decrease</strong>: In gradient descent, the function value decreases with each iteration due to the fixed step size \(t \leq \frac{1}{L}\). This means:</li> </ul> \[f(x^{(1)}) \geq f(x^{(2)}) \geq \cdots \geq f(x^{(k)}).\] <p>Thus, the latest value \(f(x^{(k)})\) is the smallest among all iterations.</p> <ul> <li><strong>Averaging the Function Values</strong>: The sum of the differences \(f(x^{(i)}) - f(x^*)\) over all \(k\) iterations can be written as:</li> </ul> \[\frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right),\] <p>which represents the average difference between the function values at each iteration and the optimal value \(f(x^*)\).</p> <ul> <li><strong>Bounding the Smallest Term by the Average</strong>: Since \(f(x^{(k)})\) is the smallest value (due to monotonic decrease), it cannot exceed the average value. In mathematical terms:</li> </ul> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right).\] <ul> <li> <p><strong>Intuition Behind the Inequality</strong>: This inequality reflects a simple fact: the smallest value in a decreasing sequence of numbers is less than or equal to their average. For example, if we have values \(10, 8, 7, 6\), the smallest value (6) will always be less than or equal to the average of these values.</p> </li> <li> <p><strong>Significance in Gradient Descent</strong>: This inequality is important because it allows us to bound the final iterate \(f(x^{(k)})\) using the sum of all previous iterations.</p> </li> </ul> <h6 id="step-8-final-substitution-to-derive-the-convergence-result"><strong>Step 8: Final Substitution to Derive the Convergence Result</strong></h6> <p>From the telescoping sum, we have:</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Using the inequality:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right),\] <p>we substitute the bound on the sum into this expression:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \cdot \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Since \(\|x^{(k)} - x^*\|_2^2 \geq 0\), we drop this term to get the worst-case bound:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2tk}.\] <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>We have derived the convergence guarantee for gradient descent with a fixed step size \(t \leq \frac{1}{L}\). The final result:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t k},\] <p>shows that the function value \(f(x^{(k)})\) decreases towards the optimal value \(f(x^*)\) at a rate proportional to \(O(1/k)\). This rate depends on the step size \(t\) and the initial distance \(\|x^{(0)} - x^*\|_2^2\).</p> <p>The result highlights that gradient descent converges reliably under the conditions of convexity, differentiability, and Lipschitz continuity of the gradient. As \(k \to \infty\), the function value approaches the optimal value, demonstrating the effectiveness of gradient descent for optimization problems with these properties.</p> <hr/> <p>Next,</p> <ul> <li>Convergence of gradient descent with adaptive step size</li> <li>Strongly convex - “linear convergence” rate</li> </ul> <h4 id="convergence-of-gradient-descent-with-adaptive-step-size"><strong>Convergence of gradient descent with adaptive step size</strong></h4> <p>In the above section, we derived the convergence rate for gradient descent with a <strong>fixed step size</strong>. In this part, we extend this analysis to the case where the step size is chosen adaptively using a <strong>backtracking line search</strong>. This method ensures that the step size decreases as necessary to guarantee sufficient decrease in the objective function at each iteration.</p> <h6 id="step-1-setup-and-assumptions"><strong>Step 1: Setup and Assumptions</strong></h6> <p>Consider a differentiable convex function \(f: \mathbb{R}^n \to \mathbb{R}\) with a <strong>Lipschitz continuous gradient</strong>. That is, for any two points \(x, y \in \mathbb{R}^n\),</p> \[\|\nabla f(x) - \nabla f(y)\|_2 \leq L \|x - y\|_2,\] <p>where \(L\) is the <strong>Lipschitz constant</strong> of the gradient.</p> <p>Let \(x^*\) be the minimizer of \(f\), and let \(x^{(i)}\) represent the iterates of gradient descent. The update rule for gradient descent with backtracking line search is:</p> \[x^{(i+1)} = x^{(i)} - t_i \nabla f(x^{(i)}),\] <p>where \(t_i\) is the step size at iteration \(i\), chosen adaptively using the backtracking procedure.</p> <h6 id="step-2-descent-lemma"><strong>Step 2: Descent Lemma</strong></h6> <p>In the case of gradient descent with a <strong>fixed step size</strong> \(t\), we know from the <strong>descent lemma</strong> (for smooth convex functions) that:</p> \[f(x^{(i+1)}) \leq f(x^{(i)}) - t \|\nabla f(x^{(i)})\|_2^2 + \frac{L}{2} t^2 \|\nabla f(x^{(i)})\|_2^2.\] <p>This inequality states that at each iteration, the function value decreases by a term proportional to the gradient’s squared norm, and this decrease depends on the step size \(t\).</p> <h6 id="step-3-backtracking-line-search"><strong>Step 3: Backtracking Line Search</strong></h6> <p>With <strong>backtracking line search</strong>, the step size \(t_i\) is chosen at each iteration to ensure sufficient decrease in the function value. Specifically, the step size is selected such that:</p> \[f(x^{(i+1)}) \leq f(x^{(i)}) + \alpha t_i \nabla f(x^{(i)})^T \nabla f(x^{(i)}),\] <p>where \(0 &lt; \alpha &lt; 1\) is a constant. The backtracking line search ensures that \(t_i\) satisfies the condition:</p> \[t_i \leq \frac{1}{L}.\] <p>Thus, the step size at each iteration is bounded by \(\frac{1}{L}\), which prevents the gradient from changing too rapidly and ensures that the update does not overshoot the optimal point.</p> <p><strong>Why “Adaptive”?</strong></p> <p>The step size is called <strong>adaptive</strong> because it changes at each iteration depending on the function’s behavior. If the function is steep or the gradient is large, the backtracking line search may choose a smaller step size to avoid overshooting. If the function is shallow or the gradient is small, it might allow a larger step size. This adaptive process uses a parameter \(\beta\) to control how the step size is reduced when the decrease condition is not met.</p> <h6 id="step-4-backtracking-process-and-beta"><strong>Step 4: Backtracking Process and \(\beta\)</strong></h6> <p>The process of backtracking works as follows:</p> <ul> <li> <p><strong>Initial Step Size</strong>: Start with an initial guess for the step size, typically \(t_0 = 1\).</p> </li> <li> <p><strong>Condition Check</strong>: Check whether the condition</p> </li> </ul> \[f(x^{(i+1)}) \leq f(x^{(i)}) + \alpha t_i \nabla f(x^{(i)})^T \nabla f(x^{(i)})\] <p>holds. If it does, accept \(t_i\); if not, reduce the step size.</p> <ul> <li><strong>Reduce Step Size</strong>: If the condition is not satisfied, reduce the step size \(t_i\) by a factor \(\beta\):</li> </ul> \[t_{i+1} = \beta t_i,\] <p>where \(\beta\) is a constant between 0 and 1 (usually around 0.5 or 0.8). This step size reduction continues until the condition is met.</p> <ul> <li><strong>Accept the Step Size</strong>: Once the condition is satisfied, the current \(t_i\) is accepted for the update.</li> </ul> <p>The use of \(\beta\) helps to ensure that the step size does not become too large, allowing the algorithm to converge smoothly without overshooting.</p> <h6 id="step-5-bounding-the-convergence"><strong>Step 5: Bounding the Convergence</strong></h6> <p>Now, let’s derive the convergence bound for gradient descent with backtracking line search. From the descent lemma, the change in the function value at each iteration can be bounded as:</p> \[f(x^{(i+1)}) - f(x^{(i)}) \leq - t_i \|\nabla f(x^{(i)})\|_2^2 \left( 1 - \frac{L}{2} t_i \right).\] <p>Because the backtracking line search ensures that \(t_i \leq t_{\text{min}} = \min\left( 1, \frac{\beta}{L} \right)\), we can bound the function value decrease as:</p> \[f(x^{(i+1)}) - f(x^{(i)}) \leq - t_{\text{min}} \|\nabla f(x^{(i)})\|_2^2 \left( 1 - \frac{L}{2} t_{\text{min}} \right).\] <p>This shows that the function value decreases at each iteration, with the step size \(t_{\text{min}}\) controlling the rate of decrease.</p> <p>Now, if you observe carefully, the equation above closely resembles the one we encountered in the fixed step size proof. The only minor difference is that \(t\) has been replaced with \(t_{\text{min}}\). Therefore, we can follow the same steps as in the fixed step size case and eventually arrive at the following result:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t_{\text{min}} k}.\] <p>This shows that by adaptively choosing the step size, we can achieve a convergence rate similar to that of the fixed step size approach, but without needing to manually set a fixed value for ( t ).</p> <p><strong>Quick Note:</strong> I’m still not completely satisfied with the proof for Adaptive Step Size. I’ll be working on refining the explanation further and will update you with any improvements.</p> <h5 id="and-finally"><strong>And finally…</strong></h5> <p>We’ve reached the end of this blog post! A huge kudos to you for making it all the way through and sticking with me. The reason we went through all of this is that understanding such proofs will lay the foundation for exploring the intricate details that drive machine learning and produce its remarkable results. To truly dive into ML research, we need to immerse ourselves in these depths and make it happen.</p> <p>So, take a well-deserved break, and in the next post, we’ll delve into the tips and tricks of SGD that are widely practiced in the industry. Until then, take care and see you soon!</p> <h5 id="references"><strong>References:</strong></h5> <ul> <li><a href="https://nyu-cs2565.github.io/mlcourse-public/2024-fall/lectures/lec02/gradient_descent_converge.pdf"> Gradient Descent: Convergence Analysis - Ryan Tibshirani</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.]]></summary></entry><entry><title type="html">Gradient Descent and Second-Order Optimization - A Thorough Comparison</title><link href="https://monishver11.github.io/blog/2024/gd-tips/" rel="alternate" type="text/html" title="Gradient Descent and Second-Order Optimization - A Thorough Comparison"/><published>2024-12-29T01:44:00+00:00</published><updated>2024-12-29T01:44:00+00:00</updated><id>https://monishver11.github.io/blog/2024/gd-tips</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/gd-tips/"><![CDATA[ <p>The concept discussed below may already be familiar to you, but it might appear a bit different in this blog. This content is adapted from the reference material - <a href="https://leon.bottou.org/publications/pdf/tricks-2012.pdf">Stochastic Gradient Descent Tricks</a> we received for Gradient Descent (GD) and Stochastic Gradient Descent (SGD) tips, written by <a href="https://leon.bottou.org/">L´eon Bottou</a>. It’s a well-written piece with many theoretical aspects that are often overlooked when applying GD in machine learning. To be honest, I still don’t fully grasp all of it, but I hope that as I continue on this learning journey, I’ll come to understand most of it and be able to make sense of it.</p> <p>This blog post, along with the next one, will serve as my personal notes on the material. Another reason for sharing this content is to familiarize ourselves with the different notations commonly used in machine learning research. Before diving directly into the SGD tips and tricks from the material, I felt it was important to revisit Gradient Descent (GD) as described in the text. In machine learning, the same concepts are often presented using various notations, which can be confusing if you’re not prepared.</p> <p>Think of it this way: just as there is a base language with many different slangs or colloquialisms, in math, the core concepts remain the same, but the notations can vary depending on who’s explaining them or for what purpose. So, consider this as the same concept, expressed in a different notation—another perspective on the same idea. That’s it—let’s get started!</p> <hr/> <h4 id="1-gradient-descent-gd"><strong>1. Gradient Descent (GD)</strong></h4> <h6 id="objective"><strong>Objective</strong></h6> <p>Minimize the empirical risk \(E_n(f_w)\).</p> <h6 id="update-rule"><strong>Update Rule</strong></h6> \[w_{t+1} = w_t - \gamma \frac{1}{n} \sum_{i=1}^n \nabla_w Q(z_i, w_t),\] <p>where:</p> <ul> <li>\(w_t\): Current weights at iteration \(t\).</li> <li>\(\gamma\): Learning rate (a small positive scalar).</li> <li>\(\nabla_w Q(z_i, w_t)\): Gradient of the loss function \(Q\) with respect to \(w_t\) for data point \(z_i\).</li> </ul> <p><strong>Convergence Requirements</strong>:</p> <ul> <li>\(w_0\) (initial weights) close to the optimum.</li> <li>\(\gamma\) small enough.</li> </ul> <p><strong>Performance</strong>:</p> <ul> <li>Achieves <strong>linear convergence</strong>, meaning the error decreases exponentially with iterations. The convergence rate is denoted as \(\rho\), so:</li> </ul> \[-\log \rho \sim t,\] <p>where \(\rho\) represents the residual error.</p> <hr/> <h4 id="2-second-order-gradient-descent-2gd"><strong>2. Second-Order Gradient Descent (2GD)</strong></h4> <h6 id="improvement"><strong>Improvement</strong></h6> <p>Instead of using a scalar learning rate \(\gamma\), introduce a positive definite matrix \(\Gamma_t\):</p> \[w_{t+1} = w_t - \Gamma_t \frac{1}{n} \sum_{i=1}^n \nabla_w Q(z_i, w_t).\] <ul> <li>\(\Gamma_t\): Approximates the inverse of the Hessian matrix of the cost function at the optimum.</li> <li>The Hessian is the second derivative of the cost function, capturing curvature information.</li> </ul> <h6 id="advantages"><strong>Advantages</strong></h6> <ul> <li>The algorithm accounts for the curvature of the cost function, leading to more informed updates.</li> <li>When \(\Gamma_t\) is exactly the inverse of the Hessian:</li> <li><strong>Convergence is quadratic</strong>, meaning:</li> </ul> \[-\log \log \rho \sim t,\] <p>where the error decreases much faster than linear convergence.</p> <ul> <li>If the cost function is quadratic and the scaling matrix \(\Gamma_t\) is exact, the optimum is reached in <strong>one iteration</strong>.</li> </ul> <h6 id="assumptions-for-quadratic-convergence"><strong>Assumptions for Quadratic Convergence</strong></h6> <ul> <li>Smoothness of the cost function.</li> <li>\(w_0\) close enough to the optimum.</li> </ul> <h6 id="intuition-behind-quadratic-convergence"><strong>Intuition Behind Quadratic Convergence</strong></h6> <ul> <li>In GD, the learning rate \(\gamma\) is fixed and doesn’t adapt to the problem’s geometry, leading to slower convergence in certain directions.</li> <li>In 2GD, the matrix \(\Gamma_t\) adapts to the curvature of the cost function:</li> <li>Allows larger steps in flat directions.</li> <li>Takes smaller steps in steep directions.</li> <li>This results in significantly faster convergence.</li> </ul> <hr/> <h4 id="follow-up-gradient-descent-and-second-order-gradient-descent"><strong>Follow-Up: Gradient Descent and Second-Order Gradient Descent</strong></h4> <p>This below section answers follow-up questions about the convergence behavior of Gradient Descent (GD) and the role of the inverse Hessian in Second-Order Gradient Descent (2GD).</p> <h5 id="1-how-does-linear-convergence-in-gd-lead-to-exponential-error-reduction"><strong>1. How does linear convergence in GD lead to exponential error reduction?</strong></h5> <h6 id="recap-of-linear-convergence"><strong>Recap of Linear Convergence</strong></h6> <p>Linear convergence means that the error at iteration \(t\) is proportional to the error at iteration \(t-1\), scaled by a constant \(\rho\):</p> \[\|w_t - w^*\| \leq \rho \|w_{t-1} - w^*\|, \quad \text{where } 0 &lt; \rho &lt; 1.\] <h6 id="derivation-of-exponential-error-reduction">Derivation of Exponential Error Reduction</h6> <p>Let’s derive how the error becomes proportional to \(\rho^t\) after \(t\) iterations:</p> <ul> <li>From the recurrence relation:</li> </ul> \[\|w_t - w^*\| \leq \rho \|w_{t-1} - w^*\|.\] <p>Expanding this iteratively:</p> \[\|w_t - w^*\| \leq \rho (\|w_{t-2} - w^*\|) \leq \rho^2 \|w_{t-2} - w^*\|.\] <ul> <li>Generalizing this pattern:</li> </ul> \[\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|,\] <p>where \(\|w_0 - w^*\|\) is the initial error at \(t = 0\).</p> <ul> <li>Since \(\rho &lt; 1\), \(\rho^t\) decreases exponentially as \(t\) increases. This shows the error reduces at an exponential rate in terms of the number of iterations.</li> </ul> <hr/> <h5 id="2-what-is-the-inverse-of-the-hessian-matrix"><strong>2. What is the inverse of the Hessian matrix?</strong></h5> <h6 id="hessian-matrix"><strong>Hessian Matrix</strong></h6> <p>The Hessian matrix is a second-order derivative matrix of the cost function \(Q(w)\), defined as:</p> \[H = \nabla^2_w Q(w),\] <p>where each entry \(H_{ij} = \frac{\partial^2 Q(w)}{\partial w_i \partial w_j}\) captures how the gradient changes with respect to each pair of weights.</p> <h6 id="inverse-of-the-hessian"><strong>Inverse of the Hessian</strong></h6> <p>The inverse of the Hessian, \(H^{-1}\), rescales the gradient updates based on the curvature of the cost function:</p> <ul> <li>In directions where the curvature is steep, \(H^{-1}\) reduces the step size.</li> <li>In flatter directions, \(H^{-1}\) increases the step size.</li> </ul> <p>This adjustment improves convergence by adapting the optimization step to the geometry of the cost function.</p> <hr/> <h5 id="3-how-does-using-the-inverse-hessian-converge-faster"><strong>3. How does using the inverse Hessian converge faster?</strong></h5> <h6 id="faster-convergence-with-2gd"><strong>Faster Convergence with 2GD</strong></h6> <p>In <strong>Second-Order Gradient Descent (2GD)</strong>:</p> \[w_{t+1} = w_t - H^{-1} \nabla_w Q(w_t).\] <p>This update accounts for the curvature of the cost function.</p> <h6 id="quadratic-convergence"><strong>Quadratic Convergence</strong></h6> <ul> <li>Near the optimum \(w^*\), the cost function can be locally approximated as quadratic:</li> </ul> \[Q(w) \approx \frac{1}{2}(w - w^*)^T H (w - w^*),\] <p>where \(H\) is the Hessian at \(w^*\).</p> <ul> <li>The gradient of the cost is:</li> </ul> \[\nabla_w Q(w) = H (w - w^*).\] <ul> <li>Substituting this gradient into the 2GD update:</li> </ul> \[w_{t+1} = w_t - H^{-1} H (w_t - w^*).\] <ul> <li>Simplifies to:</li> </ul> \[w_{t+1} = w^*.\] <p>This shows that in the best case (when the cost is exactly quadratic and \(H^{-1}\) is exact), the algorithm converges in <strong>one iteration</strong>.</p> <hr/> <h5 id="4-summary-of-convergence-behavior"><strong>4. Summary of Convergence Behavior</strong></h5> <ul> <li><strong>Gradient Descent (GD)</strong>: <ul> <li>Linear convergence: \(\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|\).</li> <li>Error decreases exponentially at a rate \(\rho\), where \(\rho\) depends on the learning rate and the condition number of the Hessian.</li> </ul> </li> <li><strong>Second-Order Gradient Descent (2GD)</strong>: <ul> <li>Quadratic convergence: \(\|w_t - w^*\| \sim (\text{error})^2\) at each iteration.</li> <li>When the cost is quadratic and \(H^{-1}\) is exact, the algorithm converges in one step.</li> </ul> </li> </ul> <hr/> <h4 id="more-details-linear-vs-quadratic-convergence-in-optimization">More Details: Linear vs. Quadratic Convergence in Optimization</h4> <h5 id="1-linear-convergence-in-gradient-descent"><strong>1. Linear Convergence in Gradient Descent</strong></h5> <h6 id="key-idea"><strong>Key Idea:</strong></h6> <p>Gradient Descent (GD) decreases the error at a fixed proportion \(\rho\) per iteration:</p> \[\|w_t - w^*\| \leq \rho \|w_{t-1} - w^*\|, \quad \text{where } 0 &lt; \rho &lt; 1.\] <h6 id="step-by-step-derivation"><strong>Step-by-Step Derivation:</strong></h6> <ul> <li><strong>Iterative Expansion</strong>: Expanding the recurrence:</li> </ul> \[\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|,\] <p>where \(\|w_0 - w^*\|\) is the initial error.</p> <ul> <li><strong>Take the Logarithm</strong>: Apply the natural logarithm to both sides:</li> </ul> \[\log \|w_t - w^*\| \leq \log (\rho^t \|w_0 - w^*\|).\] <ul> <li><strong>Simplify Using Logarithm Rules</strong>: Using \(\log (ab) = \log a + \log b\) and \(\log (\rho^t) = t \log \rho\), we get:</li> </ul> \[\log \|w_t - w^*\| \leq t \log \rho + \log \|w_0 - w^*\|.\] <ul> <li><strong>Why Does \(t \log \rho\) Decrease Linearly?</strong> <ul> <li>The parameter \(\rho\) satisfies \(0 &lt; \rho &lt; 1\), so \(\log \rho &lt; 0\).</li> <li>As \(t\) increases, \(t \log \rho\) becomes a larger negative number, reducing the value of \(\log \|w_t - w^*\|\).</li> <li>Since \(\log \rho\) is a constant, the term \(t \log \rho\) depends <strong>linearly on \(t\)</strong>:</li> </ul> \[t \log \rho = (\text{constant}) \cdot t, \quad \text{where constant} = \log \rho.\] </li> <li><strong>Interpretation of Convergence Rate</strong>: <ul> <li>From the error bound \(\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|\), we see exponential error decay with \(t\).</li> <li>Taking the logarithm leads to a linear relationship in \(t\):</li> </ul> </li> </ul> \[\log \|w_t - w^*\| \sim t \log \rho.\] <ul> <li>This behavior is summarized as:</li> </ul> \[-\log \rho \sim t.\] <h5 id="2-quadratic-convergence-in-second-order-gradient-descent"><strong>2. Quadratic Convergence in Second-Order Gradient Descent</strong></h5> <h6 id="key-idea-1"><strong>Key Idea:</strong></h6> <p>In Second-Order Gradient Descent (2GD), the error at each step is proportional to the <strong>square</strong> of the error at the previous step:</p> \[\|w_t - w^*\| \sim (\|w_{t-1} - w^*\|)^2.\] <h6 id="step-by-step-derivation-1"><strong>Step-by-Step Derivation:</strong></h6> <ul> <li><strong>Iterative Expansion</strong>: Rewriting the error at step \(t\) in terms of the initial error \(\|w_0 - w^*\|\):</li> </ul> \[\|w_t - w^*\| \sim (\|w_{t-1} - w^*\|)^2 \sim \left((\|w_{t-2} - w^*\|)^2\right)^2 \sim \dots \sim (\|w_0 - w^*\|)^{2^t}.\] <p>Thus:</p> \[\|w_t - w^*\| \sim (\|w_0 - w^*\|)^{2^t}.\] <ul> <li><strong>Take the Logarithm</strong>: Apply the natural logarithm:</li> </ul> \[\log \|w_t - w^*\| \sim 2^t \log \|w_0 - w^*\|.\] <ul> <li><strong>Take Another Logarithm</strong>: To analyze the rate of convergence, take the logarithm again:</li> </ul> \[\log \log \|w_t - w^*\| \sim \log (2^t) + \log \log \|w_0 - w^*\|.\] <p>Using \(\log (2^t) = t \log 2\), we simplify:</p> \[\log \log \|w_t - w^*\| \sim t + \log \log \|w_0 - w^*\|.\] <ul> <li><strong>Interpretation of Convergence Rate</strong>: <ul> <li>From the error bound \(\|w_t - w^*\| \sim (\|w_0 - w^*\|)^{2^t}\), we see super-exponential error decay.</li> <li>Taking the logarithm of the logarithm shows linear growth in \(t\):</li> </ul> </li> </ul> \[\log \log \|w_t - w^*\| \sim t.\] <ul> <li>This behavior is expressed as:</li> </ul> \[-\log \log \rho \sim t.\] <h6 id="summary-of-convergence-behavior"><strong>Summary of Convergence Behavior</strong></h6> <h3 id="summary-of-convergence-behavior-1"><strong>Summary of Convergence Behavior</strong></h3> <table> <thead> <tr> <th><strong>Convergence Type</strong></th> <th><strong>Error Decay</strong></th> <th><strong>Logarithmic Analysis</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gradient Descent (GD)</strong></td> <td>\(|w_t - w^*| \sim \rho^t\)</td> <td>\(\log |w_t - w^*| \sim t\)</td> </tr> <tr> <td><strong>Second-Order Gradient Descent (2GD)</strong></td> <td>\(|w_t - w^*| \sim (|w_0 - w^*|)^{2^t}\)</td> <td>\(\log \log |w_t - w^*| \sim t\)</td> </tr> </tbody> </table> <hr/> <p>Next, SGD Tricks and Tips:</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.]]></summary></entry><entry><title type="html">Understanding Stochastic Gradient Descent (SGD)</title><link href="https://monishver11.github.io/blog/2024/SGD/" rel="alternate" type="text/html" title="Understanding Stochastic Gradient Descent (SGD)"/><published>2024-12-28T03:42:00+00:00</published><updated>2024-12-28T03:42:00+00:00</updated><id>https://monishver11.github.io/blog/2024/SGD</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/SGD/"><![CDATA[<p>In the last blog, we introduced <strong>Stochastic Gradient Descent (SGD)</strong> as a computationally efficient optimization method. In this post, we’ll dive deeper into the mechanics of SGD, exploring its nuances, trade-offs, and how it compares to other gradient descent variants. Let’s unravel the details and gain a comprehensive understanding of these optimization techniques.</p> <h4 id="noisy-gradient-descent"><strong>“Noisy” Gradient Descent</strong></h4> <p>Instead of computing the exact gradient at every step, <strong>noisy gradient descent</strong> estimates the gradient using random subsamples. Surprisingly, this approximation often works well.</p> <p><strong>Why Does It Work?</strong></p> <p>Gradient descent is inherently iterative, meaning it has the chance to recover from previous missteps at each step. Leveraging noisy estimates can speed up the process without significantly impacting the final results.</p> <hr/> <h4 id="mini-batch-gradient-descent"><strong>Mini-batch Gradient Descent</strong></h4> <p>The <strong>full gradient</strong> for a dataset \(D_n = (x_1, y_1), \dots, (x_n, y_n)\) is given by:</p> \[\nabla \hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \nabla_w \ell(f_w(x_i), y_i)\] <p>This requires the entire dataset, which can be computationally expensive. To mitigate this, we use a <strong>mini-batch</strong> of size \(N\), a random subset of the data:</p> \[\nabla \hat{R}_N(w) = \frac{1}{N} \sum_{i=1}^N \nabla_w \ell(f_w(x_{m_i}), y_{m_i})\] <p>Here, \((x_{m_1}, y_{m_1}), \dots, (x_{m_N}, y_{m_N})\) is the mini-batch.</p> <hr/> <h4 id="batch-vs-stochastic-methods"><strong>Batch vs. Stochastic Methods</strong></h4> <p><strong>Rule of Thumb:</strong></p> <ul> <li><strong>Stochastic methods</strong> perform well far from the optimum but struggle as we approach it.</li> <li><strong>Batch methods</strong> excel near the optimum due to more precise gradient calculations.</li> </ul> <h4 id="mini-batch-gradient-properties"><strong>Mini-batch Gradient Properties</strong></h4> <ul> <li>The mini-batch gradient is an <strong>unbiased estimator</strong> of the full gradient, meaning on average, the gradient computed using a minibatch (a small, random subset of the dataset) gives the same direction of descent as the gradient computed using the entire dataset.</li> </ul> \[\mathbb{E}[\nabla \hat{R}_N(w)] = \nabla \hat{R}_n(w)\] <ul> <li> <p>This implies that while individual minibatch gradients may vary due to the randomness of the sample, their expected value matches the full batch gradient. This property allows Stochastic Gradient Descent (SGD) to make consistent progress toward the optimum without requiring computation over the entire dataset in each iteration.</p> </li> <li> <p>Larger mini-batches result in better estimates but are slower to compute:</p> </li> </ul> \[\text{Var}[\nabla \hat{R}_N(w)] = \frac{1}{N} \text{Var}[\nabla \hat{R}_i(w)]\] <ul> <li>This is because averaging over more samples reduces randomness. Specifically, the variance is scaled by \(1/𝑁\), meaning larger minibatches produce more accurate and stable gradient estimates, closer to the full batch gradient.</li> </ul> <p><strong>Tradeoffs of minibatch size:</strong></p> <ul> <li><strong>Larger \(N\):</strong> Better gradient estimate, slower computation.</li> <li><strong>Smaller \(N\):</strong> Faster computation, noisier gradient estimates.</li> </ul> <hr/> <h4 id="convergence-of-sgd"><strong>Convergence of SGD</strong></h4> <p>To ensure convergence, <strong>diminishing step sizes</strong> like \(\eta_k = 1/k\) are often used. While gradient descent (GD) theoretically converges faster than SGD:</p> <ul> <li><strong>GD</strong> is efficient near the minimum due to higher accuracy.</li> <li><strong>SGD</strong> is more practical for large-scale problems where high accuracy is unnecessary.</li> </ul> <p>In practice, SGD with <strong>fixed step sizes</strong> works well and can be adjusted using techniques like <strong>staircase decay</strong> or <strong>inverse time decay</strong> (\(1/t\)).</p> <h6 id="sgd-algorithm-with-mini-batches"><strong>SGD Algorithm with Mini-batches</strong></h6> <ol> <li>Initialize \(w = 0\).</li> <li>Repeat: <ul> <li>Randomly sample \(N\) points from \(D_n\): \({(x_i, y_i)}_{i=1}^N\).</li> <li>Update weights:</li> </ul> \[w \leftarrow w - \eta \left( \frac{1}{N} \sum_{i=1}^N \nabla_w \ell(f_w(x_i), y_i) \right)\] </li> </ol> <hr/> <h5 id="why-diminishing-step-sizes-theoretical-aspects"><strong>Why Diminishing Step Sizes? (Theoretical Aspects)</strong></h5> <p>If \(f\) is \(L\)-smooth and convex, and the variance of \(\nabla f(x^{(k)})\) is bounded</p> \[\text{Var}(\nabla f(x^{(k)})) \leq \sigma^2\] <p>, then SGD with step size</p> \[\eta \leq \frac{1}{L}\] <p>satisfies:</p> \[\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2] \leq \frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k} + \frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\] <p><strong>Breaking it Down:</strong></p> <ol> <li><strong>L-Smooth and Convex Function</strong>: <ul> <li>A function ( f ) is <strong>smooth</strong> if its gradient doesn’t change too rapidly. Specifically, an \(L\)-smooth function means that the gradient’s rate of change is bounded by a constant \(L\).</li> <li>A <strong>convex</strong> function means that it has a single global minimum, making optimization easier because we don’t have to worry about getting stuck in local minima.</li> </ul> </li> <li><strong>Variance of Gradient</strong>: <ul> <li>The gradient at each step of SGD might not be exact. The variance \(\text{Var}(\nabla f(x^{(k)}))\) measures the “noise” or fluctuations in the gradient estimate. A smaller variance means the gradient is more stable.</li> </ul> </li> </ol> <p><strong>What Does the Formula Mean?</strong></p> <p>The formula provides an upper bound on the expected squared magnitude of the gradient:</p> \[\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2] \leq \frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k} + \frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\] <ul> <li> <p><strong>Left Side</strong>: \(\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2]\) represents the minimum expected squared gradient magnitude. A smaller value indicates that the gradient is approaching zero, meaning we’re getting closer to the optimal solution.</p> </li> <li> <p><strong>Right Side</strong>:</p> <ul> <li>The first term \(\frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k}\) reflects how the gap between the initial point \(x^{(0)}\) and the optimal solution \(x^*\) decreases over time. The more steps we take (i.e., the larger the sum of the step sizes \(\eta_k\)), the smaller the gap becomes.</li> <li>The second term \(\frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\) accounts for the variance in the gradient. If the step size doesn’t decrease over time, this variance term grows, which can destabilize the optimization process. The numerator \(\sum_k \eta_k^2\) grows faster than the denominator \(\sum_k \eta_k\), so increasing step sizes overall increases the second term. So, this term will dominate if the step size does not decrease.</li> </ul> </li> </ul> <p><strong>Intuition Behind Diminishing Step Sizes:</strong></p> <ul> <li> <p><strong>Without diminishing step sizes</strong> - If you keep taking large steps, especially when close to the minimum, you risk overshooting the optimal solution. Large gradients or noisy estimates can lead to erratic behavior.</p> </li> <li> <p><strong>With diminishing step sizes</strong> - As we get closer to the minimum, reducing the step size helps take smaller, more controlled steps. This reduces the variance (noise) in the gradient and makes the convergence process smoother and more stable.</p> </li> </ul> <p><strong>So, now why diminish step sizes?</strong></p> <p>Diminishing step sizes are important because:</p> <ul> <li>Early on, larger steps help explore the solution space and make significant progress.</li> <li>As you approach the optimal solution, smaller steps are needed to fine-tune the result and avoid overshooting. This balance helps the optimization process converge efficiently while maintaining stability.</li> </ul> <p>More on the mathematical details of convergence will be covered in a separate blog post. For now, the key intuition to keep in mind is that diminishing step sizes help strike a balance between exploration (larger steps) and stability (smaller steps), leading to smoother convergence.</p> <hr/> <h4 id="summary"><strong>Summary</strong></h4> <p>Gradient descent variants provide trade-offs in speed, accuracy, and computational cost:</p> <ul> <li><strong>Full-batch gradient descent:</strong> Uses the entire dataset for gradient computation, yielding precise updates but high computational cost.</li> <li><strong>Mini-batch gradient descent:</strong> Balances computational efficiency and gradient accuracy by using subsets of data.</li> <li><strong>Stochastic gradient descent (SGD):</strong> Uses a single data point (\(N = 1\)) for updates, making it highly efficient but noisy.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GD_Variations.webp" sizes="95vw"/> <img src="/assets/img/GD_Variations.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GD_Variations" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Batch Vs Stochastic Vs Mini-Batch GD </div> <p>When referring to SGD, always clarify the batch size to avoid ambiguity. Modern machine learning heavily relies on SGD due to its time and memory efficiency, especially for large-scale problems.</p> <hr/> <h5 id="example-logistic-regression-with-ell_2-regularization"><strong>Example: Logistic Regression with \(\ell_2\)-Regularization</strong></h5> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_1-480.webp 480w,/assets/img/SGD_Comp_1-800.webp 800w,/assets/img/SGD_Comp_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SGD_Comp_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_2-480.webp 480w,/assets/img/SGD_Comp_2-800.webp 800w,/assets/img/SGD_Comp_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SGD_Comp_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_3-480.webp 480w,/assets/img/SGD_Comp_3-800.webp 800w,/assets/img/SGD_Comp_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SGD_Comp_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Convergence Speed(1), Computational Efficiency(2) and Rate at Near Optimum(3) of different GD approaches </div> <ul> <li><strong>Batch methods:</strong> Converge faster near the optimum.</li> <li><strong>Stochastic methods:</strong> Are computationally efficient, especially for large datasets.</li> </ul> <p>Understanding these trade-offs helps in choosing the right approach for different scenarios.</p> <hr/> <p>In the next blog, we’ll explore <strong>Gradient Descent Convergence Theorems</strong> and how to intuitively make sense out of it! See you.</p> <h6 id="image-credits"><strong>Image Credits:</strong></h6> <ul> <li><a href="https://alwaysai.co/blog/what-is-gradient-descent">Batch Vs Stochastic Vs Mini-Batch GD</a></li> <li><a href="https://www.stat.berkeley.edu/~ryantibs/">Example from Ryan Tibshirani</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).]]></summary></entry><entry><title type="html">Gradient Descent - A Detailed Walkthrough</title><link href="https://monishver11.github.io/blog/2024/gradient-descent/" rel="alternate" type="text/html" title="Gradient Descent - A Detailed Walkthrough"/><published>2024-12-25T19:01:00+00:00</published><updated>2024-12-25T19:01:00+00:00</updated><id>https://monishver11.github.io/blog/2024/gradient-descent</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/gradient-descent/"><![CDATA[<p>In our last blog post, we discussed Empirical Risk Minimization (ERM). Let’s build on that foundation by exploring a concrete example: <strong>Linear Least Squares Regression</strong>. This will help us understand how gradient descent fits into the optimization landscape.</p> <h4 id="linear-least-squares-regression"><strong>Linear Least Squares Regression</strong></h4> <p><strong>Problem Setup</strong></p> <p>We aim to minimize the empirical risk using linear regression. Here’s the setup:</p> <ul> <li><strong>Loss function</strong>: \(\ell(\hat{y}, y) = (\hat{y} - y)^2\)</li> <li><strong>Hypothesis space</strong>: \(\mathcal{F} = \{ f : \mathbb{R}^d \to \mathbb{R} \mid f(x) = w^\top x, \, w \in \mathbb{R}^d \}\)</li> <li><strong>Data set</strong>: \(\mathcal{D}_n = \{(x_1, y_1), \dots, (x_n, y_n)\}\) Our goal is to find the ERM solution: \(\hat{f} \in \mathcal{F}\). <strong>Objective Function</strong></li> </ul> <p>We want to find the function in \(\mathcal{F}\), parametrized by \(w \in \mathbb{R}^d\), that minimizes the empirical risk:</p> \[\hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n (w^\top x_i - y_i)^2\] <p>This leads to the optimization problem: \(\min_{w \in \mathbb{R}^d} \hat{R}_n(w)\)</p> <p>Although ordinary least squares (OLS) offers a closed-form solution (<strong>refer to the provided resource to understand OLS</strong>), gradient descent proves more versatile, particularly when closed-form solutions are not feasible.</p> <hr/> <h3 id="gradient-descent"><strong>Gradient Descent</strong></h3> <p>Gradient descent is a powerful optimization technique for unconstrained problems.</p> <h4 id="unconstrained-optimization-setting"><strong>Unconstrained Optimization Setting</strong></h4> <p>We assume the objective function \(f : \mathbb{R}^d \to \mathbb{R}\) is <strong>differentiable</strong>, and we aim to find:</p> \[x^* = \arg \min_{x \in \mathbb{R}^d} f(x)\] <h4 id="the-gradient"><strong>The Gradient</strong></h4> <p>The gradient is a fundamental concept in optimization. For a differentiable function \(f : \mathbb{R}^d \to \mathbb{R}\), the gradient at a point \(x_0 \in \mathbb{R}^d\) is denoted by \(\nabla f(x_0)\). It represents the vector of partial derivatives of \(f\) with respect to each dimension of \(x\):</p> \[\nabla f(x_0) = \left[ \frac{\partial f}{\partial x_1}(x_0), \frac{\partial f}{\partial x_2}(x_0), \dots, \frac{\partial f}{\partial x_d}(x_0) \right]\] <h6 id="key-points"><strong>Key points:</strong></h6> <ul> <li>The gradient points in the direction of the steepest <strong>increase</strong> of the function \(f(x)\) starting from \(x_0\).</li> <li>The <strong>magnitude</strong> of the gradient indicates how steep the slope is in that direction.</li> </ul> <p>For example, consider a 2D function \(f(x, y)\). The gradient at a point \((x_0, y_0)\) is:</p> \[\nabla f(x_0, y_0) = \left[ \frac{\partial f}{\partial x}(x_0, y_0), \frac{\partial f}{\partial y}(x_0, y_0) \right]\] <p>This tells us how \(f\) changes with respect to \(x\) and \(y\) near \((x_0, y_0)\).</p> <h6 id="importance-in-optimization"><strong>Importance in Optimization</strong></h6> <p>The gradient is crucial because it provides the direction in which the function \(f(x)\) increases most rapidly. To minimize \(f(x)\):</p> <ul> <li>We move in the <strong>opposite</strong> direction of the gradient, as this is where the function decreases most rapidly.</li> </ul> <h6 id="geometric-interpretation"><strong>Geometric Interpretation</strong></h6> <p>Imagine a 3D surface representing a function \(f(x, y)\). At any point on the surface:</p> <ul> <li>The gradient vector points <strong>uphill</strong>, perpendicular to the contour lines (or level curves) of the function.</li> <li>To find a minimum, we “descend” by moving in the <strong>opposite direction</strong> of the gradient vector.</li> </ul> <p>This understanding lays the foundation for applying gradient descent effectively in optimization problems.</p> <hr/> <h3 id="gradient-descent-algorithm"><strong>Gradient Descent Algorithm</strong></h3> <p>To iteratively minimize \(f(x)\), follow these steps:</p> <ol> <li><strong>Initialize</strong>: \(x \leftarrow 0\)</li> <li><strong>Repeat</strong>: \(x \leftarrow x - \eta \nabla f(x)\) <ul> <li>until a stopping criterion is met.</li> </ul> </li> </ol> <p>Here, \(\eta\) is the <strong>step size</strong> (or <strong>learning rate</strong>). Choosing \(\eta\) appropriately is critical to avoid divergence or slow convergence. “Step size” is also referred to as “learning rate” in neural networks literature.</p> <div align="center"> <img src="https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif" alt="Gradient Descent GIF" width="500"/> <p>Path of a Gradient Descent Algorithm</p> </div> <h4 id="insights-into-gradient-descent"><strong>Insights into Gradient Descent</strong></h4> <h6 id="step-size"><strong>Step Size</strong></h6> <ul> <li><strong>Fixed step size</strong>: Works if small enough.</li> <li>If \(\eta\) is too large, the process might diverge.</li> <li>Experimenting with multiple step sizes is often necessary.</li> <li>Big vs. Small Steps: <ul> <li><strong>Big steps</strong>: In flat regions where the gradient is small, larger steps accelerate convergence.</li> <li><strong>Small steps</strong>: In steep regions where the gradient is large, smaller steps ensure stability and prevent overshooting.</li> <li>Adaptive methods like Adam or RMSprop leverage this intuition by dynamically adjusting the step size based on the gradient’s magnitude or past behavior.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GD_Learning_Rate-480.webp 480w,/assets/img/GD_Learning_Rate-800.webp 800w,/assets/img/GD_Learning_Rate-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/GD_Learning_Rate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GD_Learning_Rate" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Step size variations. Image credit: https://medium.com/@yennhi95zz </div> <h6 id="convergence"><strong>Convergence</strong></h6> <p>Gradient descent converges to a stationary point (where the derivative is zero) for differentiable functions. These stationary points could be:</p> <ul> <li>Local minima</li> <li>Local maxima</li> <li>Saddle points</li> </ul> <p>For convex functions(Added Reference), gradient descent can converge to the global minimum.</p> <hr/> <p>The following theorems are often overlooked when learning about gradient descent. We’ll dive into them in detail in a separate blog, but for now, give them a quick read and continue.</p> <h4 id="theorem-convergence-of-gradient-descent-with-fixed-step-size"><strong>Theorem: Convergence of Gradient Descent with Fixed Step Size</strong></h4> <p>Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is convex and differentiable, and \(\nabla f\) is Lipschitz continuous with constant \(L &gt; 0\) (i.e., \(f\) is L-smooth). This means:</p> \[\|\nabla f(x) - \nabla f(x')\| \leq L \|x - x'\|\] <p>for any \(x, x' \in \mathbb{R}^d\).</p> <p><strong>Result:</strong></p> <p>If gradient descent uses a fixed step size \(\eta \leq \frac{1}{L}\), then:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|^2}{2\eta k}\] <p><strong>Implications:</strong></p> <ul> <li>Gradient descent is <strong>guaranteed to converge</strong> under these conditions.</li> <li>The convergence rate is \(O(1/k)\)</li> </ul> <hr/> <h4 id="strongly-convex-functions"><strong>Strongly Convex Functions</strong></h4> <p>A function \(f\) is \(\mu\)-strongly convex if:</p> \[f(x') \geq f(x) + \nabla f(x) \cdot (x' - x) + \frac{\mu}{2} \|x - x'\|^2\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Strongly_Convex-480.webp 480w,/assets/img/Strongly_Convex-800.webp 800w,/assets/img/Strongly_Convex-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Strongly_Convex.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Strongly_Convex" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Convex and Strongly Convex Curve </div> <h4 id="convergence-theorem-for-strongly-convex-functions"><strong>Convergence Theorem for Strongly Convex Functions</strong></h4> <p>If \(f\) is both \(L\)-smooth and \(\mu\)-strongly convex, with step size \(0 &lt; \eta \leq \frac{1}{L}\), then gradient descent achieves convergence with the following inequality:</p> \[\|x^{(k)} - x^*\|^2 \leq (1 - \eta \mu)^k \|x^{(0)} - x^*\|^2\] <p>This implies <strong>linear convergence</strong>, but it depends on \(\mu\). If the estimate of µ is bad then the rate is not great.</p> <hr/> <h4 id="stopping-criterion"><strong>Stopping Criterion</strong></h4> <ul> <li>Stop when \(\|\nabla f(x)\|_2 \leq \epsilon\), where \(\epsilon\) is a small threshold(of our choice). <strong>Why?</strong> At a local minimum, \(\nabla f(x) = 0\). If the gradient becomes small and plateaus, further updates are unlikely to significantly reduce the objective function, so we can stop the gradient updates.</li> <li>Early Stopping <ul> <li>Evaluate the loss on validation data (unseen held-out data) after each iteration.</li> <li>Stop when the loss no longer improves or starts to worsen.</li> </ul> </li> </ul> <hr/> <h3 id="quick-recap-gradient-descent-for-erm"><strong>Quick recap: Gradient Descent for ERM</strong></h3> <p>Given a hypothesis space \(F = \{f_w : X \to Y \mid w \in \mathbb{R}^d\}\), we aim to minimize:</p> \[\hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \ell(f_w(x_i), y_i)\] <p>Gradient descent is applicable if \(\ell(f_w(x_i), y_i)\) is differentiable with respect to \(w\).</p> <h6 id="scalability"><strong>Scalability</strong></h6> <p>At each iteration, we compute the gradient at the current \(w\) as:</p> \[\nabla \hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \nabla_w \ell(f_w(x_i), y_i)\] <p>This requires \(O(n)\) computation per step, as we have to iterate over all n training points to take a single step. To scale better, alternative methods like <strong>stochastic gradient descent (SGD)</strong> can be considered.</p> <hr/> <p>Gradient descent is an indispensable tool for optimization, especially in machine learning. By understanding its principles, convergence properties, and practical considerations, we can effectively tackle a variety of optimization problems.</p> <p>Stay tuned for the next post, where we’ll explore stochastic gradient descent and its variations for scalability!</p> <hr/> <h5 id="references--good-resources-for-visualizing-gradient-descent"><strong>References &amp; Good Resources for Visualizing Gradient Descent:</strong></h5> <ul> <li><a href="https://aero-learn.imperial.ac.uk/vis/Machine%20Learning/gradient_descent_3d.html" target="_blank">3D Gradient Descent</a></li> <li><a href="https://kaggle.com/code/trolukovich/animating-gradien-descent" target="_blank">Animating Gradient Descent</a></li> <li><a href="https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c" target="_blank">A Visual Explanation of Gradient Descent Methods (Momentum, AdaGrad, RMSProp, Adam) – Towards Data Science</a> (Variations of Gradient Descent)</li> <li><a href="https://medium.com/@yennhi95zz/4-a-beginners-guide-to-gradient-descent-in-machine-learning-773ba7cd3dfe" target="_blank">Yennhi95zz, 2023. #4. A Beginner’s Guide to Gradient Descent in Machine Learning. Medium</a></li> <li><a href="https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif">Path of a Gradient Descent Algorithm</a> (Image Credit)</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[An in-depth exploration of gradient descent, including its convergence and step size considerations.]]></summary></entry><entry><title type="html">Empirical Risk Minimization (ERM)</title><link href="https://monishver11.github.io/blog/2024/ERM/" rel="alternate" type="text/html" title="Empirical Risk Minimization (ERM)"/><published>2024-12-24T23:50:00+00:00</published><updated>2024-12-24T23:50:00+00:00</updated><id>https://monishver11.github.io/blog/2024/ERM</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/ERM/"><![CDATA[<p>Continuing from our discussion of supervised learning, we now dive into <strong>Empirical Risk Minimization (ERM)</strong>. While the ultimate goal is to minimize the true risk, ERM provides a practical way to approximate this goal using finite data. Let’s break it down.</p> <hr/> <h3 id="definition-empirical-risk-minimization"><strong>Definition: Empirical Risk Minimization</strong></h3> <p>A function \(\hat{f}\) is an <strong>empirical risk minimizer</strong> if:</p> \[\hat{f} \in \arg\min_f \hat{R}_n(f),\] <p>where \(\hat{R}_n(f)\) is the empirical risk, defined over a finite sample and the minimum is taken over all functions\(f: X \to Y\). In an ideal scenario, we would want to minimize the true risk \(R(f)\). This raises a critical question:</p> <p><strong>Is the empirical risk minimizer close enough to the true risk minimizer?</strong></p> <h4 id="example-of-erm-in-action"><strong>Example of ERM in Action</strong></h4> <p>Let’s consider a simple case:</p> <ul> <li>Let \(P_X = \text{Uniform}[0,1]\) and \(Y \equiv 1\) (i.e., \(Y\) is always 1).</li> <li>A proposed prediction function:</li> </ul> \[\hat{f}(x) = \begin{cases} 1 &amp; \text{if } x \in \{0.25, 0.5, 0.75\}, \\ 0 &amp; \text{otherwise.} \end{cases}\] <p><strong>Loss Analysis:</strong></p> <p>Under both the square loss and the 0-1 loss:</p> <ul> <li><strong>Empirical Risk</strong>: 0</li> <li><strong>True Risk</strong>: 1</li> </ul> <p><strong>Explanation</strong>:</p> <ul> <li>The <strong>empirical risk</strong> measures the loss on the training data. Since \(\hat{f}(x)\) perfectly predicts the labels for the training points \(x \in \{0.25, 0.5, 0.75\}\), the empirical risk is zero. There are no errors on the observed data points. - The <strong>true risk</strong>, however, measures the loss over the entire distribution of \(P_X\). For all \(x \notin \{0.25, 0.5, 0.75\}\), \(\hat{f}(x)\) incorrectly predicts 0 instead of the correct label 1, resulting in significant errors. Since \(P_X\) is uniform over \([0,1]\), this means \(\hat{f}(x)\) is incorrect for all others data points of the domain, leading to a true risk of 1.</li> </ul> <p>This illustrates a key problem: <strong>ERM can lead to overfitting by simply memorizing the training data.</strong></p> <hr/> <h3 id="generalization-improving-erm"><strong>Generalization: Improving ERM</strong></h3> <p>In the above example, ERM failed to generalize to unseen data. To improve generalization, we must “smooth things out”—a process that spreads and extrapolates information from observed parts of the input space \(X\) to unobserved parts.</p> <p>One solution is <strong>Constrained ERM</strong>: Instead of minimizing empirical risk over all possible functions, we restrict our search to a subset of functions, known as a <strong>hypothesis space</strong>.</p> <h4 id="hypothesis-spaces"><strong>Hypothesis Spaces</strong></h4> <p>A <strong>hypothesis space</strong> \(\mathcal{F}\) is a set of prediction functions mapping \(X \to Y\) that we consider when applying ERM.</p> <p><strong>Desirable Properties of a Hypothesis Space</strong></p> <ul> <li>Includes only functions with the desired “regularity” (e.g., smoothness or simplicity).</li> <li>Is computationally tractable (efficient algorithms exist for finding the best function in \(\mathcal{F}\)).</li> </ul> <p>In practice, much of machine learning involves designing appropriate hypothesis spaces for specific tasks.</p> <h4 id="constrained-erm"><strong>Constrained ERM</strong></h4> <p>Given a hypothesis space \(\mathcal{F}\), the empirical risk minimizer in \(\mathcal{F}\) is defined as:</p> \[\hat{f}_n \in \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i),\] <p>where \(\ell(f(x), y)\) is the loss function.</p> <p>Similarly, the true risk minimizer in \(\mathcal{F}\) is:</p> \[f^*_{\mathcal{F}} \in \arg\min_{f \in \mathcal{F}} \mathbb{E}[\ell(f(x), y)].\] <hr/> <h3 id="excess-risk-decomposition"><strong>Excess Risk Decomposition</strong></h3> <p>We analyze the performance of ERM through <strong>excess risk decomposition</strong>, which breaks down the gap between the true risk(e.g., the function returned by ERM) and the risk of the Bayes optimal function:</p> <p><strong>Again, Definitions</strong></p> <ul> <li> <p><strong>Bayes Optimal Function</strong>:</p> \[f^* = \arg\min_f \mathbb{E}[\ell(f(x), y)]\] </li> <li> <p><strong>Risk Minimizer in \(\mathcal{F}\)</strong>:</p> \[f_{\mathcal{F}} = \arg\min_{f \in \mathcal{F}} \mathbb{E}[\ell(f(x), y)]\] </li> <li> <p><strong>ERM Solution</strong>:</p> \[\hat{f}_n = \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)\] </li> </ul> <h4 id="excess-risk-decomposition-for-erm"><strong>Excess Risk Decomposition for ERM</strong></h4> <p><strong>Definition</strong></p> <p>The <strong>excess risk</strong> measures how much worse the risk of a function \(f\) is compared to the risk of the Bayes optimal function \(f^*\), which minimizes the true risk. Mathematically, it is defined as:</p> \[\text{Excess Risk}(f) = R(f) - R(f^*)\] <p>where:</p> <ul> <li>\(R(f)\) is the true risk of the function \(f\).</li> <li>\(R(f^*)\) is the Bayes risk, i.e., the lowest achievable risk.</li> </ul> <p><strong>Can Excess Risk Be Negative?</strong> No, the excess risk can never be negative because the Bayes optimal function \(f^*\) achieves the minimum possible risk by definition. For any other function \(f\), the risk \(R(f)\) will be equal to or greater than \(R(f^*)\).</p> <h5 id="decomposition-of-excess-risk-for-erm"><strong>Decomposition of Excess Risk for ERM</strong></h5> <p>For the empirical risk minimizer \(\hat{f}_n\), the excess risk can be decomposed as follows:</p> \[\text{Excess Risk}(\hat{f}_n) = R(\hat{f}_n) - R(f^*) = \underbrace{R(\hat{f}_n) - R(f_\mathcal{F})}_{\text{Estimation Error}} + \underbrace{R(f_\mathcal{F}) - R(f^*)}_{\text{Approximation Error}}\] <p>where:</p> <ul> <li>\(f_\mathcal{F}\) is the best function within the chosen hypothesis space \(\mathcal{F}\).</li> <li><strong>Estimation Error</strong>: This term captures the error due to estimating the best function \(f_\mathcal{F}\) using finite training data.</li> <li><strong>Approximation Error</strong>: This term reflects the penalty for restricting the search to the hypothesis space \(\mathcal{F}\) instead of considering all possible functions.</li> </ul> <p><strong>Key Insight: Tradeoff Between Errors</strong> There is always a <strong>tradeoff</strong> between approximation and estimation errors:</p> <ul> <li>A larger hypothesis space \(\mathcal{F}\) reduces approximation error but increases estimation error (due to greater model complexity).</li> <li>A smaller hypothesis space \(\mathcal{F}\) reduces estimation error but increases approximation error (due to limited flexibility).</li> </ul> <p>This tradeoff is crucial when designing models and choosing hypothesis spaces.</p> <hr/> <h3 id="erm-in-practice"><strong>ERM in Practice</strong></h3> <p>In real-world machine learning, finding the exact ERM solution is challenging. We often settle for an approximate solution:</p> <ul> <li>Let \(\tilde{f}_n\) be the function returned by an optimization algorithm.</li> <li>The <strong>optimization error</strong> is:</li> </ul> \[\text{Optimization Error} = R(\tilde{f}_n) - R(\hat{f}_n)\] <p>where:</p> <ul> <li>\(\tilde{f}_n\)is the function returned by the optimization method.</li> <li>\(\hat{f}_n\)is the empirical risk minimizer.</li> </ul> <h5 id="practical-decomposition"><strong>Practical Decomposition</strong></h5> <p>For \(\tilde{f}_n\), the excess risk can be further decomposed as:</p> \[\text{Excess Risk}(\tilde{f}_n) = \underbrace{R(\tilde{f}_n) - R(\hat{f}_n)}_{\text{Optimization Error}} + \underbrace{R(\hat{f}_n) - R(f_{\mathcal{F}})}_{\text{Estimation Error}} + \underbrace{R(f_{\mathcal{F}}) - R(f^*)}_{\text{Approximation Error}}.\] <hr/> <h3 id="summary-erm-overview"><strong>Summary: ERM Overview</strong></h3> <p>To apply ERM in practice:</p> <ol> <li><strong>Choose a loss function</strong> \(\ell(f(x), y)\).</li> <li><strong>Define a hypothesis space</strong> \(\mathcal{F}\) that balances approximation and estimation error.</li> <li><strong>Use an optimization method</strong> to find \(\hat{f}_n = \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)\) (or an approximate solution \(\tilde{f}_n\)).</li> </ol> <p>As the size of training data increases, we can use larger hypothesis spaces to reduce approximation error while keeping estimation error manageable.</p> <hr/> <h3 id="conclusion"><strong>Conclusion</strong></h3> <p>Empirical Risk Minimization (ERM) provides a foundational framework for supervised learning by optimizing a model’s performance on training data. However, achieving a balance between approximation, estimation, and optimization errors is key to building effective models. This naturally raises the question: <strong>How do we efficiently minimize empirical risk in practice, especially for complex models and large datasets?</strong></p> <p>In the next blog, we’ll dive into <strong>Gradient Descent</strong>, one of the most powerful and widely used optimization algorithms for minimizing risk, and explore how it enables us to tackle the challenges of ERM. Stay tuned and see you! 👋</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.]]></summary></entry><entry><title type="html">Understanding the Supervised Learning Setup</title><link href="https://monishver11.github.io/blog/2024/supervised-learning/" rel="alternate" type="text/html" title="Understanding the Supervised Learning Setup"/><published>2024-12-24T15:35:00+00:00</published><updated>2024-12-24T15:35:00+00:00</updated><id>https://monishver11.github.io/blog/2024/supervised-learning</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/supervised-learning/"><![CDATA[<p>Supervised learning is a cornerstone of machine learning, enabling systems to learn from labeled data to make predictions or decisions. In this post, we will explore the various components and formalizations of supervised learning to build a solid foundation.</p> <h4 id="goals-in-supervised-learning"><strong>Goals in Supervised Learning</strong></h4> <p>In supervised learning problems, we typically aim to:</p> <ul> <li><strong>Make a decision:</strong> For instance, determining whether to move an email to a spam folder.</li> <li><strong>Take an action:</strong> As in self-driving cars, deciding when to make a right turn.</li> <li><strong>Reject a hypothesis:</strong> Such as testing the hypothesis that \(\theta = 0\) in classical statistics.</li> <li><strong>Produce some output:</strong> Examples include identifying whose face is in an image, translating a Japanese sentence into Hindi, or predicting the location of a storm an hour into the future.</li> </ul> <p>Each of these goals involves predicting or generating some form of output based on given inputs.</p> <h5 id="labels-the-key-to-supervised-learning"><strong>Labels: The Key to Supervised Learning</strong></h5> <p>Supervised learning involves pairing inputs with <strong>labels</strong>, which serve as the ground truth. Examples of labels include:</p> <ul> <li>Whether or not a picture contains an animal.</li> <li>The storm’s location one hour after a query.</li> <li>Which, if any, of the suggested URLs were selected.</li> </ul> <p>These labels allow us to evaluate the performance of our predictions systematically.</p> <hr/> <h4 id="evaluation-criterion"><strong>Evaluation Criterion</strong></h4> <p>The next step in supervised learning is finding <strong>optimal outputs</strong> under various definitions of optimality. Some examples of evaluation criteria include:</p> <ul> <li><strong>Classification Accuracy:</strong> Is the predicted class correct?</li> <li><strong>Exact Match:</strong> Does the transcription exactly match the spoken words?</li> <li><strong>Partial Credit:</strong> How do we account for partially correct answers (e.g., getting some words right)?</li> <li><strong>Prediction Distance:</strong> How far is the storm’s actual location from the predicted one?</li> <li><strong>Density Prediction:</strong> How likely is the storm’s actual location under the predicted distribution?</li> </ul> <p>These criteria ensure that we can quantitatively measure the performance of our models.</p> <hr/> <h4 id="typical-sequence-of-events"><strong>Typical Sequence of Events</strong></h4> <p>Supervised learning problems can often be formalized through the following sequence:</p> <ol> <li><strong>Observe Input (\(x\)):</strong> Receive an input data point.</li> <li><strong>Predict Output (\(\hat{y}\)):</strong> Use a prediction function to generate an output.</li> <li><strong>Observe Label (\(y\)):</strong> Compare the predicted output with the true label.</li> <li><strong>Evaluate Output:</strong> Assess the prediction’s quality based on the label.</li> </ol> <p>This sequence is at the heart of most supervised learning frameworks.</p> <hr/> <h4 id="formalizing-supervised-learning"><strong>Formalizing Supervised Learning</strong></h4> <p>A <strong>prediction function</strong> is a mathematical function \(f: X \to Y\) that takes an input \(x \in X\) and produces an output \(\hat{y} \in Y\).</p> <p>A <strong>loss function</strong> evaluates the discrepancy between the predicted output \(\hat{y}\) and the true outcome \(y\). It quantifies the “cost” of making incorrect predictions.</p> <h6 id="the-goal-optimal-prediction"><strong>The Goal: Optimal Prediction</strong></h6> <p>The primary goal is to find the <strong>optimal prediction function</strong>. The intuition is simple: If we can evaluate how good a prediction function is, we can turn this into an optimization problem.</p> <ul> <li>The loss function \(\ell\) evaluates a single output.</li> <li>To evaluate the prediction function as a whole, we need to formalize the concept of “average performance.”</li> </ul> <h6 id="data-generating-distribution"><strong>Data Generating Distribution</strong></h6> <p>Assume there exists a data-generating distribution \(P_{X \times Y}\). All input-output pairs \((x, y)\) are generated independently and identically distributed (i.i.d.) from this distribution.</p> <p>A common objective is to have a prediction function \(f(x)\) that performs well <strong>on average</strong>:</p> \[\ell(f(x), y)\] <p>is small, in some sense.</p> <h6 id="risk-definition"><strong>Risk Definition</strong></h6> <p>The <strong>risk</strong> of a prediction function \(f: X \to Y\) is defined as:</p> \[R(f) = \mathbb{E}_{(x, y) \sim P_{X \times Y}} [\ell(f(x), y)].\] <p>In words, this is the expected loss of \(f\) over the data-generating distribution \(P_{X \times Y}\). However, since we do not know \(P_{X \times Y}\), we cannot compute this expectation directly. Instead, we estimate it.</p> <hr/> <h4 id="the-bayes-prediction-function"><strong>The Bayes Prediction Function</strong></h4> <p><strong>Definition</strong></p> <p>The <strong>Bayes prediction function</strong> \(f^*: X \to Y\) achieves the minimal risk among all possible functions:</p> \[f^* \in \underset{f}{\text{argmin}} \ R(f),\] <p>where the minimum is taken over all functions from \(X\) to \(Y\).</p> <p><strong>Bayes Risk</strong></p> <p>The risk associated with the Bayes prediction function is called the <strong>Bayes risk</strong>. This function is often referred to as the “target function” because it represents the best possible predictor.</p> <hr/> <h4 id="example-multiclass-classification"><strong>Example: Multiclass Classification</strong></h4> <p>In multiclass classification, the output space is:</p> \[Y = \{1, 2, \dots, k\}.\] <p>The <strong>0-1 loss</strong> function is defined as:</p> \[\ell(\hat{y}, y) = \mathbb{1}[\hat{y} \neq y] := \begin{cases} 1 &amp; \text{if } \hat{y} \neq y, \\ 0 &amp; \text{otherwise.} \end{cases}\] <ul> <li>Here, \(\mathbb{1}[\hat{y} \neq y]\) is an <strong>indicator function</strong>. It returns a value of 1 when the condition \(\hat{y} \neq y\) is true (i.e., the prediction is incorrect) and 0 otherwise. This signifies whether the prediction is correct or incorrect and is commonly used to measure classification errors.</li> </ul> <p>The risk \(R(f)\) under the 0-1 loss can be expanded as follows:</p> \[R(f) = \mathbb{E}[\mathbb{1}[f(x) \neq y]] = \mathbb{P}(f(x) \neq y),\] <p>where:</p> <ul> <li>\(\mathbb{E}[\mathbb{1}[f(x) \neq y]]\) represents the expected value of the indicator function, which counts the proportion of incorrect predictions.</li> <li>\(\mathbb{P}(f(x) \neq y)\) is the probability of the prediction \(f(x)\) being different from the true label \(y\).</li> </ul> <p>Further, this can be rewritten using the decomposition of probabilities:</p> \[R(f) = 0 \cdot \mathbb{P}(f(x) = y) + 1 \cdot \mathbb{P}(f(x) \neq y),\] <p>which simplifies back to:</p> \[R(f) = \mathbb{P}(f(x) \neq y).\] <p><strong>Explanation</strong></p> <ul> <li>The term \(0 \cdot \mathbb{P}(f(x) = y)\) accounts for the cases where the prediction is correct (loss is 0).</li> <li>The term \(1 \cdot \mathbb{P}(f(x) \neq y)\) accounts for the cases where the prediction is incorrect (loss is 1).</li> </ul> <p>Thus, \(R(f)\) directly measures the <strong>misclassification error rate</strong>, which is the probability of the model making an incorrect prediction.</p> <p><strong>Bayes Prediction Function</strong></p> <p>The Bayes prediction function returns the most likely class:</p> \[f^*(x) \in \underset{1 \leq c \leq k}{\text{argmax}} \ P(y = c \mid x).\] <hr/> <h4 id="estimating-risk"><strong>Estimating Risk</strong></h4> <p>We cannot compute the true risk \(R(f) = \mathbb{E}[\ell(f(x), y)]\) because the true distribution \(P_{X \times Y}\) is unknown. However, we can estimate it.</p> <p>Assume we have sample data:</p> \[D_n = \{(x_1, y_1), \dots, (x_n, y_n)\},\] <p>where the samples are i.i.d. from \(P_{X \times Y}\). By the strong law of large numbers, the empirical average of losses converges to the expected value. If \(z_1, . . . , z_n\) are i.i.d. with expected value \(\mathbb{E}[z]\), then</p> \[\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n z_i = \mathbb{E}[z],\] <p>with probability 1.</p> <p>This leads us to the concept of <strong>empirical risk</strong> and its minimization, which we will explore in the next post.</p> <hr/> <p>In the next blog, we will dive into <strong>empirical risk minimization</strong> and how it helps solve supervised learning problems effectively.</p> <p><strong>Stay tuned!</strong></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.]]></summary></entry><entry><title type="html">Timeline of Machine Learning History</title><link href="https://monishver11.github.io/blog/2024/ml-history/" rel="alternate" type="text/html" title="Timeline of Machine Learning History"/><published>2024-12-23T21:59:00+00:00</published><updated>2024-12-23T21:59:00+00:00</updated><id>https://monishver11.github.io/blog/2024/ml-history</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/ml-history/"><![CDATA[<p>Machine learning has come a long way, evolving through decades of research and innovation. This timeline highlights the pivotal moments that have defined the field.</p> <h4 id="1910s1940s-early-computational-foundations"><strong>1910s–1940s: Early Computational Foundations</strong></h4> <ul> <li> <p><strong>1913:</strong> <code class="language-plaintext highlighter-rouge">Markov Chains</code> <br/> Andrey Markov introduces techniques later known as Markov chains, fundamental to many machine learning algorithms.</p> </li> <li> <p><strong>1936:</strong> <code class="language-plaintext highlighter-rouge">Turing's Theory of Computation</code><br/> Alan Turing proposes the theory of computation, forming the foundation for modern computing and machine learning.</p> </li> <li> <p><strong>1940:</strong> <code class="language-plaintext highlighter-rouge">ENIAC</code><br/> The first electronic general-purpose computer is created, paving the way for future computational advancements.</p> </li> <li> <p><strong>1943:</strong> <code class="language-plaintext highlighter-rouge">McCulloch-Pitts Model</code><br/> Walter Pitts and Warren McCulloch publish the first mathematical model of a neural network, laying the foundation for neural networks.</p> </li> <li> <p><strong>1949:</strong> <code class="language-plaintext highlighter-rouge">Hebbian Learning</code><br/> Donald Hebb publishes “The Organization of Behavior,” introducing concepts crucial to neural network development.</p> </li> </ul> <hr/> <h4 id="1950s1960s-foundations-of-artificial-intelligence"><strong>1950s–1960s: Foundations of Artificial Intelligence</strong></h4> <ul> <li> <p><strong>1950:</strong> <code class="language-plaintext highlighter-rouge">Turing Test</code><br/> Alan Turing proposes the Turing Test, a benchmark for machine intelligence.</p> </li> <li> <p><strong>1951:</strong> <code class="language-plaintext highlighter-rouge">SNARC</code><br/> Marvin Minsky and Dean Edmonds build SNARC, the first artificial neural network machine.</p> </li> <li> <p><strong>1952:</strong> <code class="language-plaintext highlighter-rouge">First Learning Program</code><br/> Arthur Samuel writes the first computer program capable of learning, a checkers-playing program.</p> </li> <li> <p><strong>1956:</strong> <code class="language-plaintext highlighter-rouge">Dartmouth Conference</code><br/> The term “Artificial Intelligence” is coined, marking the birth of AI as a field.</p> </li> <li> <p><strong>1957:</strong> <code class="language-plaintext highlighter-rouge">Perceptron</code><br/> Frank Rosenblatt invents the perceptron, an early type of neural network capable of binary classification.</p> </li> <li> <p><strong>1963:</strong> <code class="language-plaintext highlighter-rouge">Machine Learning in Games</code><br/> Donald Michie creates a machine that uses reinforcement learning to play Tic-tac-toe.</p> </li> <li> <p><strong>1967:</strong> <code class="language-plaintext highlighter-rouge">Nearest Neighbor Algorithm</code><br/> The Nearest Neighbor algorithm is developed, marking the birth of pattern recognition in computers.</p> </li> <li> <p><strong>1969:</strong> <code class="language-plaintext highlighter-rouge">Limitations of Neural Networks</code><br/> Marvin Minsky and Seymour Papert publish “Perceptrons,” highlighting limitations of early neural networks.</p> </li> </ul> <hr/> <h4 id="1970s1980s-growth-and-challenges"><strong>1970s–1980s: Growth and Challenges</strong></h4> <ul> <li> <p><strong>1970s:</strong> <code class="language-plaintext highlighter-rouge">First AI Winter</code><br/> Funding and interest in AI declined due to unmet expectations and computational limitations.</p> </li> <li> <p><strong>1979:</strong> <code class="language-plaintext highlighter-rouge">Stanford Cart</code><br/> Stanford University invents the “Stanford Cart,” an early autonomous mobile robot.</p> </li> <li> <p><strong>1981:</strong> <code class="language-plaintext highlighter-rouge">Explanation-Based Learning</code><br/> Gerald Dejong introduces the concept of explanation-based learning.</p> </li> <li> <p><strong>1985:</strong> <code class="language-plaintext highlighter-rouge">NetTalk</code><br/> Terry Sejnowski invents NetTalk, demonstrating machine learning of pronunciation.</p> </li> <li> <p><strong>1988:</strong> <code class="language-plaintext highlighter-rouge">Universal Approximation Theorem</code><br/> Kurt Hornik proves the universal approximation theorem for neural networks.</p> </li> <li> <p><strong>1989:</strong> <code class="language-plaintext highlighter-rouge">CNN for Handwriting Recognition</code><br/> Yann LeCun, Yoshua Bengio, and Patrick Haffner demonstrate CNNs for handwriting recognition.</p> </li> <li> <p><strong>1989:</strong> <code class="language-plaintext highlighter-rouge">Q-learning</code><br/> Christopher Watkins develops Q-learning, advancing reinforcement learning.</p> </li> </ul> <hr/> <h4 id="1990s-statistical-learning-and-commercial-ai"><strong>1990s: Statistical Learning and Commercial AI</strong></h4> <ul> <li> <p><strong>1992:</strong> <code class="language-plaintext highlighter-rouge">TD-Gammon</code><br/> Gerald Tesauro invents TD-Gammon, a backgammon program using neural networks.</p> </li> <li> <p><strong>1997:</strong> <code class="language-plaintext highlighter-rouge">Deep Blue Defeats Chess Champion</code><br/> IBM’s Deep Blue defeats world chess champion Garry Kasparov, demonstrating AI in games.</p> </li> <li> <p><strong>1997:</strong> <code class="language-plaintext highlighter-rouge">LSTMs Introduced</code><br/> Sepp Hochreiter and Jürgen Schmidhuber invent Long Short-Term Memory (LSTM) networks.</p> </li> <li> <p><strong>1998:</strong> <code class="language-plaintext highlighter-rouge">MNIST Database Released</code> Yann LeCun releases the MNIST database, a benchmark for handwriting recognition.</p> </li> <li> <p><strong>1998:</strong> <code class="language-plaintext highlighter-rouge">Furby Released</code><br/> Tiger Electronics releases Furby, introducing simple AI to the mass market.</p> </li> <li> <p><strong>1999:</strong> <code class="language-plaintext highlighter-rouge">AIBO Robot Dog</code><br/> Sony launches AIBO, showcasing AI in consumer robotics.</p> </li> </ul> <hr/> <h4 id="2000s-big-data-and-ml-techniques"><strong>2000s: Big Data and ML Techniques</strong></h4> <ul> <li> <p><strong>2000:</strong> <code class="language-plaintext highlighter-rouge">Nomad Robot</code><br/> The Nomad robot explores Antarctica, becoming the first robot to discover a meteorite.</p> </li> <li> <p><strong>2002:</strong> <code class="language-plaintext highlighter-rouge">Torch Library Released</code><br/> The Torch machine learning library is first released, enabling research in ML.</p> </li> <li> <p><strong>2009:</strong> <code class="language-plaintext highlighter-rouge">Netflix Prize</code><br/> Netflix awards $1 million for improving its recommendation system.</p> </li> </ul> <hr/> <h4 id="2010s-the-deep-learning-revolution"><strong>2010s: The Deep Learning Revolution</strong></h4> <ul> <li> <p><strong>2010:</strong> <code class="language-plaintext highlighter-rouge">Kaggle Launch</code><br/> Kaggle, a platform for machine learning competitions, is launched.</p> </li> <li> <p><strong>2010:</strong> <code class="language-plaintext highlighter-rouge">Kinect for Xbox</code><br/> Microsoft releases Kinect, showcasing advanced computer vision capabilities.</p> </li> <li> <p><strong>2011:</strong> <code class="language-plaintext highlighter-rouge">IBM Watson Wins Jeopardy!</code><br/> IBM Watson defeats human champions, showcasing NLP and ML capabilities.</p> </li> <li> <p><strong>2012:</strong> <code class="language-plaintext highlighter-rouge">AlexNet Wins ImageNet</code><br/> Deep CNNs significantly outperformed traditional approaches, heralding the deep learning era.</p> </li> <li> <p><strong>2013:</strong> <code class="language-plaintext highlighter-rouge">Deep Reinforcement Learning</code><br/> DeepMind introduces deep reinforcement learning, advancing RL applications.</p> </li> <li> <p><strong>2013:</strong> <code class="language-plaintext highlighter-rouge">Word2Vec</code><br/> Google introduces Word2Vec, a tool for vectorizing natural language.</p> </li> <li> <p><strong>2018:</strong> <code class="language-plaintext highlighter-rouge">Alibaba's AI</code><br/> Alibaba’s AI outscores humans on Stanford University’s reading comprehension test.</p> </li> </ul> <hr/> <h4 id="2020s-large-scale-ai-and-generative-models"><strong>2020s: Large-Scale AI and Generative Models</strong></h4> <ul> <li> <p><strong>2020:</strong> <code class="language-plaintext highlighter-rouge">GPT-3 Released</code><br/> OpenAI’s large-scale language model demonstrated the power of generative pre-trained transformers.</p> </li> <li> <p><strong>2020:</strong> <code class="language-plaintext highlighter-rouge">Turing NLG</code> Microsoft introduces Turing Natural Language Generation.</p> </li> <li> <p><strong>2022:</strong> <code class="language-plaintext highlighter-rouge">AlphaFold Breakthrough</code><br/> DeepMind solved the protein folding problem, revolutionizing biology with ML.</p> </li> <li> <p><strong>2023:</strong> <code class="language-plaintext highlighter-rouge">Generative AI Adoption</code><br/> Widespread use of diffusion models and ChatGPT showcased the practical impact of generative AI.</p> </li> </ul> <hr/> <h4 id="2024s-cutting-edge-ai-innovations"><strong>2024s: Cutting-Edge AI Innovations</strong></h4> <ul> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">OpenAI's O1 Model</code><br/> Advanced reasoning capabilities in mathematics and coding, enhancing AI’s problem-solving skills.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Google DeepMind's GenCast</code><br/> Improved weather predictions to optimize agriculture and disaster preparedness.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Microsoft's Copilot Vision</code><br/> AI integration with digital environments to boost productivity.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">AI Video Creation Tools</code><br/> Transformation of content creation with tools like Google’s Veo and OpenAI’s Sora.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Anthropic's Claude Chatbot</code><br/> Enhanced AI safety and reliability for critical applications like disaster response.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Multimodal AI Advancements</code><br/> Integration of text, audio, and visual inputs in AI models like ChatGPT-4.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Small Language Models (SLMs) Rise</code><br/> Increased popularity of efficient AI models that require fewer computing resources.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Customizable Generative AI</code><br/> Development of tailored AI systems for niche markets and specific user needs.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Geo-Llama</code><br/> Advanced AI technique for generating realistic simulated data on human movement in urban settings.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">GPT-4 Enhancements</code><br/> Improved emotional recognition capabilities from a third-person perspective.</p> </li> </ul> <p>The list of discoveries/events mentioned is extensive i guess, and apologies if I’ve missed any significant developments. The field of AI is advancing at a rapid pace, and we are eagerly awaiting the first steps toward AGI. As my focus remains on machine learning, I aim to contribute to this vibrant community, and I hope you’re as excited about the future of AI as I am. That’s likely why you’re reading this now. I wish you all the best and invite you to dive deeper into the realm of supervised learning in my next blog.</p> <p><strong>Stay tuned, and I’ll see you there!</strong></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[A concise timeline of machine learning's history, showcasing key milestones and breakthroughs that shaped the field.]]></summary></entry><entry><title type="html">Advanced Probability Concepts for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/probability-2/" rel="alternate" type="text/html" title="Advanced Probability Concepts for Machine Learning"/><published>2024-12-22T19:21:00+00:00</published><updated>2024-12-22T19:21:00+00:00</updated><id>https://monishver11.github.io/blog/2024/probability-2</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/probability-2/"><![CDATA[<h3 id="bayes-rule-and-associated-properties-a-key-concept"><strong>Bayes’ Rule and Associated Properties: A Key Concept</strong></h3> <p>Bayes’ Rule is a foundational concept in probability theory that plays a critical role in machine learning, especially in tasks involving classification, decision-making, and model inference. It provides a mathematical framework to update our beliefs about a hypothesis based on new evidence.</p> <h4 id="what-is-bayes-rule"><strong>What is Bayes’ Rule?</strong></h4> <p>Bayes’ Rule describes the relationship between conditional probabilities. It allows us to reverse conditional probabilities, which can be very useful in machine learning when we need to compute the probability of a certain hypothesis given observed data.</p> <p>Mathematically, Bayes’ Rule is expressed as:</p> \[P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}\] <p>Where:</p> <ul> <li>\(P(A \mid B)\) is the <strong>posterior probability</strong>: the probability of the hypothesis \(A\) being true given the evidence \(B\).</li> <li>\(P(B \mid A)\) is the <strong>likelihood</strong>: the probability of observing the evidence \(B\) given that the hypothesis \(A\) is true.</li> <li>\(P(A)\) is the <strong>prior probability</strong>: the initial probability of the hypothesis \(A\) before seeing any evidence.</li> <li>\(P(B)\) is the <strong>evidence</strong> or <strong>normalizing constant</strong>: the total probability of observing the evidence across all possible hypotheses.</li> </ul> <p><strong>To follow along, consider this analogy:</strong></p> <p>Imagine you’re trying to diagnose whether someone has a certain disease. You have:</p> <ul> <li>The prior probability of having the disease (\(P(A)\)), which could be based on general statistics about the disease.</li> <li>The likelihood (\(P(B \mid A)\)), which is the probability that a person with the disease would test positive on a medical test.</li> <li>The evidence (\(P(B)\)), which is the total probability that anyone, sick or healthy, would test positive.</li> </ul> <p>Bayes’ Rule helps us combine this information to update our belief about the probability of the disease (hypothesis \(A\)) given the test result (evidence \(B\)).</p> <h4 id="why-is-bayes-rule-crucial-in-machine-learning"><strong>Why is Bayes’ Rule Crucial in Machine Learning?</strong></h4> <p>Bayes’ Rule is central to a variety of machine learning models, particularly in probabilistic and Bayesian approaches. Some key applications include:</p> <ol> <li> <p><strong>Naive Bayes’ Classifier</strong>: In supervised learning, the Naive Bayes’ classifier uses Bayes’ Rule to classify data based on conditional probabilities. It assumes independence between features, simplifying the computation of probabilities.</p> </li> <li> <p><strong>Model Inference and Parameter Estimation</strong>: Bayesian methods in machine learning, like Bayesian neural networks, use Bayes’ Rule to update the distribution of model parameters as new data is observed, instead of relying on point estimates.</p> </li> <li> <p><strong>Decision Theory</strong>: Bayes’ Rule helps in decision-making processes by quantifying the uncertainty associated with different outcomes, especially when there is a probabilistic component to the environment or model.</p> </li> </ol> <h4 id="associated-properties-of-bayes-rule"><strong>Associated Properties of Bayes’ Rule</strong></h4> <ol> <li> <p><strong>Bayes’ Theorem for Multiple Events</strong>: Bayes’ Rule can be extended to more complex situations, such as when dealing with multiple hypotheses or events. This is useful when making predictions over many possible outcomes or when dealing with complex models in machine learning.</p> </li> <li> <p><strong>Conjugacy</strong>: In some models, certain prior distributions are chosen because they lead to mathematical simplicity when combined with Bayes’ Rule. These priors are called <strong>conjugate priors</strong>. For example, in Gaussian processes, using a conjugate prior for the likelihood of Gaussian data results in a simpler update process.</p> </li> <li> <p><strong>The Law of Total Probability</strong>: Bayes’ Rule is closely related to the Law of Total Probability, which decomposes the total probability of an event into a sum of conditional probabilities. This can be useful when considering multiple sources of evidence or when performing integration in complex models.</p> </li> </ol> <p>Bayes’ Rule is an indispensable tool in machine learning for reasoning about uncertainty, updating beliefs with new evidence, and making decisions in the face of incomplete information.</p> <hr/> <h3 id="joint-probability-and-independence"><strong>Joint Probability and Independence:</strong></h3> <p>In machine learning, understanding how different events relate to each other is critical. Two key concepts that help us analyze these relationships are <strong>joint probability</strong> and <strong>independence</strong>.</p> <h4 id="what-is-joint-probability"><strong>What is Joint Probability?</strong></h4> <p>Joint probability refers to the probability of two or more events occurring simultaneously. In other words, it is the likelihood that multiple events happen at the same time, and is often represented as \(P(A \cap B)\) for two events \(A\) and \(B\).</p> <p>Mathematically, the joint probability of events \(A\) and \(B\) is defined as:</p> \[P(A \cap B) = P(A \mid B) \cdot P(B) = P(B \mid A) \cdot P(A)\] <p>This equation shows that the joint probability of two events \(A\) and \(B\) can be computed by multiplying the conditional probability of one event given the other by the probability of the second event. The reverse relationship also holds true.</p> <p><strong>Now, consider this analogy:</strong></p> <p>Imagine you’re rolling two dice and are interested in the probability that the first die shows a 4 and the second die shows a 6. The joint probability, \(P(\text{Die 1 = 4 and Die 2 = 6})\), is the probability of both events happening at once. Since the dice rolls are independent, we can compute this as the product of the individual probabilities:</p> \[P(\text{Die 1 = 4}) \cdot P(\text{Die 2 = 6})\] <p>This product gives the likelihood that both dice will show these values simultaneously.</p> <h4 id="why-is-joint-probability-important-in-machine-learning"><strong>Why is Joint Probability Important in Machine Learning?</strong></h4> <p>Joint probability is used to model relationships between different features or variables in a dataset. Some applications include:</p> <ol> <li> <p><strong>Multivariate Probability Models</strong>: In many machine learning problems, we are dealing with multiple features simultaneously. Joint probability helps model the dependencies between these features, which is essential for tasks like classification or clustering. For example, in a classification task, joint probabilities allow us to compute the likelihood of a particular outcome given multiple features.</p> </li> <li> <p><strong>Markov Chains</strong>: In sequence-based tasks like time series forecasting, the joint probability of a sequence of events (e.g., states in a Markov Chain) is crucial in determining the probability distribution over future states based on previous ones.</p> </li> </ol> <h4 id="what-is-independence"><strong>What is Independence?</strong></h4> <p>Two events \(A\) and \(B\) are said to be <strong>independent</strong> if the occurrence of one event does not affect the probability of the other event occurring. Mathematically, two events are independent if:</p> \[P(A \cap B) = P(A) \cdot P(B)\] <p>This property is a key assumption in many machine learning algorithms, especially those based on probabilistic reasoning.</p> <p><strong>For Intuition, think it this way:</strong></p> <p>Think of tossing a coin and rolling a die. The outcome of the coin toss does not affect the outcome of the die roll. These two events are independent, meaning the joint probability can be computed as the product of their individual probabilities. So, the probability of getting heads on the coin toss and a 6 on the die roll is:</p> \[P(\text{Heads}) \cdot P(\text{Die = 6})\] <h4 id="why-is-independence-important-in-machine-learning"><strong>Why is Independence Important in Machine Learning?</strong></h4> <p>Independence is a simplifying assumption in many machine learning models and can significantly reduce the complexity of computations:</p> <ol> <li> <p><strong>Naive Bayes Classifier</strong>: The Naive Bayes classifier makes a strong independence assumption—that the features are conditionally independent given the class. This simplifies the computation of joint probabilities for multiple features and makes the model efficient even with high-dimensional data.</p> </li> <li> <p><strong>Factorization</strong>: In probabilistic models, assuming independence allows for factorizing the joint probability distribution into simpler, more manageable parts. This can be particularly useful in situations like generative models or in deep learning when modeling complex dependencies in large datasets.</p> </li> <li> <p><strong>Feature Independence</strong>: In feature engineering, assuming that features are independent can help simplify model design and speed up training. It’s often used as a heuristic, particularly when exploring models like Gaussian Mixture Models (GMM) or Hidden Markov Models (HMM).</p> </li> </ol> <h4 id="conditional-independence"><strong>Conditional Independence</strong></h4> <p>While not the same as plain independence, <strong>conditional independence</strong> is another important concept. Two events \(A\) and \(B\) are conditionally independent given a third event \(C\) if:</p> \[P(A \cap B \mid C) = P(A \mid C) \cdot P(B \mid C)\] <p>This property is widely used in Bayesian networks and machine learning models to break down complex dependencies into simpler conditional ones.</p> <hr/> <h3 id="conditional-probability-and-conditional-distributions-building-blocks-for-predictive-models"><strong>Conditional Probability and Conditional Distributions: Building Blocks for Predictive Models</strong></h3> <p>Conditional probability and conditional distributions concepts help us refine predictions based on additional information and allow us to build more accurate, data-driven models by considering how the likelihood of one event changes when we know about the occurrence of another.</p> <h4 id="what-is-conditional-probability"><strong>What is Conditional Probability?</strong></h4> <p>Conditional probability is the probability of an event occurring given that another event has already occurred. In other words, it quantifies the likelihood of an event, assuming that certain information is known. It is expressed as:</p> \[P(A \mid B) = \frac{P(A \cap B)}{P(B)}\] <p>Where:</p> <ul> <li>\(P(A \mid B)\) is the <strong>conditional probability</strong> of event \(A\) given event \(B\).</li> <li>\(P(A \cap B)\) is the <strong>joint probability</strong> of both \(A\) and \(B\) occurring.</li> <li>\(P(B)\) is the <strong>probability</strong> of event \(B\).</li> </ul> <p>This formula helps us understand how the occurrence of one event (\(B\)) affects the likelihood of another event (\(A\)).</p> <p><strong>Intuition and Analogy for Conditional Probability</strong></p> <p>Imagine you’re at a concert and you’re interested in the probability that a person will be wearing a red T-shirt, given that they are in the front row. Without any information, the probability of someone wearing a red T-shirt might be 30%. But if you know that the person is in the front row (which could imply a certain type of concertgoer), this could affect the probability—perhaps fans who are in the front row are more likely to wear red.</p> <p>This situation is an example of <strong>conditional probability</strong>, where the event \(A\) (person wearing a red T-shirt) is conditioned on the event \(B\) (person being in the front row). Conditional probability helps refine your predictions based on new information.</p> <h4 id="why-is-conditional-probability-important-in-machine-learning"><strong>Why is Conditional Probability Important in Machine Learning?</strong></h4> <p>Conditional probability is essential in many machine learning models for predicting outcomes based on known data. Key applications include:</p> <ol> <li> <p><strong>Classification and Regression</strong>: In supervised learning, we use conditional probability to predict the class label (in classification) or continuous values (in regression) based on observed features. For instance, in logistic regression, we compute the conditional probability of a binary outcome given certain feature values.</p> </li> <li> <p><strong>Naive Bayes Classifier</strong>: The Naive Bayes algorithm, which assumes conditional independence of features, uses conditional probabilities to predict class labels. It calculates the probability of the class label given the observed features using Bayes’ Theorem.</p> </li> <li> <p><strong>Bayesian Inference</strong>: In Bayesian methods, we continuously update the probability of a hypothesis based on new data. This is done using conditional probabilities and allows for probabilistic reasoning in models such as Bayesian networks.</p> </li> </ol> <h4 id="what-are-conditional-distributions"><strong>What are Conditional Distributions?</strong></h4> <p>A <strong>conditional distribution</strong> is a probability distribution of a subset of variables given the values of other variables. It generalizes conditional probability to the case of multiple random variables and helps us understand how the distribution of one variable changes when the values of others are known.</p> <p>For example, if you have two variables \(X\) and \(Y\), the conditional distribution of \(X\) given \(Y = y\) is the probability distribution of \(X\) when you know that \(Y\) takes the specific value \(y\).</p> <p>Mathematically, a conditional distribution is denoted as:</p> \[P(X \mid Y = y)\] <p>Where:</p> <ul> <li>\(P(X \mid Y = y)\) is the conditional distribution of \(X\) given that \(Y = y\).</li> <li>This describes how the distribution of \(X\) changes when \(Y\) is fixed at a particular value.</li> </ul> <p><strong>Intuition and Analogy for Conditional Distributions</strong></p> <p>Consider a scenario where you’re trying to predict a person’s income (\(X\)) based on their level of education (\(Y\)). If you know that someone has a college degree, the distribution of their income (the possible range of incomes they could have) will be different than if you only know their high school education level.</p> <p>In this case, \(P(X \mid Y)\) would describe the distribution of income (\(X\)) conditional on a specific level of education (\(Y\)). The distribution will shift depending on the value of \(Y\), helping refine your predictions of income based on known education levels.</p> <h4 id="why-are-conditional-distributions-important-in-machine-learning"><strong>Why are Conditional Distributions Important in Machine Learning?</strong></h4> <p>Conditional distributions are vital in machine learning for understanding relationships between features and predicting outcomes. Some key uses include:</p> <ol> <li> <p><strong>Generative Models</strong>: In models like Gaussian Mixture Models (GMM) or Hidden Markov Models (HMM), conditional distributions are used to model how data points (such as observations or states) are generated given certain parameters.</p> </li> <li> <p><strong>Bayesian Networks</strong>: In Bayesian networks, the conditional distributions represent the probabilistic dependencies between variables. Each node (representing a random variable) has a conditional distribution based on its parent nodes, and the overall network structure allows us to compute the joint distribution of all variables.</p> </li> <li> <p><strong>Expectation-Maximization (EM) Algorithm</strong>: The EM algorithm, used for unsupervised learning and model fitting, relies on conditional distributions to estimate parameters in models with missing or incomplete data. The E-step computes the conditional distributions of hidden variables given the observed data.</p> </li> </ol> <p>While conditional probability allows us to adjust our expectations based on new information, conditional distributions give us a broader view of how data is distributed when specific conditions are known.</p> <hr/> <h3 id="law-of-total-probability-a-fundamental-tool-for-dealing-with-uncertainty"><strong>Law of Total Probability: A Fundamental Tool for Dealing with Uncertainty</strong></h3> <p>The Law of Total Probability is a key principle that allows us to compute the probability of an event by considering all possible ways that event could occur, based on different conditions or scenarios. This law is often used in machine learning when dealing with complex models where outcomes depend on multiple factors, or when some information is missing or unknown.</p> <h4 id="what-is-the-law-of-total-probability"><strong>What is the Law of Total Probability?</strong></h4> <p>It helps us calculate the probability of an event by partitioning the sample space into different mutually exclusive events and then summing up the probabilities of the event occurring in each of these partitions.</p> <p>Mathematically, the law is expressed as:</p> \[P(A) = \sum_{i} P(A \mid B_i) P(B_i)\] <p>Where:</p> <ul> <li>\(P(A)\) is the total probability of event \(A\).</li> <li>\(B_1, B_2, \dots, B_n\) are a partition of the sample space, meaning these events are mutually exclusive and exhaustive (they cover all possible outcomes).</li> <li>\(P(A \mid B_i)\) is the <strong>conditional probability</strong> of \(A\) given \(B_i\), i.e., the probability of \(A\) occurring under the condition that \(B_i\) occurs.</li> <li>\(P(B_i)\) is the probability of event \(B_i\).</li> </ul> <p>The law essentially breaks down the probability of \(A\) into cases based on different conditions \(B_1, B_2, \dots, B_n\), and then combines them weighted by the likelihood of each condition.</p> <p><strong>So, how to internalize this idea:</strong></p> <p>Imagine you are trying to determine the probability that a customer will purchase a product \(A\), but you have different types of customers \(B_1, B_2, \dots, B_n\) (e.g., based on their age group, spending history, etc.). The probability that a customer purchases the product will vary depending on their group. The Law of Total Probability tells you to:</p> <ol> <li>Calculate the probability of purchasing given each customer type (e.g., \(P(A \mid B_1)\), \(P(A \mid B_2)\), etc.).</li> <li>Multiply these by the probability of each customer type occurring (e.g., \(P(B_1)\), \(P(B_2)\)).</li> <li>Sum these products to find the total probability of purchasing across all customer types.</li> </ol> <p>In this way, the law allows you to compute the overall probability by considering all relevant scenarios (customer types) and weighing them accordingly.</p> <h4 id="why-is-the-law-of-total-probability-important-in-machine-learning"><strong>Why is the Law of Total Probability Important in Machine Learning?</strong></h4> <p>In machine learning, the Law of Total Probability is widely used for various tasks, especially in probabilistic modeling, classification, and predictive analytics. Some key applications include:</p> <ol> <li> <p><strong>Bayesian Inference</strong>: When updating beliefs about a hypothesis (or class) based on new data, the total probability is calculated over all possible hypotheses or classes. This helps refine predictions and is foundational in models such as <strong>Naive Bayes</strong>.</p> </li> <li> <p><strong>Handling Missing Data</strong>: In models dealing with missing data, the law helps to marginalize over the unknown values by considering all possible ways the data could be missing. For example, in the <strong>Expectation-Maximization (EM)</strong> algorithm, the law is used to estimate the missing values based on the observed data.</p> </li> <li> <p><strong>Class Conditional Probability in Classification</strong>: In classification problems, especially when working with multiple classes, the law allows the decomposition of class probabilities into conditional probabilities based on different features, facilitating the calculation of total class probabilities.</p> </li> </ol> <p><strong>Example: Applying the Law of Total Probability</strong></p> <p>Let’s consider an example in a classification task. Suppose you are trying to predict whether a customer will buy a product \(A\) (event \(A\)), and you have two features that classify the customer: whether they are a <strong>new customer</strong> (\(B_1\)) or a <strong>returning customer</strong> (\(B_2\)).</p> <p>The Law of Total Probability helps you compute the total probability of purchasing the product, considering both new and returning customers:</p> \[P(\text{Buy}) = P(\text{Buy} \mid \text{New Customer}) \cdot P(\text{New Customer}) + P(\text{Buy} \mid \text{Returning Customer}) \cdot P(\text{Returning Customer})\] <p>Here:</p> <ul> <li>\(P(\text{Buy} \mid \text{New Customer})\) is the probability that a new customer buys the product.</li> <li>\(P(\text{New Customer})\) is the probability that the customer is new.</li> <li>Similarly, \(P(\text{Buy} \mid \text{Returning Customer})\) and \(P(\text{Returning Customer})\) are for the returning customers.</li> </ul> <p>This allows you to compute the total probability of a customer buying the product, considering both customer types.</p> <h4 id="connection-with-conditional-probability"><strong>Connection with Conditional Probability</strong></h4> <p>The Law of Total Probability is built on conditional probability. It helps us to marginalize over unknown or unobserved conditions, ensuring we account for all possible scenarios that could influence the event of interest.</p> <p>For example, in a machine learning model that makes predictions based on different feature values, the law allows us to break down the total probability of an outcome by conditioning on the feature values and summing over all possible feature combinations.</p> <p>Whether you are building a Bayesian model, dealing with missing data, or predicting outcomes in complex scenarios, the Law of Total Probability provides a systematic way to combine multiple probabilities and refine your model’s predictions.</p> <hr/> <h3 id="expectation-and-variance-essential-measures"><strong>Expectation and Variance: Essential Measures</strong></h3> <p>They provide valuable insights into the behavior of data and are widely used in machine learning to understand the characteristics of models, assess uncertainty, and make predictions. Here’s a breakdown of each concept and its relevance to machine learning.</p> <h4 id="what-is-expectation"><strong>What is Expectation?</strong></h4> <p>The <strong>expectation</strong> (or <strong>mean</strong>) of a random variable represents its <strong>average</strong> or <strong>central tendency</strong>. It is the weighted average of all possible values that the variable can take, where the weights are given by the probabilities of these values.</p> <p>For a discrete random variable \(X\), the expectation \(E(X)\) is defined as:</p> \[E(X) = \sum_{i} x_i P(x_i)\] <p>Where:</p> <ul> <li>\(x_i\) are the possible values that \(X\) can take.</li> <li>\(P(x_i)\) is the probability of \(X\) taking the value \(x_i\).</li> </ul> <p>For a continuous random variable with probability density function \(f(x)\), the expectation is:</p> \[E(X) = \int_{-\infty}^{\infty} x f(x) \, dx\] <p><strong>Intuition for Expectation:</strong></p> <p>Think of expectation as the “balance point” of a distribution. For example, if you were to imagine a physical rod with different weights placed at various points, the <strong>center of mass</strong> of the rod would represent the expectation.</p> <p>In machine learning, the expectation helps us understand the <strong>average behavior</strong> of the data. For instance, in regression tasks, the expectation of the target variable provides a baseline prediction.</p> <h4 id="what-is-variance"><strong>What is Variance?</strong></h4> <p>The <strong>variance</strong> of a random variable quantifies the spread or dispersion of the variable around its expectation. A high variance indicates that the values are widely spread out, while a low variance indicates that the values are clustered around the mean.</p> <p>For a discrete random variable \(X\), the variance \(\text{Var}(X)\) is defined as:</p> \[\text{Var}(X) = E[(X - E(X))^2] = \sum_{i} (x_i - E(X))^2 P(x_i)\] <p>For a continuous random variable:</p> \[\text{Var}(X) = \int_{-\infty}^{\infty} (x - E(X))^2 f(x) \, dx\] <p>Alternatively, variance can also be computed as:</p> \[\text{Var}(X) = E(X^2) - (E(X))^2\] <p>Where \(E(X^2)\) is the expectation of \(X^2\), i.e., the expected value of the square of \(X\).</p> <p><strong>Intuition for Variance:</strong></p> <p>Variance tells us about the <strong>spread</strong> of the data. Imagine measuring the height of a group of people:</p> <ul> <li>If everyone has a similar height, the variance will be low.</li> <li>If the group includes both very short and very tall individuals, the variance will be high.</li> </ul> <p>In machine learning, variance provides insights into <strong>model uncertainty</strong>. High variance in a model’s predictions indicates overfitting, while low variance suggests underfitting.</p> <h4 id="why-are-expectation-and-variance-important-in-machine-learning"><strong>Why Are Expectation and Variance Important in Machine Learning?</strong></h4> <ol> <li><strong>Expectation</strong>: <ul> <li><strong>Model Evaluation</strong>: Used as a baseline for evaluating model predictions (e.g., in regression tasks).</li> <li><strong>Loss Functions</strong>: Central to defining loss functions like Mean Squared Error (MSE).</li> <li><strong>Feature Engineering</strong>: Understanding the average behavior of features aids in creating or selecting the most informative ones.</li> </ul> </li> <li><strong>Variance</strong>: <ul> <li><strong>Bias-Variance Tradeoff</strong>: Balancing model complexity to avoid overfitting (high variance) or underfitting (low variance).</li> <li><strong>Model Complexity</strong>: Guides the choice of model complexity (e.g., simpler models like linear regression have lower variance).</li> <li><strong>Uncertainty Estimation</strong>: Quantifies confidence in probabilistic models like Gaussian Processes.</li> <li><strong>Performance Metrics</strong>: Used in cross-validation to measure consistency across datasets.</li> </ul> </li> </ol> <hr/> <h3 id="covariance-and-correlation-measuring-relationships-between-variables"><strong>Covariance and Correlation: Measuring Relationships Between Variables</strong></h3> <p>Covariance and correlation are statistical tools used to understand the relationships between two random variables. In machine learning, these concepts are essential for identifying feature interactions, reducing dimensionality, and improving model performance.</p> <h4 id="what-is-covariance"><strong>What is Covariance?</strong></h4> <p>Covariance measures the <strong>direction</strong> of the linear relationship between two variables, indicating whether they increase or decrease together.</p> <p>For two random variables \(X\) and \(Y\), the covariance is defined as:</p> \[\text{Cov}(X, Y) = E\left[(X - E(X))(Y - E(Y))\right]\] <p>Where:</p> <ul> <li>\(E(X)\) and \(E(Y)\) are the expectations of \(X\) and \(Y\).</li> <li>\((X - E(X))\) and \((Y - E(Y))\) represent deviations from their means.</li> </ul> <p><strong>Interpretation</strong>:</p> <ul> <li>\(\text{Cov}(X, Y) &gt; 0\): Positive relationship (as \(X\) increases, \(Y\) tends to increase).</li> <li>\(\text{Cov}(X, Y) &lt; 0\): Negative relationship (as \(X\) increases, \(Y\) tends to decrease).</li> <li>\(\text{Cov}(X, Y) = 0\): No linear relationship.</li> </ul> <h4 id="what-is-correlation"><strong>What is Correlation?</strong></h4> <p>Correlation is a <strong>scaled version of covariance</strong> that provides the strength and direction of the relationship on a fixed scale \([-1, 1]\).</p> <p>The <strong>Pearson correlation coefficient</strong> is defined as:</p> \[\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\] <p>Where:</p> <ul> <li>\(\sigma_X\) and \(\sigma_Y\) are the standard deviations of \(X\) and \(Y\).</li> </ul> <p><strong>Interpretation</strong>:</p> <ul> <li>\(\rho(X, Y) = 1\): Perfect positive linear relationship.</li> <li>\(\rho(X, Y) = -1\): Perfect negative linear relationship.</li> <li>\(\rho(X, Y) = 0\): No linear relationship.</li> </ul> <h4 id="key-differences"><strong>Key Differences</strong></h4> <table> <thead> <tr> <th><strong>Aspect</strong></th> <th><strong>Covariance</strong></th> <th><strong>Correlation</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Scale</strong></td> <td>Depends on the units of variables</td> <td>Unitless, standardized</td> </tr> <tr> <td><strong>Range</strong></td> <td>\((-\infty, \infty)\)</td> <td>\([-1, 1]\)</td> </tr> <tr> <td><strong>Use Case</strong></td> <td>Direction of relationship</td> <td>Strength and direction combined</td> </tr> </tbody> </table> <p>//</p> <h4 id="applications-in-machine-learning"><strong>Applications in Machine Learning</strong></h4> <ol> <li><strong>Feature Relationships</strong>: <ul> <li>Covariance highlights how features interact.</li> <li>Correlation quantifies redundancy or relevance.</li> </ul> </li> <li><strong>Feature Selection</strong>: <ul> <li>Retain features with high correlation to the target.</li> <li>Remove features with high inter-correlation to reduce multicollinearity.</li> </ul> </li> <li><strong>Dimensionality Reduction</strong>: <ul> <li><strong>Principal Component Analysis (PCA)</strong> uses covariance or correlation matrices to identify directions of maximum variance.</li> </ul> </li> </ol> <hr/> <h3 id="central-limit-theorem-the-foundation-of-statistical-inference"><strong>Central Limit Theorem: The Foundation of Statistical Inference</strong></h3> <p>The <strong>Central Limit Theorem (CLT)</strong> explains why normal distributions appear so frequently in practice and is key for making inferences about data.</p> <h4 id="what-is-the-central-limit-theorem"><strong>What is the Central Limit Theorem?</strong></h4> <p>The <strong>Central Limit Theorem</strong> states that for a population with a finite mean \(\mu\) and variance \(\sigma^2\), the distribution of the <strong>sample mean</strong> from sufficiently large random samples will approximate a <strong>normal distribution</strong>, regardless of the original distribution of the population.</p> <p>Mathematically, if \(X_1, X_2, \dots, X_n\) are i.i.d. random variables drawn from a population with mean \(\mu\) and variance \(\sigma^2\), the sample mean \(\bar{X}\) is defined as:</p> \[\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i\] <p>As the sample size \(n\) increases, the sample mean has the following properties:</p> <ul> <li>The <strong>mean</strong> of \(\bar{X}\) is \(\mu\) (the population mean).</li> <li>The <strong>variance</strong> of \(\bar{X}\) is \(\frac{\sigma^2}{n}\), meaning the variance decreases as \(n\) increases.</li> <li>The distribution of \(\bar{X}\) approaches a <strong>normal distribution</strong> with mean \(\mu\) and variance \(\frac{\sigma^2}{n}\), and the <strong>standard deviation</strong> becomes \(\frac{\sigma}{\sqrt{n}}\), called the <strong>standard error</strong>.</li> </ul> <p><strong>How do we remember this?</strong></p> <p>Imagine you are sampling from a non-normal distribution, such as the distribution of ages in a city. A small sample might produce a skewed or non-normal distribution. However, as you increase the sample size, the distribution of the sample mean will become increasingly normal, regardless of the original distribution shape.</p> <p>This phenomenon is like averaging noisy measurements in engineering. A single measurement might be noisy, but averaging multiple measurements reduces the noise, making the result more predictable and normally distributed.</p> <h4 id="why-is-the-central-limit-theorem-important-in-machine-learning"><strong>Why is the Central Limit Theorem Important in Machine Learning?</strong></h4> <p>The Central Limit Theorem is foundational in statistics and machine learning for the following reasons:</p> <ol> <li><strong>Foundation for Inference</strong>: <ul> <li>The CLT enables statistical inference techniques like hypothesis testing and confidence intervals. When drawing random samples, the sample mean will follow a normal distribution, allowing for probabilistic statements about population parameters.</li> </ul> </li> <li><strong>Simplifying Assumptions</strong>: <ul> <li>Many machine learning algorithms assume normality (e.g., linear regression). The CLT allows us to assume that, for sufficiently large datasets, estimators of model parameters will follow a normal distribution, making them easier to analyze.</li> </ul> </li> <li><strong>Sample Size Considerations</strong>: <ul> <li>The CLT shows that, for large datasets, we can assume normality even if the underlying data is non-normal. As the sample size increases, algorithms become more stable and their performance becomes more predictable.</li> </ul> </li> </ol> <p><strong>Example of the Central Limit Theorem in Practice</strong></p> <p>Imagine you are analyzing <strong>house prices</strong> in a city, and the distribution of house prices is highly skewed due to a few luxury homes. You want to estimate the average house price.</p> <ul> <li> <p><strong>Without CLT</strong>: The highly skewed data would result in a mean that doesn’t reflect the typical house price.</p> </li> <li> <p><strong>With CLT</strong>: By taking random samples, computing the mean for each sample, and repeating the process many times, the distribution of sample means will become normal, even though the underlying distribution of house prices is skewed. The sample mean will be a more reliable estimator of the population mean, allowing for more accurate confidence intervals.</p> </li> </ul> <h4 id="central-limit-theorem-in-machine-learning"><strong>Central Limit Theorem in Machine Learning</strong></h4> <p>The CLT is useful in several machine learning contexts:</p> <ol> <li><strong>Regression Models</strong>: <ul> <li>In linear regression, the CLT implies that, for large sample sizes, the distribution of estimated coefficients will be approximately normal. This enables the use of confidence intervals to assess uncertainty in the coefficients.</li> </ul> </li> <li><strong>Bootstrap Methods</strong>: <ul> <li>The CLT is essential for bootstrap resampling methods, which estimate the variability of a statistic (like the mean) by repeatedly sampling from the data. Due to the CLT, the distribution of these sample statistics will be approximately normal.</li> </ul> </li> <li><strong>Confidence Intervals and Hypothesis Testing</strong>: <ul> <li>Many machine learning techniques rely on the CLT to estimate confidence intervals and perform hypothesis testing. For example, in regression, the standard error of the coefficients is derived from the CLT.</li> </ul> </li> </ol> <h4 id="conditions-for-the-central-limit-theorem"><strong>Conditions for the Central Limit Theorem</strong></h4> <p>For the CLT to hold, the following conditions are necessary:</p> <ol> <li><strong>Independence</strong>: The samples must be independent.</li> <li><strong>Sample Size</strong>: The sample size should be large enough. Typically, a sample size of 30 or more is considered sufficient for the CLT to apply.</li> <li><strong>Finite Variance</strong>: The population must have a finite variance.</li> </ol> <p>By leveraging the CLT, you can make reliable estimates, perform hypothesis testing, and create models that work well, even when the underlying data distribution is non-normal.</p> <hr/> <p>Finally, we’ve explored key concepts in probability theory that are important to machine learning. From understanding the basics of probability distributions and Bayes’ Theorem to the relationships between variables through covariance and correlation, these concepts provide the mathematical layer for building robust models. Finally, the Central Limit Theorem ties everything together, offering insight into statistical inference and ensuring that predictions and model estimates are reliable.</p> <p>In the next post, we’ll continue diving deeper into the brief history of machine learning and what are we upto currently.</p> <p><strong>See you there!</strong></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog explores key probability theory concepts, from distributions and Bayes' Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.]]></summary></entry><entry><title type="html">Understanding the Basics of Probability Theory for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/probability-1/" rel="alternate" type="text/html" title="Understanding the Basics of Probability Theory for Machine Learning"/><published>2024-12-22T11:55:00+00:00</published><updated>2024-12-22T11:55:00+00:00</updated><id>https://monishver11.github.io/blog/2024/probability-1</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/probability-1/"><![CDATA[<p>Probability theory forms the foundation of machine learning by enabling models to quantify uncertainty and make evidence-based predictions. This article delves into core probability concepts, providing explanations, examples, and analogies designed for clarity and practical sense.</p> <hr/> <h3 id="probability-definition-interpretation-and-basic-axioms"><strong>Probability: Definition, Interpretation, and Basic Axioms</strong></h3> <p>In simple terms, <strong>probability</strong> is a measure of the likelihood of an event occurring. It quantifies uncertainty by assigning a number between 0 and 1, where:</p> <ul> <li>A probability of <strong>0</strong> means the event is impossible.</li> <li>A probability of <strong>1</strong> means the event is certain.</li> <li>Values between 0 and 1 represent the likelihood of an event, with higher values indicating greater likelihood.</li> </ul> <p>Mathematically, probability is defined as a function:\(P: \mathcal{F} \to [0, 1]\) where \(\mathcal{F}\) is a collection of events, and \(P\) assigns a value to each event representing its likelihood.</p> <h4 id="interpretation-of-probability"><strong>Interpretation of Probability</strong></h4> <ol> <li><strong>Frequentist Interpretation</strong>: Probability is the long-run relative frequency of an event occurring after repeated trials. Example: In flipping a fair coin many times, the probability of getting heads is \(0.5\), as heads appear in roughly 50% of the flips.</li> <li><strong>Bayesian Interpretation</strong>: Probability reflects a degree of belief or confidence in an event occurring, updated as new evidence becomes available. Example: If the chance of rain tomorrow is initially assigned as \(0.7\), this reflects 70% confidence based on current information.</li> </ol> <h4 id="basic-axioms-of-probability"><strong>Basic Axioms of Probability</strong></h4> <p>The three fundamental axioms govern how probabilities are assigned:</p> <ol> <li><strong>Non-negativity</strong>: \(P(E) \geq 0 \quad \text{for all events } E\). Probabilities cannot be negative.</li> <li><strong>Normalization</strong>: \(P(S) = 1\). Here, \(S\) is the <strong>sample space</strong> (all possible outcomes). The probability of \(S\) is 1, as one outcome must occur. Example: For a die roll, \(S = \{1, 2, 3, 4, 5, 6\}\), and \(P(S) = 1\).</li> <li> <p><strong>Additivity</strong>:</p> \[P(E_1 \cup E_2) = P(E_1) + P(E_2) \quad \text{if } E_1 \text{ and } E_2 \text{ are mutually exclusive}\] <p>For mutually exclusive events \(E_1\) and \(E_2\), the probability of either occurring is the sum of their individual probabilities. <strong>Example</strong>: For a die roll, let \(E_1 = \{1\}\) and \(E_2 = \{2\}\): \(P(E_1 \cup E_2) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3}\)</p> </li> </ol> <h4 id="consequences-of-the-axioms"><strong>Consequences of the Axioms</strong></h4> <ol> <li><strong>Complementary Rule:</strong> \(P(E^c) = 1 - P(E)\). The probability of the complement of \(E\) (event not occurring) equals \(1\) minus \(P(E)\). Example: If \(P(\text{rain}) = 0.8\), then \(P(\text{no rain}) = 1 - 0.8 = 0.2\).</li> <li> <p><strong>Addition Rule for Non-Mutually Exclusive Events:</strong></p> \[P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)\] <p>For events that are not mutually exclusive, subtract the overlap probability. Example: Overlapping probabilities in surveys or data categorization.</p> </li> <li> <p><strong>Multiplication Rule for Independent Events:</strong></p> \[P(E_1 \cap E_2) = P(E_1) \cdot P(E_2) \quad \text{:if } E_1 \text{ and } E_2 \text{ are independent}\] <p>Example: Probability of flipping two heads with two coins:\(P(\text{heads on coin 1} \cap \text{heads on coin 2}) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\)</p> </li> <li> <p><strong>Conditional Probability:</strong></p> \[P(E_1 \mid E_2) = \frac{P(E_1 \cap E_2)}{P(E_2)} \quad \text{if } P(E_2) &gt; 0\] <p>Conditional probability calculates \(E_1\)’s likelihood given \(E_2\) has occurred. Example: Used in models like <strong>Naive Bayes</strong> for predictions under given conditions.</p> </li> </ol> <h4 id="why-is-probability-important-in-machine-learning"><strong>Why is Probability Important in Machine Learning?</strong></h4> <p>Probability theory plays a critical role in machine learning by enabling:</p> <ol> <li><strong>Modeling Uncertainty</strong>: Essential in probabilistic models like Bayesian networks and Hidden Markov Models.</li> <li><strong>Decision Making</strong>: Used in reinforcement learning for action selection under uncertainty.</li> <li><strong>Risk Assessment and Confidence</strong>: Provides confidence intervals and helps quantify prediction risks.</li> <li><strong>Bayesian Inference</strong>: Updates beliefs about parameters or predictions using new data.</li> </ol> <hr/> <h3 id="random-variables-discrete-and-continuous"><strong>Random Variables: Discrete and Continuous</strong></h3> <p>Building upon the foundational axioms of probability, understanding <strong>random variables</strong> is the next critical step. Random variables bridge the gap between abstract probabilistic events and numerical representation, enabling a deeper connection between data and mathematical models.</p> <p>A <strong>random variable</strong> is a variable whose possible values are outcomes of a random process or experiment. It maps outcomes from a probabilistic event to real numbers, playing a central role in probability theory and machine learning. Random variables are typically categorized into two types: <strong>discrete</strong> and <strong>continuous</strong>.</p> <p><strong>Intuition for Random Variables</strong></p> <p>Think of a random variable as a “number generator” that transforms outcomes of a random process into numbers. For instance:</p> <ul> <li>In a dice roll, the outcome (e.g., rolling a 4) is translated to the random variable \(X = 4\).</li> <li>In measuring rainfall, the amount (e.g., 12.5 mm) is assigned to the random variable \(X = 12.5\). This abstraction helps in applying mathematical operations and deriving distributions.</li> </ul> <h4 id="discrete-random-variables"><strong>Discrete Random Variables</strong></h4> <p>A <strong>discrete random variable</strong> takes on a countable number of distinct values. These values are often integers or counts, representing outcomes that are distinct and separate from one another. For example, the number of heads obtained when flipping a coin multiple times is a discrete random variable.</p> <p><strong>Characteristics</strong>:</p> <ul> <li><strong>Countable Outcomes</strong>: The possible values can be listed, even if the list is infinite (e.g., the number of calls received by a call center).</li> <li><strong>Probability Mass Function (PMF)</strong>: The probability distribution of a discrete random variable is described by a probability mass function (PMF), which assigns probabilities to each possible value. The PMF satisfies:</li> </ul> \[P(X = x) \geq 0 \quad \text{for all } x\] \[\sum_{x} P(X = x) = 1\] <p>Here, the sum is over all possible values \(x\) that the random variable can take.</p> <p><strong>Examples of Discrete Random Variables</strong>:</p> <ol> <li> <p><strong>Number of heads in coin flips</strong>: Let \(X\) be the number of heads in 3 flips of a fair coin. The possible values of \(X\) are \(0, 1, 2,\) and \(3\). The probabilities for each outcome can be computed using the binomial distribution.</p> </li> <li> <p><strong>Number of customers arriving at a store</strong>: Let \(X\) represent the number of customers who arrive at a store during a 1-hour period. If customers arrive independently with an average rate of 3 per hour, \(X\) could follow a <strong>Poisson distribution</strong>.</p> </li> </ol> <h4 id="continuous-random-variables"><strong>Continuous Random Variables</strong></h4> <p>A <strong>continuous random variable</strong> can take on an infinite number of possible values within a given range. These values are not countable but form a continuum. For example, the height of a person is a continuous random variable because it can take any value within a range, such as \(5.6\) feet, \(5.65\) feet, \(5.654\) feet, and so on.</p> <p><strong>Characteristics</strong>:</p> <ul> <li><strong>Uncountably Infinite Outcomes</strong>: The possible values form a continuum, often represented by intervals on the real number line.</li> <li><strong>Probability Density Function (PDF)</strong>: The probability distribution of a continuous random variable is described by a probability density function (PDF). Unlike a PMF, the PDF does not give the probability of any specific outcome but rather the probability of the random variable falling within a certain range. The total area under the PDF curve is equal to 1, and the probability of the variable falling in an interval \([a, b]\) is given by:</li> </ul> \[P(a \leq X \leq b) = \int_{a}^{b} f_X(x) \, dx\] <p>where \(f_X(x)\) is the PDF of \(X\).</p> <p><strong>Examples of Continuous Random Variables</strong>:</p> <ol> <li><strong>Height of a person</strong>: The height \(X\) of a person can take any value within a realistic range (e.g., between 4 and 7 feet). The exact value is not countable, and it is typically modeled by a normal distribution.</li> <li><strong>Time taken to complete a task</strong>: If you measure the time \(X\) taken by someone to complete a task, this can take any value (e.g., 2.5 minutes, 2.55 minutes, etc.). The distribution could be modeled using an exponential or normal distribution, depending on the scenario.</li> </ol> <h4 id="why-are-random-variables-important-in-machine-learning"><strong>Why are Random Variables Important in Machine Learning?</strong></h4> <ol> <li><strong>Modeling Uncertainty</strong>: In machine learning, random variables allow us to model uncertainty in data and predictions. For instance, in regression models, the target variable is often modeled as a random variable with some uncertainty, usually represented as a continuous distribution.</li> <li><strong>Bayesian Inference</strong>: In Bayesian models, parameters are treated as random variables, and their distributions are updated with new data. This allows for probabilistic reasoning and uncertainty quantification in predictions.</li> <li><strong>Stochastic Processes</strong>: Many machine learning algorithms, such as those in reinforcement learning or Monte Carlo simulations, involve <strong>stochastic processes</strong>, where future states or outcomes are modeled as random variables with given distributions.</li> </ol> <hr/> <h3 id="more-on-probability-distribution-and-types"><strong>More on Probability Distribution and Types</strong></h3> <p>A probability distribution describes how probabilities are assigned to different possible outcomes of a random variable. It provides a mathematical function that represents the likelihood of each possible value the random variable can take. In simpler terms, it tells us how likely each outcome of an experiment or process is.</p> <p>Formally, A <strong>probability distribution</strong> of a random variable \(X\) is a function that provides the probabilities of occurrence of different possible outcomes for the random variable. Depending on the nature of the random variable, the probability distribution can take different forms:</p> <ol> <li><strong>Discrete Probability Distribution</strong>: This applies when the random variable can only take on a finite or countably infinite number of values (e.g., the number of heads in a coin flip). The distribution is described by a <strong>probability mass function (PMF)</strong>.</li> <li><strong>Continuous Probability Distribution</strong>: This applies when the random variable can take on an infinite number of values within a range (e.g., the height of a person). The distribution is described by a <strong>probability density function (PDF)</strong>.</li> </ol> <p>For both types, the total probability across all possible outcomes must sum (or integrate) to 1:</p> <ul> <li> <p>For discrete distributions:</p> \[\sum_{x} P(X = x) = 1\] </li> <li> <p>For continuous distributions:</p> \[\int_{-\infty}^{\infty} f_X(x) \, dx = 1\] <p>where \(f_X(x)\) is the probability density function.</p> </li> </ul> <h5 id="1-discrete-probability-distributions"><strong>1. Discrete Probability Distributions</strong></h5> <p>Discrete distributions are used when the random variable can take a countable number of distinct values. Some common discrete probability distributions are:</p> <ul> <li><strong>Bernoulli Distribution</strong>: Models a binary outcome (success/failure, \(1/0\)) of a single trial. The probability of success is \(p\), and the probability of failure is \(1 - p\). The PMF is: \(P(X = 1) = p, \quad P(X = 0) = 1 - p\) <ul> <li><strong>Example</strong>: The outcome of a coin flip (Heads = 1, Tails = 0).</li> </ul> </li> <li> <p><strong>Binomial Distribution</strong>: Describes the number of successes in a fixed number of independent Bernoulli trials. The random variable \(X\) counts the number of successes. The PMF is:</p> \[P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}\] <p>where \(n\) is the number of trials, \(p\) is the probability of success, and \(k\) is the number of successes.</p> <ul> <li><strong>Example</strong>: The number of heads in 10 coin flips.</li> </ul> </li> <li> <p><strong>Poisson Distribution</strong>: Models the number of events occurring in a fixed interval of time or space, given a constant average rate of occurrence. The PMF is:</p> \[P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}\] <p>where \(\lambda\) is the average rate of occurrence (mean), and \(k\) is the number of events.</p> <ul> <li><strong>Example</strong>: The number of phone calls received by a call center in an hour.</li> </ul> </li> </ul> <h5 id="2-continuous-probability-distributions">2. <strong>Continuous Probability Distributions</strong></h5> <p>Continuous distributions are used when the random variable can take any value within a given range or interval. These distributions are described by probability density functions (PDFs), where the probability of any single point is zero, and probabilities are calculated over intervals. Some common continuous probability distributions include:</p> <ul> <li> <p><strong>Uniform Distribution</strong>: A continuous distribution where all values within a given interval are equally likely. The PDF is:</p> \[f_X(x) = \frac{1}{b-a} \quad \text{for} \ a \leq x \leq b\] <ul> <li><strong>Example</strong>: The time it takes for a bus to arrive, uniformly distributed between 5 and 15 minutes.</li> </ul> </li> <li> <p><strong>Normal (Gaussian) Distribution</strong>: A continuous distribution that is symmetric and bell-shaped. It is fully described by its mean \(\mu\) and standard deviation \(\sigma\). The PDF is:</p> \[f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\] <ul> <li><strong>Example</strong>: The distribution of heights in a population.</li> </ul> </li> <li><strong>Exponential Distribution</strong>: A continuous distribution often used to model the time between events in a Poisson process (events happening at a constant rate). The PDF is: \(f_X(x) = \lambda e^{-\lambda x} \quad \text{for} \ x \geq 0\) <ul> <li><strong>Example</strong>: The time between arrivals of customers at a service station.</li> </ul> </li> <li> <p><strong>Gamma Distribution</strong>: A generalization of the exponential distribution, used for modeling the sum of multiple exponentially distributed random variables. Its PDF is:</p> \[f_X(x) = \frac{x^{k-1} e^{-x/\theta}}{\Gamma(k) \theta^k} \quad \text{for} \ x \geq 0\] <ul> <li><strong>Example</strong>: The waiting time until a certain number of events occur in a Poisson process.</li> </ul> </li> <li> <p><strong>Beta Distribution</strong>: A continuous distribution on the interval \([0, 1]\), often used to model probabilities and proportions. Its PDF is:</p> \[f_X(x) = \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha, \beta)}\] <ul> <li><strong>Example</strong>: Modeling the proportion of customers who prefer a certain product in a market research study.</li> </ul> </li> </ul> <h4 id="why-are-probability-distributions-important-in-machine-learning"><strong>Why are Probability Distributions Important in Machine Learning?</strong></h4> <ol> <li><strong>Modeling Uncertainty</strong>: Many machine learning models assume that the data follows a certain probability distribution (e.g., Gaussian distribution in linear regression).</li> <li><strong>Inference and Prediction</strong>: In probabilistic models, such as Bayesian inference or Hidden Markov Models, understanding the probability distributions of variables allows for reasoning about uncertainty and making predictions based on observed data.</li> <li><strong>Risk Analysis</strong>: Distributions help quantify the risk or uncertainty in machine learning predictions. For example, a model’s output might be a probability distribution over potential outcomes, providing insights into the confidence of predictions.</li> </ol> <h3 id="cumulative-distribution-function-cdf"><strong>Cumulative Distribution Function (CDF)</strong></h3> <p>CDF is closely related to the <strong>Probability Density Function (PDF)</strong> in the case of continuous random variables and the <strong>Probability Mass Function (PMF)</strong> for discrete variables. The CDF gives the cumulative probability that a random variable takes a value less than or equal to a particular point.</p> <p>For a random variable \(X\), the <strong>Cumulative Distribution Function (CDF)</strong>, denoted by \(F_X(x)\), is defined as the probability that the random variable \(X\) takes a value less than or equal to \(x\). Formally:</p> \[F_X(x) = P(X \leq x)\] <p>The CDF is a function that provides the cumulative probability up to a point \(x\) and is computed by integrating (for continuous variables) or summing (for discrete variables) the corresponding probability distributions.</p> <h4 id="properties-of-the-cdf"><strong>Properties of the CDF</strong></h4> <ol> <li> <p><strong>Non-decreasing</strong>: The CDF is a non-decreasing function, meaning that the probability increases as \(x\) increases:</p> \[F_X(x_1) \leq F_X(x_2) \quad \text{if} \ x_1 \leq x_2\] </li> <li> <p><strong>Range</strong>: The CDF always lies within the range \([0, 1]\) : \(0 \leq F_X(x) \leq 1 \quad \text{for all} \ x\)</p> </li> <li><strong>Limits</strong>: <ul> <li>As \(x \to -\infty\), the CDF tends to 0: \(\lim_{x \to -\infty} F_X(x) = 0\)</li> <li>As \(x \to \infty\), the CDF tends to 1: \(\lim_{x \to \infty} F_X(x) = 1\)</li> </ul> </li> <li><strong>Continuity</strong>: <ul> <li>For <strong>continuous random variables</strong>, the CDF is continuous and smooth.</li> <li>For <strong>discrete random variables</strong>, the CDF is a step function, with jumps corresponding to the probabilities at specific points.</li> </ul> </li> </ol> <h4 id="cdf-for-discrete-random-variables"><strong>CDF for Discrete Random Variables</strong></h4> <p>For a <strong>discrete random variable</strong> \(X\), the CDF is computed by summing the probabilities given by the PMF. If the possible values of \(X\) are \(x_1, x_2, \dots\), the CDF is:</p> \[F_X(x) = P(X \leq x) = \sum_{x_i \leq x} P(X = x_i)\] <p><strong>Example</strong> Consider a discrete random variable \(X\) that represents the outcome of a fair 6-sided die. The possible values for \(X\) are \(1, 2, 3, 4, 5, 6\), each with a probability of \(\frac{1}{6}\). The CDF for \(X\) is:</p> \[F_X(x) = \begin{cases} 0 &amp; \text{for} \ x &lt; 1 \\ \frac{1}{6} &amp; \text{for} \ 1 \leq x &lt; 2 \\ \frac{2}{6} &amp; \text{for} \ 2 \leq x &lt; 3 \\ \frac{3}{6} &amp; \text{for} \ 3 \leq x &lt; 4 \\ \frac{4}{6} &amp; \text{for} \ 4 \leq x &lt; 5 \\ \frac{5}{6} &amp; \text{for} \ 5 \leq x &lt; 6 \\ 1 &amp; \text{for} \ x \geq 6 \end{cases}\] <h4 id="cdf-for-continuous-random-variables"><strong>CDF for Continuous Random Variables</strong></h4> <p>For a <strong>continuous random variable</strong> \(X\), the CDF is obtained by integrating the PDF:</p> \[F_X(x) = P(X \leq x) = \int_{-\infty}^{x} f_X(t) \, d t\] <p><strong>Example</strong> For a continuous random variable \(X\) that follows a <strong>uniform distribution</strong> on the interval \([0, 1]\), the PDF is:</p> \[f_X(x) = \begin{cases} 1 &amp; \text{for} \ 0 \leq x \leq 1 \\ 0 &amp; \text{otherwise} \end{cases}\] <p>The CDF is:</p> \[F_X(x) = \begin{cases} 0 &amp; \text{for} \ x &lt; 0 \\ x &amp; \text{for} \ 0 \leq x \leq 1 \\ 1 &amp; \text{for} \ x &gt; 1 \end{cases}\] <p>This shows that for values of \(x\) between 0 and 1, the probability increases linearly from 0 to 1.</p> <h4 id="relationship-between-pdf-and-cdf"><strong>Relationship Between PDF and CDF</strong></h4> <p>For a continuous random variable \(X\), the PDF is the derivative of the CDF:</p> \[f_X(x) = \frac{d}{dx} F_X(x)\] <p>Conversely, the CDF can be obtained by integrating the PDF:</p> \[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\] <h4 id="why-is-the-cdf-important-in-machine-learning"><strong>Why is the CDF Important in Machine Learning?</strong></h4> <ol> <li><strong>Data Interpretation</strong>: The CDF provides a clear interpretation of the distribution of data and is useful for understanding the likelihood of a random variable being less than or equal to a specific value.</li> <li><strong>Probabilistic Decision Making</strong>: The CDF helps integrate outcomes for decision-making in models like <strong>Naive Bayes</strong> or <strong>Bayesian networks</strong>.</li> </ol> <h4 id="how-pmf-pdf-and-cdf-interrelate"><strong>How PMF, PDF, and CDF Interrelate</strong></h4> <ul> <li><strong>Discrete Variables</strong>: <ul> <li>PMF: \(P(X = x_i)\)</li> <li>CDF:</li> </ul> \[F_X(x) = \sum_{x_i \leq x} P(X = x_i)\] </li> <li><strong>Continuous Variables</strong>: <ul> <li>PDF: \(f_X(x)\)</li> <li>CDF:</li> </ul> \[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\] <ul> <li>PDF from CDF:</li> </ul> \[f_X(x) = \frac{d}{dx} F_X(x)\] </li> </ul> <h3 id="choosing-the-right-distribution-in-ml"><strong>Choosing the Right Distribution in ML</strong></h3> <p>Choosing the appropriate probability distribution for a given machine learning (ML) problem is crucial to making accurate predictions. Each probability distribution captures a unique set of characteristics regarding the randomness and uncertainty in the data. A proper understanding of these distributions can directly influence the performance and efficiency of your model. Here’s a more detailed look at the various distributions commonly used in ML:</p> <ul> <li> <p><strong>Bernoulli/Binomial Distributions:</strong> These are useful for binary or count outcomes. The <strong>Bernoulli distribution</strong> models a single trial with two possible outcomes, often labeled as success (1) or failure (0). The <strong>Binomial distribution</strong> generalizes this by modeling the number of successes in a fixed number of independent Bernoulli trials. These distributions are especially useful in classification tasks, such as predicting whether an email is spam or not.</p> </li> <li> <p><strong>Gaussian (Normal) Distribution:</strong> The Gaussian or <strong>normal distribution</strong> is one of the most widely used distributions in statistics and machine learning, especially when the data exhibits natural variability. It is characterized by its symmetric bell-shaped curve, defined by its mean and standard deviation. It’s particularly useful when you have continuous data that tends to cluster around a central value, such as the distribution of heights in a population or the error terms in regression models. Many machine learning algorithms, such as <strong>linear regression</strong> and <strong>k-nearest neighbors (KNN)</strong>, assume that the underlying data follows a Gaussian distribution.</p> </li> <li> <p><strong>Poisson/Exponential Distributions:</strong> These distributions model events that occur over time or space, where the events happen at a constant average rate. The <strong>Poisson distribution</strong> models the number of events happening in a fixed interval of time or space (e.g., the number of customer arrivals at a service station). The <strong>Exponential distribution</strong> is often used to model the time between events in a Poisson process. Both distributions are important in scenarios involving queues or event-based systems, like predicting the time between customer purchases or server failures in network systems.</p> </li> </ul> <h4 id="why-does-choosing-the-right-distribution-matter">Why Does Choosing the Right Distribution Matter?</h4> <ol> <li><strong>Tailoring Models to Data:</strong> Understanding the underlying distribution of the data helps in selecting the right model for your problem. For example, if the data is normally distributed, using models like <strong>linear regression</strong> (which assumes normality of residuals) can result in more accurate predictions. On the other hand, when data is binary (e.g., yes/no outcomes), a <strong>logistic regression</strong> or <strong>Bernoulli distribution</strong> approach would be more appropriate.</li> <li><strong>Improving Model Efficiency:</strong> When we align the assumptions of a machine learning algorithm with the real-world distribution of the data, models tend to be more efficient and require less computation. For instance, algorithms that work with Gaussian-distributed data can be optimized to take advantage of the symmetry of the distribution, leading to faster convergence in training.</li> <li><strong>Quantifying Uncertainty:</strong> Different distributions provide unique ways to handle uncertainty in predictions. For example, when working with <strong>Poisson distributions</strong>, we can predict the expected number of events in a fixed period with a known variance. In contrast, the <strong>Exponential distribution</strong> models waiting times, making it suitable for applications like survival analysis or reliability engineering.</li> </ol> <h4 id="further-exploration"><strong>Further Exploration</strong></h4> <p>While this overview has touched on the most commonly used distributions, the world of probability distributions in machine learning is vast. As you dive deeper into various ML topics, you’ll encounter additional distributions tailored to specific data types and problems. Understanding how these distributions behave allows you to refine your models for more accurate, effective predictions.</p> <p>If you’re eager to explore further, we will be diving deeper into these distributions as we continue our series on probability theory. The next section will introduce even more important concepts, so stay tuned for that!</p> <p><strong>See you in the next post!</strong></p> <h3 id="references"><strong>References:</strong></h3> <ul> <li>Add links for common distributions for making it visually imaginable and relatable!</li> <li>Go through it one more last time for corrections.</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog explores essential probability concepts and their significance in machine learning.]]></summary></entry></feed>