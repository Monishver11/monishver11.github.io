<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-12T00:36:18+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A clear, theory-focused approach to machine learning, designed to take you beyond the basics. </subtitle><entry><title type="html">Multiclass Logistic Regression &amp;amp; Multiclass Perceptron Algorithm</title><link href="https://monishver11.github.io/blog/2025/multiclass-loss/" rel="alternate" type="text/html" title="Multiclass Logistic Regression &amp;amp; Multiclass Perceptron Algorithm"/><published>2025-04-12T00:32:00+00:00</published><updated>2025-04-12T00:32:00+00:00</updated><id>https://monishver11.github.io/blog/2025/multiclass-loss</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/multiclass-loss/"><![CDATA[<p>In real-world machine learning problems, we often need to classify data into multiple categories, not just two. While binary classification is a fundamental building block, it‚Äôs crucial to understand how we can extend these ideas to handle multiple classes. This transition from binary to multiclass classification is what we‚Äôll explore in this blog. We‚Äôll start by revisiting <strong>binary logistic regression</strong>, then step into <strong>multiclass logistic regression</strong>, and finally discuss how we can generalize algorithms like the perceptron for multiclass classification.</p> <hr/> <h5 id="binary-logistic-regression-recap"><strong>Binary Logistic Regression Recap</strong></h5> <p>Let‚Äôs begin with the most basic form of classification: binary logistic regression.</p> <p>Given an input \(x\), our goal is to predict whether it belongs to class 1 or class 0. The function we use for binary classification is called the <strong>sigmoid function</strong>, which outputs a probability between 0 and 1:</p> \[f(x) = \sigma(z) = \frac{1}{1 + \exp(-z)} = \frac{1}{1 + \exp(-w^\top x - b)} \quad (1)\] <p>The output \(f(x)\) represents the probability of class 1. The probability of the other class (class 0) is simply:</p> \[1 - f(x) = \frac{\exp(-w^\top x - b)}{1 + \exp(-w^\top x - b)} = \frac{1}{1 + \exp(w^\top x + b)} = \sigma(-z) \quad (2)\] <p>Another way to think about this is that one class corresponds to the parameters \(w\) and \(b\), while the other class corresponds to the parameters \(-w\) and \(-b\). This helps set the foundation for extending this concept to multiple classes.</p> <hr/> <h5 id="extending-to-multiclass-logistic-regression"><strong>Extending to Multiclass Logistic Regression</strong></h5> <p>Now that we have a solid understanding of binary logistic regression, let‚Äôs consider the case where we have more than two classes. This is where <strong>multiclass logistic regression</strong> comes in. For each class \(c\), we assign a weight vector \(w_c\) and a bias \(b_c\). The probability of belonging to class \(c\) given an input \(x\) is computed using the <strong>softmax function</strong>:</p> \[f_c(x) = \frac{\exp(w_c^\top x + b_c)}{\sum_{c'} \exp(w_{c'}^\top x + b_{c'})} \quad (3)\] <p>This formulation, known as <strong>softmax regression</strong>, allows us to calculate the probability for each class and select the class with the highest probability.</p> <h5 id="the-loss-function"><strong>The Loss Function</strong></h5> <p>To train the model, we use a <strong>cross-entropy loss</strong> function, which measures how well the model‚Äôs predicted probabilities match the true labels. Given a dataset \(\{(x^{(i)}, y^{(i)})\}\), the loss is defined as:</p> \[L = \sum_i -\log f_{y^{(i)}}(x^{(i)})\] <p>This loss function encourages the model to assign higher probabilities to the correct class. The gradient of the loss with respect to the pre-activation (logits) is:</p> \[\frac{\partial L}{\partial z} = f - y\] <p><strong>Derivation:</strong></p> <p>Assume the true class is \(k = y^{(i)}\). The loss for this example is:</p> \[\ell^{(i)} = -\log f_k^{(i)}\] <p>Substituting in the softmax definition:</p> \[\ell^{(i)} = -\log\left( \frac{\exp(z_k^{(i)})}{\sum_{j=1}^C \exp(z_j^{(i)})} \right) = -z_k^{(i)} + \log \left( \sum_{j=1}^C \exp(z_j^{(i)}) \right)\] <p>We differentiate the loss \(\ell^{(i)}\) with respect to each logit \(z_c^{(i)}\). There are two cases:</p> <ul> <li><strong>Case 1: \(c = k\) (the correct class)</strong></li> </ul> \[\frac{\partial \ell^{(i)}}{\partial z_k^{(i)}} = -1 + \frac{\exp(z_k^{(i)})}{\sum_{j=1}^C \exp(z_j^{(i)})} = f_k^{(i)} - 1\] <ul> <li><strong>Case 2: \(c \ne k\)</strong></li> </ul> \[\frac{\partial \ell^{(i)}}{\partial z_c^{(i)}} = \frac{\exp(z_c^{(i)})}{\sum_{j=1}^C \exp(z_j^{(i)})} = f_c^{(i)}\] <p>We can express both cases together using the one-hot encoded label vector \(y^{(i)}\):</p> \[\frac{\partial \ell^{(i)}}{\partial z^{(i)}} = f^{(i)} - y^{(i)}\] <p>Now, let \(f\) and \(y\) now represent the matrices of predicted probabilities and one-hot labels over the entire dataset. Then the total loss is:</p> \[L = \sum_{i=1}^N \ell^{(i)} = -\sum_{i=1}^N \log f_{y^{(i)}}^{(i)}\] <p>By stacking all gradients, the overall gradient of the loss with respect to the logits becomes:</p> \[\frac{\partial L}{\partial z} = f - y\] <p>This fully vectorized form allows efficient implementation and is similar to the gradient descent update used in binary logistic regression but generalized to multiple classes.</p> <hr/> <h5 id="quick-comparison-to-one-vs-all-ova-approach"><strong>Quick Comparison to One-vs-All (OvA) Approach</strong></h5> <p>In many multiclass problems, instead of learning a separate model for each class, we can use the <strong>One-vs-All (OvA)</strong> strategy. In OvA, we train a binary classifier for each class, where the classifier tries to distinguish one class from all others. The base hypothesis space in this case is:</p> \[\mathcal{H} = \{ h: \mathcal{X} \to \mathbb{R} \} \quad \text{(score functions)}\] <p>For \(k\) classes, the <strong>multiclass hypothesis space</strong> is:</p> \[\mathcal{F} = \left\{ x \mapsto \arg\max_i h_i(x) \ \big| \ h_1, \ldots, h_k \in \mathcal{H} \right\}\] <p>Intuitively, each function \(h_i(x)\) scores how likely \(x\) belongs to class \(i\). During training, we want each classifier to output positive values for examples from its own class and negative values for examples from all other classes. At test time, the classifier that outputs the highest score determines the predicted class.</p> <hr/> <h5 id="multiclass-perceptron-generalizing-the-perceptron-algorithm"><strong>Multiclass Perceptron: Generalizing the Perceptron Algorithm</strong></h5> <p>The classic Perceptron algorithm is designed for binary classification, but it can be naturally extended to multiclass problems. In the multiclass setting, instead of a single weight vector, we maintain <strong>one weight vector per class</strong>.</p> <p>For each class \(i\), we define a <strong>linear scoring function</strong>:</p> \[h_i(x) = w_i^\top x, \quad w_i \in \mathbb{R}^d\] <p>Given an input \(x\), the model predicts the class with the highest score:</p> \[\hat{y} = \arg\max_{i} w_i^\top x\] <p>The algorithm proceeds iteratively, updating the weights when it makes a mistake:</p> <ol> <li><strong>Initialize</strong>: Set all weight vectors to zero, \(w_i = 0\) for all classes $i$.</li> <li>For \(T\) iterations over the training set: <ul> <li>For each training example \((x, y)\): <ul> <li>Predict the label: \(\hat{y} = \arg\max_{i} w_i^\top x\)</li> <li>If \(\hat{y} \neq y\) (i.e., the prediction is incorrect): <ul> <li><strong>Promote</strong> the correct class:<br/> \(w_y \leftarrow w_y + x\)</li> <li><strong>Demote</strong> the incorrect prediction: \(w_{\hat{y}} \leftarrow w_{\hat{y}} - x\)</li> </ul> </li> </ul> </li> </ul> </li> </ol> <p>This update increases the score for the true class and decreases the score for the incorrect one, helping the model learn to separate them better in future iterations.</p> <h5 id="rewrite-the-scoring-function"><strong>Rewrite the scoring function</strong></h5> <p>When the number of classes \(k\) is large, storing and updating \(k\) separate weight vectors can become computationally expensive. To address this, we can rewrite the scoring function in a more compact form using a <strong>shared weight vector</strong>.</p> <p>We define a <strong>joint feature map</strong> \(\psi(x, i)\) that combines both the input \(x\) and a class label \(i\). Then, the score for class \(i\) can be written as:</p> \[h_i(x) = w_i^\top x = w^\top \psi(x, i) \tag{4}\] <p>Now, instead of maintaining a separate \(w_i\) for each class, we use <strong>a single global weight vector</strong> \(w\) that interacts with \(\psi(x, i)\) to compute scores for all classes:</p> \[h(x, i) = w^\top \psi(x, i) \tag{5}\] <p>This transformation allows us to use a single weight vector for all classes, which significantly reduces memory usage and computational complexity.</p> <p><strong>Concrete Example</strong></p> <p>Let:</p> <ul> <li>Input vector \(x \in \mathbb{R}^2\), e.g.,</li> </ul> \[x = \begin{bmatrix} 1 \\ 2 \end{bmatrix}\] <ul> <li>Number of classes \(k = 3\)</li> </ul> <p>We define \(\psi(x, i)\) as a vector in \(\mathbb{R}^{2k}\) (i.e., 6 dimensions). It places \(x\) into the block corresponding to class \(i\) and zeros elsewhere.</p> <p>For example, for class \(i = 2\):</p> \[\psi(x, 2) = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 2 \\ 0 \\ 0 \\ \end{bmatrix}\] <p>Let \(w \in \mathbb{R}^6\) (since \(x \in \mathbb{R}^2\) and \(k = 3\)):</p> \[w = \begin{bmatrix} 0.5 \\ -1.0 \\ 0.2 \\ 0.3 \\ -0.4 \\ 1.0 \\ \end{bmatrix}\] <p>To compute the score for class 2:</p> \[h(x, 2) = w^\top \psi(x, 2)\] <p>Only the block for class 2 is active:</p> \[h(x, 2) = [0.2, 0.3]^\top \cdot [1, 2] = 0.2 \cdot 1 + 0.3 \cdot 2 = 0.8\] <p>We can now compute scores for all classes:</p> <ul> <li> <p>Class 1 uses block: \([0.5, -1.0]\)</p> \[h(x, 1) = 0.5 \cdot 1 + (-1.0) \cdot 2 = -1.5\] </li> <li> <p>Class 2: (already computed) \(0.8\)</p> </li> <li> <p>Class 3 uses block: \([-0.4, 1.0]\)</p> \[h(x, 3) = -0.4 \cdot 1 + 1.0 \cdot 2 = 1.6\] </li> </ul> <p>For final prediction, we select the class with the highest score:</p> \[\hat{y} = \arg\max_i h(x, i) = 3\] <p>So, for input \(x = [1, 2]\), the predicted class is <strong>3</strong>.</p> <p>And suppose the true label is:</p> \[y = 2\] <p>Since \(\hat{y} \ne y\), the prediction is incorrect.</p> <p>The classic multiclass Perceptron updates:</p> <ul> <li><strong>Promote</strong> the correct class (add input to the correct class block)</li> <li><strong>Demote</strong> the predicted class (subtract input from the predicted class block)</li> </ul> <p>Using the joint feature map:</p> \[w \leftarrow w + \psi(x, y) - \psi(x, \hat{y})\] <p>In our case:</p> <ul> <li> \[\psi(x, y) = \psi(x, 2) = [0, 0, 1, 2, 0, 0]^\top\] </li> <li> \[\psi(x, \hat{y}) = \psi(x, 3) = [0, 0, 0, 0, 1, 2]^\top\] </li> </ul> <p>Then:</p> \[w_{\text{new}} = w + \psi(x, 2) - \psi(x, 3)\] <p>Apply this update:</p> \[w = \begin{bmatrix} 0.5 \\ -1.0 \\ 0.2 \\ 0.3 \\ -0.4 \\ 1.0 \\ \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \\ 1 \\ 2 \\ 0 \\ 0 \\ \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 2 \\ \end{bmatrix} = \begin{bmatrix} 0.5 \\ -1.0 \\ 1.2 \\ 2.3 \\ -1.4 \\ -1.0 \\ \end{bmatrix}\] <p>This update increases the score for the correct class (2) and decreases the score for the incorrect prediction (3), just like the original multiclass Perceptron but using a single shared weight vector and structured feature representation.</p> <hr/> <h5 id="formalising-via-multivector-construction"><strong>Formalising via Multivector Construction</strong></h5> <p>Consider a simple example where \(x \in \mathbb{R}^2\) and we have 3 classes \(Y = \{1, 2, 3\}\). Suppose we stack the weight vectors for each class together in the following way:</p> \[w = \begin{pmatrix} -\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}, 0, 1, \frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2} \end{pmatrix}^\top\] <p>Now, define the feature map \(\Psi(x, y)\) as follows:</p> <ul> <li> \[\Psi(x, 1) = (x_1, x_2, 0, 0, 0, 0)\] </li> <li> \[\Psi(x, 2) = (0, 0, x_1, x_2, 0, 0)\] </li> <li> \[\Psi(x, 3) = (0, 0, 0, 0, x_1, x_2)\] </li> </ul> <p>The dot product between the weight vector \(w\) and the feature map \(\Psi(x, y)\) is then:</p> \[\langle w, \Psi(x, y) \rangle = \langle w_y, x \rangle\] <p>This approach allows us to represent all classes using a single weight vector, which is more efficient and scalable.</p> <p>With the multivector construction in place, the multiclass perceptron algorithm can be rewritten as follows:</p> <ol> <li><strong>Initialize</strong> the weight vector \(w = 0\).</li> <li>For \(T\) iterations, repeat the following for each training example \((x, y)\): <ul> <li>Predict \(\hat{y} = \arg\max_{y'} w^\top \psi(x, y')\) (choose the class with the highest score).</li> <li>If \(\hat{y} \neq y\): <ul> <li>Update the weight vector: \(w \leftarrow w + \psi(x, y)\).</li> <li>Update the weight vector: \(w \leftarrow w - \psi(x, \hat{y})\).</li> </ul> </li> </ul> </li> </ol> <p>This version of the algorithm is computationally efficient and scales well to large datasets.</p> <p><strong>Question</strong>: What is the <strong>base binary classification problem</strong> in multiclass perceptron?</p> <p><strong>Answer</strong>: At each update step, the multiclass Perceptron reduces to a binary classification problem between the correct class \(y\) and the predicted class \(\hat{y}\). The model must adjust the weights so that \(y\) scores higher than \(\hat{y}\) ‚Äî just like in a binary classification setting where one class must be separated from another.</p> <hr/> <h5 id="feature-engineering-for-multiclass-tasks"><strong>Feature Engineering for Multiclass Tasks</strong></h5> <p>To apply the multivector construction in practice, we need to define meaningful and informative features that capture the relationship between the input and each possible class. This is especially important in structured prediction tasks like <strong>part-of-speech (POS) tagging</strong>.</p> <p>Suppose our input space \(X\) consists of all possible words, and our label space \(Y\) contains the categories {NOUN, VERB, ADJECTIVE, ADVERB, etc.}. Each input word needs to be classified into one of these grammatical categories.</p> <p>We can define features that depend on both the input word and the target label ‚Äî a natural fit for the joint feature map \(\Psi(x, y)\) introduced earlier. For example, some useful features might include:</p> <ul> <li>Whether the word is exactly a specific token (e.g., ‚Äúrun‚Äù, ‚Äúapple‚Äù)</li> <li>Whether the word ends in certain suffixes (e.g., ‚Äúly‚Äù for adverbs)</li> <li>Capitalization or presence of digits (in named entity recognition)</li> </ul> <p>Here are a few sample features written in the multivector style:</p> <ul> <li> \[\psi_1(x, y) = 1[x = \text{apple} \land y = \text{NOUN}]\] </li> <li> \[\psi_2(x, y) = 1[x = \text{run} \land y = \text{NOUN}]\] </li> <li> \[\psi_3(x, y) = 1[x = \text{run} \land y = \text{VERB}]\] </li> <li> \[\psi_4(x, y) = 1[x \text{ ends in } \text{ly} \land y = \text{ADVERB}]\] </li> </ul> <p>Each of these features ‚Äúactivates‚Äù only when both the input word and the predicted class match a certain pattern. This is perfectly aligned with the multivector framework, where the model learns weights for specific combinations of features and labels.</p> <h5 id="feature-templates"><strong>Feature Templates</strong></h5> <p>In real-world applications, especially in natural language processing (NLP), we rarely hand-code features for every word. Instead, we use <strong>feature templates</strong> that automatically generate features from observed patterns.</p> <p><strong>What is a Feature Template?</strong></p> <p>A feature template is a <strong>rule or function</strong> that, given an input and a label, produces one or more binary features of the form \(\psi(x, y)\).</p> <p>Templates help create thousands or even millions of features in a structured and consistent way.</p> <p>Let‚Äôs say we want to predict the POS tag for the word <strong>‚Äúrunning‚Äù</strong> in the sentence:</p> <blockquote> <p>I am <strong>running</strong> late.</p> </blockquote> <p>We might use the following templates:</p> <hr/> <table> <thead> <tr> <th>Template Description</th> <th>Template Rule</th> <th>Example Feature</th> </tr> </thead> <tbody> <tr> <td>Current word</td> <td>\(\psi(x, y) = 1[x = w \land y = y']\)</td> <td>\(x = \text{"running"}, y = \text{VERB}\)</td> </tr> <tr> <td>Word suffix (3 chars)</td> <td>\(\psi(x, y) = 1[x[-3:] = s \land y = y']\)</td> <td>\(x[-3:] = \text{"ing"}, y = \text{VERB}\)</td> </tr> <tr> <td>Previous word is ‚Äúam‚Äù</td> <td>\(\psi(x, y) = 1[\text{prev}(x) = \text{"am"} \land y = y']\)</td> <td>\(y = \text{VERB}\)</td> </tr> <tr> <td>Is capitalized</td> <td>\(\psi(x, y) = 1[x[0].\text{isupper()} \land y = y']\)</td> <td>‚Äî</td> </tr> <tr> <td>Prefix (first 2 letters)</td> <td>\(\psi(x, y) = 1[x[:2] = p \land y = y']\)</td> <td>\(x[:2] = \text{"ru"}, y = \text{VERB}\)</td> </tr> </tbody> </table> <hr/> <p>Each of these templates would produce many feature instances across a dataset ‚Äî and each instance activates only when the corresponding condition holds.</p> <p><strong>Integration with the Model</strong></p> <p>In the multivector model, we don‚Äôt store a giant feature matrix explicitly. Instead, we treat each <strong>feature-label pair</strong> \(\psi(x, y)\) as a key that can be mapped to an <strong>index</strong> in a long feature vector. This is done using either:</p> <ul> <li>A <strong>dictionary lookup</strong>, if we predefine all feature-label pairs, or</li> <li>A <strong>hash function</strong>, if we want to compute the index on the fly (common in online or large-scale settings)</li> </ul> <p><strong>Why is this needed?</strong></p> <p>When \(\psi(x, y)\) is represented as a very large sparse vector (e.g. size 100,000+), we don‚Äôt want to store all zeros. So instead, we store only the <strong>nonzero features</strong> ‚Äî each one identified by its <strong>feature name and associated label</strong>.</p> <p>Say we define a feature template:</p> <ul> <li>‚ÄúDoes the word end with ‚Äòing‚Äô?‚Äù</li> </ul> <p>Then for the input word <strong>‚Äúrunning‚Äù</strong>, and possible labels:</p> <ul> <li> \[\psi(x = \text{running}, y = \text{VERB}) = 1[\text{suffix} = ing \land y = \text{VERB}]\] </li> <li> \[\psi(x = \text{running}, y = \text{NOUN}) = 1[\text{suffix} = ing \land y = \text{NOUN}]\] </li> </ul> <p>These are <strong>two different features</strong>, because they are tied to different labels.</p> <p>We can assign an index to each:</p> <table> <thead> <tr> <th>Feature Name</th> <th>Label</th> <th>Combined Key</th> <th>Index</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">suffix=ing</code></td> <td>VERB</td> <td><code class="language-plaintext highlighter-rouge">suffix=ing_VERB</code></td> <td>1921</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">suffix=ing</code></td> <td>NOUN</td> <td><code class="language-plaintext highlighter-rouge">suffix=ing_NOUN</code></td> <td>2390</td> </tr> </tbody> </table> <p>So \(\psi(x, y)\) is implemented as:</p> <ul> <li>A vector of size (say) 50,000,</li> <li>With a single non-zero at position 1921 or 2390, depending on the label,</li> <li>And the model‚Äôs weight vector \(w\) has learned weights at those positions.</li> <li> <p>During prediction, we compute:</p> \[\hat{y} = \arg\max_{y'} w^\top \psi(x, y')\] </li> </ul> <p>This is how the model can <strong>distinguish between ‚Äúing‚Äù being a verb signal vs a noun signal</strong>, just by associating label-specific versions of the feature. And this feature-to-index mapping is what makes it possible to use linear classifiers with sparse high-dimensional features efficiently.</p> <p><strong>So, Why Feature Templates Matter?</strong></p> <ul> <li>They <strong>automate</strong> feature construction and ensure consistency across training and test data.</li> <li>They <strong>generalize well</strong> ‚Äî e.g., instead of memorizing that ‚Äúrunning‚Äù is a verb, a suffix-based feature can generalize that any word ending in ‚Äúing‚Äù is likely a verb.</li> <li>They are <strong>language-agnostic</strong> to some extent ‚Äî and can be extended to other structured tasks like NER, chunking, or even machine translation.</li> </ul> <p>This feature-based view, combined with the multivector construction, gives us a powerful and scalable way to build multiclass classifiers, especially in domains like NLP where feature engineering plays a key role.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>We covered how multiclass classification can be tackled using multiclass loss and perceptron algorithms. We highlighted the importance of feature engineering, specifically through feature templates, which help automatically create relevant features for each class. This approach enables efficient, scalable models, especially in tasks like POS tagging. By mapping feature-label pairs to indices, we can handle large datasets without excessive memory usage.</p> <p>Having seen how to generalize the perceptron algorithm, we‚Äôll now move on to explore how <strong>Support Vector Machines (SVMs)</strong> can be extended to handle multiclass classification. Stay tuned and Take care!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.]]></summary></entry><entry><title type="html">Multiclass Classification - Overview</title><link href="https://monishver11.github.io/blog/2025/multiclass/" rel="alternate" type="text/html" title="Multiclass Classification - Overview"/><published>2025-02-23T02:15:00+00:00</published><updated>2025-02-23T02:15:00+00:00</updated><id>https://monishver11.github.io/blog/2025/multiclass</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/multiclass/"><![CDATA[<h4 id="motivation"><strong>Motivation</strong></h4> <p>So far, most of the classification algorithms we have encountered focus on <strong>binary classification</strong>, where the goal is to distinguish between two classes. For instance, sentiment analysis classifies text as either <strong>positive or negative</strong>, while spam filters differentiate between <strong>spam and non-spam</strong> emails.</p> <p>However, real-world problems often involve more than two categories, making binary classification insufficient. <strong>Document classification</strong> may require labeling articles into over <strong>ten</strong> categories, <strong>object recognition</strong> must identify objects among <strong>thousands of classes</strong>, and <strong>face recognition</strong> must distinguish between <strong>millions</strong> of individuals.</p> <p>As the number of classes grows, several challenges arise. <strong>Computation cost</strong> increases significantly, especially when training separate models for each class. Additionally, some classes might have far fewer examples than others, leading to <strong>class imbalance</strong> issues. Finally, in some cases, <strong>different types of errors</strong> have varying consequences‚Äîfor example, misidentifying a handwritten ‚Äú3‚Äù as an ‚Äú8‚Äù might be less problematic than confusing a stop sign with a yield sign in an autonomous driving system.</p> <p>Given these challenges, we need effective strategies to extend binary classification techniques to handle multiple classes efficiently.</p> <hr/> <h4 id="reducing-multiclass-to-binary-classification"><strong>Reducing Multiclass to Binary Classification</strong></h4> <p>A natural way to approach multiclass classification is by reducing it to <strong>multiple binary classification problems</strong>. One naive approach is to represent each class using a unique binary code and train a classifier to predict the code (000, 001, 010). Another approach is to apply <strong>regression</strong>, where each class is assigned a numerical value (1.0, 2.0, 3.0), and the model predicts a continuous number that is rounded to the nearest class. However, these methods often fail in practice due to poor generalization.</p> <p>Instead, two well-established techniques are commonly used:</p> <ol> <li><strong>One-vs-All (OvA), also called One-vs-Rest</strong></li> <li><strong>All-vs-All (AvA), also known as One-vs-One</strong></li> </ol> <p>These methods decompose the problem into smaller binary classification tasks while ensuring that the final model can still distinguish between all classes.</p> <hr/> <h4 id="one-vs-all-ova"><strong>One-vs-All (OvA)</strong></h4> <p>The <strong>One-vs-All</strong> (OvA) approach works by training <strong>one binary classifier per class</strong>. Each classifier is responsible for distinguishing a single class from all the others.</p> <h5 id="training"><strong>Training</strong></h5> <p>Given a dataset with \(k\) classes, we train \(k\) separate classifiers:</p> <ul> <li>Each classifier \(h_i\) learns to recognize class \(i\) as <strong>positive (+1)</strong> while treating all other classes as <strong>negative (-1)</strong>.</li> <li>Formally, each classifier is a function \(h_i: X \to \mathbb{R}\), where a higher score indicates a higher likelihood of belonging to class \(i\).</li> </ul> <h5 id="prediction"><strong>Prediction</strong></h5> <p>When a new input \(x\) is given, we evaluate all \(k\) classifiers and select the class with the highest score:</p> \[h(x) = \arg\max_{i \in \{1, \dots, k\}} h_i(x)\] <p>If multiple classifiers output the same score, we can resolve ties arbitrarily.</p> <h5 id="example-3-class-problem"><strong>Example: 3-Class Problem</strong></h5> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multiclass-1-480.webp 480w,/assets/img/Multiclass-1-800.webp 800w,/assets/img/Multiclass-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multiclass-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multiclass-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Consider a classification task with three categories: <strong>cats, dogs, and rabbits</strong>. Using OvA, we train three classifiers:</p> <ol> <li>A classifier that distinguishes <strong>cats</strong> from <strong>dogs and rabbits</strong>.</li> <li>A classifier that distinguishes <strong>dogs</strong> from <strong>cats and rabbits</strong>.</li> <li>A classifier that distinguishes <strong>rabbits</strong> from <strong>cats and dogs</strong>.</li> </ol> <p>At test time, each classifier produces a score, and the class with the highest score is selected.</p> <p>However, this method has some limitations. If the data is <strong>not linearly separable</strong>, the decision boundaries can become ambiguous. Additionally, if one class has far fewer examples than others, the classifier for that class might be undertrained, leading to <strong>class imbalance issues</strong>.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multiclass-2-480.webp 480w,/assets/img/Multiclass-2-800.webp 800w,/assets/img/Multiclass-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multiclass-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multiclass-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h4 id="all-vs-all-ava"><strong>All-vs-All (AvA)</strong></h4> <p>A more refined approach is <strong>All-vs-All (AvA)</strong>, also known as <strong>One-vs-One</strong>. Instead of training one classifier per class, we train a <strong>separate classifier for each pair of classes</strong>.</p> <h5 id="training-1"><strong>Training</strong></h5> <p>For a dataset with \(k\) classes, we train \(\frac{k(k-1)}{2}\) binary classifiers:</p> <ul> <li>Each classifier \(h_{ij}\) is trained to distinguish class \(i\) from class \(j\).</li> <li>Formally, each classifier \(h_{ij}: X \to \mathbb{R}\) outputs a score, where a positive value indicates class \(i\), and a negative value indicates class \(j\).</li> </ul> <h5 id="prediction-1"><strong>Prediction</strong></h5> <p>At test time, each classifier makes a decision between its assigned two classes. Each class receives a <strong>vote</strong> based on the number of times it wins in pairwise comparisons. The final prediction is the class with the most votes:</p> \[h(x) = \arg\max_{i \in \{1, \dots, k\}} \sum_{j \ne i} [ \underbrace{h_{ij}(x)\mathbb{I}\{i &lt; j\}}_{\text{class } i \text{ is } +1} - \underbrace{h_{ji}(x)\mathbb{I}\{j &lt; i\}}_{\text{class } i \text{ is } -1} ]\] <p>Again, in scenarios where multiple classes receive the same number of votes, a tournament-style approach can be used to break ties. Here, classes compete in a series of pairwise matchups, and the winner of each round advances until only one class remains‚Äîthe final prediction.</p> <h5 id="example-4-class-problem"><strong>Example: 4-Class Problem</strong></h5> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multiclass-3-480.webp 480w,/assets/img/Multiclass-3-800.webp 800w,/assets/img/Multiclass-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multiclass-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multiclass-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Suppose we have four classes. Using AvA, we train six classifiers:</p> <ol> <li>A classifier for <strong>cats vs. dogs</strong></li> <li>A classifier for <strong>cats vs. rabbits</strong></li> <li>A classifier for <strong>cats vs. birds</strong></li> <li>A classifier for <strong>dogs vs. rabbits</strong></li> <li>A classifier for <strong>dogs vs. birds</strong></li> <li>A classifier for <strong>rabbits vs. birds</strong></li> </ol> <p>At test time, each classifier votes for a class, and the class with the most votes is chosen as the final prediction.</p> <p>This method has <strong>better decision boundaries</strong> than OvA because it learns finer distinctions between pairs of classes. However, it is <strong>computationally expensive</strong> for large \(k\), as the number of classifiers grows quadratically.</p> <hr/> <h4 id="ova-vs-ava-trade-offs"><strong>OvA vs. AvA: Trade-offs</strong></h4> <p>Both approaches have their own advantages and limitations. In general:</p> <ul> <li><strong>OvA is simpler</strong> and requires fewer models, but it suffers from <strong>class imbalance issues</strong>.</li> <li><strong>AvA provides better decision boundaries</strong> but is <strong>computationally expensive</strong>, especially for large numbers of classes.</li> </ul> <p>The following table summarizes the computational complexity of both methods:</p> <hr/> <table> <thead> <tr> <th>¬†</th> <th><strong>OvA</strong></th> <th><strong>AvA</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Training Complexity</strong></td> <td>\(O(k B_{\text{train}}(n))\)</td> <td>\(O(k^2 B_{\text{train}}(n/k))\)</td> </tr> <tr> <td><strong>Testing Complexity</strong></td> <td>\(O(k B_{\text{test}})\)</td> <td>\(O(k^2 B_{\text{test}})\)</td> </tr> <tr> <td><strong>Challenges</strong></td> <td>Class imbalance, poor calibration</td> <td>Small training sets, tie-breaking</td> </tr> </tbody> </table> <hr/> <ul> <li> <p><strong>\(k\)</strong>: The <strong>number of classes</strong> in the multiclass classification problem.</p> </li> <li> <p><strong>\(B_{\text{train}}(n)\)</strong>: The <strong>computational cost of training</strong> a binary classifier on <strong>\(n\)</strong> training examples.</p> </li> <li> <p><strong>\(B_{\text{test}}\)</strong>: The <strong>computational cost of testing</strong> a single input using one binary classifier.</p> </li> <li>For <strong>One-vs-All (OvA)</strong>: <ul> <li><strong>Training</strong>: <ul> <li>Train \(k\) classifiers, each on the <strong>entire dataset</strong> with \(n\) examples.</li> <li><strong>Total training cost</strong>: \(O(k \cdot B_{\text{train}}(n))\)</li> </ul> </li> <li><strong>Testing</strong>: <ul> <li>For each new input, all \(k\) classifiers are evaluated.</li> <li><strong>Total testing cost per input</strong>: \(O(k \cdot B_{\text{test}})\)</li> </ul> </li> </ul> </li> <li>For <strong>All-vs-All (AvA)</strong>: <ul> <li><strong>Training</strong>: <ul> <li>Train \(\frac{k(k-1)}{2} \approx O(k^2)\) classifiers.</li> <li>Each classifier uses data from only <strong>two classes</strong>, approximately \(n/k\) examples.</li> <li><strong>Total training cost</strong>: \(O(k^2 \cdot B_{\text{train}}(n/k))\)</li> </ul> </li> <li><strong>Testing</strong>: <ul> <li>For each new input, all \(O(k^2)\) classifiers are evaluated.</li> <li><strong>Total testing cost per input</strong>: \(O(k^2 \cdot B_{\text{test}})\)</li> </ul> </li> </ul> </li> </ul> <p>While these reduction-based approaches work well for <strong>small numbers of classes</strong>, they become impractical when scaling to large datasets.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>We explored two fundamental approaches for handling multiclass classification by <strong>reducing it to multiple binary classification problems</strong>:</p> <ul> <li><strong>One-vs-All (OvA)</strong>: Train one classifier per class.</li> <li><strong>All-vs-All (AvA)</strong>: Train one classifier per pair of classes.</li> </ul> <p>Although these methods are simple and effective for small datasets, they become computationally expensive as the number of classes grows. For example, <strong>ImageNet contains over 20,000 categories, and Wikipedia has over 1 million topics</strong>, making reduction-based methods infeasible.</p> <h5 id="whats-next"><strong>What‚Äôs Next?</strong></h5> <p>To overcome these challenges, we need classification algorithms that <strong>directly generalize binary classification to multiple classes</strong> without breaking the problem into smaller binary tasks. In the next post, we‚Äôll explore these approaches and how they scale efficiently to large datasets. Stay tuned and See you! üëã</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.]]></summary></entry><entry><title type="html">Gaussian Regression - A Bayesian Approach to Linear Regression</title><link href="https://monishver11.github.io/blog/2025/gaussian-regression/" rel="alternate" type="text/html" title="Gaussian Regression - A Bayesian Approach to Linear Regression"/><published>2025-02-08T16:59:00+00:00</published><updated>2025-02-08T16:59:00+00:00</updated><id>https://monishver11.github.io/blog/2025/gaussian-regression</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/gaussian-regression/"><![CDATA[<p>Gaussian regression provides a <strong>probabilistic perspective</strong> on linear regression, enabling us to <strong>model uncertainty</strong> in our predictions. Unlike traditional regression techniques that provide only point estimates, Gaussian regression models the <strong>entire distribution</strong> over possible predictions.</p> <p>In this post, we will explore Gaussian regression through a <strong>step-by-step example in one dimension</strong>, gradually building intuition before moving to the <strong>Bayesian posterior update</strong> with multiple observations.</p> <hr/> <h5 id="gaussian-regression-in-one-dimension-problem-setup"><strong>Gaussian Regression in One Dimension: Problem Setup</strong></h5> <p>To understand Gaussian regression, let‚Äôs consider a simple <strong>one-dimensional regression task</strong>. Suppose our <strong>input space</strong> is:</p> \[X = [-1,1]\] <p>and the <strong>output space</strong> is:</p> \[Y \subseteq \mathbb{R}\] <p>We assume that for a given input \(x\), the corresponding output \(y\) is generated according to a <strong>linear model</strong> with additive Gaussian noise:</p> \[y = w_0 + w_1 x + \varepsilon, \quad \text{where} \quad \varepsilon \sim \mathcal{N}(0, 0.2^2)\] <p>In other words, the output is a <strong>linear function</strong> of \(x\) with coefficients \((w_0, w_1)\) and <strong>Gaussian noise</strong> \(\varepsilon\) with variance \(0.2^2\).</p> <p>Alternatively, we can express this in <strong>probabilistic form</strong> as:</p> \[y \mid x, w_0, w_1 \sim \mathcal{N}(w_0 + w_1 x, 0.2^2)\] <p>where the mean is given by the <strong>linear function</strong> \(w_0 + w_1 x\), and the variance captures the <strong>uncertainty in our observations</strong>.</p> <p>Now, the question is: <strong>how do we model our belief about the parameters \(w_0\) and \(w_1\)?</strong></p> <hr/> <h5 id="prior-distribution-over-parameters"><strong>Prior Distribution Over Parameters</strong></h5> <p>Before observing any data, we assume a <strong>prior belief</strong> about the parameters \(w = (w_0, w_1)\). A common choice is to model the parameters as a <strong>zero-mean Gaussian distribution</strong>:</p> \[w = (w_0, w_1) \sim \mathcal{N}(0, \frac{1}{2} I)\] <p>This prior reflects our initial assumption that \(w_0\) and \(w_1\) are likely to be small, centered around zero, with <strong>independent Gaussian uncertainty</strong>.</p> <h5 id="visualizing-the-prior"><strong>Visualizing the Prior</strong></h5> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Guassian_Regression-1-480.webp 480w,/assets/img/Guassian_Regression-1-800.webp 800w,/assets/img/Guassian_Regression-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Guassian_Regression-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Guassian_Regression-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The <strong>left plot</strong> below illustrates samples drawn from the prior distribution over \(w\).</li> <li>The <strong>right plot</strong> shows <strong>random linear functions</strong> \(y(x) = E[y \mid x, w] = w_0 + w_1 x\) drawn from this prior distribution \((w ‚àº p(w))\).</li> </ul> <p>Since no observations have been made yet, these functions represent <strong>potential hypotheses</strong> about the true relationship between \(x\) and \(y\).</p> <hr/> <h5 id="updating-beliefs-posterior-after-one-observation"><strong>Updating Beliefs: Posterior After One Observation</strong></h5> <p>Once we collect data, we can update our belief about \(w\) using <strong>Bayes‚Äô theorem</strong>. Suppose we observe a <strong>single training point</strong> \((x_1, y_1)\).</p> <p>This leads to a <strong>posterior distribution</strong> over \(w\), incorporating the evidence from our observation.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Guassian_Regression-2-480.webp 480w,/assets/img/Guassian_Regression-2-800.webp 800w,/assets/img/Guassian_Regression-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Guassian_Regression-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Guassian_Regression-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The <strong>left plot</strong> below shows the updated posterior distribution over \(w\), where the <strong>white cross</strong> represents the true underlying parameters.</li> <li>The <strong>right plot</strong> now shows <strong>updated predictions</strong> based on this posterior. Blue circle indicates the training observation. The red lines represent sampled functions \(y(x) = E[y \mid x, w]\) drawn from the <strong>posterior</strong> \((w ‚àº p(w \vert D) )\) instead of the prior.</li> </ul> <h5 id="key-observations-after-one-data-point"><strong>Key Observations After One Data Point</strong></h5> <ol> <li>The <strong>posterior distribution is more concentrated</strong>, reflecting reduced uncertainty about \(w\).</li> <li>Predictions near the observed point \(x_1\) are more certain, while uncertainty remains high elsewhere.</li> </ol> <hr/> <h5 id="adding-more-observations-improved-predictions"><strong>Adding More Observations: Improved Predictions</strong></h5> <p>Let‚Äôs now extend this idea by considering multiple observations.</p> <p><strong>Posterior After Two Observations</strong></p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Guassian_Regression-3-480.webp 480w,/assets/img/Guassian_Regression-3-800.webp 800w,/assets/img/Guassian_Regression-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Guassian_Regression-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Guassian_Regression-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>With two training points, the posterior further <strong>shrinks</strong>, indicating <strong>increased confidence</strong> in our estimate of \(w\).</li> <li>The red curves on the right become more <strong>aligned</strong>, meaning the model has more confidence in its predictions.</li> </ul> <p><strong>Posterior After Twenty Observations</strong></p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Guassian_Regression-4-480.webp 480w,/assets/img/Guassian_Regression-4-800.webp 800w,/assets/img/Guassian_Regression-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Guassian_Regression-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Guassian_Regression-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>When we have <strong>twenty observations</strong>, our uncertainty about \(w\) significantly decreases.</li> <li>The predicted functions become <strong>highly concentrated</strong>, meaning the model has effectively <strong>learned the underlying relationship</strong> between \(x\) and \(y\).</li> </ul> <p>This progression illustrates a fundamental principle in Bayesian regression: <strong>as we collect more data, the posterior distribution sharpens, leading to more confident predictions.</strong></p> <hr/> <p>So, Gaussian regression provides a <strong>Bayesian approach to linear regression</strong>, allowing us to <strong>incorporate prior knowledge</strong> and <strong>update beliefs</strong> as new data arrives.</p> <p><strong>Key takeaways:</strong></p> <ol> <li>We started with a <strong>prior distribution</strong> over parameters \(w\).</li> <li>We incorporated <strong>one observation</strong>, updating our belief using the <strong>posterior distribution</strong>.</li> <li>As we collected <strong>more data</strong>, our predictions became <strong>more confident</strong>, reducing uncertainty in our model.</li> </ol> <p>This framework not only provides <strong>point predictions</strong> but also <strong>quantifies uncertainty</strong>, making it particularly useful for applications where knowing confidence levels is crucial.</p> <p>Next up, we will derive the <strong>closed-form posterior distribution</strong> for Gaussian regression and explore its connection to <strong>ridge regression</strong>.</p> <hr/> <h4 id="deriving-the-general-posterior-distribution-in-gaussian-regression"><strong>Deriving the General Posterior Distribution in Gaussian Regression</strong></h4> <p>We begin by defining the prior belief about our parameters. Suppose we have a <strong>prior distribution</strong> over the weights:</p> \[w \sim \mathcal{N}(0, \Sigma_0)\] <p>where \(\Sigma_0\) represents the prior covariance matrix, encoding our initial uncertainty about \(w\). Given an <strong>observed dataset</strong> \(D = \{(x_i, y_i)\}_{i=1}^{n}\), we assume the following <strong>likelihood model</strong>:</p> \[y_i \mid x_i, w \sim \mathcal{N}(w^T x_i, \sigma^2)\] <p>where:</p> <ul> <li>\(X\) is the <strong>design matrix</strong>, consisting of all input feature vectors \(x_i\),</li> <li>\(y\) is the <strong>response column vector</strong>, representing the observed outputs,</li> <li>\(\sigma^2\) represents the <strong>variance of the noise</strong> in the observations.</li> </ul> <p>Using <strong>Bayes‚Äô theorem</strong>, we update our belief about \(w\) after observing data \(D\), leading to a <strong>Gaussian posterior distribution</strong>:</p> \[w \mid D \sim \mathcal{N}(\mu_P, \Sigma_P)\] <p>where the posterior mean \(\mu_P\) and covariance \(\Sigma_P\) are given by:</p> \[\mu_P = \left( X^T X + \sigma^2 \Sigma_0^{-1} \right)^{-1} X^T y\] \[\Sigma_P = \left( \sigma^{-2} X^T X + \Sigma_0^{-1} \right)^{-1}\] <p>The posterior mean \(\mu_P\) provides the <strong>best estimate</strong> of \(w\) given the data, while the posterior covariance \(\Sigma_P\) captures the <strong>uncertainty</strong> in our estimation.</p> <hr/> <h5 id="maximum-a-posteriori-map-estimation-and-its-connection-to-ridge-regression"><strong>Maximum A Posteriori (MAP) Estimation and Its Connection to Ridge Regression</strong></h5> <p>While the posterior distribution fully describes the uncertainty in \(w\), in practice, we often seek a <strong>point estimate</strong>. The <strong>MAP (Maximum A Posteriori) estimate</strong> of \(w\) is simply the <strong>posterior mean</strong>:</p> \[\hat{w} = \mu_P = \left( X^T X + \sigma^2 \Sigma_0^{-1} \right)^{-1} X^T y\] <p>Now, let‚Äôs assume an <strong>isotropic prior</strong> variance:</p> \[\Sigma_0 = \frac{\sigma^2}{\lambda} I\] <p>Plugging this into the MAP estimate simplifies it to:</p> \[\hat{w} = \left( X^T X + \lambda I \right)^{-1} X^T y\] <p>which is precisely the <strong>ridge regression solution</strong>.</p> <p>Thus, ridge regression can be interpreted as a <strong>Bayesian approach</strong> where we assume a Gaussian prior on the weights with variance controlled by \(\lambda\). The larger \(\lambda\), the stronger our prior belief that \(w\) should remain small, leading to <strong>regularization</strong>.</p> <p>This is because the isotropic Gaussian prior on \(w\) has variance \(\frac{\sigma^2}{\lambda}\). As \(\lambda\) increases, the prior variance decreases, implying a stronger belief that the weights \(w\) are closer to zero (more regularization). This results in a simpler model with smaller coefficients, reducing overfitting by penalizing large weights.</p> <hr/> <h5 id="understanding-the-posterior-density-and-ridge-regression"><strong>Understanding the Posterior Density and Ridge Regression</strong></h5> <p>To further illustrate the connection between MAP estimation and ridge regression, let‚Äôs look at the <strong>posterior density</strong> of \(w\) under the prior assumption \(\Sigma_0 = \frac{\sigma^2}{\lambda} I\):</p> \[p(w \mid D) \propto \underbrace{\exp \left( - \frac{\lambda}{2 \sigma^2} \|w\|^2 \right)}_{\text{Prior}} \underbrace{\prod_{i=1}^{n} \exp \left( - \frac{(y_i - w^T x_i)^2}{2 \sigma^2} \right)}_{\text{Likelihood}}\] <p>To find the <strong>MAP estimate</strong>, we minimize the <strong>negative log-posterior</strong>:</p> \[\hat{w}_{\text{MAP}} = \arg\min_{w \in \mathbb{R}^d} \left[ -\log p(w \mid D) \right]\] \[\hat{w}_{\text{MAP}} = \arg\min_{w \in \mathbb{R}^d} \left[ \underbrace{\sum_{i=1}^{n} (y_i - w^T x_i)^2}_{\text{Log Likelihood}} + \underbrace{\lambda \|w\|^2}_{\text{Log Prior}} \right]\] <p>This objective function is exactly the <strong>ridge regression loss function</strong>, which balances the <strong>data likelihood</strong> (sum of squared errors) with a <strong>penalty on the weight magnitude</strong>.</p> <p>Thus, we see that <strong>MAP estimation in Bayesian regression is equivalent to ridge regression in frequentist statistics</strong>.</p> <hr/> <h5 id="predictive-posterior-distribution-and-uncertainty-quantification"><strong>Predictive Posterior Distribution and Uncertainty Quantification</strong></h5> <p>Now that we have obtained the posterior distribution of \(w\), we can compute the <strong>predictive distribution</strong> for a <strong>new input point</strong> \(x_{\text{new}}\). Instead of providing just a single point estimate, Bayesian regression gives a <strong>distribution over predictions</strong>, reflecting our confidence.</p> <p>The predictive distribution is given by:</p> \[p(y_{\text{new}} \mid x_{\text{new}}, D) = \int p(y_{\text{new}} \mid x_{\text{new}}, w) p(w \mid D) dw\] <p>Averages over prediction for each \(w\), weighted by posterior distribution.</p> <hr/> <p>The equation represents the <strong>predictive distribution</strong> for a new observation \(y_{\text{new}}\) given a new input \(x_{\text{new}}\) and the data \(D\).</p> <p>It combines:</p> <ol> <li> <p><strong>Likelihood</strong> \(p(y_{\text{new}} \mid x_{\text{new}}, w)\): This term describes how likely the new output \(y_{\text{new}}\) is for a given set of model weights \(w\) and the new input \(x_{\text{new}}\).</p> </li> <li> <p><strong>Posterior</strong> \(p(w \mid D)\): This is the distribution of the model weights \(w\) after observing the data \(D\), representing the uncertainty in the weights.</p> </li> </ol> <p>The integral averages the likelihood of the new observation over all possible values of the model parameters \(w\), weighted by the posterior distribution \(p(w \mid D)\), because we don‚Äôt know the exact value of \(w\) ‚Äî it can vary based on the data \(D\).</p> <p>Here‚Äôs why this happens:</p> <ol> <li> <p><strong>Uncertainty in the weights</strong>: In Bayesian regression, we have uncertainty about the model parameters (weights) \(w\), and the posterior distribution \(p(w \mid D)\) reflects our belief about the possible values of \(w\), given the observed data \(D\).</p> </li> <li> <p><strong>Prediction is uncertain</strong>: For a new input \(x_{\text{new}}\), the prediction \(y_{\text{new}}\) depends on the model parameters \(w\), and since we have uncertainty in \(w\), we can‚Äôt give a single prediction. Instead, we need to compute the distribution over all possible values of \(y_{\text{new}}\) corresponding to all possible values of \(w\).</p> </li> <li> <p><strong>Integrating over the posterior</strong>: The integral averages over all possible values of \(w\) because we want to account for the uncertainty in \(w\), which is captured by the posterior distribution \(p(w \mid D)\). Each value of \(w\) contributes to the prediction \(y_{\text{new}}\) according to its likelihood, and the posterior distribution tells us how probable each \(w\) is. By integrating, we are essentially weighing each possible prediction by how likely the corresponding weight \(w\) is under the posterior distribution.</p> </li> </ol> <p>Thus, the integral provides the <strong>expected prediction</strong> by combining the likelihood of the new data with the uncertainty about the model parameters. This results in a distribution for the prediction, reflecting both the model‚Äôs uncertainty and the uncertainty in the new data.</p> <hr/> <p>Since both the likelihood and posterior are Gaussian, the predictive distribution is also Gaussian:</p> \[y_{\text{new}} \mid x_{\text{new}}, D \sim \mathcal{N} (\eta_{\text{new}}, \sigma_{\text{new}}^2)\] <p>where:</p> \[\eta_{\text{new}} = \mu_P^T x_{\text{new}}\] \[\sigma_{\text{new}}^2 = x_{\text{new}}^T \Sigma_P x_{\text{new}} + \sigma^2\] <p>The predictive variance \(\sigma_{\text{new}}^2\) consists of two terms:</p> <ol> <li>\(x_{\text{new}}^T \Sigma_P x_{\text{new}}\) ‚Äì <strong>Uncertainty due to finite data</strong>, representing how much we should trust our estimate of \(w\) (or simply uncertainty from the variance of \(w\)).</li> <li>\(\sigma^2\) ‚Äì <strong>Inherent observation noise</strong>, representing the irreducible error in predicting \(y\).</li> </ol> <p>This decomposition highlights how <strong>Bayesian regression naturally incorporates uncertainty in predictions</strong>.</p> <hr/> <h5 id="why-bayesian-regression-is-powerful"><strong>Why Bayesian Regression is Powerful</strong></h5> <p>One major advantage of Bayesian regression over traditional point estimation methods is that it <strong>provides uncertainty estimates</strong>. So, With predictive distributions, we can give mean prediction with error bands.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Guassian_Regression-5-480.webp 480w,/assets/img/Guassian_Regression-5-800.webp 800w,/assets/img/Guassian_Regression-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Guassian_Regression-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Guassian_Regression-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This is particularly useful in applications where knowing <strong>confidence intervals</strong> is crucial, such as:</p> <ul> <li><strong>Medical diagnostics</strong>, where uncertainty in predictions affects decision-making.</li> <li><strong>Financial modeling</strong>, where risk estimation is key.</li> <li><strong>Autonomous systems</strong>, where knowing when to be uncertain improves safety.</li> </ul> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>In this post, we derived the <strong>posterior distribution</strong> for Gaussian regression, connected <strong>MAP estimation to ridge regression</strong>, and explored the <strong>predictive posterior distribution</strong>, which quantifies uncertainty.</p> <p>Bayesian regression provides a <strong>principled way</strong> to incorporate prior beliefs and quantify uncertainty, making it an essential tool in machine learning.</p> <p>Next, we‚Äôll take a brief pause from probabilistic machine learning methods, as we‚Äôve covered the essentials thoroughly, and shift our focus to a new topic: <strong>Multiclass Classification</strong>. Stay tuned, and keep learning and growing!üöÄ</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.]]></summary></entry><entry><title type="html">My Understanding of ‚ÄúEfficient Algorithms for Online Decision Problems‚Äù Paper</title><link href="https://monishver11.github.io/blog/2025/FPL-proof/" rel="alternate" type="text/html" title="My Understanding of ‚ÄúEfficient Algorithms for Online Decision Problems‚Äù Paper"/><published>2025-02-05T04:08:00+00:00</published><updated>2025-02-05T04:08:00+00:00</updated><id>https://monishver11.github.io/blog/2025/FPL-proof</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/FPL-proof/"><![CDATA[<p>Here is the link to the paper - <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/2005-Efficient_Algorithms_for_Online_Decision_Problems.pdf">Efficient Algorithms for Online Decision Problems</a>. I highly recommend reading through the entire paper once or twice before continuing with this blog.</p> <h4 id="1-introduction"><strong>1. Introduction</strong></h4> <p>This paper explores <strong>online decision problems</strong>, where decisions must be made sequentially without knowing future costs. The goal is to ensure that the total cost incurred is <strong>close to the best fixed decision in hindsight</strong>.</p> <h5 id="key-contributions"><strong>Key Contributions:</strong></h5> <ul> <li>Introducing <strong>Follow the Perturbed Leader (FPL)</strong>, a computationally efficient alternative to <strong>exponential weighting</strong> methods like Weighted Majority.</li> <li>Extending FPL to <strong>structured problems</strong> (e.g., <strong>shortest paths</strong>, <strong>tree-based search</strong>, <strong>adaptive Huffman coding</strong>).</li> <li>Demonstrating that FPL achieves <strong>low regret</strong> while remaining computationally efficient.</li> <li>Showing how FPL can be applied <strong>even when exact offline solutions are infeasible</strong>, by leveraging approximation algorithms.</li> </ul> <h5 id="2-online-decision-problem-setup"><strong>2. Online Decision Problem Setup</strong></h5> <p><strong>Problem Definition</strong></p> <ul> <li>At each time step \(t\), a <strong>cost vector</strong> \(s_t\) is revealed.</li> <li>The algorithm picks a <strong>decision \(d_t\)</strong> from a set \(D\).</li> <li> <p>The objective is to minimize the cumulative cost: \(\sum_{t=1}^{T} d_t \cdot s_t\) compared to the <strong>best single decision in hindsight</strong>:</p> \[\min_{d \in D} \sum_{t=1}^{T} d \cdot s_t.\] </li> </ul> <p><strong>Example: The Experts Problem</strong></p> <ul> <li>Suppose there are <strong>\(n\) experts</strong> providing recommendations.</li> <li>Each expert incurs a cost at each time step.</li> <li>The goal is to perform <strong>as well as the best expert</strong>.</li> </ul> <p>Traditional solutions use <strong>exponential weighting</strong> but are computationally expensive. <strong>FPL</strong> provides a <strong>simpler and faster</strong> alternative.</p> <h5 id="3-follow-the-perturbed-leader-fpl-algorithm"><strong>3. Follow the Perturbed Leader (FPL) Algorithm</strong></h5> <p><strong>Key Idea</strong></p> <ul> <li>Instead of following the exact best decision so far (<strong>Follow the Leader, FTL</strong>), which can lead to excessive switching, we introduce <strong>random perturbations</strong>.</li> <li>This smooths decision-making and prevents adversarial manipulation.</li> </ul> <p><strong>Algorithm</strong></p> <ol> <li>Compute cumulative costs for each decision: \(c_t(e) = \sum_{\tau=1}^{t} s_\tau(e)\)</li> <li>Add <strong>random perturbation</strong> \(p_t(e)\) drawn from an exponential distribution: \(\tilde{c}_t(e) = c_t(e) + p_t(e).\)</li> <li>Choose the decision <strong>with the lowest perturbed cost</strong>.</li> </ol> <p><strong>Why Does This Work?</strong></p> <ul> <li>Reduces <strong>frequent switching</strong> of decisions.</li> <li>Makes the algorithm <strong>less predictable</strong> to adversarial environments.</li> <li>Maintains a <strong>low regret bound</strong>.</li> </ul> <h5 id="4-regret-analysis-of-fpl"><strong>4. Regret Analysis of FPL</strong></h5> <p>The regret measures how much worse FPL is compared to the <strong>best decision in hindsight</strong>.</p> \[E[\text{Cost of FPL}] - \min_{d \in D} \sum_{t=1}^{T} d \cdot s_t.\] <p><strong>Step 1: The ‚ÄúBe the Leader‚Äù Algorithm</strong></p> <ul> <li>The <strong>hypothetical</strong> ‚ÄúBe the Leader‚Äù algorithm always picks the best decision so far: \(d_t = M(s_{1:t})\)</li> <li>It incurs <strong>zero regret</strong>: \(\sum_{t=1}^{T} M(s_{1:t}) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T}.\)</li> <li>However, it <strong>switches too often</strong>.</li> </ul> <p><strong>Step 2: Introducing Perturbations</strong></p> <p>FPL <strong>adds perturbations</strong> to cumulative costs before selecting the leader: \(M(s_{1:t-1} + p_t).\) The regret bound becomes:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + \eta R A T + D / \eta.\] <p><strong>Step 3: Choosing the Optimal Perturbation Scale</strong></p> <p>To balance stability and adaptation, we set: \(\eta = \sqrt{\frac{D}{RAT}}.\) This leads to the <strong>final regret bound</strong>: \(O(\sqrt{T}).\) This ensures <strong>sublinear regret</strong>, meaning FPL performs nearly as well as the best expert over time.</p> <h5 id="5-applying-fpl-to-structured-problems"><strong>5. Applying FPL to Structured Problems</strong></h5> <p>FPL can be extended to <strong>problems beyond expert selection</strong>, such as <strong>shortest paths</strong>.</p> <p><strong>Online Shortest Paths Problem</strong></p> <ul> <li>Given a <strong>graph</strong> with edge costs changing over time.</li> <li>Each round, the algorithm must select a <strong>path from source \(s\) to destination \(t\)</strong>.</li> <li>The naive approach (treating paths as independent experts) is <strong>computationally infeasible</strong>.</li> </ul> <p><strong>Efficient Approach: FPL at the Edge Level</strong></p> <ol> <li>Instead of applying FPL to entire paths, apply <strong>perturbations at the edge level</strong>.</li> <li>Compute <strong>perturbed edge costs</strong>: \(\tilde{c}_t(e) = c_t(e) + p_t(e).\)</li> <li><strong>Compute the shortest path</strong> based on these perturbed edge costs.</li> <li>Select the <strong>shortest path</strong>.</li> </ol> <p><strong>Why Does This Work?</strong></p> <ul> <li><strong>Avoids exponential complexity</strong> by working at the edge level.</li> <li><strong>Efficiently computed</strong> using shortest path algorithms (e.g., Dijkstra‚Äôs).</li> <li><strong>Regret bound remains low</strong>: \((1 + \epsilon) \times (\text{Best Path Cost}) + O(m n \log n).\)</li> </ul> <p><strong>Other Structured Problems</strong></p> <p>FPL has also been applied to:</p> <ul> <li><strong>Tree search</strong>: Efficiently updating search trees.</li> <li><strong>Adaptive Huffman coding</strong>: Dynamically optimizing prefix codes.</li> <li><strong>Online approximation algorithms</strong>: Extending FPL when exact offline solutions are infeasible.</li> </ul> <h5 id="6-summary"><strong>6. Summary</strong></h5> <ul> <li>FPL is a simple, efficient alternative to exponential weighting methods.</li> <li>It achieves regret \(O(\sqrt{T})\), ensuring performance close to the best decision in hindsight.</li> <li>The perturbation method generalizes to structured problems like shortest paths.</li> <li>FPL is computationally feasible even when the number of decisions is large.</li> </ul> <hr/> <blockquote> <p>Follow Up Questions;</p> </blockquote> <h4 id="understanding-the-additive-analysis-and-regret-bounds"><strong>Understanding the Additive Analysis and Regret Bounds</strong></h4> <p>The <strong>Additive Analysis</strong> section in the paper derives a regret bound for the <strong>Follow the Perturbed Leader (FPL)</strong> algorithm. The key goal is to compare the <strong>cumulative cost of FPL</strong> to the <strong>best fixed decision in hindsight</strong>.</p> <h5 id="key-notation"><strong>Key Notation</strong></h5> <p>Before deriving the regret bound, let‚Äôs clarify the notation:</p> <ul> <li>\(s_t\): <strong>State (cost vector) at time \(t\)</strong> <ul> <li>At each time step, we observe a cost vector \(s_t\), where each component represents the cost of a different decision.</li> </ul> </li> <li>\(d_t\): <strong>Decision made at time \(t\)</strong> <ul> <li>The action chosen by the algorithm at time \(t\).</li> </ul> </li> <li>\(M(x)\): <strong>Best fixed decision in hindsight</strong> <ul> <li>Given a total cost vector \(x\), \(M(x)\) returns the best decision: \(M(x) = \arg\min_{d \in D} d \cdot x\)</li> </ul> </li> <li>\(s_{1:T}\): <strong>Total cost vector over all \(T\) time steps</strong> <ul> <li>This is simply the sum of all cost vectors: \(s_{1:T} = s_1 + s_2 + \dots + s_T\)</li> </ul> </li> <li>\(p_t\): <strong>Random perturbation added at time \(t\)</strong> <ul> <li>Introduced to smooth out decision-making and prevent frequent switches.</li> </ul> </li> </ul> <h5 id="goal-regret-minimization"><strong>Goal: Regret Minimization</strong></h5> <p>We define regret as the difference between FPL‚Äôs total cost and the best fixed decision in hindsight:</p> \[E \left[ \sum_{t=1}^{T} d_t \cdot s_t \right] - \min_{d \in D} \sum_{t=1}^{T} d \cdot s_t.\] <p><strong>Step 1: The Hypothetical ‚ÄúBe the Leader‚Äù Algorithm</strong></p> <p>To analyze FPL, we first consider a <strong>hypothetical ‚ÄúBe the Leader‚Äù algorithm</strong>, which always picks the <strong>best decision so far</strong>:</p> \[d_t = M(s_{1:t}).\] <p>The key property of this algorithm is:</p> \[\sum_{t=1}^{T} M(s_{1:t}) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T}.\] <p><strong>Why Does This Hold?</strong></p> <ul> <li>The <strong>best decision in hindsight</strong> is optimal for the full sequence.</li> <li>If we could always ‚Äúbe the leader,‚Äù we would incur <strong>zero regret</strong>.</li> <li>However, this algorithm <strong>switches too frequently</strong>, making it unstable.</li> </ul> <p><strong>Step 2: Introducing Perturbations</strong></p> <p>FPL smooths out decision-making by adding <strong>random perturbations</strong> before selecting the leader:</p> \[M(s_{1:t-1} + p_t).\] <p>The analysis shows:</p> \[\sum_{t=1}^{T} M(s_{1:t} + p_t) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T} + D \sum_{t=1}^{T} |p_t - p_{t-1}|_\infty.\] <p><strong>Key Insight</strong></p> <ul> <li>The first term on the RHS is the cost of the <strong>best decision in hindsight</strong>.</li> <li>The second term represents the <strong>additional cost due to perturbations</strong>.</li> <li>This term grows at most as \(O(\sqrt{T})\), ensuring that regret remains <strong>sublinear</strong>.</li> </ul> <p><strong>Step 3: Bounding the Impact of Perturbations</strong></p> <p>The final step is to bound the effect of perturbations:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + \eta R A T + D / \eta.\] <p>where:</p> <ul> <li>\(\eta\) is a tuning parameter controlling the size of perturbations.</li> <li>\(R, A, D\) are problem-dependent constants.</li> </ul> <p><strong>Choosing the Optimal \(\eta\)</strong></p> <p>To minimize regret, we set:</p> \[\eta = \sqrt{\frac{D}{RAT}}.\] <p>Plugging this into the regret bound:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + 2 \sqrt{DRAT}.\] <p><strong>Final Regret Bound</strong> \(O(\sqrt{T}).\)</p> <p>This ensures <strong>sublinear regret</strong>, meaning that over time, <strong>FPL performs nearly as well as the best fixed decision</strong>.</p> <p><strong>Key Takeaways</strong></p> <ol> <li>‚ÄúBe the Leader‚Äù is optimal but switches too often.</li> <li>FPL adds perturbations to smooth decision-making.</li> <li>The regret bound is controlled by the trade-off between making stable choices and avoiding excessive randomness.</li> <li>Choosing \(\eta\) optimally gives a regret bound of \(O(\sqrt{T})\), ensuring long-term efficiency.</li> </ol> <hr/> <h4 id="understanding-the-key-notation-and-be-the-leader-algorithm"><strong>Understanding the Key Notation and ‚ÄúBe the Leader‚Äù Algorithm</strong></h4> <p>This section provides a clear explanation of the <strong>Key Notation</strong> and the <strong>‚ÄúBe the Leader‚Äù algorithm</strong> used in the <strong>Additive Analysis</strong> of the paper.</p> <h5 id="1-key-notation-clarified"><strong>1. Key Notation (Clarified)</strong></h5> <p>To understand the regret bound derivation, let‚Äôs first clarify the key mathematical symbols.</p> <h5 id="online-decision-problem-setup"><strong>Online Decision Problem Setup</strong></h5> <p>In an <strong>online decision problem</strong>, we repeatedly make decisions without knowing future costs. Our goal is to minimize <strong>total cost</strong> over time.</p> <hr/> <table> <thead> <tr> <th>Symbol</th> <th>Definition</th> </tr> </thead> <tbody> <tr> <td>\(s_t\)</td> <td><strong>State (cost vector) at time \(t\)</strong>, representing the cost of each decision at step \(t\).</td> </tr> <tr> <td>\(d_t\)</td> <td><strong>Decision chosen at time \(t\)</strong> (e.g., selecting an expert or a path).</td> </tr> <tr> <td>\(M(x)\)</td> <td><strong>Best fixed decision in hindsight</strong>, meaning the best decision if we knew all costs in advance.</td> </tr> <tr> <td>\(s_{1:T}\)</td> <td><strong>Total cost vector over \(T\) rounds</strong>, defined as \(s_{1:T} = \sum_{t=1}^{T} s_t\).</td> </tr> <tr> <td>\(p_t\)</td> <td><strong>Random perturbation added at time \(t\)</strong> to smooth decision-making.</td> </tr> </tbody> </table> <hr/> <h5 id="example-experts-problem"><strong>Example: Experts Problem</strong></h5> <ul> <li>Suppose we are choosing between <strong>two experts</strong> (A and B).</li> <li>Each expert has a different cost at each time step.</li> <li>We want to <strong>pick the expert that minimizes the total cost over time</strong>.</li> </ul> <hr/> <table> <thead> <tr> <th>Time \(t\)</th> <th>Expert A‚Äôs Cost \(s_t(A)\)</th> <th>Expert B‚Äôs Cost \(s_t(B)\)</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> </tr> <tr> <td>\(t = 2\)</td> <td>0.5</td> <td>0.2</td> </tr> <tr> <td>\(t = 3\)</td> <td>0.4</td> <td>0.5</td> </tr> <tr> <td>\(t = 4\)</td> <td>0.2</td> <td>0.3</td> </tr> </tbody> </table> <hr/> <ul> <li><strong>Without perturbations</strong>, we would always select the expert with the lowest cumulative cost.</li> <li>However, <strong>this can cause excessive switching</strong>, which leads to instability.</li> </ul> <p><strong>2. Step 1: The ‚ÄúBe the Leader‚Äù Algorithm</strong></p> <p>The <strong>‚ÄúBe the Leader‚Äù algorithm</strong> is a <strong>hypothetical strategy</strong> where we always choose the <strong>best decision so far</strong>.</p> <p><strong>How It Works</strong></p> <p>At time \(t\), select:</p> \[d_t = M(s_{1:t}),\] <p>meaning:</p> <ul> <li>Choose the <strong>decision that has had the lowest total cost so far</strong>.</li> <li>This ensures <strong>no regret</strong> because we are always picking the best option up to that point.</li> </ul> <p><strong>Key Property</strong></p> \[\sum_{t=1}^{T} M(s_{1:t}) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T}.\] <p><strong>Why is this true?</strong></p> <ol> <li>The <strong>best decision in hindsight</strong> is optimal for the full sequence.</li> <li>If we could always ‚Äúbe the leader,‚Äù we would incur <strong>zero regret</strong>.</li> <li>However, this algorithm <strong>switches decisions too frequently</strong>, making it unstable.</li> </ol> <p><strong>Example Calculation</strong></p> <hr/> <table> <thead> <tr> <th>Time \(t\)</th> <th>Cumulative Cost \(A\)</th> <th>Cumulative Cost \(B\)</th> <th>Chosen Expert</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> <td><strong>A</strong></td> </tr> <tr> <td>\(t = 2\)</td> <td>0.8</td> <td>0.6</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 3\)</td> <td>1.2</td> <td>1.1</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 4\)</td> <td>1.4</td> <td>1.4</td> <td><strong>Switching rapidly!</strong></td> </tr> </tbody> </table> <hr/> <ul> <li>The leader <strong>switches frequently</strong> whenever cumulative costs change slightly.</li> <li>This is problematic, especially in <strong>adversarial settings</strong>, because an adversary can force unnecessary switches.</li> </ul> <p><strong>3. Why Do We Need Perturbations?</strong></p> <p>The <strong>‚ÄúBe the Leader‚Äù</strong> algorithm <strong>switches too often</strong>, making it inefficient.</p> <p><strong>Follow the Perturbed Leader (FPL) Fixes This</strong></p> <p>To avoid excessive switching, <strong>FPL adds random perturbations</strong> before selecting the leader:</p> \[M(s_{1:t-1} + p_t).\] <p>This <strong>smooths out decision-making</strong>:</p> <ul> <li><strong>Prevents rapid switches</strong> caused by small cost changes.</li> <li><strong>Balances stability and adaptability</strong>.</li> <li><strong>Maintains a low regret bound</strong>.</li> </ul> <p><strong>Key Insight</strong></p> <ul> <li><strong>FPL ensures that decisions do not fluctuate excessively</strong>.</li> <li><strong>Adding perturbations leads to a regret bound of \(O(\sqrt{T})\), ensuring long-term efficiency</strong>.</li> </ul> <hr/> <h5 id="understanding-step-2-and-step-3-in-additive-analysis"><strong>Understanding Step 2 and Step 3 in Additive Analysis</strong></h5> <p>In the <strong>Additive Analysis</strong> section, Steps 2 and 3 are crucial for deriving the regret bound for <strong>Follow the Perturbed Leader (FPL)</strong>. These steps show how perturbations help smooth decision-making while maintaining low regret.</p> <hr/> <p><strong>Step 2: Introducing Perturbations</strong></p> <p>The issue with the <strong>‚ÄúBe the Leader‚Äù</strong> algorithm is that it <strong>switches too frequently</strong>, leading to instability. To fix this, <strong>Follow the Perturbed Leader (FPL)</strong> <strong>adds small random perturbations</strong> to past costs before selecting the leader:</p> \[d_t = M(s_{1:t-1} + p_t).\] <p><strong>Effect of Perturbations</strong></p> <p>Instead of choosing the decision with the exact lowest cumulative cost, FPL selects the decision <strong>with the lowest perturbed cost</strong>:</p> \[\tilde{c}_t(e) = c_t(e) + p_t(e).\] <p>This ensures:</p> <ol> <li><strong>Fewer unnecessary switches</strong>: Small cost fluctuations no longer cause frequent decision changes.</li> <li><strong>Better robustness against adversarial cost sequences</strong>.</li> </ol> <p><strong>Key Inequality</strong></p> <p>The analysis shows:</p> \[\sum_{t=1}^{T} M(s_{1:t} + p_t) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T} + D \sum_{t=1}^{T} |p_t - p_{t-1}|_\infty.\] <p><strong>Breaking It Down</strong></p> <ul> <li><strong>LHS</strong>: The total cost incurred by FPL.</li> <li><strong>RHS</strong>: The total cost of the best decision in hindsight <strong>plus an extra term due to perturbations</strong>.</li> <li><strong>The second term</strong> captures the additional cost introduced by randomness.</li> </ul> <p>Since perturbations are drawn from a well-chosen distribution, their effect remains <strong>small</strong> (bounded by \(O(\sqrt{T})\)).</p> <p><strong>Step 3: Bounding the Impact of Perturbations</strong></p> <p>The final step is to <strong>quantify how much extra cost perturbations introduce</strong>.</p> <p>The key regret bound derived is:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + \eta R A T + D / \eta.\] <p>where:</p> <ul> <li>\(\eta\) controls <strong>how large the perturbations are</strong>.</li> <li>\(R, A, D\) are constants depending on the problem setup.</li> </ul> <p><strong>Choosing the Optimal Perturbation Scale</strong></p> <p>To balance stability and adaptation, they choose:</p> \[\eta = \sqrt{\frac{D}{RAT}}.\] <p>Plugging this into the regret bound:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + 2 \sqrt{DRAT}.\] <p><strong>Final Regret Bound</strong> \(O(\sqrt{T}).\)</p> <p>This means that <strong>the regret grows sublinearly</strong>, ensuring that over time, <strong>FPL performs nearly as well as the best fixed decision</strong>.</p> <hr/> <h5 id="understanding-fpl-with-a-worked-out-example"><strong>Understanding FPL with a Worked-Out Example</strong></h5> <p>To better understand <strong>Follow the Perturbed Leader (FPL)</strong>, let‚Äôs go through a <strong>step-by-step numerical example</strong>.</p> <p><strong>Problem Setup</strong></p> <ul> <li>We have <strong>two experts</strong>: <strong>A</strong> and <strong>B</strong>.</li> <li>Each expert incurs a cost at each time step.</li> <li>We must <strong>pick one expert per round</strong> without knowing future costs.</li> <li>Our goal is to minimize <strong>total cost over \(T = 4\) rounds</strong>.</li> </ul> <p><strong>Cost Sequence</strong></p> <hr/> <table> <thead> <tr> <th>Time \(t\)</th> <th>Expert A‚Äôs Cost \(s_t(A)\)</th> <th>Expert B‚Äôs Cost \(s_t(B)\)</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> </tr> <tr> <td>\(t = 2\)</td> <td>0.5</td> <td>0.2</td> </tr> <tr> <td>\(t = 3\)</td> <td>0.4</td> <td>0.5</td> </tr> <tr> <td>\(t = 4\)</td> <td>0.2</td> <td>0.3</td> </tr> </tbody> </table> <hr/> <ul> <li><strong>Without perturbations</strong>, FPL would always pick the expert with the lowest cumulative cost.</li> <li>However, this leads to <strong>frequent switching</strong>.</li> </ul> <p><strong>Step 1: Follow the Leader (FTL) - No Perturbation</strong></p> <p>If we naively follow the leader <strong>without perturbations</strong>, we get:</p> <table> <thead> <tr> <th>Time \(t\)</th> <th>Cumulative Cost \(A\)</th> <th>Cumulative Cost \(B\)</th> <th>Chosen Expert</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> <td><strong>A</strong></td> </tr> <tr> <td>\(t = 2\)</td> <td>0.8</td> <td>0.6</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 3\)</td> <td>1.2</td> <td>1.1</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 4\)</td> <td>1.4</td> <td>1.4</td> <td><strong>Switching rapidly!</strong></td> </tr> </tbody> </table> <p><strong>Problem with FTL</strong></p> <ul> <li>The algorithm <strong>switches too frequently</strong>, making it unstable.</li> <li>Small cost differences cause unnecessary <strong>leader changes</strong>.</li> <li>This is <strong>bad in adversarial settings</strong>, where cost sequences can be manipulated.</li> </ul> <p><strong>Step 2: Follow the Perturbed Leader (FPL) - Adding Perturbations</strong></p> <p>Now, let‚Äôs <strong>add perturbations</strong>.</p> <ul> <li>We <strong>randomly sample perturbations</strong> \(p_t(A)\) and \(p_t(B)\) from an <strong>exponential distribution</strong>.</li> <li>Suppose we get: \(p_1(A) = 0.1, \quad p_1(B) = 0.2\)</li> </ul> <p><strong>Step 2.1: Compute Perturbed Costs</strong></p> <hr/> <table> <thead> <tr> <th>Time \(t\)</th> <th>Cumulative Cost \(A\)</th> <th>Cumulative Cost \(B\)</th> <th>Perturbed \(A\)</th> <th>Perturbed \(B\)</th> <th>Chosen Expert</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> <td><strong>0.4</strong></td> <td><strong>0.6</strong></td> <td><strong>A</strong></td> </tr> <tr> <td>\(t = 2\)</td> <td>0.8</td> <td>0.6</td> <td><strong>0.9</strong></td> <td><strong>0.8</strong></td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 3\)</td> <td>1.2</td> <td>1.1</td> <td><strong>1.3</strong></td> <td><strong>1.2</strong></td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 4\)</td> <td>1.4</td> <td>1.4</td> <td><strong>1.5</strong></td> <td><strong>1.6</strong></td> <td><strong>A</strong></td> </tr> </tbody> </table> <hr/> <p><strong>Step 2.2: What Changed?</strong></p> <ul> <li><strong>FPL smooths out decisions</strong>: The perturbations prevent unnecessary switching.</li> <li><strong>Perturbations reduce instability</strong>: Instead of switching too frequently (like FTL), FPL <strong>stabilizes</strong>.</li> <li><strong>Better stability ‚Üí Lower regret!</strong></li> </ul> <p><strong>Step 3: Computing the Regret</strong></p> <p><strong>Step 3.1: Compute Cost of FPL</strong></p> <p>The total cost incurred by <strong>FPL</strong>: \(\sum_{t=1}^{T} \text{Chosen Expert's Cost}\) Using the decisions above:</p> \[\text{Total Cost of FPL} = 0.3 + 0.2 + 0.5 + 0.2 = 1.2.\] <p><strong>Step 3.2: Compute Cost of Best Expert in Hindsight</strong></p> <p>If we had <strong>perfect hindsight</strong>, we would choose the best expert who had the <strong>lowest total cost over all \(T\) rounds</strong>.</p> \[\text{Total Cost of Best Expert} = \min(1.4, 1.4) = 1.4.\] <p><strong>Step 3.3: Compute Regret</strong></p> <p>Regret is the difference between FPL and the best fixed decision:</p> \[\text{Regret} = E[\text{Cost of FPL}] - \text{Cost of Best Expert}.\] <p>Since FPL <strong>performs better than the best single expert</strong>, it actually has <strong>negative regret in this case!</strong> In general, the regret is <strong>bounded by \(O(\sqrt{T})\), ensuring FPL converges to the best decision in hindsight over time.</strong></p> <hr/> <h5 id="extending-fpl-to-structured-problems-online-shortest-paths"><strong>Extending FPL to Structured Problems: Online Shortest Paths</strong></h5> <p>One of the major contributions of the paper is extending <strong>Follow the Perturbed Leader (FPL)</strong> beyond the <strong>experts setting</strong> to more <strong>structured problems</strong>, such as <strong>online shortest paths</strong>. This is important because treating every possible path as an independent expert is computationally infeasible when the number of paths is exponential in the number of edges.</p> <p><strong>1. The Online Shortest Path Problem</strong></p> <p><strong>Problem Setup</strong></p> <ul> <li>Given a <strong>directed graph</strong> with \(n\) nodes and \(m\) edges.</li> <li>Each edge \(e\) has a time cost \(c_t(e)\) at each time step \(t\).</li> <li>The goal is to <strong>select a path from source \(s\) to destination \(t\)</strong> at each time step without knowing future costs.</li> <li>After selecting a path, we observe the costs of all edges.</li> </ul> <p><strong>Objective</strong></p> <p>We want to ensure that the total travel time over \(T\) rounds is close to the best <strong>single</strong> path in hindsight.</p> <p><strong>Challenges</strong></p> <ul> <li>The number of possible paths grows exponentially with the number of nodes, making <strong>treating paths as experts infeasible</strong>.</li> <li>Instead of treating whole paths as independent decisions, we need a way to <strong>apply FPL efficiently at the edge level</strong>.</li> </ul> <p><strong>2. Applying FPL to Online Shortest Paths</strong></p> <p><strong>Na√Øve Approach (Infeasible)</strong></p> <ul> <li>If we were to apply <strong>vanilla FPL</strong> directly, we would: <ol> <li>Treat each <strong>entire path</strong> as an ‚Äúexpert.‚Äù</li> <li>Maintain cumulative travel time for every path.</li> <li>Apply perturbations to total path costs.</li> <li>Choose the best path.</li> </ol> </li> <li><strong>Problem:</strong> The number of paths grows exponentially, making this computationally <strong>infeasible</strong>.</li> </ul> <p><strong>Efficient Approach: FPL at the Edge Level</strong></p> <p>To make FPL work efficiently, we <strong>apply perturbations to edges instead of whole paths</strong>:</p> <p><strong>Follow the Perturbed Leading Path (FPL for Paths)</strong></p> <p><strong>At each time step \(t\):</strong></p> <ol> <li><strong>For each edge \(e\)</strong>, draw a random perturbation \(p_t(e)\) from an exponential distribution.</li> <li>Compute <strong>perturbed edge costs</strong>: \(\tilde{c}_t(e) = c_t(e) + p_t(e)\)</li> <li><strong>Find the shortest path</strong> using these perturbed edge costs.</li> <li>Choose the <strong>shortest path</strong> in the graph based on the perturbed edge weights.</li> </ol> <p><strong>Why Does This Work?</strong></p> <ul> <li>Since perturbations are applied <strong>at the edge level</strong>, we avoid maintaining explicit costs for all paths.</li> <li>The standard <strong>shortest path algorithm (e.g., Dijkstra‚Äôs)</strong> can efficiently compute the best path at each step.</li> <li> <p>Theoretical guarantees remain valid: The expected regret is at most:</p> \[(1 + \epsilon) \times (\text{Best Path Cost}) + O(m n \log n)\] <p>where \(m\) is the number of edges and \(n\) is the number of nodes.</p> </li> </ul> <p><strong>3. Understanding the Regret Bound</strong></p> <p>We now derive the <strong>regret bound</strong> for online shortest paths.</p> <p><strong>Key Definitions</strong></p> <ul> <li>Let \(P_t\) be the path chosen by FPL at time \(t\).</li> <li> <p>Let \(P^*\) be the best fixed path in hindsight, i.e., the path with the lowest total cost over \(T\) rounds:</p> \[P^* = \arg\min_{P} \sum_{t=1}^{T} c_t(P).\] </li> <li> <p>The regret of FPL is:</p> \[\sum_{t=1}^{T} c_t(P_t) - \sum_{t=1}^{T} c_t(P^*).\] </li> </ul> <p><strong>Applying FPL Analysis</strong></p> <ul> <li>The perturbation ensures that <strong>bad paths are not chosen too often</strong> and <strong>good paths are discovered quickly</strong>.</li> <li>The additional regret due to perturbation grows as \(O(m n \log n)\), meaning it is still <strong>sublinear in \(T\)</strong>.</li> </ul> <p>Thus, FPL guarantees:</p> \[E[\text{Total Cost of FPL}] \leq (1 + \epsilon) \times \text{Best Path Cost} + O(m n \log n).\] <p><strong>4. Why is This Important?</strong></p> <p><strong>1. Generalization to Graph-Structured Problems</strong></p> <ul> <li>This method can be applied to <strong>any structured problem where the decision space is large</strong>.</li> <li>Example: Instead of treating each full <strong>decision tree</strong> as an expert, perturbations can be applied at <strong>the node level</strong>.</li> </ul> <p><strong>2. Computational Efficiency</strong></p> <ul> <li>Unlike <strong>exponential weighting algorithms</strong> (which require maintaining weights for each path), this approach <strong>only requires standard shortest-path computations</strong>.</li> <li><strong>Time complexity:</strong> Runs in <strong>\(O(m)\)</strong> (if using Bellman-Ford) or <strong>\(O(m + n \log n)\)</strong> (if using Dijkstra‚Äôs).</li> </ul> <p><strong>3. Practical Use Cases</strong></p> <ul> <li><strong>Network Routing:</strong> Selecting optimal paths in a <strong>dynamic network</strong>.</li> <li><strong>Robot Navigation:</strong> Choosing paths in a changing environment.</li> <li><strong>Traffic Prediction:</strong> Adjusting routes based on real-time conditions.</li> </ul> <p><strong>5. Summary</strong></p> <p><strong>Problem</strong>: Online shortest path selection where edge costs change over time.<br/> <strong>FPL Extension</strong>: Instead of treating full paths as experts, <strong>apply perturbations to edges</strong>.<br/> <strong>Algorithm</strong>:</p> <ol> <li>Add <strong>random noise</strong> to <strong>edge costs</strong>.</li> <li>Compute the <strong>shortest path</strong> with the perturbed costs.</li> <li>Follow that path.<br/> <strong>Regret Bound</strong>: <ul> <li><strong>Competitive with the best fixed path</strong> in hindsight.</li> <li><strong>Extra cost</strong> due to perturbations is <strong>small</strong> (only \(O(m n \log n)\)).<br/> <strong>Key Benefit</strong>: <strong>Works efficiently</strong> even when the number of paths is exponential.</li> </ul> </li> </ol> <hr/>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Paper"/><summary type="html"><![CDATA[A breakdown of Follow the Perturbed Leader (FPL) from Kalai & Vempala‚Äôs (2005) paper, "Efficient Algorithms for Online Decision Problems." This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.]]></summary></entry><entry><title type="html">Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning</title><link href="https://monishver11.github.io/blog/2025/FPL/" rel="alternate" type="text/html" title="Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning"/><published>2025-02-01T17:50:00+00:00</published><updated>2025-02-01T17:50:00+00:00</updated><id>https://monishver11.github.io/blog/2025/FPL</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/FPL/"><![CDATA[<p>Online learning is a fascinating area of machine learning where an agent makes sequential decisions, aiming to minimize loss over time. Unlike traditional supervised learning, where a model is trained on a fixed dataset, online learning involves continuously adapting based on feedback from past decisions. One of the simplest and most natural strategies in this setting is <strong>Follow the Leader (FL)</strong>‚Äîa method that always picks the action that has performed best in hindsight.</p> <p>While FL is intuitive, it has critical weaknesses that make it unreliable in adversarial settings. To address these limitations, a more robust strategy called <strong>Follow the Perturbed Leader (FPL)</strong> was introduced, which introduces randomness to stabilize decision-making. In this post, we will explore the core ideas behind FL and FPL, develop their mathematical formulations, and derive their performance bounds.</p> <h5 id="learning-from-structure-decomposing-losses"><strong>Learning from Structure: Decomposing Losses</strong></h5> <p>Many learning problems can be framed in terms of minimizing cumulative loss, which often decomposes naturally across substructures. For instance, in structured prediction tasks, the total loss may be expressed as a sum over smaller components such as:</p> <ul> <li>The sum of losses along edges in a tree.</li> <li>The sum of losses along a specific path in a graph.</li> <li>The sum of individual substructure losses in a discrete optimization problem.</li> <li>The cumulative loss incurred in an expert setting, where multiple strategies are available, and the goal is to learn from the best-performing one.</li> </ul> <p>This decomposition plays a key role in designing online learning algorithms, as it allows us to analyze decision-making strategies in a structured manner.</p> <h4 id="the-follow-the-leader-fl-strategy"><strong>The Follow the Leader (FL) Strategy</strong></h4> <p>To formalize the <strong>Follow the Leader</strong> approach, let‚Äôs consider a general linear decision problem. At each round \(t\), a player selects a decision \(w_t\) from a set \(W\). Once the decision is made, the environment reveals a loss vector \(x_t\), and the player incurs a loss given by the inner product:</p> \[L_t = w_t \cdot x_t\] <p>The objective is to minimize the cumulative loss over \(T\) rounds:</p> \[L_T = \sum_{t=1}^{T} w_t \cdot x_t\] <p>Alternatively, we can evaluate performance through <strong>regret</strong>, which measures how much worse our strategy is compared to the best fixed action in hindsight:</p> \[\text{Regret}_T = L_T - \min_{w \in W} \sum_{t=1}^{T} w \cdot x_t\] <p>The <strong>Follow the Leader</strong> strategy takes a straightforward approach: at each step, it selects the action that has performed best so far. Mathematically, this means choosing:</p> \[w_t = \arg\min_{w \in W} \sum_{s=1}^{t-1} w \cdot x_s\] <p>This approach, sometimes referred to as <strong>fictitious play</strong>, seems reasonable‚Äîafter all, picking the historically best-performing action should be a good idea, right? However, FL has a major flaw: it can be <strong>highly unstable</strong>. In adversarial settings, small changes in past losses can cause large shifts in decisions, leading to poor performance.</p> <p>To illustrate this, consider a simple scenario where we alternate between two actions. If FL starts with an initial action and then alternates between two different ones, it can end up incurring a loss of 1 at every round, whereas a fixed expert strategy would accumulate much less loss. This instability calls for a more robust alternative.</p> <h4 id="follow-the-perturbed-leader-fpl-adding-randomization"><strong>Follow the Perturbed Leader (FPL): Adding Randomization</strong></h4> <p>A simple yet effective modification to FL is <strong>Follow the Perturbed Leader (FPL)</strong>. The idea is to <strong>introduce random noise</strong> before selecting the best action, preventing the algorithm from making overly rigid choices based on past data.</p> <p>Instead of selecting the exact leader, FPL picks an action by solving:</p> \[w_t = \arg\min_{w \in W} \left( \sum_{s=1}^{t-1} w \cdot x_s + w \cdot p_t \right)\] <p>where \(p_t\) is a random perturbation added to the cumulative loss. This small tweak dramatically improves performance in adversarial settings by preventing sudden shifts in decisions.</p> <p>A common choice for the perturbation is uniform noise:</p> \[p_t \sim U([0, 1/\epsilon]^N)\] <p>which ensures that no two actions have exactly the same cumulative loss, effectively breaking ties in a randomized manner. Another approach is to use <strong>multiplicative perturbations</strong>, where the noise follows a Laplacian distribution:</p> \[f(x) = \frac{\epsilon}{2} e^{-\epsilon \|x\|_1}\] <p>This version, referred to as <strong>FPL</strong>*, has even stronger guarantees and is particularly effective in adversarial settings. The theoretical analysis of FPL dates back to <strong>Hannan (1957)</strong> and was later refined by <strong>Kalai &amp; Vempala (2004)</strong>.</p> <hr/> <blockquote> <p>Follow-Up Questions!</p> </blockquote> <h5 id="is-fpl-an-online-learning-strategy-similar-to-the-randomized-weighted-majority-algorithm-rwma-or-is-it-something-else-designed-for-specific-cases"><strong>Is FPL an online learning strategy similar to the Randomized Weighted Majority Algorithm (RWMA), or is it something else designed for specific cases?</strong></h5> <p>Yes, Follow the Perturbed Leader (FPL) is an online learning strategy, and it shares some similarities with the Randomized Weighted Majority Algorithm (RWMA) in that both incorporate randomness to improve decision-making and achieve better regret bounds. However, their underlying mechanisms differ.</p> <ul> <li> <p><strong>RWMA (Randomized Weighted Majority Algorithm)</strong>:<br/> This is an expert-based algorithm where the learner maintains a set of expert weights, updating them multiplicatively based on incurred losses. RWMA is particularly effective in adversarial settings and can be viewed as a special case of Exponentiated Gradient (EG) methods. Its main idea is to give more weight to the experts who perform well, based on past performance.</p> </li> <li> <p><strong>FPL (Follow the Perturbed Leader)</strong>:<br/> FPL, on the other hand, introduces random perturbations to the past loss values before selecting the best action. This process smooths decision-making, avoiding the instability seen in Follow the Leader (FL) while keeping the simplicity of choosing the best historical action.</p> </li> </ul> <p>FPL is especially useful when working with linear loss functions and structured decision spaces, such as combinatorial optimization problems (e.g., shortest paths, spanning trees). It is closely related to mirror descent and regularization-based algorithms and, in some cases, can be viewed as an implicit form of regularization via perturbations.</p> <p><strong>Key Differences:</strong></p> <ul> <li><strong>RWMA</strong> is expert-based, where weights are adjusted multiplicatively based on performance.</li> <li><strong>FPL</strong> uses random perturbations to past losses, making it more general and suitable for structured decision-making problems, particularly in combinatorial settings.</li> </ul> <p>In summary, while both algorithms use randomness to stabilize learning, RWMA is more tailored to expert settings, while FPL is a broader decision-making framework effective for combinatorial and structured problems.</p> <h5 id="what-do-we-mean-by-structured-problems"><strong>What Do We Mean by Structured Problems?</strong></h5> <p>A <strong>structured problem</strong> refers to a decision-making setting where the available choices or actions have an underlying structure, often governed by combinatorial, geometric, or graph-based constraints. Instead of choosing from a simple finite set of actions (like in a standard multi-armed bandit or expert setting), the learner must select complex objects such as:</p> <ul> <li><strong>Paths in a graph</strong> (e.g., shortest path routing)</li> <li><strong>Spanning trees</strong> (e.g., network design)</li> <li><strong>Matchings in a bipartite graph</strong> (e.g., job allocation)</li> <li><strong>Binary vectors with constraints</strong> (e.g., feature selection)</li> <li><strong>Matrices or sequences</strong> (e.g., scheduling problems)</li> </ul> <p>In these cases, the decision space is often exponentially large, making direct enumeration or simple expert-based approaches infeasible.</p> <hr/> <h5 id="why-does-flfpl-apply-to-structured-problems"><strong>Why Does FL/FPL Apply to Structured Problems?</strong></h5> <p>The <strong>Follow the Leader (FL)</strong> and <strong>Follow the Perturbed Leader (FPL)</strong> strategies naturally extend to structured problems because:</p> <ol> <li> <p><strong>Linear Loss Structure</strong>: Many structured problems can be formulated in terms of a linear loss function over the structure. For example, if selecting a path in a graph, the total loss might be the sum of edge losses along the path. FL/FPL directly works with such additive losses.</p> </li> <li> <p><strong>Combinatorial Decision Spaces</strong>: Since the space of possible decisions is structured (e.g., spanning trees, paths), explicitly maintaining weights over all choices, as in the Weighted Majority Algorithm, is impractical. Instead, FL/FPL selects actions by solving an optimization problem over the structured space.</p> </li> <li> <p><strong>Computational Feasibility</strong>: The FL and FPL updates involve solving an argmin optimization problem over past losses:</p> </li> </ol> \[w_t = \arg \min_{w \in W} \sum_{s=1}^{t-1} w \cdot x_s\] <p>This optimization often reduces to a well-known combinatorial problem, such as minimum spanning tree, shortest path, or maximum matching, which can be solved efficiently using algorithms from combinatorial optimization.</p> <hr/> <h5 id="how-does-flfpl-work-in-structured-settings"><strong>How Does FL/FPL Work in Structured Settings?</strong></h5> <ul> <li> <p><strong>Follow the Leader (FL)</strong>:<br/> FL picks the structure (e.g., path, tree) that has accumulated the lowest loss so far. However, FL is unstable in adversarial settings because a small change in losses can drastically change the optimal structure.</p> </li> <li> <p><strong>Follow the Perturbed Leader (FPL)</strong>:<br/> FPL introduces random perturbations to smooth decision-making. The learner solves:</p> </li> </ul> \[w_t = \arg \min_{w \in W} \sum_{s=1}^{t-1} w \cdot x_s + w \cdot p_t\] <p>where \(p_t\) is a noise vector. This prevents overfitting to small loss differences and ensures better stability.</p> <hr/> <p><strong>Key Takeaways:</strong></p> <ul> <li><strong>FL/FPL</strong> applies well to structured problems where decisions involve combinatorial choices rather than simple finite sets.</li> <li><strong>FL</strong> works when losses are additive over substructures, but it is unstable in adversarial settings.</li> <li><strong>FPL</strong> improves stability by adding randomness, making it a powerful tool for structured online learning.</li> </ul> <p>This is why FL/FPL is commonly used in structured online learning, where actions have combinatorial dependencies, and stability is crucial.</p> <h5 id="understanding-structured-problems-an-intuition"><strong>Understanding Structured Problems: An Intuition</strong></h5> <p>Imagine you‚Äôre navigating a city using Google Maps, and every road has a <strong>traffic delay</strong> (which changes over time).</p> <p>Your goal is to pick the best route (shortest travel time) every day. The total delay you experience is the sum of individual road delays along your chosen path.</p> <p>This is an example of a <strong>structured decision problem</strong>, because:</p> <ul> <li>You‚Äôre not picking a single element (e.g., a traffic light or a road segment).</li> <li>You‚Äôre picking a structured object‚Äîa full <strong>path</strong> (set of roads that form a connected route).</li> <li>The <strong>loss</strong> (delay) of a path is determined by the sum of its parts (individual road delays).</li> </ul> <hr/> <h5 id="contrast-with-expert-based-settings"><strong>Contrast with Expert-Based Settings</strong></h5> <p>Now, contrast this with a typical <strong>expert-based setting</strong>, like a multi-armed bandit:</p> <ul> <li>If Google simply gave you a few pre-defined <strong>expert routes</strong>, and you chose among them, it wouldn‚Äôt be flexible.</li> <li>If a road suddenly gets blocked, <strong>all routes</strong> using that road become bad at once, and the whole system collapses.</li> <li><strong>Expert algorithms</strong> assume fixed actions, but in <strong>structured problems</strong>, you can dynamically build better solutions.</li> </ul> <p>Instead of treating entire routes as fixed ‚Äúexperts,‚Äù <strong>FL/FPL</strong> can adaptively construct the best route based on updated delays.</p> <hr/> <h5 id="why-flfpl-is-better-for-structured-problems"><strong>Why FL/FPL is Better for Structured Problems</strong></h5> <p><strong>Expert-Based Methods Struggle with Exponentially Large Action Spaces</strong></p> <p>If we use an <strong>expert-based algorithm</strong>, we‚Äôd need to maintain weights over all possible routes, but the number of possible paths grows exponentially. Instead of tracking every possible path, <strong>FL/FPL</strong> works directly with roads (substructures), summing losses efficiently.</p> <p><strong>Follow the Leader (FL) Uses the Best Past Route</strong></p> <p>In each round, FL picks the route with the <strong>lowest total delay</strong> so far. However, FL is <strong>unstable</strong>‚Äîif one road suddenly gets bad, it can completely switch to a totally different route, causing large fluctuations.</p> <p><strong>Follow the Perturbed Leader (FPL) Adds Stability</strong></p> <p>Instead of blindly trusting past delays, <strong>FPL adds a small random perturbation</strong> to each road‚Äôs delay before choosing a route. This prevents overcommitting to minor variations and makes the algorithm <strong>more robust</strong> to sudden changes.</p> <p><strong>Final Takeaway:</strong></p> <ul> <li><strong>FL/FPL</strong> works well for structured problems where decisions are not independent (like picking a single stock) but instead composed of multiple interacting elements (like building a portfolio or choosing a route).</li> <li><strong>FL</strong> is too <strong>reactive</strong>, making drastic changes when small variations happen.</li> <li><strong>FPL</strong> stabilizes decisions, avoiding erratic shifts and leading to <strong>better long-term performance</strong>.</li> </ul> <p>This is why <strong>FL/FPL</strong> is widely used in <strong>structured online learning</strong>, like routing, combinatorial optimization, and dynamic decision-making.</p> <hr/> <h5 id="what-happens-if-we-stop-adding-perturbation-after-some-rounds-t-that-is-when-we-have-achieved-good-low-regret"><strong>What happens if we stop adding perturbation after some rounds \(T\), that is, when we have achieved good low regret?</strong></h5> <p>Great question! If you stop adding perturbations after some round \(T\) when you‚Äôve achieved good performance and low regret, the behavior of the algorithm will change, and it will essentially revert to the <strong>Follow the Leader (FTL)</strong> strategy. This transition can have interesting consequences.</p> <p><strong>Understanding the Effect of Stopping Perturbations</strong></p> <ol> <li><strong>Initial Phase with Perturbations (Exploration):</strong> <ul> <li>When you apply perturbations, the <strong>Follow the Perturbed Leader (FPL)</strong> strategy introduces randomness into the decision-making process. This helps the algorithm <strong>explore</strong> different experts and prevents it from sticking to an expert that might perform poorly in the future, despite performing well in the past.</li> <li>The perturbations encourage the algorithm to occasionally ‚Äúswitch‚Äù experts, even if the cumulative loss of one expert is slightly better than the others. This helps to <strong>avoid overfitting</strong> to a single expert and <strong>explores new possibilities</strong>, which is particularly helpful if the environment is non-stationary or if the best expert changes over time.</li> </ul> </li> <li><strong>After Stopping Perturbations:</strong> <ul> <li>Once you stop adding perturbations after round \(T\), the algorithm will no longer introduce any randomness and will instead follow the expert with the lowest cumulative loss up until that point. Effectively, the algorithm will behave like <strong>Follow the Leader (FTL)</strong> after the perturbation phase.</li> <li><strong>FTL</strong> always follows the expert with the <strong>best cumulative performance</strong> (i.e., the lowest cumulative loss). If one expert has been performing better consistently, the algorithm will stick to that expert.</li> </ul> </li> </ol> <p><strong>Possible Outcomes of Stopping Perturbations</strong></p> <p>Let‚Äôs consider a few different scenarios that can arise depending on when you stop perturbing and how the environment behaves.</p> <ol> <li><strong>Environment is Stationary (Best Expert Doesn‚Äôt Change)</strong> <ul> <li><strong>Situation</strong>: If the best expert has been consistently the best throughout all rounds, stopping the perturbations at round \(T\) will result in the algorithm behaving like <strong>FTL</strong> from round \(T\) onward.</li> <li><strong>Result</strong>: The algorithm will follow the best expert without any risk of choosing a suboptimal one. The cumulative loss will continue to accumulate for the best expert, and the regret will stay small or grow slowly.</li> <li><strong>Performance</strong>: If the best expert is already clearly the best by round \(T\), the algorithm will perform <strong>optimally</strong> from round \(T\) onward (i.e., no more regret). The cumulative regret will be close to the best expert‚Äôs cumulative loss, but there might still be a small amount of regret during the exploration phase before round \(T\).</li> </ul> </li> <li><strong>Environment is Non-Stationary (Best Expert Changes Over Time)</strong> <ul> <li><strong>Situation</strong>: If the best expert changes over time (e.g., due to shifts in the environment or external factors), stopping perturbations at round \(T\) might cause the algorithm to get stuck with the wrong expert.</li> <li><strong>Result</strong>: Once you stop perturbing, the algorithm might follow an expert that <strong>was the best up to round \(T\)</strong> but is no longer the best expert from that point onward.</li> <li><strong>Performance</strong>: In this case, the algorithm could suffer from <strong>increased regret</strong> after round \(T\), because it will stick with an expert that was optimal earlier but is now suboptimal. The regret will start to grow again because the algorithm has <strong>stopped exploring</strong> and is no longer adapting to changes in the environment.</li> <li><strong>Consequence</strong>: The algorithm will lose the flexibility that perturbations gave it, and it might be too slow to react to changes in the best expert. The regret could potentially grow at a <strong>linear rate</strong> after perturbations stop, similar to <strong>FTL</strong>.</li> </ul> </li> <li><strong>Regret After Stopping Perturbations</strong> <ul> <li>If the perturbations were helping to <strong>adapt</strong> to changes in the environment, stopping them could result in the <strong>regret increasing</strong> once the environment changes and the algorithm locks onto a potentially suboptimal expert.</li> <li>The cumulative regret after round \(T\) (where perturbations stop) would depend on how well the algorithm was able to adapt and learn the best expert <strong>before</strong> the perturbations were turned off.</li> </ul> </li> </ol> <p><strong>Long-Term Regret Bound After Stopping Perturbations</strong></p> <p>If you stop perturbing after \(T\) rounds, the regret bound will likely change.</p> <ul> <li>Before round \(T\), the regret was bounded by the <strong>\(O(\sqrt{T \log N})\)</strong> bound, due to the perturbations helping with exploration and avoiding suboptimal sticking to an expert.</li> <li>After round \(T\), once perturbations are stopped, the algorithm behaves like <strong>Follow the Leader (FTL)</strong>, and the regret bound could transition to something like:</li> </ul> \[R(T') \leq O(T' - T)\] <p>where \(T'\) is the number of rounds after you stop perturbations. Essentially, the regret will grow linearly with the number of rounds after \(T\), because the algorithm no longer explores and may follow a suboptimal expert.</p> <p>Thus, the overall regret after \(T\) rounds will be:</p> \[R(T) \leq O(\sqrt{T \log N}) + O(T' - T)\] <p>where:</p> <ul> <li>The first term accounts for the exploration phase with perturbations.</li> <li>The second term captures the linear regret that may occur once perturbations are turned off.</li> </ul> <p><strong>Optimizing the Stopping Point:</strong> To minimize the long-term regret after perturbations stop, you would want to stop perturbing at a point where:</p> <ol> <li><strong>The best expert has stabilized</strong> ‚Äî meaning the algorithm has successfully identified the best expert up until round \(T\), and the environment is <strong>either stationary or predictable</strong>.</li> <li><strong>The expert switching rate is low</strong> ‚Äî if the best expert has already been chosen and the environment isn‚Äôt changing much, then stopping perturbations at round \(T\) will not lead to significant regret in the long run.</li> </ol> <p><strong>Summary of Outcomes</strong></p> <ul> <li><strong>If the environment is stationary</strong>, stopping perturbations at round \(T\) when good performance has been achieved is <strong>beneficial</strong> and will result in very low regret going forward, since the algorithm will just follow the best expert.</li> <li><strong>If the environment is non-stationary</strong>, stopping perturbations could lead to <strong>higher regret</strong> since the algorithm will stop adapting to new changes and could get stuck with a suboptimal expert.</li> <li>The <strong>regret bound</strong> will change after you stop perturbations. Initially, regret grows sublinearly (\(O(\sqrt{T \log N})\)), but after stopping perturbations, regret may grow linearly, like <strong>FTL</strong> (\(O(T' - T)\)).</li> </ul> <hr/> <h4 id="theoretical-bounds-for-fpl"><strong>Theoretical Bounds for FPL</strong></h4> <p>A crucial advantage of FPL is that it achieves <strong>sublinear regret</strong>, meaning that over time, its cumulative loss approaches that of the best fixed decision. We now present regret bounds for both the <strong>additive</strong> and <strong>multiplicative</strong> versions of FPL.</p> <h5 id="additive-fpl-bound"><strong>Additive FPL Bound</strong></h5> <p>For a fixed \(\epsilon &gt; 0\), the expected cumulative loss of <strong>FPL with additive perturbations</strong> is bounded as:</p> \[E[\mathcal{L}_T] \leq \mathcal{L}_T^{\min} + \epsilon R X_1 T + \frac{W_1}{\epsilon}\] <p>By choosing an optimal \(\epsilon\) value:</p> \[\epsilon = \sqrt{\frac{W_1}{R X_1 T}}\] <p>we obtain the bound:</p> \[E[\mathcal{L}_T] \leq \mathcal{L}_T^{\min} + 2 \sqrt{X_1 W_1 R T}\] <p>which ensures that the regret grows sublinearly with \(T\), implying that as time progresses, the algorithm performs nearly as well as the best fixed action in hindsight.</p> <h5 id="multiplicative-fpl-bound"><strong>Multiplicative FPL* Bound</strong></h5> <p>For <strong>FPL</strong>* with multiplicative perturbations, the expected cumulative loss is bounded by:</p> \[E[\mathcal{L}_T] \leq \mathcal{L}_T^{\min} + 4 \sqrt{\mathcal{L}_T^{\min} X_1 W_1 (1 + \log N)} + 4 X_1 W_1 (1 + \log N)\] <p>where the optimal choice of \(\epsilon\) is:</p> \[\epsilon = \min \left( \frac{1}{2X_1}, \sqrt{\frac{W_1(1 + \log N)}{X_1 \mathcal{L}_T^{\min}}} \right)\] <p>This bound highlights the benefits of <strong>multiplicative perturbations</strong>, which further stabilize decision-making in adversarial scenarios.</p> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>The <strong>Follow the Leader (FL)</strong> algorithm, while simple, suffers from instability in adversarial settings. By introducing <strong>random perturbations</strong>, the <strong>Follow the Perturbed Leader (FPL)</strong> approach overcomes these weaknesses, ensuring better performance over time.</p> <ul> <li><strong>Additive perturbations</strong> smooth out decision-making, reducing instability.</li> <li><strong>Multiplicative perturbations (FPL*)</strong> provide even stronger guarantees, particularly in adversarial environments.</li> </ul> <p>By leveraging <strong>randomization</strong>, FPL achieves <strong>better regret bounds</strong>, making it a powerful tool in online learning. These ideas form the foundation for many modern online learning algorithms, and understanding them provides valuable insight into sequential decision-making strategies.</p> <p>If you‚Äôre interested in diving deeper into online learning, stay tuned for more explorations into regret minimization techniques and advanced algorithms!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/2005-Efficient_Algorithms_for_Online_Decision_Problems.pdf">Efficient algorithms for online decision problems</a></li> </ul>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.]]></summary></entry><entry><title type="html">Bayesian Conditional Models</title><link href="https://monishver11.github.io/blog/2025/bayes-conditional-models/" rel="alternate" type="text/html" title="Bayesian Conditional Models"/><published>2025-01-31T19:37:00+00:00</published><updated>2025-01-31T19:37:00+00:00</updated><id>https://monishver11.github.io/blog/2025/bayes-conditional-models</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/bayes-conditional-models/"><![CDATA[<p>In machine learning, making predictions is not just about estimating the most likely outcome. It‚Äôs also about understanding <strong>uncertainty</strong> and making informed decisions based on available data. Traditional <strong>frequentist methods</strong> typically estimate a single best-fit parameter using approaches like Maximum Likelihood Estimation (MLE). While effective, this approach does not quantify the uncertainty in parameter estimates or predictions.</p> <p>Bayesian conditional models, on the other hand, take a <strong>probabilistic approach</strong>. Instead of committing to a single parameter estimate, they maintain a <strong>distribution over possible parameters</strong>. By incorporating prior beliefs and updating them as new data arrives, Bayesian models allow us to make <strong>predictions that inherently capture uncertainty</strong>. This is achieved through <strong>posterior predictive distributions</strong>, which average over all possible models rather than selecting just one.</p> <p>In this post, we will explore Bayesian conditional models in depth‚Äîhow they work, how they differ from frequentist approaches, and how they allow for <strong>more robust decision-making under uncertainty</strong>.</p> <h4 id="bayesian-conditional-models-the-basics"><strong>Bayesian Conditional Models: The Basics</strong></h4> <p>To set up the problem, consider the following:</p> <ul> <li><strong>Input space</strong>: \(X = \mathbb{R}^d\), representing feature vectors.</li> <li><strong>Outcome space</strong>: \(Y = \mathbb{R}\), representing target values.</li> </ul> <p>A <strong>Bayesian conditional model</strong> consists of two main components:</p> <ol> <li> <p>A <strong>parametric family</strong> of conditional probability densities:</p> \[\{ p(y \mid x, \theta) : \theta \in \Theta \}\] </li> <li> <p>A <strong>prior distribution</strong> \(p(\theta)\), which represents our beliefs about \(\theta\) before observing any data.</p> </li> </ol> <p>The prior acts as a <strong>regularization mechanism</strong>, preventing overfitting by incorporating external knowledge into our model. Once we observe data, we update this prior to obtain a <strong>posterior distribution</strong> over the parameters.</p> <hr/> <p><strong>Q: How does the prior prevent overfitting?</strong><br/> The prior \(p(\theta)\) assigns probability to different parameter values before seeing any data. This prevents the model from fitting noise in the data by <strong>restricting extreme values</strong> of \(\theta\). When combined with the likelihood, it balances between prior beliefs and observed data.</p> <p><strong>Q: Why does this help?</strong></p> <ul> <li>It <strong>controls model complexity</strong>, ensuring we don‚Äôt fit spurious patterns.</li> <li>It <strong>biases the model toward reasonable solutions</strong>, especially in low-data regimes.</li> <li>It <strong>smooths predictions</strong>, preventing sharp jumps caused by noisy observations.</li> </ul> <p><strong>Q: What happens after observing data?</strong><br/> The prior is updated using Bayes‚Äô rule to form the <strong>posterior</strong>:</p> \[p(\theta \mid D) \propto p(D \mid \theta) p(\theta)\] <p>This posterior now reflects both the <strong>initial beliefs</strong> and the <strong>information from the data</strong>, striking a balance between flexibility and regularization.</p> <p><strong>Q: How is this similar to frequentist regularization?</strong><br/> In frequentist methods, regularization terms (e.g., L2 in ridge regression) <strong>penalize large parameter values</strong>. Bayesian priors achieve a similar effect, but instead of a fixed penalty, they provide a <strong>probabilistic framework</strong> that adapts as more data is observed.</p> <p>Thus, the prior serves as a <strong>principled way to regularize models</strong>, ensuring robustness while allowing adaptation as more evidence accumulates.</p> <hr/> <h5 id="the-posterior-distribution"><strong>The Posterior Distribution</strong></h5> <p>The <strong>posterior distribution</strong> is the foundation of Bayesian inference. It represents our updated belief about the parameter \(\theta\) after observing data \(D\). Using <strong>Bayes‚Äô theorem</strong>, we compute:</p> \[p(\theta \mid D, x) \propto p(D \mid \theta, x) p(\theta)\] <p>where:</p> <ul> <li>\(p(D \mid \theta, x)\) is the <strong>likelihood function</strong> \(L_D(\theta)\), describing how likely the data is given the parameter \(\theta\).</li> <li>\(p(\theta)\) is the <strong>prior</strong> distribution, encoding our prior knowledge about \(\theta\).</li> </ul> <p>This updated posterior distribution allows us to make <strong>probabilistically sound predictions</strong> while explicitly incorporating uncertainty.</p> <h5 id="estimating-parameters-point-estimates"><strong>Estimating Parameters: Point Estimates</strong></h5> <p>While Bayesian inference provides a full posterior distribution over \(\theta\), sometimes we may need a single point estimate. Different choices arise depending on the loss function we minimize:</p> <ul> <li> <p><strong>Posterior mean</strong>:</p> \[\hat{\theta} = \mathbb{E}[\theta \mid D, x]\] <p>This minimizes squared error loss.</p> </li> <li> <p><strong>Posterior median</strong>:</p> \[\hat{\theta} = \text{median}(\theta \mid D, x)\] <p>This minimizes absolute error loss.</p> </li> <li> <p><strong>Maximum a posteriori (MAP) estimate</strong>:</p> \[\hat{\theta} = \arg\max_{\theta \in \Theta} p(\theta \mid D, x)\] <p>This finds the most probable parameter value under the posterior.</p> </li> </ul> <p>Each approach has its advantages, and the choice depends on the <strong>application and the cost of different types of errors</strong>.</p> <h4 id="bayesian-prediction-function"><strong>Bayesian Prediction Function</strong></h4> <p>The goal of any supervised learning method is to learn a function that maps input \(x \in X\) to a distribution over outputs \(Y\). The key difference between frequentist and Bayesian approaches lies in how they achieve this.</p> <h5 id="frequentist-approach"><strong>Frequentist Approach</strong></h5> <p>In a frequentist framework:</p> <ol> <li>We choose a <strong>hypothesis space</strong>‚Äîa family of conditional probability densities.</li> <li>We estimate a single best-fit parameter \(\hat{\theta}(D)\) using MLE or another optimization method.</li> <li>We make predictions using \(p(y \mid x, \hat{\theta}(D))\), ignoring uncertainty in \(\theta\).</li> </ol> <h5 id="bayesian-approach"><strong>Bayesian Approach</strong></h5> <p>In contrast, Bayesian methods:</p> <ol> <li>Define a <strong>parametric family</strong> of conditional densities \(\{ p(y \mid x, \theta) : \theta \in \Theta \}\).</li> <li>Specify a <strong>prior distribution</strong> \(p(\theta)\).</li> <li>Instead of selecting a single best-fit \(\theta\), integrate over all possible parameters using the posterior.</li> </ol> <p>This results in a <strong>predictive distribution</strong> that <strong>preserves model uncertainty</strong> rather than discarding it.</p> <h5 id="the-prior-and-posterior-predictive-distributions"><strong>The Prior and Posterior Predictive Distributions</strong></h5> <p>Even before observing any data, we can make predictions using the <strong>prior predictive distribution</strong>:</p> \[p(y \mid x) = \int p(y \mid x, \theta) p(\theta) d\theta\] <p>This represents an average over all conditional densities, weighted by the prior \(p(\theta)\). Once we observe data \(D\), we compute the <strong>posterior predictive distribution</strong>:</p> \[p(y \mid x, D) = \int p(y \mid x, \theta) p(\theta \mid D) d\theta\] <p>This distribution takes into account both the likelihood and prior, providing <strong>updated predictions</strong> that reflect the data.</p> <p>[How to make intuitive sense of this? and What happens if we do this? and What if not?]</p> <hr/> <p><strong>Q: How does the prior predictive distribution get its value? What does it mean to make predictions before observing data, and how does this function account for it?</strong></p> <p>The prior predictive distribution represents predictions before observing data, and it accounts for the uncertainty in the model parameters by averaging over all possible values of the parameters based on the prior distribution. It essentially captures the expected predictions by integrating over the entire parameter space, weighted by the prior beliefs about the parameters.</p> <p>Mathematically, the prior predictive distribution is given by: \(p(y \mid x) = \int p(y \mid x, \theta) p(\theta) d\theta\)</p> <p>Here‚Äôs how it works:</p> <ol> <li> <p><strong>Prior Distribution \(p(\theta)\):</strong><br/> This reflects our beliefs about the parameters \(\theta\) before any data is observed. It could be based on prior knowledge or assumptions about the parameters‚Äô likely values.</p> </li> <li> <p><strong>Likelihood \(p(y \mid x, \theta)\):</strong><br/> This describes the model that predicts the outcome \(y\) given the input data \(x\) and the parameters \(\theta\). It represents the relationship between the parameters and the predicted outcomes.</p> </li> <li> <p><strong>Prior Predictive Distribution:</strong><br/> The integral sums over all possible values of \(\theta\), weighted by the prior distribution \(p(\theta)\), and gives the expected outcome \(y\). It represents predictions before any data is observed by averaging over the entire parameter space as described by the prior distribution.</p> </li> </ol> <p><strong>Conceptually:</strong></p> <ul> <li><strong>Predictions before observing data</strong> means we are making predictions based on our beliefs about the parameters, without any data to inform us.</li> <li>The prior predictive distribution is essentially a <strong>preliminary prediction</strong> that incorporates uncertainty about the parameters, providing a forecast based on the prior assumptions, rather than the actual data.</li> </ul> <p><strong>Example:</strong></p> <p>If we were predicting the height of individuals based on age and gender, the prior predictive distribution would give us an expected distribution of heights based on our prior assumptions about average height and variation, before any actual data on height is observed.</p> <p><strong>Why is it useful?</strong></p> <p>The prior predictive distribution gives us an initial understanding of what predictions might look like before data is available, incorporating prior knowledge about the parameters. However, once data is observed, this prediction is updated using the posterior predictive distribution, which integrates both prior beliefs and observed data.</p> <p><strong>Q: Why use the posterior predictive distribution?</strong></p> <ul> <li>It refines predictions using observed data.</li> <li>It accounts for uncertainty by integrating over posterior \(p(\theta \mid D)\).</li> <li>It prevents overconfident predictions from a single parameter estimate.</li> </ul> <p><strong>Q: What if we don‚Äôt use it?</strong></p> <ul> <li>Using only the prior predictive distribution leads to uninformed predictions.</li> <li>Relying on a single \(\theta\) (e.g., MLE) ignores uncertainty, increasing overconfidence.</li> <li>Ignoring parameter uncertainty may lead to suboptimal decisions.</li> </ul> <p><strong>Q: Is Integrating Over \(\theta\) the Same as Marginalizing It?</strong></p> <p>Yes, integrating over \(\theta\) in Bayesian inference is effectively <strong>marginalizing</strong> it out. When computing the <strong>posterior predictive distribution</strong>,</p> \[p(y \mid x, D) = \int p(y \mid x, \theta) p(\theta \mid D) d\theta\] <p>we sum (integrate) over all possible values of \(\theta\), weighted by their posterior probability \(p(\theta \mid D)\). This removes \(\theta\) as an explicit parameter, ensuring predictions reflect all plausible values rather than relying on a single estimate. In contrast, frequentist methods select a single \(\hat{\theta}\) (e.g., MLE or MAP), which does not account for uncertainty in \(\theta\). By marginalizing \(\theta\), Bayesian inference naturally incorporates parameter uncertainty, leading to more robust and well-calibrated predictions.</p> <p><strong>Takeaway:</strong> The posterior predictive distribution provides well-calibrated, data-driven predictions while maintaining uncertainty estimates.</p> <blockquote> <p>Bayesian Analogy: A Detective Solving a Case</p> </blockquote> <h5 id="1-prior--what-you-know-before-the-investigation"><strong>1. Prior ‚Äì What You Know Before the Investigation</strong></h5> <p>Imagine you‚Äôre a detective assigned to a case. Before you‚Äôve looked at any clues or evidence (i.e., before observing any data), you have some <strong>prior beliefs</strong> based on your experience or intuition about the suspect.</p> <p>For example, maybe based on past cases, you believe the suspect is likely to be someone in their 30s (that‚Äôs your <strong>prior</strong> belief). It could be based on things like:</p> <ul> <li>Crime trends (e.g., most crimes in this area are committed by people in their 30s).</li> <li>Hunches or experience (e.g., in your line of work, you‚Äôve seen that younger suspects tend to get caught more easily, so older individuals are more likely to be the culprits).</li> </ul> <p>This <strong>prior belief</strong> about who the suspect might be is like the <strong>prior distribution</strong> in Bayesian statistics‚Äîit‚Äôs your <strong>best guess</strong> before you have any real evidence (data).</p> <h5 id="2-likelihood--how-the-clues-fit-the-suspect"><strong>2. Likelihood ‚Äì How the Clues Fit the Suspect</strong></h5> <p>Now, you start finding <strong>clues</strong> (data) that might suggest a certain suspect. The clues don‚Äôt give you the full picture, but they help you refine your guess.</p> <p>Let‚Äôs say you find a footprint at the crime scene, and based on your knowledge, the likelihood that someone in their 30s leaves this kind of print is relatively high. But the likelihood is not zero for other age groups either‚Äîit‚Äôs just higher for people in their 30s.</p> <p>In Bayesian terms, <strong>likelihood</strong> is how <strong>likely</strong> it is to see the data (e.g., the footprint) given different possible values for your parameters (e.g., the age of the suspect). You‚Äôre comparing the fit of each possible age (parameter) to the actual clue.</p> <h5 id="3-posterior--your-updated-belief-after-seeing-the-clues"><strong>3. Posterior ‚Äì Your Updated Belief After Seeing the Clues</strong></h5> <p>Once you have both your <strong>prior belief</strong> and the <strong>clues</strong>, you combine them to get a better sense of who the suspect might be. This process is called <strong>updating your belief</strong>.</p> <p>So, after considering the clue (e.g., the footprint), you revise your initial guess. Maybe, now that you know the footprint matches your original suspicion of a person in their 30s, you <strong>update</strong> your belief to make it even <strong>stronger</strong>.</p> <p>In Bayesian terms, this is the <strong>posterior distribution</strong>: it‚Äôs the updated belief about the parameters (e.g., the suspect‚Äôs age) <strong>after incorporating the new data (evidence)</strong>. The posterior combines your <strong>prior</strong> belief and the <strong>likelihood</strong> of the evidence, giving you a new <strong>posterior</strong> that reflects both.</p> <h5 id="4-integrating--considering-all-possibilities"><strong>4. Integrating ‚Äì Considering All Possibilities</strong></h5> <p>Finally, to update your belief, you need to <strong>integrate</strong> all the possibilities. For example, you might not be 100% sure that the suspect is in their 30s, but you know that they‚Äôre more likely to be in that age group than in their 40s or 20s. You <strong>integrate</strong> over all the possible ages by weighing them by how probable each one is (based on the prior belief and likelihood).</p> <p>This is where the <strong>integration</strong> comes in. In Bayesian terms, you‚Äôre averaging over all possible values (ages) to get the best <strong>overall</strong> estimate of the suspect‚Äôs age (which is the posterior). You‚Äôre not just picking the most likely answer; you‚Äôre considering all the possibilities and combining them in a way that incorporates both your prior and the evidence you‚Äôve gathered.</p> <hr/> <h5 id="making-point-predictions-from-py-mid-x-d"><strong>Making Point Predictions from \(p(y \mid x, D)\)</strong></h5> <p>Once we have the full predictive distribution, we can extract <strong>point predictions</strong> depending on the loss function we wish to minimize:</p> <ul> <li> <p><strong>Mean prediction</strong> (minimizing squared error loss):</p> \[\mathbb{E}[y \mid x, D]\] </li> <li> <p><strong>Median prediction</strong> (minimizing absolute error loss):</p> \[\text{median}(y \mid x, D)\] </li> <li> <p><strong>Mode (MAP estimate of \(y\))</strong> (minimizing 0/1 loss):</p> \[\arg\max_{y \in Y} p(y \mid x, D)\] </li> </ul> <p>Each of these choices is derived directly from the <strong>posterior predictive distribution</strong>, making Bayesian methods highly flexible for different objectives.</p> <hr/> <blockquote> <p>Okay, everything makes sense now‚Äîat least somewhat. But what‚Äôs the real difference between all these Bayesian concepts we‚Äôve covered?</p> </blockquote> <p>Bayesian Conditional Models, Bayes Point Estimation, and Bayesian Decision Theory are all part of the broader Bayesian framework, but they serve different purposes. Here‚Äôs how they differ:</p> <h5 id="1-bayesian-conditional-models-bcm--a-probabilistic-approach-to-prediction"><strong>1. Bayesian Conditional Models (BCM) ‚Äì A Probabilistic Approach to Prediction</strong></h5> <p>Bayesian Conditional Models focus on modeling <strong>conditional distributions</strong> of an outcome \(Y\) given an input \(X\). Instead of choosing a single best function or parameter, BCM maintains a <strong>distribution over possible models</strong> and integrates over uncertainty.</p> <ul> <li><strong>Key Idea</strong>: Instead of selecting a fixed hypothesis (as in frequentist methods), we consider an entire <strong>distribution over models</strong> and use it for making predictions.</li> <li><strong>Mathematical Formulation</strong>: <ul> <li> <p><strong>Prior Predictive Distribution</strong> (before observing data):</p> \[p(y | x) = \int p(y | x, \theta) p(\theta) d\theta\] </li> <li> <p><strong>Posterior Predictive Distribution</strong> (after observing data \(D\)):</p> \[p(y | x, D) = \int p(y | x, \theta) p(\theta | D) d\theta\] </li> </ul> </li> <li><strong>Relation to Other Concepts</strong>: BCM extends Bayesian inference to <strong>predictive modeling</strong>, ensuring that uncertainty is incorporated directly into the predictions.</li> </ul> <h5 id="2-bayes-point-estimation-bpe--a-single-best-estimate-of-parameters"><strong>2. Bayes Point Estimation (BPE) ‚Äì A Single Best Estimate of Parameters</strong></h5> <p>Bayes Point Estimation, in contrast, is about finding a <strong>single ‚Äúbest‚Äù estimate</strong> for the model parameters \(\theta\), given the posterior distribution \(p(\theta \mid D)\). It‚Äôs a simplification of full Bayesian inference when we need a point estimate rather than an entire distribution.</p> <ul> <li><strong>Key Idea</strong>: Instead of integrating over all possible parameters, we select a <strong>single representative parameter</strong> from the posterior.</li> <li><strong>Common Choices</strong>: <ul> <li> <p><strong>Posterior Mean</strong>:</p> \[\hat{\theta} = \mathbb{E}[\theta \mid D]\] <p>(Minimizes squared error)</p> </li> <li> <p><strong>Posterior Median</strong>:</p> \[\hat{\theta} = \text{median}(\theta \mid D)\] <p>(Minimizes absolute error)</p> </li> <li> <p><strong>Maximum a Posteriori (MAP) Estimate</strong>:</p> \[\hat{\theta} = \arg\max_{\theta} p(\theta \mid D)\] <p>(Maximizes posterior probability)</p> </li> </ul> </li> <li><strong>Difference from BCM</strong>: BCM keeps the full predictive distribution, while BPE collapses uncertainty into a single parameter choice.</li> </ul> <h5 id="3-bayesian-decision-theory-bdt--making-optimal-decisions-with-uncertainty"><strong>3. Bayesian Decision Theory (BDT) ‚Äì Making Optimal Decisions with Uncertainty</strong></h5> <p>Bayesian Decision Theory extends Bayesian inference to <strong>decision-making</strong>. It incorporates a <strong>loss function</strong> to determine the best action given uncertain outcomes.</p> <ul> <li><strong>Key Idea</strong>: Instead of just estimating parameters, we aim to make an <strong>optimal decision</strong> that minimizes expected loss.</li> <li> <p><strong>Mathematical Formulation</strong>: Given a loss function \(L(a, y)\) for action \(a\) and outcome \(y\), the optimal action is:</p> \[a^* = \arg\min_a \mathbb{E}[L(a, Y) \mid D]\] </li> <li><strong>Relation to BCM</strong>: <ul> <li>BCM provides a <strong>full predictive distribution</strong> of \(Y\), which is then used in BDT to make optimal decisions.</li> <li>If we only care about a <strong>single estimate</strong>, we apply Bayes Point Estimation within BDT.</li> </ul> </li> </ul> <h5 id="summary-of-differences"><strong>Summary of Differences</strong></h5> <hr/> <table> <thead> <tr> <th>Concept</th> <th>Focus</th> <th>Key Idea</th> <th>Output</th> </tr> </thead> <tbody> <tr> <td><strong>Bayesian Conditional Models (BCM)</strong></td> <td>Predicting \(Y\) given \(X\)</td> <td>Maintain a <strong>distribution over possible models</strong></td> <td>A full <strong>predictive distribution</strong> \(p(y \vert x, D)\)</td> </tr> <tr> <td><strong>Bayes Point Estimation (BPE)</strong></td> <td>Estimating model parameters \(\theta\)</td> <td>Choose a <strong>single best estimate</strong> from the posterior</td> <td>A point estimate \(\hat{\theta}\) (e.g., posterior mean, MAP)</td> </tr> <tr> <td><strong>Bayesian Decision Theory (BDT)</strong></td> <td>Making optimal decisions</td> <td>Select the <strong>best action</strong> based on a loss function</td> <td>An action \(a^*\) that minimizes expected loss</td> </tr> </tbody> </table> <hr/> <p>So, <strong>Bayesian Conditional Models are a more general framework</strong> that encompasses both Bayesian Point Estimation and Bayesian Decision Theory as special cases when we either want a point estimate or a decision-making strategy.</p> <h5 id="practical-applications-of-bayesian-conditional-models"><strong>Practical Applications of Bayesian Conditional Models</strong></h5> <p>Bayesian conditional models are widely used in various fields where uncertainty plays a crucial role:</p> <ul> <li><strong>Medical Diagnosis &amp; Healthcare</strong>: Bayesian models help in probabilistic disease prediction, patient risk assessment, and adaptive clinical trials where data is limited.</li> <li><strong>Finance &amp; Risk Management</strong>: Used for credit scoring, fraud detection, and portfolio optimization, where uncertainty in market conditions needs to be modeled explicitly.</li> <li><strong>Autonomous Systems &amp; Robotics</strong>: Bayesian approaches help robots and self-driving cars make <strong>decisions under uncertainty</strong>, such as obstacle avoidance and motion planning.</li> <li><strong>Recommendation Systems</strong>: Bayesian methods improve user personalization by adapting to changing preferences with uncertainty-aware updates.</li> </ul> <blockquote> <p>Let‚Äôs tie it all together with a story to help us feel it.</p> </blockquote> <h5 id="1-bayesian-conditional-models-bcm--predicting-the-route"><strong>1. Bayesian Conditional Models (BCM) ‚Äì Predicting the Route</strong></h5> <p>Think of a scenario where you‚Äôre planning a trip, and you need to choose a route from a starting point (X) to your destination (Y). Instead of using just one route (which could be inaccurate), you take into account a variety of possible routes and factor in your <strong>uncertainty</strong> about traffic conditions, road closures, and construction. You create a model that looks at all the possible routes, weighing each of them based on how likely they are to be optimal given the current information.</p> <ul> <li> <p><strong>Intuition</strong>: BCM is like saying, ‚ÄúI‚Äôm not sure which exact route to take, so let‚Äôs consider all the possible routes and their chances of being optimal based on my prior knowledge of traffic and construction conditions.‚Äù</p> </li> <li> <p><strong>In Bayesian Terms</strong>: You‚Äôre integrating over all possible routes (models) to get a <strong>distribution of possible outcomes</strong> (where you might end up). You‚Äôre not just picking one route, but making an informed prediction that accounts for your uncertainty.</p> </li> </ul> <h5 id="2-bayes-point-estimation-bpe--choosing-the-best-route"><strong>2. Bayes Point Estimation (BPE) ‚Äì Choosing the Best Route</strong></h5> <p>Now imagine you‚Äôve gathered more information, such as current traffic reports and road conditions. You can now estimate the ‚Äúbest‚Äù route to take. Instead of considering every possible route, you choose the one that has the highest likelihood of being optimal given the current data.</p> <ul> <li> <p><strong>Intuition</strong>: BPE is like saying, ‚ÄúGiven what I know right now, the best choice is to take Route A. It might not be the perfect route, but it‚Äôs the one that seems most likely to get me to my destination quickly based on current data.‚Äù</p> </li> <li> <p><strong>In Bayesian Terms</strong>: You‚Äôre using the <strong>posterior distribution</strong> of routes and picking a <strong>single estimate</strong> (the route you think will be best). This is either the route with the <strong>posterior mean</strong>, the <strong>maximum a posteriori estimate (MAP)</strong>, or another representative value from the distribution.</p> </li> </ul> <h5 id="3-bayesian-decision-theory-bdt--choosing-the-optimal-action-based-on-costs"><strong>3. Bayesian Decision Theory (BDT) ‚Äì Choosing the Optimal Action Based on Costs</strong></h5> <p>Now, let‚Äôs introduce a <strong>cost</strong> to the decision-making. Imagine you‚Äôre trying to not only get to your destination as quickly as possible, but you also want to minimize costs‚Äîwhether that‚Äôs the cost of time, fuel, or stress. The optimal decision isn‚Äôt just about picking the quickest route, but about minimizing your <strong>expected cost</strong> (which could involve trade-offs, like a longer route with less traffic vs. a shorter one with more congestion).</p> <ul> <li> <p><strong>Intuition</strong>: BDT is like saying, ‚ÄúGiven that I want to minimize both my time and stress, I will pick the route that‚Äôs expected to cost me the least overall, even if it‚Äôs not the fastest.‚Äù</p> </li> <li> <p><strong>In Bayesian Terms</strong>: You‚Äôre using the <strong>predictive distribution</strong> (like BCM) to understand all possible outcomes, and then making a decision by minimizing the <strong>expected loss</strong> (cost) based on the uncertainty about your outcomes.</p> </li> </ul> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>Bayesian conditional models provide a <strong>principled and uncertainty-aware</strong> approach to prediction. Unlike frequentist methods, which estimate a single best-fit parameter, Bayesian inference maintains <strong>a full distribution over parameters</strong> and updates beliefs as new data arrives. This allows for <strong>more robust, probabilistically grounded predictions</strong>, making Bayesian methods an essential tool in modern machine learning.</p> <p>By integrating over possible hypotheses rather than committing to one, Bayesian models naturally <strong>quantify uncertainty</strong> and adapt to new information, making them particularly useful in scenarios with limited data or high variability.</p> <p>Next up, we‚Äôll use all of this to tackle <strong>Gaussian linear regression</strong>. Stay tuned, and see you in the next oneüëã!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.]]></summary></entry><entry><title type="html">On-line to Batch Conversion</title><link href="https://monishver11.github.io/blog/2025/online-to-batch/" rel="alternate" type="text/html" title="On-line to Batch Conversion"/><published>2025-01-30T15:00:00+00:00</published><updated>2025-01-30T15:00:00+00:00</updated><id>https://monishver11.github.io/blog/2025/online-to-batch</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/online-to-batch/"><![CDATA[<p>In previous sections, we explored various algorithms for online learning, such as the <strong>Perceptron</strong> and <strong>Winnow algorithms</strong>, analyzing their performance within the <strong>mistake model</strong>‚Äîa setting where no assumptions are made about how the training sequence is generated.</p> <p>A natural question arises:</p> <blockquote> <p>Can these algorithms be leveraged to derive hypotheses with <strong>small generalization error</strong> in a standard <strong>stochastic setting</strong>?<br/> Moreover, how can the intermediate hypotheses they generate be combined to form an <strong>accurate predictor</strong>?</p> </blockquote> <p>These questions are the focus of this section.</p> <hr/> <h5 id="problem-setup"><strong>Problem Setup</strong></h5> <p>We consider a <strong>supervised learning</strong> setting where we have a labeled sample:</p> \[S = \{(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)\} \subset (X \times Y)^T\] <p>drawn <strong>i.i.d.</strong> from some unknown but fixed distribution \(D\).</p> <p>Let \(H\) be a <strong>hypothesis class</strong> consisting of functions mapping \(X\) to \(Y'\), and let \(L: Y' \times Y \to \mathbb{R}^+\) be a <strong>bounded loss function</strong>, meaning there exists a constant \(M \geq 0\) such that:</p> \[L \leq M.\] <p>An <strong>online learning algorithm</strong> \(A\) sequentially processes the dataset \(S\), generating hypotheses:</p> \[h_1, h_2, \dots, h_T \in H.\] <p>It starts with an initial hypothesis \(h_1\) and updates it after processing each training example \((x_t, y_t)\), for \(t \in [T]\).</p> <hr/> <h4 id="regret-and-generalization-error"><strong>Regret and Generalization Error</strong></h4> <p>Before we tackle the question we started with, let‚Äôs first clarify why it‚Äôs important to address it. Also, let‚Äôs define the key terms and concepts to ensure everything is clear and aligned as we proceed.</p> <h5 id="1-regret">1. <strong>Regret:</strong></h5> <p>The <strong>regret</strong> of an algorithm is a measure of how much worse the algorithm‚Äôs performance is compared to the best possible hypothesis in the class \(H\), in hindsight.</p> <p>Formally, it‚Äôs defined as:</p> \[R_T = \sum_{t=1}^{T} L(h_t(x_t), y_t) - \min_{h \in H} \sum_{t=1}^{T} L(h(x_t), y_t)\] <p>Let‚Äôs break this down:</p> <ul> <li>\(h_t(x_t)\) is the prediction of the algorithm at time \(t\), after processing the \(t\)-th training example \((x_t, y_t)\).</li> <li>The term \(\sum_{t=1}^{T} L(h_t(x_t), y_t)\) is the total loss the algorithm incurs by predicting \(h_t(x_t)\) for each training example.</li> <li>\(\min_{h \in H} \sum_{t=1}^{T} L(h(x_t), y_t)\) represents the loss of the best hypothesis \(h \in H\) if it were chosen in advance and used to predict every \(x_t\) in the sequence.</li> </ul> <p><strong>Why regret matters:</strong><br/> Regret tells us how far off the algorithm‚Äôs performance is compared to the best possible performance it could have had with a perfect hypothesis. If the regret is small, it means the algorithm is making predictions that are close to the best possible predictions, at least in terms of total loss.</p> <h5 id="2-generalization-error">2. <strong>Generalization Error:</strong></h5> <p>The <strong>generalization error</strong> of a hypothesis \(h \in H\) refers to how well it performs on unseen data from the same distribution \(D\), and it‚Äôs typically measured by the <strong>expected loss</strong>:</p> \[R(h) = \mathbb{E}_{(x,y) \sim D} [L(h(x), y)]\] <p>In other words, the generalization error measures how well the hypothesis \(h\) performs on future data (or on data drawn from the same distribution) as opposed to just the training data. This is crucial because we want our hypothesis to not just perform well on the training set (which can be overfit) but also to generalize well to new, unseen examples.</p> <p><strong>How does this tie into the problem setup?</strong><br/> The goal in this section is to bound the <strong>average generalization error</strong> of the sequence of hypotheses \(h_1, h_2, \dots, h_T\) generated by the algorithm \(A\). You want to know if the algorithm can generate hypotheses whose <strong>average loss</strong> on the training dataset \(S\) is a good predictor of their <strong>generalization error</strong>‚Äîmeaning, will the algorithm‚Äôs performance on the training set reflect its performance on unseen data?</p> <h5 id="connecting-regret-to-generalization-error"><strong>Connecting Regret to Generalization Error:</strong></h5> <p>Now, let‚Äôs dive into the key insight of this section:</p> <ul> <li>The algorithm generates a sequence of hypotheses \(h_1, h_2, \dots, h_T\), but we don‚Äôt necessarily care about each individual hypothesis. Rather, we care about the <strong>average performance</strong> over all of them.</li> </ul> <p>Let‚Äôs assume the <strong>average loss</strong> of the hypotheses on the training data is:</p> \[\hat{L} = \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t)\] <p>The idea is that, if the average loss \(\hat{L}\) is small (i.e., the algorithm has done well on the training set), then the <strong>generalization error</strong> (i.e., the expected loss on unseen data) should also be small. The question here is how small, and whether we can bound this generalization error using the average loss on the training set.</p> <h5 id="key-insights-and-the-challenge"><strong>Key Insights and the Challenge:</strong></h5> <p>The big challenge in this problem setup is to establish a <strong>connection between regret and generalization error</strong>. Specifically, you are interested in showing that even though the algorithm has no prior knowledge of the distribution \(D\), the average loss of the hypotheses it generates will give you a good idea of their <strong>generalization performance</strong>.</p> <h5 id="how-do-we-approach-this"><strong>How do we approach this?</strong></h5> <p>To answer the question of how to use the average loss to bound the generalization error, we might use tools from <strong>PAC learning</strong> (Probably Approximately Correct) theory, where we can bound the generalization error of a hypothesis by looking at its empirical loss on the training set and how much variability exists due to the randomness in the sampling process.</p> <p>In simple terms:</p> <ul> <li>If the training loss is small, can we show that the hypothesis will likely also perform well on unseen data?</li> <li>Can we bound how much the training loss differs from the generalization error for the hypotheses \(h_1, h_2, \dots, h_T\)?</li> </ul> <p>This section would likely explore these questions in greater detail, using concepts such as <strong>concentration inequalities</strong> (e.g., Hoeffding‚Äôs inequality) or tools from statistical learning theory (e.g., uniform convergence) to establish these bounds.</p> <h5 id="key-takeaways"><strong>Key Takeaways:</strong></h5> <ul> <li><strong>Regret</strong> measures how well the algorithm‚Äôs performance compares to the best possible hypothesis over the sequence.</li> <li><strong>Generalization error</strong> measures how well a hypothesis performs on new, unseen data.</li> <li>The main goal is to relate the <strong>average loss</strong> of the hypotheses generated by the online algorithm to their <strong>generalization error</strong>, so that we can prove the algorithm produces good generalizers, not just good fitters to the training data.</li> </ul> <p>Alright, we‚Äôre all set! Now, let‚Äôs take a look at the actual bounds that are available.</p> <hr/> <h4 id="generalization-error-bound"><strong>Generalization Error Bound</strong></h4> <p>The following lemma provides a bound on the average generalization error in terms of the empirical loss:</p> <h5 id="lemma-814"><strong>Lemma 8.14</strong></h5> <p>Let \(S = \{(x_1,y_1), ..., (x_T,y_T)\}\) be an i.i.d. sample from \(D\), and let \(h_1, ..., h_T\) be the sequence of hypotheses generated by an online algorithm \(A\) processing \(S\). If the loss function is bounded by \(M\), then for any \(\delta &gt; 0\), with probability at least \(1 - \delta\):</p> \[\frac{1}{T} \sum_{t=1}^{T} R(h_t) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log(1/\delta)}{T}}\] <h5 id="proof-sketch"><strong>Proof Sketch:</strong></h5> <p>We begin by defining the random variable:</p> \[V_t = R(h_t) - L(h_t(x_t), y_t)\] <p>This represents the difference between the <strong>true expected loss</strong> (generalization error) of \(h_t\) and the <strong>empirical loss</strong> observed on the sample \((x_t, y_t)\).</p> <p><strong>Step 1: Establishing Expectation Property</strong></p> <p>Since the data points \((x_t, y_t)\) are drawn <strong>i.i.d.</strong> from the distribution \(D\), the expectation of the empirical loss over the randomness in the sample should match the expected loss:</p> \[\mathbb{E}[L(h_t(x_t), y_t) | h_t] = R(h_t)\] <p>Rearranging this gives:</p> \[\mathbb{E}[V_t | x_1, ..., x_{t-1}] = R(h_t) - \mathbb{E}[L(h_t(x_t), y_t) | h_t] = 0\] <p>Thus, the sequence \(\{V_t\}_{t=1}^{T}\) forms a <strong>martingale difference sequence</strong>, meaning that the expected value of \(V_t\) at any step remains zero given all past observations. This ensures that there is no systematic drift in expectations.</p> <p>This means that knowing past values does not help predict the next value‚Äôs expectation‚Äîit remains centered around zero. No systematic drift means that the sequence does not consistently increase or decrease over time; it behaves like a fair game in probability.</p> <p><strong>Step 2: Bounding the Range of \(V_t\)</strong></p> <p>Since the loss function is <strong>bounded</strong> by \(M\), we get:</p> \[L(h_t(x_t), y_t) \in [0, M] \quad \Rightarrow \quad V_t = R(h_t) - L(h_t(x_t), y_t) \in [-M, +M]\] <p>This ensures that \(V_t\) is always within a fixed range.</p> <p><strong>Step 3: Applying Azuma‚Äôs Inequality</strong></p> <p>Azuma‚Äôs inequality states that for a martingale difference sequence \(V_t\) with bounded differences \(\vert V_t \vert \leq M\), the probability of the empirical mean deviating from its expectation satisfies:</p> \[P\left( \frac{1}{T} \sum_{t=1}^{T} V_t \geq \epsilon \right) \leq \exp \left( -\frac{2T\epsilon^2}{4M^2} \right)\] <p>Rearranging the denominator:</p> \[P\left( \frac{1}{T} \sum_{t=1}^{T} V_t \geq \epsilon \right) \leq \exp \left( -\frac{T\epsilon^2}{2M^2} \right)\] <p><strong>Step 4: Solving for \(\epsilon\)</strong></p> <p>We now set the right-hand side equal to \(\delta &gt; 0\):</p> \[\exp \left( -\frac{T\epsilon^2}{2M^2} \right) = \delta\] <p>Taking the natural logarithm on both sides:</p> \[-\frac{T\epsilon^2}{2M^2} = \log \delta\] <p>Solving for \(\epsilon\):</p> \[\epsilon = M \sqrt{\frac{2 \log(1/\delta)}{T}}\] <p><strong>Step 5: Concluding the Bound</strong></p> <p>Since \(\frac{1}{T} \sum_{t=1}^{T} V_t\) represents the difference between the <strong>average generalization error</strong> and the <strong>empirical average loss</strong>, we obtain:</p> \[\frac{1}{T} \sum_{t=1}^{T} R(h_t) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log(1/\delta)}{T}}\] <p>This completes the proof.</p> <hr/> <p>Before we move forward, let‚Äôs break down what all of this means and how we‚Äôre using it to reach our desired outcome.</p> <h5 id="why-does-the-martingale-difference-property-allow-us-to-apply-azumas-inequality"><strong>Why does the martingale difference property allow us to apply Azuma‚Äôs inequality?</strong></h5> <ul> <li>The key idea of a <strong>martingale difference sequence</strong> is that at each step, the expectation of the difference \(V_t\) is <strong>zero</strong> given past observations.</li> <li>This means that on average, the sequence doesn‚Äôt drift systematically in any direction‚Äîit‚Äôs <strong>balanced</strong> in expectation.</li> <li>Moreover, since each \(V_t\) is <strong>bounded</strong> within \([-M, M]\), there are no extreme jumps.</li> <li><strong>Azuma‚Äôs inequality</strong> applies precisely in such cases‚Äîit tells us that despite small fluctuations at each step, the <strong>cumulative deviation from zero</strong> remains <strong>small with high probability</strong>.</li> </ul> <p>üí° <strong>Analogy:</strong> Imagine you‚Äôre walking on a balance beam with a safety harness.</p> <ul> <li>You might take small steps to the left or right, but <strong>each step is unbiased</strong>, meaning you‚Äôre not favoring one direction over time.</li> <li>Also, your steps are limited in size (bounded).</li> <li><strong>Azuma‚Äôs inequality is like saying that, with high probability, you won‚Äôt drift too far from the center because of these properties.</strong></li> </ul> <h5 id="how-does-this-help-bound-generalization-error"><strong>How does this help bound generalization error?</strong></h5> <ul> <li>The empirical loss over the training sample is what we observe, but what we <strong>really care about</strong> is the generalization error (expected loss over unseen data).</li> <li>The sum \(\sum_{t=1}^{T} V_t\) captures how much our empirical loss <strong>deviates</strong> from the true generalization error.</li> <li>Since we established that this deviation behaves like a <strong>martingale difference sequence</strong>, we can now use Azuma‚Äôs inequality to show that this deviation is <strong>small with high probability</strong>.</li> <li><strong>This means that our empirical loss is a good estimate of the true generalization error, up to a small correction term!</strong></li> </ul> <p>üí° <strong>Analogy:</strong> Think of a weather forecast model trained on past temperature data.</p> <ul> <li>If the model is unbiased and doesn‚Äôt systematically overestimate or underestimate temperatures, then <strong>on average</strong>, its predictions should be close to reality.</li> <li><strong>Azuma‚Äôs inequality tells us that, with high probability, the difference between past observed temperatures (empirical loss) and future actual temperatures (generalization error) remains small.</strong></li> <li>This justifies why our training loss is a <strong>reliable estimate</strong> of test loss.</li> </ul> <hr/> <h4 id="application-averaging-hypotheses"><strong>Application: Averaging Hypotheses</strong></h4> <p>When the loss function is <strong>convex</strong> in its first argument, we can bound the generalization error of the <strong>average hypothesis</strong>:</p> \[\bar{h} = \frac{1}{T} \sum_{t=1}^{T} h_t\] <p>Since expectation is <strong>linear</strong>, we can use <strong>Jensen‚Äôs inequality</strong>, which states that for a convex function \(f\):</p> \[f\left(\frac{1}{T} \sum_{t=1}^{T} x_t\right) \leq \frac{1}{T} \sum_{t=1}^{T} f(x_t)\] <p>Applying this to the generalization error:</p> \[R(\bar{h}) = \mathbb{E}_{(x,y) \sim D} [L(\bar{h}(x), y)]\] <p>By convexity of \(L\), we get:</p> \[R(\bar{h}) \leq \frac{1}{T} \sum_{t=1}^{T} R(h_t)\] <p>From our previous bound:</p> \[\frac{1}{T} \sum_{t=1}^{T} R(h_t) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log(1/\delta)}{T}}\] <p>Thus, we obtain:</p> \[R(\bar{h}) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log(1/\delta)}{T}}\] <p><strong>Key Insights:</strong></p> <ul> <li>The generalization error of the <strong>averaged</strong> hypothesis is controlled by the <strong>average loss</strong> of the individual hypotheses plus a small correction term.</li> <li>This shows that averaging hypotheses is a simple yet powerful way to obtain a <strong>low generalization error</strong> in an online learning setting.</li> </ul> <h5 id="connection-to-regret-minimization"><strong>Connection to Regret Minimization</strong></h5> <p>If the online learning algorithm has <strong>small regret</strong> \(R_T\), then:</p> \[\frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) \approx \inf_{h \in H} \frac{1}{T} \sum_{t=1}^{T} L(h(x_t), y_t)\] <p>Since the second term in our bound vanishes as \(T \to \infty\), we conclude:</p> \[R(\bar{h}) \approx \inf_{h \in H} R(h)\] <p>This means that the average hypothesis is nearly <strong>optimal</strong> in terms of generalization error.</p> <p>If the above result feels a bit half-baked for you to process, no worries‚Äîwe‚Äôll work through it and figure out what it means next.</p> <hr/> <h4 id="theorem-815-generalization-error-of-averaged-hypotheses"><strong>Theorem 8.15: Generalization Error of Averaged Hypotheses</strong></h4> <p>Let \(S = ((x_1, y_1), \dots, (x_T, y_T))\) be a labeled sample drawn <strong>i.i.d.</strong> according to distribution \(D\). Let \(L\) be a <strong>loss function</strong> that is <strong>bounded</strong> by \(M\) and <strong>convex</strong> with respect to its first argument. Consider a sequence of hypotheses \(h_1, \dots, h_T\) generated by an online learning algorithm \(A\) processing \(S\) sequentially.</p> <p>Then, for any \(\delta &gt; 0\), with probability at least \(1 - \delta\), the following bounds hold:</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log \frac{1}{\delta}}{T}}.\] \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \inf_{h \in \mathcal{H}} R(h) + \frac{R_T}{T} + 2M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <hr/> <h5 id="proof"><strong>Proof</strong></h5> <p><strong>Step 1: Bounding the Generalization Error of the Averaged Hypothesis</strong></p> <p>By the convexity of \(L\) in its first argument, for any \((x, y)\), we have (via <strong>Jensen‚Äôs inequality</strong>):</p> \[L\left(\frac{1}{T} \sum_{t=1}^{T} h_t(x), y\right) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x), y).\] <p>Taking expectations over the data distribution \(D\):</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) = \mathbb{E}_{(x,y) \sim D} \left[ L\left(\frac{1}{T} \sum_{t=1}^{T} h_t(x), y\right) \right] \leq \frac{1}{T} \sum_{t=1}^{T} R(h_t).\] <p>Using <strong>Lemma 8.14</strong>, we get:</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log \frac{1}{\delta}}{T}}.\] <p><strong>Step 2: Connection to Regret Minimization</strong></p> <p>By definition of the <strong>regret</strong> \(R_T\), for any \(\delta &gt; 0\), the following holds with probability at least \(1 - \delta/2\):</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <p>Since the learner attempts to minimize regret, we take the <strong>minimum</strong> over all hypotheses:</p> \[\frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) \leq \min_{h \in \mathcal{H}} \frac{1}{T} \sum_{t=1}^{T} L(h(x_t), y_t) + \frac{R_T}{T}.\] <p><strong>Note:</strong> Not sure how we got this. Actually, this is what we started with, right? The definition of regret as defined above when we started, just dividing by T across.</p> <p>Thus, with probability at least \(1 - \delta/2\):</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \min_{h \in \mathcal{H}} \frac{1}{T} \sum_{t=1}^{T} L(h(x_t), y_t) + \frac{R_T}{T} + M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <p>Using Hoeffding‚Äôs inequality, we can show that for any optimal hypothesis \(h^*\), with probability at least \(1 - \delta/2\):</p> \[\frac{1}{T} \sum_{t=1}^{T} L(h^*(x_t), y_t) \leq R(h^*) + M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <p>Combining these results:</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq R(h^*) + \frac{R_T}{T} + 2M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <p>By the definition of \(\inf_{h \in H} R(h)\), for any \(\epsilon &gt; 0\), there exists \(h^* \in H\) with</p> \[R(h^*) \leq \inf_{h \in H} R(h) + \epsilon.\] <p>Since \(h^*\) is chosen optimally, which gives:</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \inf_{h \in \mathcal{H}} R(h) + \frac{R_T}{T} + 2M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <p>Since the inequality holds for all \(\epsilon &gt; 0\), it proves the second statement.</p> <hr/> <h5 id="application-to-online-learning-algorithms"><strong>Application to Online Learning Algorithms</strong></h5> <p>The theorem can be applied to a variety of <strong>online regret minimization algorithms</strong>. A key case is when:</p> \[\frac{R_T}{T} = O(1/\sqrt{T}).\] <p>In particular, we can apply this theorem to the <strong>exponentially weighted average algorithm</strong>. Assuming:</p> <ul> <li>The loss function \(L\) is <strong>bounded</strong> by \(M = 1\).</li> <li>The number of rounds \(T\) is <strong>known</strong> to the algorithm.</li> </ul> <p>Then, using the <strong>regret bound from Theorem 8.6</strong>, we obtain:</p> \[R_T \leq \sqrt{(T/2) \log N}\] <p>where \(N\) is the number of experts, or the <strong>dimension of the weight vectors</strong>.</p> <p>If \(T\) is <strong>not</strong> known in advance, we can apply the <strong>doubling trick</strong> (Theorem 8.7) to derive a similar bound.</p> <p>Before we wrap up, just one last thing: Hoeffding‚Äôs Inequality. I think I‚Äôll split this blog into two parts for better readability‚Äîit‚Äôs a bit too much to cover in one post.</p> <h4 id="hoeffdings-inequality-and-its-application-in-generalization-bounds"><strong>Hoeffding‚Äôs Inequality and Its Application in Generalization Bounds</strong></h4> <p>Hoeffding‚Äôs inequality is a fundamental concentration inequality that provides a bound on the probability that the sum of bounded independent random variables deviates from its expected value.</p> <h5 id="statement-of-hoeffdings-inequality"><strong>Statement of Hoeffding‚Äôs Inequality</strong></h5> <p>Let \(X_1, X_2, \dots, X_T\) be independent random variables such that for each \(t\),</p> \[a_t \leq X_t \leq b_t.\] <p>Define the sum:</p> \[S_T = \sum_{t=1}^{T} X_t.\] <p>Then, the probability that \(S_T\) deviates from its expected value by more than \(\epsilon\) satisfies:</p> \[P\left( \left| S_T - \mathbb{E}[S_T] \right| \geq \epsilon \right) \leq 2 \exp \left( -\frac{2 \epsilon^2}{\sum_{t=1}^{T} (b_t - a_t)^2} \right).\] <p>If each \(X_t\) is bounded in the range \([a, b]\), this simplifies to:</p> \[P\left( \left| S_T - \mathbb{E}[S_T] \right| \geq \epsilon \right) \leq 2 \exp \left( -\frac{2 \epsilon^2}{T (b - a)^2} \right).\] <h5 id="application-in-generalization-bounds"><strong>Application in Generalization Bounds</strong></h5> <p>In our proof, we use Hoeffding‚Äôs inequality to control the deviation of the empirical risk from the expected risk.</p> <p>Define the random variables:</p> \[X_t = L(h^*(x_t), y_t),\] <p>where \(h^*\) is the optimal hypothesis. Since the loss function is <strong>bounded by</strong> \(M\), we have:</p> \[0 \leq X_t \leq M.\] <p>Thus, the sum:</p> \[\sum_{t=1}^{T} L(h^*(x_t), y_t)\] <p>is a sum of <strong>bounded</strong> independent random variables. Applying Hoeffding‚Äôs inequality, we get:</p> \[P\left( \left| \frac{1}{T} \sum_{t=1}^{T} L(h^*(x_t), y_t) - R(h^*) \right| \geq \epsilon \right) \leq 2 \exp \left( -\frac{2 T \epsilon^2}{M^2} \right).\] <p>Setting \(\epsilon = M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}\) and solving for \(\delta\), we obtain the bound:</p> \[P\left( \frac{1}{T} \sum_{t=1}^{T} L(h^*(x_t), y_t) \geq R(h^*) + M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}} \right) \leq \frac{\delta}{2}.\] <p>This bound ensures that, with high probability, the empirical loss is <strong>close</strong> to the expected loss, which is crucial for proving the generalization bounds of the learning algorithm.</p> <p>A few questions that i had and you might too:</p> <p><strong>Why Does the Optimal \(h^*\) Minimize the Loss?</strong><br/> The optimal hypothesis \(h^*\) is defined as the one that minimizes the <strong>expected loss</strong> over the data distribution:</p> \[h^* = \arg\min_{h \in \mathcal{H}} R(h),\] <p>where the <strong>expected loss</strong> (also called the risk) is:</p> \[R(h) = \mathbb{E}_{(x,y) \sim D} [L(h(x), y)].\] <p>Since \(h^*\) minimizes this risk, it achieves the <strong>smallest possible expected loss</strong> over all hypotheses in \(\mathcal{H}\).</p> <hr/> <p><strong>Why This Justifies Applying Hoeffding‚Äôs Inequality</strong></p> <p>We consider the empirical loss:</p> \[\frac{1}{T} \sum_{t=1}^{T} L(h^*(x_t), y_t),\] <p>which is an <strong>estimate</strong> of the true expected loss \(R(h^*)\). However, since the sample is drawn <strong>i.i.d.</strong>, there is some randomness, so the empirical loss might deviate from the expected loss.</p> <p>Since the loss function is <strong>bounded</strong> (i.e., \(0 \leq L(h^*(x), y) \leq M\)), we can apply <strong>Hoeffding‚Äôs inequality</strong> to bound this deviation. This ensures that, with high probability, the empirical loss <strong>does not stray too far</strong> from \(R(h^*)\), giving us a reliable way to estimate the generalization error.</p> <hr/> <p><strong>Intuitive Explanation</strong></p> <p>Think of this as estimating the <strong>average height of people in a city</strong> by measuring the height of a random sample. The true average height (analogous to \(R(h^*)\)) is unknown, but if we take a large enough sample, the average height in our sample (empirical loss) will be <strong>close to the true average height</strong> with high probability.</p> <p>By Hoeffding‚Äôs inequality, the probability that our sample average <strong>significantly deviates</strong> from the true mean is exponentially small, ensuring our empirical loss is a good approximation of the expected loss.</p> <hr/> <p><strong>Key Insights</strong></p> <ul> <li>\(h^*\) is optimal since it minimizes the expected loss.</li> <li>The empirical loss is a sample-based approximation of \(R(h^*)\).</li> <li>Hoeffding‚Äôs inequality guarantees that the empirical loss is close to the true loss with high probability.</li> <li>This allows us to <strong>confidently generalize from training data to unseen data</strong>.</li> </ul> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>This analysis demonstrates that online learning algorithms, despite being designed for <strong>adversarial settings</strong>, can be effectively converted into batch learning methods. By averaging the hypotheses they produce, we can derive strong <strong>generalization guarantees</strong>, bridging the gap between <strong>online and batch learning</strong> paradigms.</p> <p>This insight is particularly valuable in large-scale machine learning, where online learning methods can be computationally efficient while still ensuring strong predictive performance.</p> <h5 id="to-do">To-Do:</h5> <ul> <li>Split this Blog into two.</li> </ul> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://cs.nyu.edu/~mohri/mlbook/">Foundations of Machine Learning Book</a></li> </ul>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.]]></summary></entry><entry><title type="html">Randomized Weighted Majority Algorithm</title><link href="https://monishver11.github.io/blog/2025/RWM/" rel="alternate" type="text/html" title="Randomized Weighted Majority Algorithm"/><published>2025-01-29T16:59:00+00:00</published><updated>2025-01-29T16:59:00+00:00</updated><id>https://monishver11.github.io/blog/2025/RWM</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/RWM/"><![CDATA[<p>The <strong>Randomized Weighted Majority (RWM) algorithm</strong> is an extension of the <strong>deterministic Weighted Majority (WM) algorithm</strong>, designed to overcome its limitations in adversarial settings, particularly in the <strong>zero-one loss</strong> scenario. This post explores why the deterministic approach struggles, how randomization helps, and what makes the RWM algorithm effective.</p> <h4 id="problem-with-the-deterministic-wm-algorithm"><strong>Problem with the Deterministic WM Algorithm</strong></h4> <p>The <strong>deterministic Weighted Majority (WM) algorithm</strong> operates by maintaining a set of experts, assigning them weights, and updating these weights based on their correctness. However, this approach suffers from <strong>high regret</strong> in adversarial settings.</p> <ul> <li> <p><strong>Regret in adversarial settings</strong><br/> No deterministic algorithm can achieve a <strong>sublinear regret</strong> of \(R_T = o(T)\) for all possible sequences under zero-one loss.</p> </li> <li> <p><strong>Worst-case scenario leading to linear regret</strong><br/> If the adversary knows the algorithm‚Äôs strategy, it can force it to make mistakes at every step.</p> <ul> <li>Suppose we have two experts: one always predicts <strong>0</strong>, the other always predicts <strong>1</strong>.</li> <li>If the best expert is correct <strong>only half the time</strong>, it makes at most <strong>\(T/2\)</strong> mistakes.</li> <li>The regret is defined as: \(R_T = m_T - m_T^*\)<br/> where: <ul> <li>\(m_T\) is the number of mistakes made by the algorithm.</li> <li>\(m_T^*\) is the number of mistakes made by the best expert.</li> </ul> </li> </ul> <p>Since \(m_T^* \leq T/2\), the regret in the worst case is at least: \(R_T \geq T/2\) which grows <strong>linearly</strong> with \(T\).</p> </li> </ul> <h4 id="the-randomized-weighted-majority-algorithm"><strong>The Randomized Weighted Majority Algorithm</strong></h4> <p>To address this issue, the <strong>Randomized Weighted Majority (RWM)</strong> algorithm introduces <strong>randomness</strong> into the decision-making process. Instead of deterministically following the highest-weighted expert, it assigns a <strong>probabilistic prediction</strong> based on expert weights.</p> <h5 id="key-idea-behind-rwm"><strong>Key Idea Behind RWM</strong></h5> <ul> <li>Instead of picking the expert with the highest weight <strong>deterministically</strong>, the algorithm selects predictions <strong>probabilistically</strong>, based on expert weights.</li> <li>Experts that have made fewer mistakes are given <strong>higher weights</strong>, making them more likely to be followed.</li> <li>This <strong>randomization prevents the adversary</strong> from forcing the algorithm to always make the same mistakes.</li> </ul> <h5 id="benefits-of-randomization"><strong>Benefits of Randomization</strong></h5> <ul> <li> <p><strong>Sublinear regret in adversarial settings</strong><br/> Unlike the deterministic approach, RWM can achieve: \(R_T = O(\sqrt{T})\) making it significantly better in the long run.</p> </li> <li> <p><strong>More balanced decision-making</strong><br/> By updating expert weights probabilistically, the algorithm avoids overly trusting any one expert too soon.</p> </li> </ul> <h4 id="the-randomized-weighted-majority-algorithm-step-by-step"><strong>The Randomized Weighted Majority Algorithm: Step-by-Step</strong></h4> <p>The algorithm follows these steps:</p> <ol> <li><strong>Initialize Weights:</strong> Each expert starts with an equal weight of <strong>1</strong>.</li> <li><strong>Compute Probabilities:</strong> The probability of selecting an expert is proportional to its weight.</li> <li><strong>Make a Prediction:</strong> Instead of following a single expert, the algorithm chooses its prediction probabilistically.</li> <li><strong>Update Weights:</strong> Experts that make mistakes have their weights <strong>decreased</strong> by a factor \(\beta\), where \(0 &lt; \beta &lt; 1\).</li> </ol> <p><strong><mark>Pseudocode:</mark></strong></p> \[\begin{array}{l} \textbf{Randomized-Weighted-Majority} \ (N) \\[5pt] \quad 1. \quad \textbf{for } i \gets 1 \text{ to } N \textbf{ do} \\ \quad 2. \quad \quad w_{1,i} \gets 1 \\ \quad 3. \quad \quad p_{1,i} \gets \frac{1}{N} \\[5pt] \quad 4. \quad \textbf{for } t \gets 1 \text{ to } T \textbf{ do} \\ \quad 5. \quad \quad \textbf{Receive } l_t \\ \quad 6. \quad \quad \textbf{for } i \gets 1 \text{ to } N \textbf{ do} \\ \quad 7. \quad \quad \quad \textbf{if } (l_{t,i} = 1) \textbf{ then} \\ \quad 8. \quad \quad \quad \quad w_{t+1,i} \gets \beta w_{t,i} \\ \quad 9. \quad \quad \quad \textbf{else} \\ \quad10. \quad \quad \quad \quad w_{t+1,i} \gets w_{t,i} \\[5pt] \quad11. \quad \quad W_{t+1} \gets \sum_{i=1}^{N} w_{t+1,i} \\[5pt] \quad12. \quad \quad \textbf{for } i \gets 1 \text{ to } N \textbf{ do} \\ \quad13. \quad \quad \quad p_{t+1,i} \gets w_{t+1,i} / W_{t+1} \\[5pt] \quad14. \quad \textbf{return } \mathbf{w}_{T+1} \end{array}\] <p>At this point, we‚Äôve introduced the RWM algorithm, but a key question remains:</p> <blockquote> <p>How does randomization <strong>actually prevent</strong> the algorithm from making repeated mistakes, and how is the probabilistic selection <strong>used effectively</strong>?</p> </blockquote> <p>We‚Äôll dive into this in the next section.</p> <hr/> <h4 id="how-randomization-prevents-repeated-mistakes"><strong>How Randomization Prevents Repeated Mistakes</strong></h4> <p>The <strong>Randomized Weighted Majority (RWM)</strong> algorithm prevents repeated mistakes in adversarial settings by making predictions <strong>probabilistically based on expert weights</strong>. Here‚Äôs how this works step by step:</p> <p><strong>1. Maintaining Expert Weights</strong></p> <ul> <li>We assign an initial weight to each expert, typically \(w_i^{(1)} = 1\) for all experts \(i\).</li> <li>Over time, we <strong>update the weights</strong> of experts based on their performance, penalizing those who make mistakes.</li> </ul> <p><strong>2. Making Probabilistic Predictions</strong></p> <ul> <li>Instead of deterministically following the best expert (which an adversary could exploit), RWM <strong>randomly selects a prediction</strong> based on the current expert weights.</li> <li>The probability of choosing a particular expert‚Äôs prediction is proportional to their weight: \(P(y_t = y_i) = \frac{w_i^{(t)}}{\sum_{j=1}^{N} w_j^{(t)}}\)<br/> where \(w_i^{(t)}\) is the weight of expert \(i\) at time \(t\).</li> <li>This means that if an expert has a high weight (i.e., has made fewer mistakes), their prediction is <strong>more likely</strong> to be chosen, but not always.</li> <li>If an adversary tries to force mistakes by targeting a specific deterministic strategy, the randomization ensures that the algorithm <strong>does not always follow a single pattern</strong>, making it harder for the adversary to exploit.</li> </ul> <p><strong>3. Weight Update Rule</strong></p> <ul> <li>After making a prediction, the algorithm observes the true outcome \(y_t\).</li> <li>The weights of experts who made mistakes are <strong>exponentially decreased</strong> using a multiplicative update rule: \(w_i^{(t+1)} = w_i^{(t)} \cdot \beta^{\ell_i^{(t)}}\)<br/> where: <ul> <li>\(\ell_i^{(t)}\) is the loss (1 if the expert made a mistake, 0 otherwise),</li> <li>\(\beta \in (0,1)\) is a parameter that determines how aggressively the weights are updated.</li> </ul> </li> <li>This ensures that over time, experts who consistently make mistakes lose influence, while those with good predictions gain more say in future predictions.</li> </ul> <p><strong>4. Why This Prevents Repeated Mistakes</strong></p> <ul> <li>Since the algorithm chooses predictions probabilistically, it does not <strong>consistently</strong> make the same mistakes like a deterministic algorithm would.</li> <li>Even if an adversary tries to construct a sequence that forces a mistake, RWM‚Äôs randomization means that <strong>the same incorrect choice won‚Äôt always be made</strong>.</li> <li>Moreover, since weights adjust dynamically, experts who perform better in the long run <strong>gradually dominate</strong> the prediction process.</li> </ul> <h5 id="takeaways"><strong><mark>Takeaways:</mark></strong></h5> <ul> <li><strong>Randomization prevents predictable failures</strong>: The algorithm does not follow a fixed pattern, making it harder for an adversary to force mistakes.</li> <li><strong>Probabilities favor better experts</strong>: Instead of blindly following one expert, the algorithm balances between exploration (randomization) and exploitation (favoring high-weight experts).</li> <li><strong>Weights adjust over time</strong>: Poor-performing experts lose influence, ensuring the algorithm improves as more data is observed.</li> </ul> <p>By incorporating randomness, the <strong>Randomized Weighted Majority Algorithm</strong> provides a <strong>powerful and adaptive approach</strong> to online learning, making it a fundamental tool in adversarial learning settings.</p> <hr/> <p>Here‚Äôs an analogy to make the <strong>Randomized Weighted Majority (RWM) algorithm</strong> more intuitive:</p> <p>Imagine you are in a <strong>new city</strong> for an extended stay, and you have to decide <strong>where to eat dinner every night</strong>. There are multiple restaurants (experts), and each night, you choose one based on your past experiences.</p> <p><strong>1. Initial Equal Preference (Assigning Weights)</strong></p> <p>At the start, you <strong>have no idea</strong> which restaurant is the best. So, you assign them equal preference:</p> <ul> <li>Restaurant A, B, and C all seem equally good, so you <strong>randomly pick one</strong>.</li> </ul> <p><strong>2. Evaluating Performance (Tracking Mistakes)</strong></p> <p>Each time you eat at a restaurant, you observe whether the meal was <strong>good</strong> or <strong>bad</strong>.</p> <ul> <li>If the meal was great, you <strong>trust the restaurant more</strong>.</li> <li>If it was terrible, you <strong>trust it less</strong>.</li> </ul> <p><strong>3. Adjusting Your Choices Over Time (Weight Updates)</strong></p> <p>Instead of always sticking to a single restaurant (which might backfire if it suddenly declines in quality), you <strong>adjust your preferences probabilistically</strong>:</p> <ul> <li>If <strong>Restaurant A</strong> has served consistently good food, you start <strong>choosing it more often</strong>, but you <strong>don‚Äôt completely ignore</strong> B and C.</li> <li>If <strong>Restaurant B</strong> has had a few bad meals, you reduce your visits there <strong>but still give it a chance occasionally</strong>.</li> </ul> <p><strong>4. Why Randomization Helps</strong></p> <p>Imagine there‚Äôs a <strong>food critic (the adversary)</strong> trying to ruin your dining experience.</p> <ul> <li>If you <strong>always follow a deterministic rule</strong> (e.g., always picking the currently best restaurant), the critic can <strong>sabotage your choices</strong>‚Äîperhaps by tipping off the restaurant to serve bad food only when you visit.</li> <li>However, by <strong>randomizing your choices</strong> (with a bias toward better restaurants), the critic <strong>can‚Äôt predict where you‚Äôll go</strong>, making it much harder to force repeated bad experiences.</li> </ul> <p><strong>5. Long-Term Adaptation (Minimizing Regret)</strong></p> <p>Over time, bad restaurants get <strong>fewer chances</strong>, and good ones <strong>dominate your choices</strong>. But, because you <strong>never completely eliminate</strong> any option, you still have room to adjust if a once-bad restaurant improves.</p> <h5 id="mapping-back-to-rwm"><strong>Mapping Back to RWM</strong></h5> <ul> <li><strong>Restaurants = Experts</strong></li> <li><strong>Your decision = Algorithm‚Äôs prediction</strong></li> <li><strong>Good meal = Correct prediction (no loss)</strong></li> <li><strong>Bad meal = Mistake (loss)</strong></li> <li><strong>Reducing visits to bad restaurants = Lowering expert weights</strong></li> <li><strong>Randomly choosing where to eat = Making probabilistic predictions</strong></li> </ul> <p>By <strong>not always following the same pattern</strong>, you prevent predictable failures and <strong>gradually learn the best strategy</strong> while adapting to changes.</p> <hr/> <h4 id="randomized-weighted-majority-algorithm-regret-bound-and-proof"><strong>Randomized Weighted Majority Algorithm: Regret Bound and Proof</strong></h4> <p>The main objective of the RWM algorithm is to minimize the <strong>regret</strong>, which is the difference between the cumulative loss of the algorithm and that of the best possible decision (in hindsight) over time.</p> <p>Now, we‚Äôll dive into the <strong>regret bound</strong> for the RWM algorithm. Specifically, we‚Äôll present a theorem that gives a strong guarantee on the regret \(R_T\) of the algorithm, and follow up with a proof that demonstrates the result.</p> <h5 id="setting--notations"><strong>Setting &amp; Notations</strong></h5> <p>At each round \(t \in [T]\), an online algorithm \(A\) selects a distribution \(p_t\) over the set of actions, receives a loss vector \(\mathbf{l}_t\), whose \(i\)-th component \(l_{t,i} \in [0, 1]\) is the loss associated with action \(i\), and incurs the expected loss:</p> \[L_t = \sum_{i=1}^{N} p_{t,i} l_{t,i}\] <p>The total loss incurred by the algorithm over \(T\) rounds is:</p> \[\mathcal{L}_T = \sum_{t=1}^{T} L_t\] <p>The total loss associated with action \(i\) is:</p> \[\mathcal{L}_{T,i} = \sum_{t=1}^{T} l_{t,i}\] <p>The minimal loss of a single action is denoted by:</p> \[\mathcal{L}_{\text{min}}^T = \min_{i \in A} \mathcal{L}_{T,i}\] <p>The regret \(R_T\) of the algorithm after \(T\) rounds is typically defined as the difference between the loss of the algorithm and that of the best single action:</p> \[R_T = \mathcal{L}_T - \mathcal{L}_{\text{min}}^T\] <p><strong>Note:</strong> Whenever you‚Äôre confused by the notations of \(L\) and \(\mathcal{L}\), refer to this.</p> <hr/> <h5 id="rwm-regret-bound"><strong>RWM Regret Bound</strong></h5> <p>The following <strong>theorem</strong> provides a regret bound for the RWM algorithm, showing that the regret \(R_T\) is in \(O(\sqrt{T \log N})\), where \(T\) is the number of rounds, and \(N\) is the number of experts.</p> <p><strong>Theorem</strong> : Fix \(\beta \in [\frac{1}{2}, 1)\). Then, for any \(T \geq 1\), the loss of the algorithm \(\text{RWM}\) on any sequence of decisions can be bounded as follows:</p> \[\mathcal{L}_T \leq \frac{\log N}{1 - \beta} + (2 - \beta) \mathcal{L}^{\min}_T \tag{1}\] <p>In particular, for \(\beta = \max\left(\frac{1}{2}, 1 - \sqrt{\frac{\log N}{T}}\right)\), the loss can be further bounded as:</p> \[\mathcal{L}_T \leq \mathcal{L}^{\min}_T + 2 \sqrt{T \log N} \tag{2}\] <p>Here, \(\mathcal{L}_T\) is the total loss incurred by the algorithm till \(T\) rounds, and \(\mathcal{L}^{\min}_T\) is the minimal possible loss achievable by any expert till \(T\) rounds.</p> <hr/> <h5 id="proof-outline-deriving-the-regret-bound"><strong>Proof Outline: Deriving the Regret Bound</strong></h5> <p>The proof of this result relies on analyzing the <strong>potential function</strong> \(W_t\), which represents the total weight assigned to the experts at each round \(t\). We derive upper and lower bounds for \(W_t\) and combine them to establish the regret bound.</p> <p>Let‚Äôs walk through the key steps of the proof.</p> <hr/> <p><strong>Step 1: The Weight Update Rule</strong> The weight of expert \(i\) at round \(t+1\) is updated based on their incurred loss \(l_{t,i}\):</p> \[w_{t+1, i} = \begin{cases} w_{t, i} \cdot \beta, &amp; \text{if } l_{t, i} = 1 \\ w_{t, i}, &amp; \text{if } l_{t, i} = 0 \end{cases}\] <p>where \(\beta \in (0,1)\) is a fixed discount factor.</p> <p>The total weight at round \(t+1\) is then:</p> \[W_{t+1} = \sum_{i=1}^{N} w_{t+1, i}\] <hr/> <p><strong>Step 2: Evolution of Total Weight</strong> Using the update rule, we can express \(W_{t+1}\) in terms of \(W_t\):</p> \[W_{t+1} = \sum_{i: l_{t,i} = 0} w_{t,i} + \beta \sum_{i: l_{t,i} = 1} w_{t,i}\] \[= W_t + (\beta - 1) \sum_{i: l_{t,i} = 1} w_{t,i}\] \[= W_t + (\beta - 1) W_t \sum_{i: l_{t,i} = 1} p_{t,i}\] \[= W_t + (\beta - 1) W_t L_t\] \[= W_t (1 - (1 - \beta) L_t)\] <hr/> <p><strong>Note:</strong> If you‚Äôre unsure, refer to the items listed below, which should be used appropriately to achieve the desired result.</p> <p>Using the probability interpretation of the weights:</p> \[\sum_{i: l_{t,i}=1} w_{t,i} = W_t L_t,\] <p>where \(L_t\) is the expected loss at time \(t\):</p> \[L_t = \sum_{i=1}^{N} p_{t,i} l_{t,i}\] <p>Thus, we obtain:</p> \[W_{t+1} = W_t(1 - (1 - \beta) L_t)\] <hr/> <p>By recursion, since \(W_1 = N\), we get:</p> \[W_{T+1} = N \prod_{t=1}^{T} (1 - (1 - \beta) L_t)\] <hr/> <p><strong>Step 3: Lower Bound on \(W_{T+1}\)</strong> The minimum weight of any expert at round \(T+1\) satisfies:</p> \[W_{T+1} \geq \max_{i \in [N]} w_{T+1, i} = \beta^{\mathcal{L}_T^{\min}}\] <p>where \(\mathcal{L}_T^{\min}\) is the loss of the best expert.</p> <p>How did we arrive at this version?</p> <p>Each expert‚Äôs weight evolves according to the <strong>multiplicative update rule</strong>. If expert \(i\) incurs a loss \(l_{t,i}\) at round \(t\), its weight is updated as:</p> \[w_{t+1, i} = w_{t,i} \cdot \beta^{l_{t,i}}\] <p>where \(\beta \in (0,1]\) is the update factor.</p> <p>Define the <strong>best expert</strong> as the one with the <strong>minimum cumulative loss</strong> over \(T\) rounds. Let \(\mathcal{L}_T^{\min}\) denote this minimum loss:</p> \[\mathcal{L}_T^{\min} = \min_{i \in [N]} \sum_{t=1}^{T} l_{t,i}\] <p>For this best expert (say expert \(i^*\)), its weight after \(T\) rounds evolves as:</p> \[w_{T+1, i^*} = w_{1, i^*} \cdot \prod_{t=1}^{T} \beta^{l_{t,i^*}}\] <p>Since all experts start with an equal initial weight \(w_{1, i} = 1\) (assuming uniform initialization), we have:</p> \[w_{T+1, i^*} = \beta^{\mathcal{L}_T^{\min}}\] <p>Since the <strong>total weight</strong> at round \(T+1\) is at least the weight of the best expert, we get:</p> \[W_{T+1} = \sum_{i=1}^{N} w_{T+1, i} \geq w_{T+1, i^*} = \beta^{\mathcal{L}_T^{\min}}\] <p>Thus, the lower bound holds:</p> \[W_{T+1} \geq \beta^{\mathcal{L}_T^{\min}}\] <p>This ensures that the total weight does not shrink too fast, preserving a lower bound based on the best expert‚Äôs performance.</p> <hr/> <p><strong>Step 4: Taking Logarithms</strong> Taking the logarithm of both bounds:</p> \[\log W_{T+1} = \log N + \sum_{t=1}^{T} \log (1 - (1 - \beta) L_t)\] <p>For the second term, using the inequality \(\log(1 - x) \leq -x\) for \(x &lt; 1\), we get:</p> \[\sum_{t=1}^{T} \log (1 - (1 - \beta) L_t) \leq \sum_{t=1}^{T} - (1 - \beta) L_T = - (1 - \beta) \mathcal{L}_T\] <p>Thus,</p> \[\log W_{T+1} \leq \log N - (1 - \beta) \mathcal{L}_T.\] <p>Similarly, for the lower bound:</p> \[\log W_{T+1} \geq \mathcal{L}_T^{\min} \log \beta\] <p>Combining these,</p> \[\mathcal{L}_T^{\min} \log \beta \leq \log N - (1 - \beta) \mathcal{L}_T.\] <p>Rearranging,</p> \[\mathcal{L}_T \leq \frac{\log N}{1 - \beta} - \frac{\log \beta}{1 - \beta} \mathcal{L}_T^{\min}\] \[\mathcal{L}_T \leq \log N - \frac{\log (1 - (1 - \beta))}{1 - \beta} \mathcal{L}_T^{\min}\] <p>Again, for this second term, using the inequality \(-\log(1 - x) \leq x+x^2\) for \(x \in [0, \frac{1}{2}]\), we get:</p> \[\mathcal{L}_T \leq \frac{\log N}{1 - \beta} + (2 - \beta) \mathcal{L}_T^{\min} \tag{1}\] <p>This is the main result, and it provides a clear bound on the cumulative loss \(\mathcal{L}_T\).</p> <hr/> <p><strong>Step 5: Choosing Optimal \(\beta\)</strong></p> <p>We differentiate with respect to \(\beta\) and setting it to zero gives:</p> \[\frac{\log N}{(1 - \beta)^2} - T = 0\] <p>Solving for \(\beta\):</p> \[\beta = 1 - \sqrt{\frac{\log N}{T}} &lt; 1\] <p>If \(1 - \sqrt{\frac{\log N}{T}} \geq \frac{1}{2}\), then:</p> \[\beta_0 = 1 - \sqrt{\frac{\log N}{T}}\] <p>Otherwise, we use the boundary value \(\beta_0 = \frac{1}{2}\) is the optimal value.</p> <p>Substituting this choice in \((1)\), we get:</p> \[\mathcal{L}_T \leq \mathcal{L}_T^{\min} + 2\sqrt{T \log N} \tag{2}\] <p>Thus, the <strong>regret bound</strong> is:</p> \[R_T = \mathcal{L}_T - \mathcal{L}_T^{\min} \leq 2 \sqrt{T \log N}\] <p><strong>Key Essence:</strong></p> <ul> <li>The <strong>regret</strong> of the RWM algorithm is \(O(\sqrt{T \log N})\).</li> <li>The <strong>average regret per round \(R_T/T\)</strong> decreases as \(O(1/\sqrt{T})\).</li> </ul> <p>This result shows that RWM achieves <strong>sublinear regret</strong>, meaning that as the number of rounds \(T\) grows, the algorithm performs almost as well as the best expert.</p> <hr/> <p>Do we really grasp what this formula is conveying? It highlights a remarkable bound in online learning. Alright, let‚Äôs dig into that further.</p> <p><strong>What does sublinear regret mean?</strong></p> <p>When we say that an algorithm has <strong>sublinear regret</strong>, we mean that the total regret <strong>grows slower than the number of rounds</strong>. As the number of rounds \(T\) increases, the gap between the algorithm‚Äôs performance and the best expert‚Äôs performance doesn‚Äôt increase linearly. Instead, it grows at a slower rate (e.g., \(\sqrt{T}\)).</p> <p><strong>The meaning of the formula:</strong></p> <ul> <li> <p><strong>Regret \(O(\sqrt{T \log N})\)</strong>: This tells you that after \(T\) rounds, the total regret will grow roughly as \(\sqrt{T}\), with an additional logarithmic factor based on \(N\) (the number of possible actions). The logarithmic term grows slowly and doesn‚Äôt significantly affect the overall growth for large \(T\).</p> </li> <li> <p><strong>Average regret per round \(O(1/\sqrt{T})\)</strong>: This shows that, on average, the regret per round decreases as the number of rounds increases. As \(T\) gets larger, the average regret (the loss per round) decreases.</p> </li> </ul> <p><strong>Sublinear regret in action:</strong></p> <ol> <li><strong>At the start</strong>, when the algorithm has few rounds to learn, it might perform poorly (larger regret).</li> <li><strong>Over time</strong>, as \(T\) grows, the algorithm‚Äôs performance improves. It makes fewer mistakes as it ‚Äúlearns‚Äù from past rounds, and the regret per round decreases.</li> <li><strong>After many rounds</strong>, the algorithm performs almost as well as the best possible action, and the regret becomes quite small.</li> </ol> <p><strong>Key takeaway:</strong></p> <ul> <li><strong>Sublinear regret</strong> means that the algorithm‚Äôs performance gets closer to the best possible action as the number of rounds increases, but it does so at a slower pace than linear growth. The algorithm doesn‚Äôt just keep getting worse with more rounds; instead, it converges toward optimal performance.</li> </ul> <p><strong>Note:</strong> The bound \((2)\) assumes that the algorithm additionally receives as a parameter the number of rounds \(T\). However, as we learned from the <a href="https://monishver11.github.io/blog/2025/doubling-trick/">Doubling trick</a> in the previous blog, this requirement can be relaxed at the cost of a small constant factor increase.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>The <strong>Randomized Weighted Majority (RWM) Algorithm</strong> provides a powerful and efficient method for decision-making and prediction in online learning. The regret bound we‚Äôve derived shows that, under the right conditions, the RWM algorithm can perform nearly as well as the best possible expert in hindsight, with a regret that grows at most as \(O(\sqrt{T \log N})\).</p> <p>This result is optimal, as demonstrated by further lower bound theorems, and provides a strong theoretical guarantee for the RWM algorithm‚Äôs performance in practice.</p> <h5 id="references"><strong>References</strong></h5>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.]]></summary></entry><entry><title type="html">Bayesian Decision Theory - Concepts and Recap</title><link href="https://monishver11.github.io/blog/2025/bayes-decision-theory/" rel="alternate" type="text/html" title="Bayesian Decision Theory - Concepts and Recap"/><published>2025-01-28T16:18:00+00:00</published><updated>2025-01-28T16:18:00+00:00</updated><id>https://monishver11.github.io/blog/2025/bayes-decision-theory</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/bayes-decision-theory/"><![CDATA[<p>Bayesian decision theory is a powerful framework for making decisions under uncertainty. It provides a principled way to combine prior knowledge with observed data to make optimal choices. In this post, we‚Äôll take a closer look at its key components, revisit Bayesian point estimation, and connect these ideas to classical probability modeling. Let‚Äôs dive in!</p> <hr/> <h4 id="ingredients-of-bayesian-decision-theory"><strong>Ingredients of Bayesian Decision Theory</strong></h4> <p>At the heart of Bayesian decision theory lie several key components. First, we have the <strong>parameter space</strong>, denoted as \(\Theta\), which represents all possible values of the unknown parameter we aim to estimate or make decisions about. Next, we have the <strong>prior distribution</strong>, \(p(\theta)\), which encodes our beliefs about \(\theta\) before observing any data. This prior serves as a starting point in the Bayesian framework.</p> <p>Equally important is the <strong>action space</strong>, \(A\), which includes all possible actions we might take. To evaluate these actions, we rely on a <strong>loss function</strong>, \(\ell : A \times \Theta \to \mathbb{R}\), which quantifies the cost of taking a specific action \(a \in A\) when the true parameter value is \(\theta \in \Theta\).</p> <p>With these components, we can define the <strong>posterior risk</strong> of an action \(a \in A\), which represents the expected loss under the posterior distribution:</p> \[r(a) = \mathbb{E}[\ell(\theta, a) \mid D] = \int \ell(\theta, a) p(\theta \mid D) d\theta\] <p>The goal is to minimize this risk. The action that achieves this minimization is called the <strong>Bayes action</strong>, \(a^*\), which satisfies:</p> \[r(a^*) = \min_{a \in A} r(a)\] <h5 id="bayesian-point-estimation"><strong>Bayesian Point Estimation</strong></h5> <p>Bayesian point estimation builds upon this foundation. Imagine we have data \(D\) generated from some distribution \(p(y \mid \theta)\), where \(\theta \in \Theta\) is unknown. Our task is to find a single point estimate, \(\hat{\theta}\), that best represents \(\theta\).</p> <p>To do this, we first specify a prior distribution \(p(\theta)\) over \(\Theta\), which reflects our beliefs about \(\theta\) before observing the data. We then define a loss function, \(\ell(\hat{\theta}, \theta)\), to measure the cost of estimating \(\theta\) with \(\hat{\theta}\). Finally, we seek the point estimate \(\hat{\theta} \in \Theta\) that minimizes the posterior risk:</p> \[r(\hat{\theta}) = \mathbb{E}[\ell(\hat{\theta}, \theta) \mid D] = \int \ell(\hat{\theta}, \theta) p(\theta \mid D) d\theta.\] <h5 id="important-loss-functions-and-their-role"><strong>Important Loss Functions and Their Role</strong></h5> <p>The choice of loss function significantly influences the optimal estimate. Here are three commonly used loss functions and the corresponding Bayes actions:</p> <ul> <li> <p><strong>Squared Loss</strong>: \(\ell(\hat{\theta}, \theta) = (\theta - \hat{\theta})^2\).<br/> For squared loss, the Bayes action is the <strong>posterior mean</strong>, \(\mathbb{E}[\theta \mid D]\).</p> </li> <li> <p><strong>Zero-One Loss</strong>: \(\ell(\hat{\theta}, \theta) = 1[\theta \neq \hat{\theta}]\).<br/> For zero-one loss, the Bayes action is the <strong>posterior mode</strong>, the most probable value of \(\theta\) under the posterior.</p> </li> <li> <p><strong>Absolute Loss</strong>: \(\ell(\hat{\theta}, \theta) = |\theta - \hat{\theta}|\).<br/> For absolute loss, the Bayes action is the <strong>posterior median</strong>, the value that splits the posterior distribution into two equal halves.</p> </li> </ul> <h5 id="example-card-drawing"><strong>Example: Card Drawing</strong></h5> <p>To see this in action, consider drawing a card from a deck consisting of the values \(\{2, 3, 3, 4, 4, 5, 5, 5\}\). Suppose you are asked to guess the value of the card. Based on the posterior distribution:</p> <ul> <li>The <strong>mean</strong> of the distribution is \(3.875\).</li> <li>The <strong>mode</strong> (most frequent value) is \(5\).</li> <li>The <strong>median</strong> (middle value) is \(4\).</li> </ul> <p>This simple example highlights how different loss functions lead to different optimal estimates.</p> <h5 id="bayesian-point-estimation-with-squared-loss"><strong>Bayesian Point Estimation with Squared Loss</strong></h5> <p>We seek an action \(\hat{\theta}\) that minimizes the <strong>posterior risk</strong>, given by:</p> \[r(\hat{\theta}) = \int (\theta - \hat{\theta})^2 p(\theta | \mathcal{D}) \, d\theta\] <p>To find the optimal \(\hat{\theta}\), we differentiate:</p> \[\frac{d r(\hat{\theta})}{d\hat{\theta}} = - \int 2 (\theta - \hat{\theta}) p(\theta | \mathcal{D}) \, d\theta\] <p>Rearranging,</p> \[= -2 \int \theta p(\theta | \mathcal{D}) \, d\theta + 2\hat{\theta} \int p(\theta | \mathcal{D}) \, d\theta\] <p>Since the total probability integrates to 1,</p> \[\int p(\theta | \mathcal{D}) \, d\theta = 1,\] <p>this simplifies to:</p> \[\frac{d r(\hat{\theta})}{d\hat{\theta}} = -2 \int \theta p(\theta | \mathcal{D}) \, d\theta + 2\hat{\theta}\] <p>Setting the derivative to zero,</p> \[-2 \int \theta p(\theta | \mathcal{D}) \, d\theta + 2\hat{\theta} = 0\] <p>Solving for \(\hat{\theta}\),</p> \[\hat{\theta} = \int \theta p(\theta \mid D) d\theta = \mathbb{E}[\theta \mid D]\] <p>Thus, under squared loss, the Bayes action is the <strong>posterior mean</strong>.</p> <hr/> <h4 id="recap-and-interpretation"><strong>Recap and Interpretation</strong></h4> <p>Bayesian Decision Theory is built on a few core ideas that tie together probability, decision-making, and inference. Let‚Äôs revisit these concepts and unpack their meaning(again) all at once to gain the full picture.</p> <p><strong>Note:</strong> If you feel this isn‚Äôt necessary, feel free to skip it. However, I believe it‚Äôs helpful to reinforce these concepts periodically to build strong intuition and apply them effectively when needed.</p> <h5 id="the-prior-ptheta"><strong>The Prior (\(p(\theta)\))</strong></h5> <p>The prior represents our initial beliefs about the unknown parameter \(\theta\) before observing any data. It encapsulates what we know (or assume) about \(\theta\) based on prior knowledge, expert opinion, or historical data.</p> <p>For example, if \(\theta\) represents the probability of success in a coin toss, a reasonable prior might be a Beta distribution centered around 0.5, reflecting our belief that the coin is fair.</p> <h5 id="the-posterior-ptheta-mid-d"><strong>The Posterior (\(p(\theta \mid D)\))</strong></h5> <p>The posterior is the updated belief about \(\theta\) after observing the data \(D\). It combines the prior \(p(\theta)\) with the likelihood of the data \(p(D \mid \theta)\) using Bayes‚Äô theorem:</p> \[p(\theta \mid D) = \frac{p(D \mid \theta) p(\theta)}{p(D)}\] <p>The posterior is the foundation of all Bayesian inference. It reflects how the data has rationally updated our initial beliefs.</p> <h5 id="inferences-and-actions"><strong>Inferences and Actions</strong></h5> <p>In the Bayesian framework, all inferences (e.g., estimating \(\theta\)) and actions (e.g., making decisions) are based on the posterior distribution. This is because the posterior contains all the information we have about \(\theta\), combining both prior knowledge and observed data.</p> <p>For example, if we want to estimate \(\theta\), we might compute the posterior mean, median, or mode, depending on the loss function we choose.</p> <h5 id="no-need-to-justify-an-estimator"><strong>No Need to Justify an Estimator</strong></h5> <p>In classical statistics, we often need to justify why a particular estimator (e.g., the sample mean) is a good choice. In Bayesian statistics, this issue doesn‚Äôt arise because the estimator is derived directly from the posterior distribution, which is fully determined by the prior and the data.</p> <p>The only choices we need to make are:</p> <ol> <li>The family of distributions (e.g., Gaussian, Beta) that model the data.</li> <li>The prior distribution on the parameter space \(\Theta\).</li> </ol> <p><strong>Role of the Loss Function</strong></p> <p>The loss function \(\ell(a, \theta)\) quantifies the cost of taking action \(a\) when the true parameter is \(\theta\). It bridges the gap between inference and decision-making.</p> <p>The optimal action \(a^*\) is the one that minimizes the posterior risk, which is the expected loss under the posterior distribution:</p> \[r(a) = \mathbb{E}[\ell(a, \theta) \mid D] = \int \ell(a, \theta) p(\theta \mid D) \, d\theta\] <p>Different loss functions lead to different optimal actions. For example:</p> <ul> <li><strong>Squared loss</strong> leads to the posterior mean.</li> <li><strong>Absolute loss</strong> leads to the posterior median.</li> <li><strong>Zero-one loss</strong> leads to the posterior mode.</li> </ul> <h5 id="philosophical-interpretation"><strong>Philosophical Interpretation</strong></h5> <p>Bayesian Decision Theory is fundamentally about rational decision-making under uncertainty. It provides a coherent framework for updating beliefs and making decisions that minimize expected loss.</p> <p>Unlike frequentist methods, which focus on long-run properties of estimators, Bayesian methods focus on the current state of knowledge, as represented by the posterior distribution.</p> <h5 id="why-does-this-matter"><strong>Why Does This Matter?</strong></h5> <p>Understanding these concepts is crucial because they form the backbone of Bayesian thinking. Here‚Äôs why:</p> <ol> <li><strong>Flexibility</strong>: The Bayesian approach allows us to incorporate prior knowledge into our analysis, which can be especially useful when data is limited.</li> <li><strong>Transparency</strong>: All assumptions (e.g., the choice of prior) are explicitly stated, making the analysis transparent and interpretable.</li> <li><strong>Decision-Oriented</strong>: By focusing on minimizing expected loss, Bayesian Decision Theory directly addresses the practical goal of making optimal decisions.</li> </ol> <h5 id="example-estimating-the-mean-of-a-normal-distribution"><strong>Example: Estimating the Mean of a Normal Distribution</strong></h5> <p>Suppose we want to estimate the mean \(\theta\) of a normal distribution based on observed data \(D\). Here‚Äôs how the Bayesian approach works:</p> <ol> <li><strong>Prior</strong>: We choose a normal prior \(p(\theta) = N(\mu_0, \sigma_0^2)\), where \(\mu_0\) and \(\sigma_0^2\) reflect our initial beliefs about \(\theta\).</li> <li><strong>Likelihood</strong>: The data \(D\) is modeled as \(p(D \mid \theta) = N(\theta, \sigma^2)\).</li> <li><strong>Posterior</strong>: Using Bayes‚Äô theorem, the posterior \(p(\theta \mid D)\) is also a normal distribution, with updated mean and variance that balance the prior and the data.</li> <li><strong>Decision</strong>: If we use squared loss, the optimal estimate \(\hat{\theta}\) is the posterior mean.</li> </ol> <p>This example illustrates how the Bayesian approach seamlessly integrates prior knowledge with observed data to produce a rational and optimal estimate.</p> <h5 id="example-estimating-the-mean-of-a-normal-distribution-frequentist-approach"><strong>Example: Estimating the Mean of a Normal Distribution (Frequentist Approach)</strong></h5> <ol> <li> <p><strong>Model Assumption</strong>:<br/> Assume the data \(D\) comes from a normal distribution: \(p(D \mid \theta) = N(\theta, \sigma^2),\)<br/> where \(\theta\) is the unknown mean and \(\sigma^2\) is the known variance.</p> </li> <li> <p><strong>Estimator</strong>:<br/> Use the sample mean: \(\bar{D} = \frac{1}{n} \sum_{i=1}^n D_i\)<br/> as the estimator for \(\theta\). This is derived as the <strong>maximum likelihood estimator (MLE)</strong> because it maximizes the likelihood function:</p> \[L(\theta) = \prod_{i=1}^n p(D_i \mid \theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(D_i - \theta)^2}{2\sigma^2}\right).\] </li> <li><strong>Properties of the Estimator</strong>: <ul> <li>The sample mean \(\bar{D}\) is an <strong>unbiased estimator</strong> of \(\theta\), meaning: \(E[\bar{D}] = \theta.\)</li> <li>Its variance is: \(\text{Var}(\bar{D}) = \frac{\sigma^2}{n},\)<br/> which decreases as the sample size \(n\) increases.</li> </ul> </li> <li><strong>Decision</strong>:<br/> The sample mean \(\bar{D}\) is reported as the estimate of \(\theta\). There is no explicit loss function in the frequentist framework; instead, the sample mean is justified by its desirable properties (e.g., unbiasedness, efficiency, and consistency).</li> </ol> <p><strong>Note:</strong> If you‚Äôre unsure about this derivation from MLE, check out this <a href="https://monishver11.github.io/blog/2025/NB-continuous-features/">link</a> for clarification.</p> <p><strong><mark>Key Takeaways</mark></strong></p> <ol> <li>The prior represents initial beliefs, the posterior represents updated beliefs, and the loss function guides decision-making.</li> <li>Bayesian methods are inherently decision-oriented, focusing on minimizing expected loss.</li> <li>The only choices we need to make are the family of distributions and the prior‚Äîeverything else follows logically from these choices.</li> </ol> <hr/> <p>A few follow-up questions you might have:</p> <h5 id="explanation-of-actionsdecision-making"><strong>Explanation of ‚ÄúActions/Decision-Making‚Äù</strong></h5> <p>In the Bayesian framework, <strong>actions</strong> or <strong>decision-making</strong> refer to the choices or decisions we make based on the information encoded in the posterior distribution. These decisions could range from estimating a parameter to choosing between different courses of action based on the expected outcomes.</p> <p>For example:</p> <ul> <li>If you‚Äôre estimating a parameter \(\theta\), the <strong>action</strong> could be selecting the posterior mean, median, or mode as your estimate.</li> <li>If you‚Äôre deciding whether to launch a product, the <strong>action</strong> could involve calculating the probability of success using the posterior and deciding based on a predefined threshold.</li> <li>In medical diagnostics, the <strong>action</strong> could be choosing a treatment plan based on the likelihood of a disease inferred from the posterior.</li> </ul> <p>In essence, <strong>actions</strong> are the outcomes of the decision-making process, guided by the posterior distribution and a loss function that quantifies the cost of making an incorrect decision.</p> <h5 id="what-is-an-estimator-in-the-frequentist-approach"><strong>What is an ‚ÄúEstimator‚Äù in the frequentist approach?</strong></h5> <p>An <strong>estimator</strong> is a statistical function or rule used to estimate an unknown parameter \(\theta\) based on observed data. In frequentist statistics, estimators are often chosen based on their theoretical properties, such as:</p> <ul> <li><strong>Unbiasedness</strong>: The estimator‚Äôs expected value equals the true parameter value.</li> <li><strong>Efficiency</strong>: The estimator has the smallest possible variance among all unbiased estimators.</li> <li><strong>Consistency</strong>: The estimator converges to the true parameter value as the sample size increases.</li> </ul> <p>For example:</p> <ul> <li>The <strong>sample mean</strong> is a common estimator for the population mean.</li> <li>The <strong>sample variance</strong> is an estimator for the population variance.</li> </ul> <p>In Bayesian statistics, however, the <strong>estimator</strong> is derived directly from the posterior distribution. For instance:</p> <ul> <li>The <strong>posterior mean</strong> minimizes squared error loss.</li> <li>The <strong>posterior median</strong> minimizes absolute error loss.</li> <li>The <strong>posterior mode</strong> corresponds to the most likely value of \(\theta\).</li> </ul> <p>The key difference is that Bayesian methods do not require separate justification for an estimator because the posterior distribution naturally incorporates both the prior beliefs and observed data, making the choice of estimator a consequence of the decision-making process.</p> <hr/> <p>We‚Äôve covered most of it, right? Now, let‚Äôs revisit the foundational concepts that underpin everything we‚Äôve discussed so far, including conditional probability, the likelihood function, and MLE in a general sense.</p> <h5 id="conditional-probability-modeling"><strong>Conditional Probability Modeling</strong></h5> <p>In this context, we have:</p> <ul> <li>An <strong>input space</strong>, \(X\), which represents the features or predictors.</li> <li>An <strong>outcome space</strong>, \(Y\), which represents the possible outputs.</li> <li>An <strong>action space</strong>, \(A\), consisting of probability distributions on \(Y\).</li> </ul> <p>A prediction function \(f : X \to A\) maps each input \(x \in X\) to a distribution on \(Y\). This setup allows us to model the relationship between inputs and outputs probabilistically.</p> <p>In a parametric framework, we define a family of conditional densities:</p> \[\{p(y \mid x, \theta) : \theta \in \Theta\},\] <p>where \(p(y \mid x, \theta)\) is a density on \(Y\) for each \(x \in X\), and \(\theta\) is a parameter in the finite-dimensional space \(\Theta\). This is the common starting point for either classical or Bayesian regression.</p> <h5 id="classical-treatment-likelihood-function"><strong>Classical Treatment: Likelihood Function</strong></h5> <p>In the classical approach, we begin with data \(D = (y_1, \dots, y_n)\) and assume it is generated by the conditional density \(p(y \mid x, \theta)\). The probability of the data is:</p> \[p(D \mid x_1, \dots, x_n, \theta) = \prod_{i=1}^n p(y_i \mid x_i, \theta)\] <p>For fixed \(D\), the likelihood function is defined as:</p> \[L_D(\theta) = p(D \mid x, \theta),\] <p>where \(x = (x_1, \dots, x_n)\).</p> <h5 id="maximum-likelihood-estimator-mle"><strong>Maximum Likelihood Estimator (MLE)</strong></h5> <p>The <strong>Maximum Likelihood Estimator (MLE)</strong> for \(\theta\) is the value that maximizes the likelihood function:</p> \[\hat{\theta}_{\text{MLE}} = \arg\max_{\theta \in \Theta} L_D(\theta)\] <p>Interestingly, MLE corresponds to <strong>Empirical Risk Minimization (ERM)</strong> if we set the loss function to the negative log-likelihood. The resulting prediction function is:</p> \[\hat{f}(x) = p(y \mid x, \hat{\theta}_{\text{MLE}})\] <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>So, we‚Äôve reached the end of this blog. I know it can be confusing at times‚ÄîI‚Äôve been confused too‚Äîbut take your time to get a solid grasp on it. This is the foundation of Bayesian ML. In the past few blogs, we‚Äôve discussed these concepts, but do you recall if we‚Äôve applied them to any prediction tasks yet? The answer is no. In the next one, we‚Äôll put them into practice through Bayesian conditional models.</p> <h5 id="references"><strong>References</strong></h5>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.]]></summary></entry><entry><title type="html">Reinforcement Learning - An Introductory Guide</title><link href="https://monishver11.github.io/blog/2025/rl-intro/" rel="alternate" type="text/html" title="Reinforcement Learning - An Introductory Guide"/><published>2025-01-28T03:45:00+00:00</published><updated>2025-01-28T03:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/rl-intro</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/rl-intro/"><![CDATA[<p>Reinforcement Learning (RL) is a fascinating field that focuses on teaching agents how to make decisions based on their environment. The agent‚Äôs goal is to learn a strategy that maximizes a cumulative reward over time by interacting with its surroundings. But before we dive into the details of RL, let‚Äôs explore the fundamental concepts of decision-making and intelligence.</p> <h5 id="what-is-decision-making"><strong>What is Decision Making?</strong></h5> <p>At its core, decision-making is the process of choosing the best possible action from a set of alternatives. It lies at the heart of intelligence and is fundamental to building intelligent systems.</p> <h5 id="what-is-intelligence"><strong>What is Intelligence?</strong></h5> <p>Intelligence can be broadly defined as the ability to:</p> <ul> <li>Understand and process what‚Äôs happening around you.</li> <li>Make informed decisions.</li> <li>Reason and analyze complex situations.</li> </ul> <p>In simpler terms, intelligence enables us to learn, adapt, and tackle new or challenging scenarios effectively.</p> <h5 id="why-intelligence-and-how-does-it-work"><strong>Why Intelligence and How Does It Work?</strong></h5> <p>A strong driving force for the evolution of intelligence is survival. For instance, early single-celled organisms relied on simple hunting strategies. Over time, the emergence of multicellular organisms, like <em>C. elegans</em>, marked a leap in complexity. They developed neurons, enabling coordination and more sophisticated strategies.</p> <p>While the exact mechanisms of intelligence remain elusive, certain principles stand out:</p> <ul> <li>Neurons form the foundation of biological intelligence.</li> <li>Artificial neural networks via backpropagation, despite their inspiration from biology, cannot replicate human intelligence <strong>yet</strong>.</li> <li>Understanding the principles of intelligence is arguably more important than mimicking its mechanisms.</li> </ul> <hr/> <h4 id="six-lessons-from-babies"><strong>Six Lessons from Babies</strong></h4> <p>Developmental psychology offers valuable insights into intelligence. Here are six key lessons we can learn from how babies develop:</p> <ol> <li><strong>Be Multimodal</strong>: Babies combine sensory inputs (sight, sound, touch, etc.) to create a cohesive understanding of their environment.</li> <li><strong>Be Incremental</strong>: Learning is gradual. Babies adapt as they encounter new information. <ul> <li>Unlike i.i.d. data in supervised learning, real-world learning is sequential and non-i.i.d., posing unique challenges.</li> <li>RL algorithms often simulate i.i.d.-like scenarios to work effectively.</li> </ul> </li> <li><strong>Be Physical</strong>: Interaction with the environment is crucial. Babies learn by manipulating objects and observing the outcomes.</li> <li><strong>Explore</strong>: Exploration is central to learning. Babies experiment with their surroundings to gather information and refine their actions.</li> <li><strong>Be Social</strong>: Social interactions play a significant role. Babies learn by observing and imitating others.</li> <li><strong>Learn a Language</strong>: Language serves as a symbolic framework to organize thoughts and retrieve information efficiently.</li> </ol> <p>These principles are directly relevant to reinforcement learning (RL), where agents learn by interacting with and exploring their environment.</p> <h5 id="takeaways-thus-far"><strong>Takeaways Thus Far;</strong></h5> <ul> <li>Intelligence is fundamentally rooted in decision-making.</li> <li>Decision-making occurs at various levels, from low-level motor control to high-level reasoning and coordination.</li> <li>Algorithms for decision-making depend heavily on the specific task.</li> <li>Neuroscience, motor control, and cognitive psychology provide valuable insights for designing intelligent systems.</li> <li>Translating biological insights into computational systems remains challenging due to a lack of foundational understanding.</li> </ul> <hr/> <h4 id="a-computational-lens-on-decision-making"><strong>A Computational Lens on Decision Making</strong></h4> <p>Decision-making can be viewed through the following computational frameworks:</p> <h5 id="from-an-agents-perspective"><strong>From an Agent‚Äôs Perspective:</strong></h5> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Agent_Persepective-480.webp 480w,/assets/img/Agent_Persepective-800.webp 800w,/assets/img/Agent_Persepective-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Agent_Persepective.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Agent_Persepective" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>Sense</li> <li>Think</li> <li>Act</li> <li>Repeat</li> </ol> <h5 id="from-a-global-perspective"><strong>From a Global Perspective:</strong></h5> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Global_Persepective-480.webp 480w,/assets/img/Global_Persepective-800.webp 800w,/assets/img/Global_Persepective-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Global_Persepective.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Global_Persepective" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>Observations</li> <li>Sense ‚Üí Think ‚Üí Act</li> <li>Effects of Actions</li> <li>Repeat</li> </ol> <p>Alternate terminologies for decision-making systems include policy, strategy, and controller.</p> <h5 id="examples-of-computational-decision-making"><strong>Examples of Computational Decision Making</strong></h5> <ul> <li><strong>Atari Games (Deep RL)</strong>: Agents maximize scores by analyzing game frames and selecting actions accordingly.</li> <li><strong>Google Robots</strong>: Robots perform complex tasks using intricate joint control mechanisms.</li> <li><strong>Go</strong>: RL has enabled agents to outperform humans in this game with a vast decision space.</li> <li><strong>Dota 2</strong>: RL systems have been trained to defeat top human players by optimizing strategies in a dynamic environment. Though this success is currently limited to controlled and restricted settings.</li> </ul> <p>Reinforcement learning provides a robust framework to model decision-making by teaching agents to optimize actions based on rewards.</p> <h5 id="the-rewards-mechanism-and-sequential-decision-making"><strong>The Rewards Mechanism and Sequential Decision Making</strong></h5> <p>In RL, decision-making unfolds over time through a sequence of observations, actions, and rewards. This sequence can be represented as:</p> \[(o_1, a_1, r_1) \to (o_2, a_2, r_2) \to \dots \to (o_n, a_n, r_n)\] <p>Where:</p> <ul> <li>\(o_t\): Observation at time \(t\),</li> <li>\(a_t\): Action taken at time \(t\),</li> <li>\(r_t\): Reward received after taking action \(a_t\).</li> </ul> <p>The objective is to maximize the cumulative reward:</p> \[\max_{a_1, a_2, \dots, a_{T-1}} \sum_{t=1}^{T} r_t\] <p>The key challenge is determining the optimal actions \(a_t\) at each time step to achieve the maximum long-term reward. This leads to the concept of <em>policy optimization</em>.</p> <h5 id="planning-and-world-models"><strong>Planning and World Models</strong></h5> <p>Another approach to decision-making is through planning, where the agent uses a model of the world to simulate the effects of its actions. This allows the agent to reason about potential future states and make decisions accordingly.</p> <p>However, planning is limited in its applicability:</p> <ul> <li>It works well in discrete, well-defined environments like games.</li> <li>It struggles with complex, dynamic tasks like conversational AI or real-world robotics.</li> </ul> <h5 id="the-limits-of-current-approaches"><strong>The Limits of Current Approaches</strong></h5> <p>Despite their potential, RL and planning-based models face significant challenges:</p> <ul> <li>Real-world scenarios often lack the i.i.d. assumption that RL sometime relies on.</li> <li>Bridging the gap between controlled simulations and dynamic real-world environments remains a key hurdle.</li> </ul> <h5 id="a-note-of-caution-amidst-progress"><strong>A Note of Caution Amidst Progress</strong></h5> <p>While reinforcement learning and computational decision-making have seen remarkable progress, it‚Äôs important to recognize the challenges that remain. This brings us to <strong>Moravec‚Äôs Paradox</strong>, a fascinating insight into the nature of artificial intelligence:</p> <blockquote> <p>‚ÄúIt is comparatively easy to make computers exhibit adult-level performance on intelligence tests or play games like chess, yet it is extremely difficult to give them the skills of a one-year-old when it comes to perception and mobility.‚Äù<br/> ‚Äî <strong>Hans Moravec, 1988</strong></p> </blockquote> <p>Steven Pinker elaborated further:</p> <blockquote> <p>‚ÄúThe main lesson of thirty-five years of AI research is that the hard problems are easy, and the easy problems are hard.‚Äù<br/> ‚Äî <strong>Steven Pinker, 1994</strong></p> </blockquote> <p>What this paradox highlights is that tasks humans find effortless‚Äîsuch as walking, recognizing faces, or interacting physically with the environment‚Äîrequire immense computational power and intricate modeling to replicate in machines. Conversely, tasks like playing chess, solving mathematical problems, or optimizing game strategies are relatively easier for computers.</p> <p>This paradox underscores the fact that intelligence, especially in its perceptual and physical forms, is deeply rooted in evolutionary processes. The interplay of sensory data, motor control, and real-world adaptation‚Äîelements essential for robust intelligence‚Äîremains a significant challenge for machines.</p> <p>In reinforcement learning, we see this reflected in the difficulty of training agents to generalize to unstructured, real-world environments. RL agents perform admirably in games like <strong>Atari</strong> or <strong>Go</strong>, but replicating even the basic capabilities of a human child‚Äîlike balancing on uneven surfaces or adapting to novel stimuli‚Äîremains an open frontier.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>Reinforcement learning provides a powerful framework for decision-making, allowing agents to learn from their interactions with the environment. By leveraging concepts such as reward mechanisms, policy optimization, and planning, we have achieved significant milestones in fields like gaming, robotics, and autonomous systems.</p> <p>However, the journey toward creating truly intelligent systems is far from over. While computational models continue to evolve, there is a need for a deeper understanding of the principles of intelligence, both biological and artificial.</p> <p>As we wrap up this introduction to reinforcement learning and decision-making, it‚Äôs clear that intelligence is fundamentally about making informed decisions. Whether through supervised learning, reinforcement learning, or planning, the goal remains the same: enabling machines to reason, adapt, and thrive in dynamic environments.</p> <p>In the next post, we‚Äôll dive deeper into decision-making in supervised learning and explore how it serves as the foundation for many modern AI systems. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li></li> </ul>]]></content><author><name></name></author><category term="RL-NYU"/><category term="ML"/><summary type="html"><![CDATA[Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.]]></summary></entry></feed>