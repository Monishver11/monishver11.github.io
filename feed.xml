<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-28T02:50:56+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A space for journaling my learnings, career, thoughts, and experiences in this amazing ride of life. </subtitle><entry><title type="html">Wrapping Up Our ML Foundations Journey</title><link href="https://monishver11.github.io/blog/2025/wrapping-ml-basics/" rel="alternate" type="text/html" title="Wrapping Up Our ML Foundations Journey"/><published>2025-05-17T19:45:00+00:00</published><updated>2025-05-17T19:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/wrapping-ml-basics</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/wrapping-ml-basics/"><![CDATA[<p>After completing 51 comprehensive blog posts on machine learning fundamentals, it’s time to wrap up this series. This journey has been both challenging and immensely rewarding.</p> <p><strong>There were several key motivations behind creating this content:</strong></p> <ol> <li>To provide a <strong>structured approach</strong> that methodically builds intuition for a solid foundational understanding of machine learning</li> <li>To explain complex concepts in an <strong>accessible way</strong> that’s easy to remember and articulate</li> <li>To ensure <strong>mathematical rigor</strong> is maintained while keeping explanations clear and thorough</li> <li>To create a resource I wish I had when first learning these concepts</li> </ol> <p>I believe I’ve accomplished these goals, and I’m genuinely satisfied with the outcome.</p> <p><strong>Looking back at our path:</strong></p> <ul> <li>We began with essential <strong>mathematical prerequisites</strong> (multivariate calculus, linear algebra, probability theory) to build a strong foundation</li> <li>Progressed to the <strong>fundamentals of machine learning</strong> with supervised learning and empirical risk minimization</li> <li>Explored <strong>optimization techniques</strong> through gradient descent and stochastic gradient descent</li> <li>Examined various <strong>loss functions and regularization approaches</strong> to understand model development</li> <li>Delved into <strong>linear models</strong> and their extensions with SVMs and margin classifiers</li> <li>Advanced to <strong>nonlinear feature maps and kernels</strong> to tackle more complex problems</li> <li>Investigated <strong>probabilistic modeling and Bayesian approaches</strong> for a different perspective on machine learning</li> <li>Addressed <strong>multiclass classification</strong> methods and structured prediction</li> <li>Introduced <strong>decision trees</strong> as our first truly non-linear classifiers</li> <li>Concluded with <strong>ensemble methods</strong> from bagging and random forests to various boosting algorithms, culminating with gradient boosting</li> </ul> <p>This progression represents a logical flow from fundamentals to advanced concepts, carefully designed to build upon each previous lesson.</p> <p><strong>I have several plans for the future:</strong></p> <ol> <li> <p>I intend to cover the <strong>foundational aspects of deep learning</strong> in a similar structured fashion, though not immediately. I will ensure this content aligns well with the machine learning material we’ve already covered.</p> </li> <li> <p>Following advice from a friend, I recognize the importance of focusing on <strong>depth in ML and practical applications</strong>. I’ll be dedicating time to projects that enhance my practical experience and will share insights as they develop.</p> </li> <li> <p>I welcome suggestions for <strong>specific topics</strong> you’d like to see broken down in this style, or if you have interesting ML ideas for collaboration. Feel free to DM me.</p> </li> <li> <p>With summer break approaching, I’ll be taking some time to <strong>rest and recharge</strong> after this challenging but highly educational semester.</p> </li> </ol> <p>I’d like to express my sincere gratitude to:</p> <ul> <li>My professors for their excellent teaching of these complex subjects, especially Professor <a href="https://mengyeren.com/">Mengye Ren</a>, who taught me this course.</li> <li>Everyone who supported me throughout this journey</li> <li>You, the readers, for your engagement and feedback</li> </ul> <h5 id="final-thoughts"><strong>Final Thoughts</strong></h5> <p>This project began with the aim of clarifying machine learning concepts for myself and others. As we progressed from basic mathematical foundations all the way to advanced ensemble methods like gradient boosting, I hope these explanations have helped demystify machine learning and provided you with both theoretical understanding and practical insights.</p> <p>The complete list of topics is available at: <a href="https://monishver11.github.io/blog/category/ml-nyu/">ML-NYU Category</a> or <a href="https://drive.google.com/file/d/1t1r7w_0gSJIcaEATQlAn1gyMU8yM582v/view?usp=sharing">This list</a></p> <p>Until we meet again in future learning adventures, keep exploring, stay curious, and never stop learning!</p> <hr/>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.]]></summary></entry><entry><title type="html">Gradient Boosting in Practice</title><link href="https://monishver11.github.io/blog/2025/gb-in-practice/" rel="alternate" type="text/html" title="Gradient Boosting in Practice"/><published>2025-05-15T01:10:00+00:00</published><updated>2025-05-15T01:10:00+00:00</updated><id>https://monishver11.github.io/blog/2025/gb-in-practice</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/gb-in-practice/"><![CDATA[<p>In the previous post, we introduced Gradient Boosting with logistic loss, discussing how weak learners can be sequentially combined to minimize differentiable loss functions using functional gradient descent. Now, we turn our attention to how this elegant theory translates into robust, high-performing models in practice, particularly focusing on regularization techniques that help control overfitting and improve generalization.</p> <hr/> <h5 id="preventing-overfitting-in-gradient-boosting"><strong>Preventing Overfitting in Gradient Boosting</strong></h5> <p>While <strong>gradient boosting</strong> is often surprisingly resistant to overfitting compared to other ensemble methods like bagging, it is not immune. Its resilience stems from several key characteristics of the boosting process:</p> <ul> <li> <p><strong>Implicit Feature Selection</strong>:<br/> One of the reasons gradient boosting is relatively resistant to overfitting is that it performs <strong>feature selection implicitly</strong> during training. At each boosting iteration, the algorithm fits a weak learner often a decision tree to the current pseudo-residuals (the gradients of the loss).</p> <p>Decision trees naturally perform feature selection: they evaluate all available features and choose the one that offers the best split (i.e., the greatest reduction in loss). This means that only the most predictive features are chosen at each step. Over multiple rounds, this leads to an ensemble that selectively and adaptively focuses on the most informative parts of the input space.</p> <p>As a result, even in high-dimensional settings or with noisy features, gradient boosting can often avoid overfitting by not relying too heavily on irrelevant or redundant features. It prioritizes features that consistently contribute to reducing the loss, acting like a built-in greedy feature selection mechanism.</p> </li> <li> <p><strong>Localized Additive Updates</strong>:<br/> In gradient boosting, each weak learner (such as a shallow decision tree) is trained to correct the mistakes of the current model. Because these learners are typically low-capacity (e.g., stumps or small trees), their predictions only affect specific regions of the input space - they’re “localized” in that sense.</p> <p>As boosting progresses, the model becomes more accurate, and the residuals (errors) it tries to fix get smaller and more focused. Consequently, the subsequent learners make increasingly smaller and more targeted updates. This means that instead of making large, sweeping changes to the model, later learners adjust predictions only in regions where the model is still wrong.</p> <p>This gradual, fine-grained updating process helps avoid overfitting, as the model doesn’t overreact to noise or outliers - it incrementally improves where it matters most.</p> </li> </ul> <p>Together, these mechanisms help gradient boosting maintain a balance between flexibility and generalization. However, it can still overfit when models are overly complex or trained for too many iterations - which is why regularization techniques such as shrinkage, subsampling, and tree size constraints are essential in practice.</p> <ul> <li> <p><strong>Shrinkage</strong>: Use a small learning rate (step size) \(\lambda\) to scale the contribution of each weak learner:</p> \[f_m(x) = f_{m-1}(x) + \lambda \cdot v_m h_m(x)\] <p>Smaller \(\lambda\) slows down the learning process, often resulting in better generalization.</p> </li> <li><strong>Stochastic Gradient Boosting</strong>: Instead of using the full dataset at each boosting round, randomly sample a subset of training examples.</li> <li><strong>Feature (Column) Subsampling</strong>: Randomly select a subset of input features for each boosting iteration.</li> </ul> <p>These methods inject randomness into the learning process and reduce variance, both of which help mitigate overfitting.</p> <h5 id="step-size-as-regularization"><strong>Step Size as Regularization</strong></h5> <p>One of the simplest and most effective ways to regularize gradient boosting is by adjusting the <strong>step size</strong> or learning rate. This directly controls how far the model moves in the direction of the negative gradient at each step. Smaller step sizes lead to slower learning, but allow the model to build more nuanced approximations of the target function.</p> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-5-480.webp 480w,/assets/img/gb-5-800.webp 800w,/assets/img/gb-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Left is training set &amp; right is validation set </div> <p>This effect is clearly observed in tasks like <strong>Sinc function regression</strong>(which we saw in the last post - <a href="https://monishver11.github.io/blog/2025/binomial-boost/">BinomialBoost</a>), where the number of boosting rounds and the choice of shrinkage rate dramatically affect both training and validation performance. Lower learning rates typically result in better generalization when combined with more boosting rounds.</p> <h5 id="stochastic-gradient-boosting"><strong>Stochastic Gradient Boosting</strong></h5> <p>To improve efficiency and reduce overfitting, we can employ <strong>stochastic gradient boosting</strong> - a strategy analogous to minibatch gradient descent in optimization. So in minibatch, for each boosting round:</p> <ul> <li>Randomly sample a subset of the training data.</li> <li>Fit the base learner using this subset.</li> <li>Use it to approximate the gradient and update the model.</li> </ul> <p>Benefits of this approach include:</p> <ul> <li><strong>Regularization</strong>: Injecting randomness into the training process helps prevent overfitting.</li> <li><strong>Efficiency</strong>: Training is faster per iteration since we work with fewer data points.</li> <li><strong>Improved performance</strong>: Empirically, this often leads to better results for the same computational budget.</li> </ul> <h5 id="column-feature-subsampling"><strong>Column (Feature) Subsampling</strong></h5> <p>Another effective technique borrowed from Random Forests is <strong>column or feature subsampling</strong>. For each boosting round:</p> <ul> <li>Randomly select a subset of input features.</li> <li>Learn a weak learner using only this subset.</li> </ul> <p>This further reduces correlation between individual learners and acts as a strong regularizer. In fact, the <strong>XGBoost</strong> paper emphasizes that column subsampling can reduce overfitting <strong>even more</strong> effectively than row subsampling.</p> <p>Additionally, limiting the number of features also improves training speed, especially for high-dimensional datasets.</p> <hr/> <h5 id="summary-gradient-boosting-as-a-versatile-framework"><strong>Summary: Gradient Boosting as a Versatile Framework</strong></h5> <p>Let’s step back and summarize the key takeaways:</p> <ul> <li><strong>Motivation</strong>: Combine many weak learners (e.g., shallow trees) to build a strong predictor.</li> <li><strong>Statistical view</strong>: Fit an additive model greedily, one function at a time.</li> <li><strong>Optimization view</strong>: Boosting makes local improvement iteratively and perform gradient descent in function space.</li> </ul> <p>Gradient Boosting is a <strong>flexible and powerful meta-algorithm</strong>:</p> <ul> <li>Supports <strong>any differentiable loss function</strong></li> <li>Applicable to <strong>classification, regression, ranking, multiclass tasks</strong>, and more</li> <li>Highly <strong>scalable</strong> with implementations like <strong>XGBoost</strong>, <strong>LightGBM</strong>, and <strong>CatBoost</strong></li> </ul> <p>With proper regularization, including shrinkage, stochastic updates, and feature subsampling - Gradient Boosting becomes one of the most effective tools in the machine learning toolkit.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.]]></summary></entry><entry><title type="html">BinomialBoost</title><link href="https://monishver11.github.io/blog/2025/binomial-boost/" rel="alternate" type="text/html" title="BinomialBoost"/><published>2025-05-10T00:58:00+00:00</published><updated>2025-05-10T00:58:00+00:00</updated><id>https://monishver11.github.io/blog/2025/binomial-boost</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/binomial-boost/"><![CDATA[<p>In the previous post, we introduced the <strong>Gradient Boosting framework</strong> as functional gradient descent, where we minimize a loss function by iteratively adding base learners that approximate the negative gradient (pseudo-residuals) of the loss. We demonstrated this with the squared loss, where residuals had a direct and intuitive interpretation. In this post, we extend that idea to <strong>logistic loss</strong>, which is more appropriate for binary classification tasks. This special case is often referred to as <strong>BinomialBoost</strong>.</p> <hr/> <h5 id="logistic-loss-and-pseudo-residuals"><strong>Logistic Loss and Pseudo-Residuals</strong></h5> <p>For binary classification with labels \(Y = \{-1, 1\}\), the <strong>logistic loss</strong> is given by:</p> \[\ell(y, f(x)) = \log(1 + e^{-y f(x)})\] <p>At each boosting iteration, we need to compute the <strong>pseudo-residuals</strong>, which are the negative gradients of the loss with respect to the model’s prediction. For the \(i\)-th training example:</p> \[\begin{aligned} r_i &amp;= - \frac{\partial}{\partial f(x_i)} \ell(y_i, f(x_i)) \\ &amp;= - \frac{\partial}{\partial f(x_i)} \log(1 + e^{-y_i f(x_i)}) \\ &amp;= y_i \cdot \frac{e^{-y_i f(x_i)}}{1 + e^{-y_i f(x_i)}} \\ &amp;= \frac{y_i}{1 + e^{y_i f(x_i)}} \end{aligned}\] <p>These pseudo-residuals guide the model by indicating how each example’s prediction should be adjusted to reduce the classification loss.</p> <h5 id="step-direction-and-boosting-update"><strong>Step Direction and Boosting Update</strong></h5> <p>Once we compute the pseudo-residuals \(r_i\), the next base learner \(h_m \in \mathcal{H}\) is fit to match them in a least squares sense:</p> \[h_m = \arg\min_{h \in \mathcal{H}} \sum_{i=1}^n \left( \frac{y_i}{1 + e^{y_i f_{m-1}(x_i)}} - h(x_i) \right)^2\] <p>The model is then updated as:</p> \[f_m(x) = f_{m-1}(x) + \nu h_m(x)\] <p>where \(\nu \in (0, 1]\) is the learning rate or shrinkage parameter and \(f_{m-1}(x)\) is prediction after \(m−1\) rounds.</p> <h5 id="gradient-tree-boosting"><strong>Gradient Tree Boosting</strong></h5> <p>A particularly effective version of gradient boosting uses <strong>regression trees</strong> as base learners. The hypothesis space is:</p> \[\mathcal{H} = \{ \text{regression trees with } S \text{ terminal nodes} \}\] <ul> <li>\(S = 2\) corresponds to decision stumps, a very simple weak learner that makes a prediction based on a single feature threshold.</li> <li>Larger values of \(S\) allow more expressive trees, capable of capturing more complex interactions among features.</li> <li>Common choices for tree size: \(4 \leq S \leq 8\)</li> </ul> <p>Gradient Tree Boosting combines the predictive power of decision trees with the optimization capabilities of functional gradient descent. Each tree fits the pseudo-residuals (i.e., the gradient of the loss), and the overall model evolves by sequentially adding these trees with appropriate scaling (via step size or shrinkage).</p> <p>This approach is widely known as <strong>Gradient Tree Boosting</strong> and is implemented in various software packages:</p> <ul> <li><strong>R</strong>: <code class="language-plaintext highlighter-rouge">gbm</code></li> <li><strong>scikit-learn</strong>: <code class="language-plaintext highlighter-rouge">GradientBoostingClassifier</code>, <code class="language-plaintext highlighter-rouge">GradientBoostingRegressor</code></li> <li><strong>XGBoost</strong>, <strong>LightGBM</strong>: state-of-the-art libraries for scalable, high-performance boosting</li> </ul> <h5 id="visual-example"><strong>Visual Example;</strong></h5> <div class="row justify-content-center"> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-3-480.webp 480w,/assets/img/gb-3-800.webp 800w,/assets/img/gb-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As a simple regression example, we can use an ensemble of decision stumps to fit a noisy version of the sinc function using squared loss. Even shallow learners (depth-1 trees) become powerful when combined via boosting. Here’s what the model looks like after different boosting rounds:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-4-480.webp 480w,/assets/img/gb-4-800.webp 800w,/assets/img/gb-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Decision stumps with 1,10,50, and 100 steps </div> <p>The shrinkage parameter \(\lambda = 1\) is used in this example to simplify learning, though smaller values are typically preferred in practice to prevent overfitting.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>With all the pieces in place, we can summarize what’s needed to implement Gradient Boosting (also known as <strong>AnyBoost</strong>):</p> <ul> <li>A differentiable loss function: e.g., squared loss for regression, logistic loss for classification</li> <li>A base hypothesis space: e.g., regression trees of fixed depth</li> <li>A gradient descent procedure in function space</li> <li>Hyperparameters: step size \(\nu\), number of boosting rounds \(M\), tree size \(S\)</li> </ul> <p>This general and flexible framework can adapt to a wide variety of tasks, making Gradient Boosting one of the most versatile and powerful tools in modern machine learning.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[See how the gradient boosting framework naturally extends to binary classification using the logistic loss.]]></summary></entry><entry><title type="html">Gradient Boosting / “Anyboost”</title><link href="https://monishver11.github.io/blog/2025/gradient-boosting/" rel="alternate" type="text/html" title="Gradient Boosting / “Anyboost”"/><published>2025-05-08T16:56:00+00:00</published><updated>2025-05-08T16:56:00+00:00</updated><id>https://monishver11.github.io/blog/2025/gradient-boosting</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/gradient-boosting/"><![CDATA[<p>In our previous post, we explored how <strong>Forward Stagewise Additive Modeling (FSAM)</strong> with exponential loss recovers the AdaBoost algorithm. But FSAM is not limited to exponential loss — it can be extended to <strong>any differentiable loss</strong>, leading to the powerful and flexible framework of <strong>Gradient Boosting Machines (GBMs)</strong>.</p> <p>This post walks through the derivation of gradient boosting, starting with the squared loss, and builds toward a general functional gradient descent interpretation of boosting.</p> <hr/> <h5 id="fsam-with-squared-loss"><strong>FSAM with Squared Loss</strong></h5> <p>Let’s begin with FSAM using squared loss. At the \(m\)-th boosting round, we wish to find a new function \(v h(x)\) to add to the model \(f_{m-1}(x)\). The objective becomes:</p> \[J(v, h) = \frac{1}{n} \sum_{i=1}^n \left( y_i - \underbrace{[f_{m-1}(x_i) + v h(x_i)]}_{\text{new model}} \right)^2\] <p>If the hypothesis space \(\mathcal{H}\) is closed under rescaling (i.e., if \(h \in \mathcal{H}\) implies \(v h \in \mathcal{H}\) for all \(v \in \mathbb{R}\)), we can drop \(v\) and simplify the objective:</p> \[J(h) = \frac{1}{n} \sum_{i=1}^n \left( \underbrace{[y_i - f_{m-1}(x_i)]}_{\text{residual}} - h(x_i) \right)^2\] <p>This is just <strong>least squares regression on the residuals</strong> — fit \(h(x)\) to approximate the residuals \([y_i - f_{m-1}(x_i)]\).</p> <h5 id="interpreting-the-residuals"><strong>Interpreting the Residuals</strong></h5> <p>Let’s take a closer look at how <strong>residuals relate to gradients</strong> in the context of boosting with squared loss.</p> <p>The objective for squared loss is:</p> \[J(f) = \frac{1}{n} \sum_{i=1}^n (y_i - f(x_i))^2\] <p>This measures how far the model predictions \(f(x_i)\) are from the true labels \(y_i\) across the dataset.</p> <p>To minimize this objective, we can perform gradient descent. So we ask: <strong>what is the gradient of \(J(f)\) with respect to \(f(x_i)\)?</strong></p> <p>Let’s compute it:</p> \[\frac{\partial}{\partial f(x_i)} J(f) = \frac{\partial}{\partial f(x_i)} \left[ \frac{1}{n} \sum_{j=1}^n (y_j - f(x_j))^2 \right]\] <p>Because only the \(i\)-th term depends on \(f(x_i)\), this simplifies to:</p> \[\frac{\partial}{\partial f(x_i)} J(f) = \frac{1}{n} \cdot (-2)(y_i - f(x_i)) = -\frac{2}{n}(y_i - f(x_i))\] <p>The term \(y_i - f(x_i)\) is the <strong>residual</strong> at point \(x_i\). So we see:</p> <blockquote> <p>The residual is proportional to the <strong>negative gradient</strong> of the squared loss.</p> </blockquote> <p><strong>Why This Matters for Boosting</strong></p> <p>Gradient descent updates a parameter in the opposite direction of the gradient. Similarly, in <strong>gradient boosting</strong>, we update our model \(f\) in the direction of a function that tries to approximate the <strong>negative gradient</strong> at every data point.</p> <p>In the case of squared loss, this just means:</p> <ul> <li>Compute the residuals \(r_i = y_i - f(x_i)\).</li> <li>Fit a new base learner \(h_m\) to those residuals.</li> <li>Add \(h_m\) to the model: \(f \leftarrow f + v h_m\).</li> </ul> <p>This process mimics the behavior of <strong>gradient descent in function space</strong>.</p> <p>We can now draw an analogy:</p> <ul> <li><strong>Boosting update:</strong>  \(f \leftarrow f + \textcolor{red}{v} \textcolor{green}{h}\)</li> <li><strong>Gradient descent:</strong> \(f \leftarrow f - \textcolor{red}{\alpha} \textcolor{green}{\nabla_f J(f)}\)</li> </ul> <p><strong>Note: Observe the variables highlighted in red and green.</strong></p> <p>Where:</p> <ul> <li>\(h\) approximates the direction of steepest descent (the gradient),</li> <li>\(v\) is the step size (akin to learning rate \(\alpha\)),</li> <li>and the update improves the model’s predictions to reduce the loss.</li> </ul> <p>This perspective generalizes easily to other loss functions, which we explore in the next sections on <strong>functional gradient descent</strong>.</p> <hr/> <h5 id="functional-gradient-descent-intuition-and-setup"><strong>Functional Gradient Descent: Intuition and Setup</strong></h5> <p>To generalize FSAM to arbitrary (differentiable) loss functions, we adopt the <strong>functional gradient descent</strong> perspective.</p> <p>Suppose we have a loss function that depends on predictions \(f(x_i)\) at \(n\) training examples:</p> \[J(f) = \sum_{i=1}^n \ell(y_i, f(x_i))\] <p>Note that \(f\) is a function, but this loss only depends on \(f\) through its values on the training points. So, we can treat:</p> \[f = (f(x_1), f(x_2), \dots, f(x_n))^\top\] <p>as a vector in \(\mathbb{R}^n\), and write:</p> \[J(f) = \sum_{i=1}^n \ell(y_i, f_i)\] <p>where \(f_i := f(x_i)\). We want to minimize \(J(f)\) by updating our predictions \(f_i\) in the <strong>steepest descent direction</strong>.</p> <h5 id="unconstrained-step-direction-pseudo-residuals"><strong>Unconstrained Step Direction (Pseudo-Residuals)</strong></h5> <p>We compute the gradient of the loss with respect to each prediction:</p> \[g = \nabla_f J(f) = \left( \frac{\partial \ell(y_1, f_1)}{\partial f_1}, \dots, \frac{\partial \ell(y_n, f_n)}{\partial f_n} \right)\] <p>The <strong>negative gradient direction</strong> is:</p> \[-g = -\nabla_f J(f) = \left( -\frac{\partial \ell(y_1, f_1)}{\partial f_1}, \dots, -\frac{\partial \ell(y_n, f_n)}{\partial f_n} \right)\] <p>This tells us how to change each \(f(x_i)\) to decrease the loss — it’s the direction of steepest descent in \(\mathbb{R}^n\).</p> <p>We call this vector the <strong>pseudo-residuals</strong>. In the case of squared loss:</p> \[\ell(y_i, f_i) = (y_i - f_i)^2 \quad \Rightarrow \quad -g_i = y_i - f_i\] <p>So, pseudo-residuals coincide with actual residuals for squared loss.</p> <h5 id="projection-step-fitting-the-pseudo-residuals"><strong>Projection Step: Fitting the Pseudo-Residuals</strong></h5> <p>We now want to update the function \(f\) by stepping in direction \(-g\). However, we can’t directly take a step in \(\mathbb{R}^n\) — we must stay within our base hypothesis space \(\mathcal{H}\).</p> <p>So we find the <strong>function \(h \in \mathcal{H}\)</strong> that best fits the negative gradient at the training points. This is a projection of \(-g\) onto the function class:</p> \[h = \arg\min_{h \in \mathcal{H}} \sum_{i=1}^n \left( -g_i - h(x_i) \right)^2\] <p>This is just <strong>least squares regression</strong> of pseudo-residuals.</p> <p>Once we have \(h\), we take a step:</p> \[f \leftarrow f + v h\] <p>where \(v\) is a step size, which can either be fixed (e.g., shrinkage factor \(\lambda\)) or found via line search.</p> <p><strong>All Together;</strong></p> <ul> <li> <p><strong>Objective</strong>: \(J(f) = \sum_{i=1}^n \ell(y_i, f(x_i))\)</p> </li> <li> <p><strong>Unconstrained gradient</strong>: \(g = \nabla_f J(f) = \left( \frac{\partial \ell(y_1, f_1)}{\partial f_1}, \dots, \frac{\partial \ell(y_n, f_n)}{\partial f_n} \right) \tag{25}\)</p> </li> <li> <p><strong>Projection</strong>: \(h = \arg\min_{h \in \mathcal{H}} \sum_{i=1}^n (-g_i - h(x_i))^2 \tag{26}\)</p> </li> <li> <p><strong>Boosting update</strong>: \(f \leftarrow f + v h\)</p> </li> </ul> <p>This gives a general recipe for boosting: <strong>approximate the negative gradient with a weak learner and take a small step in that direction</strong> — hence the name <strong>gradient boosting</strong>.</p> <h5 id="functional-gradient-descent-hyperparameters-and-regularization"><strong>Functional Gradient Descent: Hyperparameters and Regularization</strong></h5> <p>Once we have our base learner \(h_m\) for iteration \(m\), we need to decide how far to move in that direction.</p> <p>We can either:</p> <ul> <li> <p><strong>Choose a step size \(v_m\) by line search</strong>: \(v_m = \arg\min_v \sum_{i=1}^n \ell\left(y_i, f_{m-1}(x_i) + v h_m(x_i)\right)\)</p> </li> <li> <p><strong>Or</strong> use a fixed step size \(v\) as a hyperparameter. Line search is not strictly necessary but can improve performance.</p> </li> </ul> <p>To <strong>regularize</strong> and control overfitting, we scale the step size with a <strong>shrinkage factor</strong> \(\lambda \in (0, 1]\):</p> \[f_m(x) = f_{m-1}(x) + \lambda v_m h_m(x)\] <p>This shrinkage slows down learning and typically improves generalization. A common choice is \(\lambda = 0.1\).</p> <p>We also need to decide:</p> <ul> <li>The <strong>number of steps \(M\)</strong> (i.e., number of boosting rounds). <ul> <li>This is typically chosen via early stopping or tuning on a validation set.</li> </ul> </li> </ul> <hr/> <h5 id="gradient-boosting-algorithm"><strong>Gradient Boosting Algorithm</strong></h5> <p>Putting everything together, the gradient boosting algorithm proceeds as follows:</p> <ol> <li> <p><strong>Initialize the model</strong> with a constant function: \(f_0(x) = \arg\min_\gamma \sum_{i=1}^n \ell(y_i, \gamma)\)</p> </li> <li> <p><strong>For</strong> \(m = 1\) to \(M\):</p> <ol> <li> <p><strong>Compute the pseudo-residuals</strong> (i.e., the negative gradients):</p> \[r_{im} = -\left[ \frac{\partial}{\partial f(x_i)} \ell(y_i, f(x_i)) \right]_{f(x_i) = f_{m-1}(x_i)}\] </li> <li> <p><strong>Fit a base learner</strong> \(h_m\) (e.g., regression tree) to the dataset \(\{(x_i, r_{im})\}_{i=1}^n\) using squared error.</p> </li> <li> <p><em>(Optional)</em> <strong>Line search</strong> to find best step size: \(v_m = \arg\min_v \sum_{i=1}^n \ell(y_i, f_{m-1}(x_i) + v h_m(x_i))\)</p> </li> <li> <p><strong>Update the model</strong>: \(f_m(x) = f_{m-1}(x) + \lambda v_m h_m(x)\)</p> </li> </ol> </li> <li> <p><strong>Return</strong> the final model: \(f_M(x)\)</p> </li> </ol> <hr/> <h5 id="conclusion-the-gradient-boosting-machine-ingredients"><strong>Conclusion: The Gradient Boosting Machine Ingredients</strong></h5> <p>To implement gradient boosting, you need:</p> <ul> <li>A loss function \(\ell(y, f(x))\) that is differentiable w.r.t. \(f(x)\)</li> <li>A base hypothesis space \(\mathcal{H}\) (e.g., decision trees) for regression</li> <li>A method to choose step size: fixed, or via line search</li> <li>A stopping criterion: number of iterations \(M\), or early stopping</li> <li>Regularization through <strong>shrinkage</strong> (\(\lambda\))</li> </ul> <p>Once these ingredients are in place, you’re ready to build powerful models with <strong>Gradient Boosting Machines (GBMs)</strong>!</p> <p>In the next post, we’ll explore specific loss functions like <strong>logistic loss</strong> for classification and how gradient boosting works in this setting.</p> <p>Take care!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.]]></summary></entry><entry><title type="html">Forward Stagewise Additive Modeling</title><link href="https://monishver11.github.io/blog/2025/FSAM/" rel="alternate" type="text/html" title="Forward Stagewise Additive Modeling"/><published>2025-05-04T23:04:00+00:00</published><updated>2025-05-04T23:04:00+00:00</updated><id>https://monishver11.github.io/blog/2025/FSAM</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/FSAM/"><![CDATA[<p>In the previous post, we saw how learning both the weights and basis functions gives rise to adaptive models — and how neural networks and decision trees fit into this framework. Now, we dive into <strong>Forward Stagewise Additive Modeling (FSAM)</strong>, a greedy algorithm that forms the foundation of <strong>Gradient Boosting</strong>.</p> <hr/> <h5 id="what-is-fsam"><strong>What is FSAM?</strong></h5> <p>Our goal is to fit a model of the form:</p> \[f(x) = \sum_{m=1}^M v_m h_m(x)\] <p>where each \(h_m \in \mathcal{H}\) is a basis function, and \(v_m\) is its weight. The key idea is to <strong>greedily</strong> fit one function at a time without changing previously fitted ones. That’s why it’s called <strong>forward stagewise</strong>.</p> <p>After \(m - 1\) steps, we have:</p> \[f_{m-1}(x) = \sum_{i=1}^{m-1} v_i h_i(x)\] <p>At step \(m\), we select a new basis function \(h_m \in \mathcal{H}\) and weight \(v_m &gt; 0\) to form:</p> \[f_m(x) = \underbrace{ f_{m-1}(x) }_{\text{fixed}} + v_m h_m(x)\] <p>We choose \((v_m, h_m)\) to minimize the loss as much as possible.</p> <h5 id="fsam-for-empirical-risk-minimization-erm"><strong>FSAM for Empirical Risk Minimization (ERM)</strong></h5> <p>Let’s apply FSAM to an ERM objective. We proceed as follows:</p> <ol> <li> <p><strong>Initialize:</strong></p> \[f_0(x) = 0\] </li> <li><strong>For</strong> \(m = 1\) to \(M\): <ul> <li> <p>Compute:</p> \[(v_m, h_m) = \arg\min_{v \in \mathbb{R}, h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n \ell\left(y_i, f_{m-1}(x_i) + v h(x_i)\right)\] </li> <li> <p>Update:</p> \[f_m(x) = f_{m-1}(x) + v_m h_m(x)\] </li> </ul> </li> <li><strong>Return</strong> \(f_M\)</li> </ol> <hr/> <h5 id="exponential-loss"><strong>Exponential Loss</strong></h5> <p>Let’s use the <strong>exponential loss</strong>:</p> \[\ell(y, f(x)) = \exp\left( -y f(x) \right)\] <p>This loss function is margin-based, it penalizes examples based on how confidently they are classified.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-2-480.webp 480w,/assets/img/gb-2-800.webp 800w,/assets/img/gb-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="fsam-with-exponential-loss"><strong>FSAM with Exponential Loss</strong></h5> <p>Apply the FSAM steps using exponential loss:</p> <ol> <li> <p><strong>Initialize:</strong></p> \[f_0(x) = 0\] </li> <li><strong>For</strong> \(m = 1\) to \(M\): <ul> <li> <p>Compute:</p> \[(v_m, h_m) = \arg\min_{v \in \mathbb{R}, h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n \exp\left(-y_i \left( f_{m-1}(x_i) + \overbrace{v h(x_i)}^{\text{new piece}} \right)\right)\] </li> <li> <p>Update:</p> \[f_m(x) = f_{m-1}(x) + v_m h_m(x)\] </li> </ul> </li> <li><strong>Return</strong> \(f_M\)</li> </ol> <hr/> <h5 id="fsam-with-exponential-loss-basis-function"><strong>FSAM with Exponential Loss: Basis Function</strong></h5> <p>We assume the base hypothesis space is:</p> \[\mathcal{H} = \left\{ h : \mathcal{X} \to \{-1, 1\} \right\}\] <p>At the \(m^\text{th}\) round, our goal is to choose \(v\) and \(h\) to minimize the following objective:</p> \[J(v, h) = \sum_{i=1}^n \exp\left[ -y_i \left( f_{m-1}(x_i) + v h(x_i) \right) \right]\] <p>We define:</p> \[w_i^m \triangleq \exp\left( -y_i f_{m-1}(x_i) \right)\] <p>This lets us re-express the objective as:</p> \[J(v, h) = \sum_{i=1}^n w_i^m \exp\left( -y_i v h(x_i) \right)\] <p>Now, because \(h(x_i) \in \{-1, 1\}\), we can split the expression into cases:</p> \[J(v, h) = \sum_{i=1}^n w_i^m \left[ \mathbb{I}(y_i = h(x_i)) e^{-v} + \mathbb{I}(y_i \ne h(x_i)) e^{v} \right]\] <p>Recall that:</p> \[\mathbb{I}(y_i = h(x_i)) = 1 - \mathbb{I}(y_i \ne h(x_i))\] <p>Using this identity, we can further simplify:</p> \[J(v, h) = \sum_{i=1}^n w_i^m \left[ (e^v - e^{-v}) \mathbb{I}(y_i \ne h(x_i)) + e^{-v} \right]\] <p>At this point, we’re ready to decide how to pick the best \(h\).</p> <p>Note that the second term of the objective function, \(e^{-v}\) is constant with respect to \(h\), and if \(v &gt; 0\), the term \(e^v - e^{-v}\) is positive. Therefore, minimizing \(J(v, h)\) is equivalent to minimizing:</p> \[\arg \min_{h \in \mathcal{H}} \sum_{i=1}^n w_i^m \mathbb{I}(y_i \ne h(x_i))\] <p>This leads to:</p> \[h_m = \arg \min_{h \in \mathcal{H}} \sum_{i=1}^n w_i^m \mathbb{I}(y_i \ne h(x_i))\] <p>We can also write this as a weighted classification error:</p> \[h_m = \arg \min_{h \in \mathcal{H}} \frac{1}{\sum_{i=1}^n w_i^m} \sum_{i=1}^n w_i^m \mathbb{I}(y_i \ne h(x_i))\] <p>In other words, \(h_m\) is the classifier that minimizes the <strong>weighted zero-one loss</strong>.</p> <hr/> <h5 id="fsam-with-exponential-loss-classifier-weights"><strong>FSAM with Exponential Loss: Classifier Weights</strong></h5> <p>Now that we’ve selected the best basis function \(h_m\), let’s figure out how to choose the best weight \(v_m\) for it.</p> <p>We define the <strong>weighted zero-one error</strong> at round \(m\) as:</p> \[\text{err}_m = \frac{\sum_{i=1}^n w_i^m \mathbb{I}(y_i \ne h(x_i))}{\sum_{i=1}^n w_i^m}\] <p>This error measures how poorly the current hypothesis \(h\) is doing, taking into account the importance (weights) of each example.</p> <p>Then, it can be shown that the optimal value for \(v_m\) — the coefficient for the current basis function is:</p> \[v_m = \frac{1}{2} \log \frac{1 - \text{err}_m}{\text{err}_m} \tag{14}\] <p>This is exactly the same form as the weight update in AdaBoost (just scaled differently). Note that if \(\text{err}_m &lt; 0.5\) — that is, our current classifier is better than random guessing — then \(v_m &gt; 0\), meaning it contributes positively.</p> <hr/> <p><strong>Derivation of the expression for the optimal classifier weights</strong></p> <p>To justify this expression for the optimal coefficient \(v_m\), recall that our goal at each round is to minimize the exponential loss objective:</p> \[J(v, h) = \sum_{i=1}^n w_i^m \left[ \exp(-v) \cdot \mathbb{I}(y_i = h(x_i)) + \exp(v) \cdot \mathbb{I}(y_i \ne h(x_i)) \right]\] <p>We can rewrite this as:</p> \[J(v, h) = \exp(-v) \sum_{i: y_i = h(x_i)} w_i^m + \exp(v) \sum_{i: y_i \ne h(x_i)} w_i^m\] <p>Define the <strong>weighted error</strong> of hypothesis \(h\) as:</p> \[\text{err}_m = \frac{\sum_{i=1}^n w_i^m \mathbb{I}(y_i \ne h(x_i))}{\sum_{i=1}^n w_i^m}\] <p>Let:</p> <ul> <li>\(W^{+} = \sum_{i: y_i = h(x_i)} w_i^m\) (correctly classified)</li> <li>\(W^{-} = \sum_{i: y_i \ne h(x_i)} w_i^m\) (misclassified)</li> </ul> <p>So the objective becomes:</p> \[J(v, h) = W^+ \exp(-v) + W^- \exp(v)\] <p>To find the optimal \(v = v_m\) for a fixed \(h = h_m\), we differentiate with respect to \(v\) and set to zero:</p> \[\frac{dJ}{dv} = -W^+ \exp(-v) + W^- \exp(v) = 0\] <p>Solving:</p> \[W^- \exp(v) = W^+ \exp(-v) \\ \Rightarrow \frac{W^-}{W^+} = \exp(-2v) \\ \Rightarrow -2v = \log\left( \frac{W^-}{W^+} \right) \\ \Rightarrow v = \frac{1}{2} \log\left( \frac{W^+}{W^-} \right)\] <p>Since:</p> \[\text{err}_m = \frac{W^-}{W^- + W^+} \quad \text{and} \quad 1 - \text{err}_m = \frac{W^+}{W^- + W^+}\] <p>We get:</p> \[\frac{W^+}{W^-} = \frac{1 - \text{err}_m}{\text{err}_m}\] <p>Therefore, the optimal weight for classifier \(h_m\) is:</p> \[v_m = \frac{1}{2} \log\left( \frac{1 - \text{err}_m}{\text{err}_m} \right) \tag{14}\] <p>This result balances the classifier’s confidence based on how well it performs: higher confidence (larger \(v_m\)) when error is low, and lower confidence when error is close to 0.5.</p> <hr/> <h5 id="fsam-with-exponential-loss-updating-example-weights"><strong>FSAM with Exponential Loss: Updating Example Weights</strong></h5> <p>After choosing the best classifier \(h_m\) and its corresponding weight \(v_m\), we now update the example weights for the next round.</p> <p>The weights at round \(m+1\) are defined as:</p> \[w_i^{m+1} \overset{\text{def}}{=} \exp\left( -y_i f_m(x_i) \right) \tag{15}\] <p>Recall that the updated model at round \(m\) is:</p> \[f_m(x_i) = f_{m-1}(x_i) + v_m h_m(x_i) \tag{16}\] <p>Substituting this into the definition of the new weights:</p> \[w_i^{m+1} = \exp\left( -y_i (f_{m-1}(x_i) + v_m h_m(x_i)) \right) \\ = \exp\left( -y_i f_{m-1}(x_i) \right) \cdot \exp\left( -y_i v_m h_m(x_i) \right) \\ = w_i^m \cdot \exp\left( -y_i v_m h_m(x_i) \right) \tag{17}\] <p>This form shows how the current round’s prediction (\(h_m(x_i)\)) affects the weight of example \(i\) moving forward.</p> <p>Let’s interpret this based on classification correctness:</p> <ul> <li> <p>If \(y_i = h_m(x_i)\) (i.e., correctly classified), then:</p> \[-y_i v_m h_m(x_i) = -v_m \cdot 1 = -v_m\] </li> <li> <p>If \(y_i \ne h_m(x_i)\) (i.e., misclassified), since both \(y_i\) and \(h_m(x_i)\) are in \(\{-1, 1\}\):</p> \[-y_i v_m h_m(x_i) = -v_m \cdot (-1) = +v_m\] </li> </ul> <p>We can now re-express the update in terms of the indicator function:</p> \[w_i^{m+1} = w_i^m \cdot \exp\left( 2v_m \mathbb{I}(y_i \ne h_m(x_i)) \right) \cdot \underbrace{\exp(-v_m)}_{\text{constant scaler}} \tag{18}\] <p>This is because:</p> <ul> <li>When \(y_i = h_m(x_i)\), \(\mathbb{I}(y_i \ne h_m(x_i)) = 0\), so the exponent is \(0\) and we just get \(w_i^m \cdot \exp(-v_m)\).</li> <li>When \(y_i \ne h_m(x_i)\), \(\mathbb{I}(y_i \ne h_m(x_i)) = 1\), so the exponent is \(2v_m\), and the weight becomes \(w_i^m \cdot \exp(v_m)\).</li> </ul> <p><strong>Interpretation</strong></p> <ul> <li><strong>Correct classification</strong>: the weight gets multiplied by \(\exp(-v_m)\) → the example becomes <strong>less important</strong>.</li> <li><strong>Misclassification</strong>: the weight gets multiplied by \(\exp(v_m)\) → the example becomes <strong>more important</strong>.</li> </ul> <p>This mechanism focuses the learner on harder examples in future rounds, by increasing their influence.</p> <blockquote> <p>The constant factor \(\exp(-v_m)\) appears in all weights and <strong>cancels out during normalization</strong>. So only the relative importance matters.</p> </blockquote> <p><strong>Connection to AdaBoost</strong></p> <p>Observe that:</p> \[2v_m = \alpha_m\] <p>This matches the AdaBoost formulation, where \(\alpha_m\) is the weight assigned to the classifier at round \(m\).</p> <p>Hence, <strong>FSAM with exponential loss recovers AdaBoost’s update rule</strong>, making it a principled derivation from a loss minimization perspective.</p> <hr/> <h5 id="why-use-exponential-loss"><strong>Why Use Exponential Loss?</strong></h5> <p>The exponential loss function is given by:</p> \[\ell_{\text{exp}}(y, f(x)) = \exp(-y f(x))\] <p>This loss has an elegant statistical interpretation. Specifically, it turns out that minimizing the expected exponential loss leads to a prediction function that estimates the <strong>log-odds</strong> of the label being positive:</p> \[f^*(x) = \frac{1}{2} \log \left( \frac{p(y = 1 \mid x)}{p(y = -1 \mid x)} \right)\] <p>This result aligns with the principle behind <strong>logistic regression</strong>, where the model estimates the log-odds of class membership. Here’s how we can justify it:</p> <p><strong>Derivation Sketch</strong></p> <p>We seek the function \(f^*(x)\) that minimizes the expected exponential loss:</p> \[\mathbb{E}_{y \sim p(y \mid x)}\left[ \exp(-y f(x)) \right] = p(y = 1 \mid x) \exp(-f(x)) + p(y = -1 \mid x) \exp(f(x))\] <p>Define:</p> <ul> <li> \[p_+ = p(y = 1 \mid x)\] </li> <li> \[p_- = p(y = -1 \mid x) = 1 - p_+\] </li> </ul> <p>So, the expected loss becomes:</p> \[L(f) = p_+ \exp(-f) + p_- \exp(f)\] <p>Take the derivative w.r.t. \(f\) and set to zero:</p> \[\frac{dL}{df} = -p_+ \exp(-f) + p_- \exp(f) = 0 \\ \Rightarrow p_- \exp(f) = p_+ \exp(-f) \\ \Rightarrow \frac{p_-}{p_+} = \exp(-2f) \\ \Rightarrow f^*(x) = \frac{1}{2} \log \left( \frac{p_+}{p_-} \right)\] <p>Thus,</p> \[f^*(x) = \frac{1}{2} \log \left( \frac{p(y = 1 \mid x)}{p(y = -1 \mid x)} \right)\] <p><strong>Interpretation</strong></p> <p>The exponential loss encourages predictions that align with the <strong>log-odds ratio</strong>. This makes it a natural choice for binary classification problems where confidence is important, and it helps explain why AdaBoost, which minimizes exponential loss, is such a powerful classifier.</p> <hr/> <h5 id="adaboost-and-exponential-loss-robustness-issues"><strong>AdaBoost and Exponential Loss: Robustness Issues</strong></h5> <p>While exponential loss has nice theoretical and computational properties, it comes with a key drawback — <strong>lack of robustness</strong>.</p> <p>Recall that the exponential loss is:</p> \[\ell_{\text{exp}}(y, f(x)) = \exp(-y f(x))\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-2-480.webp 480w,/assets/img/gb-2-800.webp 800w,/assets/img/gb-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This function grows <strong>exponentially</strong> as the margin \(y f(x)\) becomes more negative. So:</p> <ul> <li><strong>Misclassified examples</strong> (where \(y f(x) &lt; 0\)) incur <strong>very high penalties</strong>.</li> <li>As a result, <strong>outliers</strong> or <strong>noisy labels</strong> can dominate the loss and heavily influence the model.</li> </ul> <p><strong>Practical Consequences</strong></p> <ul> <li>AdaBoost (which minimizes exponential loss) tends to <strong>over-focus on misclassified points</strong>, even if they are noisy or mislabeled.</li> <li>This leads to <strong>degraded performance</strong> in datasets with: <ul> <li>High <strong>Bayes error rate</strong> (i.e., intrinsic label uncertainty),</li> <li>Significant <strong>label noise</strong>.</li> </ul> </li> </ul> <p>In contrast, <strong>logistic loss</strong> (or log-loss), used in <strong>Logistic Regression</strong>, penalizes mistakes more <strong>gradually</strong> and is <strong>more robust</strong> in such settings.</p> <p><strong>Why Still Use Exponential Loss?</strong></p> <p>Despite these robustness concerns, exponential loss has some <strong>computational advantages</strong>:</p> <ul> <li>It leads to <strong>simpler update rules</strong> (as seen in AdaBoost),</li> <li>The math works out cleanly in boosting settings,</li> <li>It’s easy to implement and analyze.</li> </ul> <p>So in summary:</p> <blockquote> <p><strong>Exponential loss</strong> is powerful and efficient but sensitive to outliers.<br/> <strong>Logistic loss</strong> is more robust, especially when the data is noisy or inherently uncertain.</p> </blockquote> <hr/> <h5 id="wrapping-up"><strong>Wrapping up</strong></h5> <p>In this post, we’ve unpacked how <strong>Forward Stagewise Additive Modeling (FSAM)</strong> builds models by greedily adding base learners — and how, when paired with the <strong>exponential loss</strong>, it naturally recovers the well-known <strong>AdaBoost</strong> algorithm.</p> <p>Key takeaways:</p> <ul> <li>FSAM provides a clean, iterative framework for model building.</li> <li>The <strong>exponential loss</strong> leads to a simple and elegant form of weight updates.</li> <li>AdaBoost emerges as a special case of FSAM with exponential loss, emphasizing misclassified examples via <strong>weighted classification error</strong> and <strong>adaptive reweighting</strong>.</li> </ul> <p>However, this formulation is limited to specific loss functions like the exponential loss.</p> <p><strong>What’s next?</strong><br/> In the upcoming post, we’ll generalize this framework to work with <strong>any differentiable loss function</strong>, leading to the powerful and flexible family of models known as <strong>Gradient Boosted Machines (GBMs)</strong>.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A clear walkthrough of FSAM and its role in boosting with exponential loss.]]></summary></entry><entry><title type="html">Introduction to Gradient Boosting</title><link href="https://monishver11.github.io/blog/2025/intro-gradient-boosting/" rel="alternate" type="text/html" title="Introduction to Gradient Boosting"/><published>2025-05-04T17:33:00+00:00</published><updated>2025-05-04T17:33:00+00:00</updated><id>https://monishver11.github.io/blog/2025/intro-gradient-boosting</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/intro-gradient-boosting/"><![CDATA[<p>In our previous discussions on ensemble methods, we explored <strong>boosting</strong> — a technique designed to reduce bias by combining several <strong>shallow decision trees</strong>, also known as <strong>weak learners</strong>. Each learner in the ensemble focuses on correcting the errors made by its predecessors. This iterative correction process results in a strong predictive model.</p> <p>One prominent boosting method we covered was <strong>AdaBoost</strong>, a powerful off-the-shelf classifier. We touched on how AdaBoost operates by minimizing an exponential loss and adjusting the sample weights accordingly.</p> <p>Now, let’s build on this foundation and transition into more general methods like <strong>Gradient Boosting</strong>, which extends the boosting idea to arbitrary differentiable loss functions.</p> <hr/> <h5 id="from-nonlinear-regression-to-gradient-boosting"><strong>From Nonlinear Regression to Gradient Boosting</strong></h5> <p>Let’s consider the problem of <strong>nonlinear regression</strong>. How can we model a function that fits complex, nonlinear data?</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gb-1-480.webp 480w,/assets/img/gb-1-800.webp 800w,/assets/img/gb-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gb-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gb-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>A common approach is to use <strong>basis function models</strong>, which transform the input into a new feature space where a linear model can be applied.</p> <h5 id="linear-models-with-basis-functions"><strong>Linear Models with Basis Functions</strong></h5> <p>We begin with a function of the form:</p> \[f(x) = \sum_{m=1}^{M} v_m h_m(x)\] <p>Here, the \(h_m(x)\) are <strong>basis functions</strong> (also called feature functions), and \(v_m\) are their corresponding coefficients.</p> <p><strong>Example</strong>: In polynomial regression, the basis functions are \(h_m(x) = x^m\).</p> <p>This method works well with standard linear model fitting techniques like <strong>least squares</strong>, <strong>lasso</strong>, or <strong>ridge regression</strong>. However, there’s a limitation: the basis functions \(h_1, \dots, h_M\) are <strong>fixed in advance</strong>.</p> <p>What if we could <strong>learn these basis functions</strong> instead of fixing them?</p> <h5 id="adaptive-basis-function-models"><strong>Adaptive Basis Function Models</strong></h5> <p>This idea leads us to <strong>adaptive basis function models</strong>, where we aim to learn both the coefficients and the basis functions:</p> \[f(x) = \sum_{m=1}^{M} v_m h_m(x), \quad \text{where } h_m \in \mathcal{H}\] <p>Here, \(\mathcal{H}\) is a <strong>base hypothesis space</strong> of functions \(h : \mathcal{X} \to \mathbb{R}\). The combined hypothesis space is:</p> \[\mathcal{F}_M = \left\{ f(x) = \sum_{m=1}^{M} v_m h_m(x) \,\middle|\, v_m \in \mathbb{R},\, h_m \in \mathcal{H} \right\}\] <p>The learnable parameters now include both the weights \(v_m\) and the functions \(h_m\) — this is what makes the model <em>adaptive</em>.</p> <p>Here’s a simple analogy to help build intuition for adaptive basis function models:</p> <blockquote> <p><strong>Think of building a house:</strong> In a linear model with fixed basis functions, it’s like choosing from a catalog of pre-made furniture — your design is limited by what’s available. In contrast, adaptive basis functions let you custom-build each piece of furniture to fit your space and needs perfectly. You’re not just choosing the weights (how much furniture to use), you’re also designing what kind of furniture works best in each room.</p> </blockquote> <p><strong>In ML terms:</strong> Instead of relying on predefined transformations of input features (e.g., fixed polynomials), adaptive models learn which transformations (basis functions) are most useful for capturing the data’s patterns.</p> <hr/> <h5 id="empirical-risk-minimization-erm"><strong>Empirical Risk Minimization (ERM)</strong></h5> <p>To train adaptive basis function models, we rely on the principle of <strong>Empirical Risk Minimization (ERM)</strong>.</p> <p>The idea is simple: among all possible functions in our hypothesis space \(\mathcal{F}_M\), we want to find the one that minimizes the <strong>average loss</strong> over our training data. The loss function \(\ell(y, f(x))\) measures how far our prediction \(f(x)\) is from the true label \(y\) — common choices include squared loss, logistic loss, or exponential loss depending on the task.</p> <p>Formally, our ERM objective is:</p> \[\hat{f} = \arg\min_{f \in \mathcal{F}_M} \frac{1}{n} \sum_{i=1}^n \ell(y_i, f(x_i))\] <p>Now, expanding \(f(x)\) as a sum over learnable basis functions, we write:</p> \[f(x) = \sum_{m=1}^{M} v_m h_m(x)\] <p>Plugging this into the ERM formulation, we get the following optimization objective:</p> \[J(v_1, \dots, v_M, h_1, \dots, h_M) = \frac{1}{n} \sum_{i=1}^n \ell \left( y_i, \sum_{m=1}^M v_m h_m(x_i) \right)\] <p>This is the <strong>ERM objective</strong> we aim to minimize. The challenge now is: how do we optimize this objective when both the weights \(v_m\) and the basis functions \(h_m\) are learnable?</p> <h5 id="gradient-based-methods"><strong>Gradient-Based Methods</strong></h5> <p>Let’s first consider the case where our base hypothesis space \(\mathcal{H}\) is <strong>parameterized</strong> by a vector \(\theta \in \mathbb{R}^b\). That is, each basis function \(h_m\) is written as:</p> \[h(x; \theta_m) \in \mathcal{H}\] <p>With this setup, our model becomes:</p> \[f(x) = \sum_{m=1}^M v_m h(x; \theta_m)\] <p>Substituting into our ERM objective gives:</p> \[J(v_1, \dots, v_M, \theta_1, \dots, \theta_M) = \frac{1}{n} \sum_{i=1}^n \ell \left( y_i, \sum_{m=1}^M v_m h(x_i; \theta_m) \right)\] <p>This formulation is powerful because <strong>we can now differentiate</strong> the loss \(\ell\) with respect to both:</p> <ul> <li>the weights \(v_m\) (which scale the outputs of each basis function), and</li> <li>the parameters \(\theta_m\) (which define the internal structure of each basis function).</li> </ul> <p>If the loss function \(\ell\) is differentiable, and the basis functions \(h(x; \theta)\) are differentiable with respect to \(\theta\), then we can apply <strong>gradient-based optimization</strong> methods such as <strong>Stochastic Gradient Descent (SGD)</strong>, <strong>Adam</strong>, or other first-order techniques to train the model end-to-end.</p> <h5 id="how-neural-networks-fit-in"><strong>How Neural Networks Fit In</strong></h5> <p>A classic example of this framework is a <strong>neural network</strong>:</p> <ul> <li>The network transforms the input \(x\) through multiple layers.</li> <li>The final output is a <strong>linear combination of the neurons in the last hidden layer</strong> — these neurons act as <strong>adaptive basis functions</strong> \(h_m(x; \theta_m)\).</li> <li>The weights \(v_m\) connect these neurons to the output layer.</li> </ul> <p>Since both the neuron activations and the loss functions (like cross-entropy or MSE) are differentiable, we can compute gradients with respect to both \(v_m\) and \(\theta_m\). This is why training neural networks using backpropagation is possible — it’s essentially solving the ERM objective using gradient-based methods.</p> <p><strong>Wait, Aren’t Neural Networks Still Using Fixed Functions?</strong></p> <p>This is a question I found myself asking — and maybe you’re wondering the same:</p> <blockquote> <p><em>“Aren’t neural networks still built from fixed components like linear combinations and activation functions? If so, are we really ‘learning’ basis functions — or just tuning weights inside a fixed structure?”</em></p> </blockquote> <p>That’s a great observation — and you’re absolutely right in noting that <strong>each neuron has a predefined form</strong>: it performs a linear combination of its inputs followed by a non-linear activation, like \(\sigma(w^T x + b)\). The architecture — layers, activation functions, etc. — is fixed before training.</p> <p>So what’s actually being learned?</p> <p>While we don’t change the <em>form</em> of the individual components, neural networks <strong>adaptively construct complex functions</strong> by <strong>composing these fixed units across layers</strong>. Each layer transforms the representation from the previous one, and through this deep composition, the network can approximate highly non-linear functions.</p> <p>Thus, neural networks fall under the broader umbrella of <strong>adaptive basis function models</strong>:</p> <blockquote> <p>We don’t learn new basis function types from scratch, but we do learn <strong>how to combine and configure</strong> them to fit the data.</p> </blockquote> <p>This makes neural networks extremely flexible — capable of learning complex patterns by tuning parameters inside a rich function space defined by their architecture.</p> <p>With all that said, if you’re not familiar with this part about neural networks, no worries — feel free to skip it for now. We’ll dive deeper into it in a dedicated section later on.</p> <p>For now, it’s important to understand that <strong>not all models support gradient-based optimization</strong>. So what happens when our basis functions are <strong>non-differentiable</strong>, like decision trees?</p> <hr/> <h5 id="when-gradient-based-methods-dont-apply"><strong>When Gradient-Based Methods Don’t Apply</strong></h5> <p>So far, we’ve relied on the assumption that our basis functions are differentiable with respect to their parameters — allowing us to optimize the ERM objective using gradient-based methods. But what if this assumption doesn’t hold?</p> <p>Let’s consider an important and widely-used case: when our base hypothesis space \(\mathcal{H}\) consists of <strong>decision trees</strong>.</p> <p>Decision trees are <strong>non-parametric models</strong>. That is, they’re not defined by a small set of continuous parameters like \(\theta \in \mathbb{R}^b\). Instead, they involve discrete decisions — splitting data based on feature thresholds, forming branches and leaf nodes. This poses two fundamental challenges:</p> <ol> <li> <p><strong>Non-differentiability</strong>: Even if we try to assign parameters to a decision tree (e.g., split thresholds, structure), the output of the tree does not change <strong>smoothly</strong> with respect to those parameters. Small changes in a split value can cause <strong>large, abrupt changes</strong> in the prediction.</p> </li> <li> <p><strong>Discontinuous prediction surfaces</strong>: Since decision trees partition the input space into disjoint regions, the function \(h(x)\) they represent is <strong>piecewise constant</strong>, which means it’s flat in regions and jumps discontinuously at split boundaries. Gradients simply don’t exist in such a surface.</p> </li> </ol> <p>Therefore, traditional gradient descent — which relies on computing derivatives — breaks down in this setting.</p> <h5 id="a-greedy-stage-wise-alternative"><strong>A Greedy, Stage-Wise Alternative</strong></h5> <p>Despite the lack of gradients, we can still make progress using a <strong>greedy, stage-wise</strong> approach inspired by <strong>AdaBoost</strong>.</p> <p>Recall how AdaBoost builds an ensemble by sequentially adding weak learners (like trees) to correct the errors of the previous ones. Even though it doesn’t explicitly minimize a loss function using gradients, it turns out that <strong>AdaBoost is implicitly minimizing exponential loss</strong> through a forward stage-wise additive modeling approach.</p> <p>This insight opens the door to generalizing the idea:</p> <blockquote> <p>Can we design a similar stage-wise additive method that <strong>explicitly minimizes a differentiable loss function</strong>, even when our base learners (like trees) are non-differentiable?</p> </blockquote> <p>The answer is <strong>yes</strong> — and this leads us to the powerful technique of <strong>Gradient Boosting</strong>.</p> <h5 id="gradient-boosting"><strong>Gradient Boosting</strong></h5> <p>Gradient Boosting combines the flexibility of additive models with the ability to optimize arbitrary differentiable loss functions — all while using <strong>non-differentiable base learners</strong> like decision trees.</p> <p>But how is that possible?</p> <blockquote> <p>The key idea is to <strong>perform gradient descent not in parameter space, but in function space</strong>.</p> </blockquote> <hr/> <p>In the next post, we’ll explore how Gradient Boosting interprets the optimization problem and fits a new base learner at each step by <strong>approximating the negative gradient</strong> of the loss with respect to the current model’s predictions.</p> <p>This clever trick allows us to iteratively improve the model — even when the individual learners don’t support gradients themselves.</p> <p>Stay tuned!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.]]></summary></entry><entry><title type="html">Boosting and AdaBoost</title><link href="https://monishver11.github.io/blog/2025/adaboost/" rel="alternate" type="text/html" title="Boosting and AdaBoost"/><published>2025-04-27T18:04:00+00:00</published><updated>2025-04-27T18:04:00+00:00</updated><id>https://monishver11.github.io/blog/2025/adaboost</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/adaboost/"><![CDATA[<p>Boosting is a powerful machine learning technique that focuses on reducing the error rate of high-bias estimators by combining many weak learners, typically trained sequentially. Unlike bagging, which trains classifiers in parallel to reduce variance, boosting focuses on improving performance by training classifiers sequentially on reweighted data. The core idea behind boosting is simple: rather than using a large and complex model that may overfit, we train a series of simpler models (typically decision trees) to improve accuracy gradually.</p> <p>In contrast to bagging’s emphasis on parallel training of multiple models on different data subsets, boosting systematically reweights the training examples after each classifier is added, directing the model’s attention to examples that previous classifiers struggled with.</p> <p><strong>Key Intuition:</strong></p> <ul> <li>A <strong>weak learner</strong> is a classifier that performs slightly better than random chance. <ul> <li>Example: A rule such as “If <code class="language-plaintext highlighter-rouge">&lt;keyword&gt;</code> then spam” or “From a friend” then “not spam”.</li> </ul> </li> <li>Weak learners focus on different parts of the data, which may be misclassified by previous models.</li> <li>The final model is a weighted combination of these weak learners, with each learner contributing differently based on its performance.</li> </ul> <p>We will explore a specific boosting algorithm: <strong>AdaBoost</strong> (Freund &amp; Schapire, 1997), which is commonly used with decision trees as weak learners.</p> <hr/> <h5 id="adaboost-setting"><strong>AdaBoost: Setting</strong></h5> <p>For binary classification, where the target variable \(Y = \{-1, 1\}\), AdaBoost uses a base hypothesis space \(H = \{h : X \rightarrow \{-1, 1\}\}\). Common choices for weak learners include:</p> <ul> <li><strong>Decision stumps</strong>: A tree with a single split.</li> <li><strong>Decision trees</strong> with a few terminal nodes.</li> <li><strong>Linear decision functions</strong>.</li> </ul> <p><strong>Weighted Training Set</strong></p> <p>Each weak learner in AdaBoost is trained on a <strong>weighted</strong> version of the training data. The training set \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\) has weights associated with each example: \(w_1, w_2, \dots, w_n\).</p> <p>The <strong>weighted empirical risk</strong> is defined as:</p> \[\hat{R}_n^w(f) \overset{\text{def}}{=} \frac{1}{W} \sum_{i=1}^{n} w_i \cdot \ell(f(x_i), y_i)\] <p>where \(W = \sum_{i=1}^{n} w_i\), and \(\ell\) is a loss function (typically 0-1 loss in the case of classification).</p> <p>Examples with larger weights have a more significant impact on the loss, guiding the model to focus on harder-to-classify examples.</p> <h5 id="adaboost-sketch-of-the-algorithm"><strong>AdaBoost: Sketch of the Algorithm</strong></h5> <p>AdaBoost works by combining several weak learners to create a strong classifier.<br/> Here’s the high-level process:</p> <ol> <li><strong>Start by assigning equal weights</strong> to all training examples:</li> </ol> \[w_1 = w_2 = \cdots = w_n = 1\] <ol> <li> <p><strong>For each boosting round</strong> \(m = 1, \dots, M\) (where \(M\) is the number of classifiers we want to train):</p> <ul> <li><strong>Train a base classifier</strong> \(G_m(x)\) on the <strong>current weighted</strong> training data.</li> <li><strong>Evaluate</strong> how well \(G_m(x)\) performs.</li> <li><strong>Increase the weights</strong> of examples that were <strong>misclassified</strong>, so the next classifier focuses more on those harder examples.</li> </ul> </li> <li> <p><strong>Aggregate the predictions</strong> from all classifiers, weighted by their accuracy:</p> </li> </ol> \[G(x) = \text{sign}\left( \sum_{m=1}^{M} \alpha_m G_m(x) \right)\] <p>The key idea is: <strong>the more accurate a base learner, the higher its influence in the final prediction</strong>.</p> <h5 id="adaboost-how-to-compute-classifier-weights"><strong>AdaBoost: How to Compute Classifier Weights</strong></h5> <p>In AdaBoost, each base classifier \(G_m\) contributes to the final prediction with a weight \(\alpha_m\).<br/> We want the following:</p> <ul> <li>\(\alpha_m\) should be <strong>non-negative</strong>.</li> <li>\(\alpha_m\) should be <strong>larger</strong> when \(G_m\) fits its weighted training data well.</li> </ul> <p>The <strong>weighted 0-1 error</strong> of the base classifier \(G_m(x)\) is computed as:</p> \[\text{err}_m = \frac{1}{W} \sum_{i=1}^{n} w_i \, \mathbf{1}\left[ y_i \neq G_m(x_i) \right]\] <p>where:</p> <ul> <li>\(W = \sum_{i=1}^{n} w_i\) is the total sum of weights.</li> <li>\(\mathbf{1}[\cdot]\) is an indicator function, equal to 1 if the condition is true, and 0 otherwise.</li> </ul> <p>Since the error is normalized by the total weight, we always have:</p> \[\text{err}_m \in [0, 1]\] <p>Once we know the error \(\text{err}_m\), we compute the weight of the classifier \(G_m\) as:</p> \[\alpha_m = \ln\left( \frac{1 - \text{err}_m}{\text{err}_m} \right)\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-4-480.webp 480w,/assets/img/ensemble-4-800.webp 800w,/assets/img/ensemble-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Higher weighted error ⇒ lower weight </div> <p><strong>Interpretation</strong>:</p> <ul> <li>If \(\text{err}_m\) is <strong>small</strong> (good classifier), then \(\alpha_m\) is <strong>large</strong>.</li> <li>If \(\text{err}_m\) is <strong>large</strong> (poor classifier), then \(\alpha_m\) is <strong>small</strong>.</li> </ul> <p>Thus, <strong>more accurate classifiers get higher voting power</strong> in the final decision.</p> <h5 id="adaboost-how-example-weights-are-updated"><strong>AdaBoost: How Example Weights Are Updated</strong></h5> <p>After training a base classifier, we update the weights of the examples to <strong>focus more on mistakes</strong>.</p> <p>Suppose \(w_i\) is the weight of example \(x_i\) <strong>before</strong> training \(G_m\). After training:</p> <ul> <li>If \(G_m\) <strong>correctly classifies</strong> \(x_i\), <strong>keep \(w_i\) the same</strong>.</li> <li>If \(G_m\) <strong>misclassifies</strong> \(x_i\), <strong>increase \(w_i\)</strong>:</li> </ul> \[w_i \leftarrow w_i \times e^{\alpha_m}\] <p>This adjustment ensures that:</p> <ul> <li><strong>Hard examples</strong> (previously misclassified) <strong>get more weight</strong> and are more likely to be correctly classified by future classifiers.</li> <li>If \(G_m\) is a <strong>strong classifier</strong> (large \(\alpha_m\)), the weight update for misclassified examples is <strong>more significant</strong>.</li> </ul> <p>Alternatively, you can think of it this way:</p> \[w_i \leftarrow w_i \times \left( \frac{1}{\text{err}_m} - 1 \right)\] <p>This reweighting step is what drives AdaBoost to sequentially <strong>correct</strong> the errors of the previous learners.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-5-480.webp 480w,/assets/img/ensemble-5-800.webp 800w,/assets/img/ensemble-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> AdaBoost: Schematic </div> <p><strong>Intuition Behind AdaBoost: Analogy</strong></p> <p>To better internalize AdaBoost, imagine the process as <strong>training a team of tutors</strong> to help a student (the model) pass an exam (classification task):</p> <ul> <li> <p><strong>First Tutor</strong>: The first tutor teaches the entire syllabus equally. After the first test, they realize the student struggles with some topics (mistakes/misclassifications).</p> </li> <li> <p><strong>Second Tutor</strong>: The second tutor <strong>focuses more heavily</strong> on the topics where the student made mistakes, spending extra time on them.</p> </li> <li> <p><strong>Third Tutor</strong>: The third tutor notices there are still lingering problems on certain topics, so they <strong>focus even more narrowly</strong> on the hardest concepts.</p> </li> <li> <p><strong>And so on…</strong></p> </li> </ul> <p>Each tutor is <strong>not perfect</strong>, but by <strong>combining their focused efforts</strong>, the student gets a much more complete understanding — better than what any single tutor could achieve alone.</p> <hr/> <h5 id="simple-mathematical-example"><strong>Simple Mathematical Example</strong></h5> <p>Let’s walk through a <strong>tiny AdaBoost example</strong> to see everything in action.</p> <p>Suppose we have 4 data points:</p> <hr/> <table> <thead> <tr> <th style="text-align: center">\(x_i\)</th> <th style="text-align: center">\(y_i\) (True Label)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">+1</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">+1</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">-1</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">-1</td> </tr> </tbody> </table> <hr/> <p><strong>Step 1: Initialization</strong></p> <p>All examples start with <strong>equal weight</strong>:</p> \[w_i = \frac{1}{4} = 0.25 \quad \text{for each } i\] <p><strong>Step 2: First Classifier \(G_1(x)\)</strong></p> <p>Suppose \(G_1\) predicts:</p> <hr/> <table> <thead> <tr> <th style="text-align: center">\(x_i\)</th> <th style="text-align: center">\(G_1(x_i)\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">+1</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">-1 ❌</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">-1</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">-1</td> </tr> </tbody> </table> <hr/> <p>It misclassifies \(x_2\).</p> <p>Compute weighted error:</p> \[\text{err}_1 = \frac{w_2}{\sum_{i=1}^{4} w_i} = \frac{0.25}{1} = 0.25\] <p>Classifier weight:</p> \[\alpha_1 = \ln\left( \frac{1 - 0.25}{0.25} \right) = \ln(3) \approx 1.0986\] <p><strong>Step 3: Update Weights</strong></p> <p>Increase the weight for the misclassified example:</p> <ul> <li>For correctly classified points \(w_i\) stays the same.</li> <li>For misclassified points:</li> </ul> \[w_i \leftarrow w_i \times e^{\alpha_1}\] <p>Thus:</p> <ul> <li>\(w_2\) (misclassified) becomes:</li> </ul> \[w_2' = 0.25 \times e^{1.0986} \approx 0.25 \times 3 = 0.75\] <ul> <li>\(w_1, w_3, w_4\) stay \(0.25\).</li> </ul> <p><strong>Normalization step</strong> (so weights sum to 1):</p> <p>Total weight:</p> \[W' = 0.25 + 0.75 + 0.25 + 0.25 = 1.5\] <p>New normalized weights:</p> <hr/> <table> <thead> <tr> <th style="text-align: center">\(x_i\)</th> <th style="text-align: center">New Weight</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">\(\frac{0.25}{1.5} \approx 0.167\)</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">\(\frac{0.75}{1.5} = 0.5\)</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">\(\frac{0.25}{1.5} \approx 0.167\)</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">\(\frac{0.25}{1.5} \approx 0.167\)</td> </tr> </tbody> </table> <hr/> <p><strong>Step 4: Second Classifier \(G_2(x)\)</strong></p> <p>Train next classifier \(G_2\) <strong>on the new weights</strong>.</p> <p>Now, \(x_2\) has the highest weight (0.5), so the model focuses more on predicting \(x_2\) correctly!</p> <p>(And the process repeats…)</p> <p><strong>Key Takeaways</strong></p> <ul> <li>AdaBoost <strong>punishes mistakes</strong> by increasing weights of misclassified examples.</li> <li>Future classifiers <strong>focus</strong> on the harder examples.</li> <li>Classifier weight \(\alpha\) depends on how good the classifier is (lower error → higher weight).</li> <li>Final prediction is:</li> </ul> \[G(x) = \text{sign}\left( \alpha_1 G_1(x) + \alpha_2 G_2(x) + \cdots + \alpha_M G_M(x) \right)\] <p>Thus, even if each individual classifier is weak, <strong>together they become strong</strong>!</p> <hr/> <h5 id="adaboost-algorithm"><strong>AdaBoost: Algorithm</strong></h5> <p>Given a training set:</p> \[\mathcal{D} = \{(x_1, y_1), \ldots, (x_n, y_n)\}.\] <p>The AdaBoost procedure works as follows:</p> <p><strong>Steps:</strong></p> <ol> <li> <p><strong>Initialize observation weights</strong>:</p> <p>Set:</p> \[w_i = 1, \quad \text{for all } i = 1, 2, \ldots, n.\] </li> <li> <p><strong>For \(m = 1\) to \(M\) (number of base classifiers)</strong>:</p> <ul> <li> <p><strong>Train</strong> a base learner on the weighted training data, obtaining a classifier \(G_m(x)\).</p> </li> <li> <p><strong>Compute the weighted empirical 0-1 risk</strong>:</p> </li> </ul> \[\text{err}_m = \frac{1}{W} \sum_{i=1}^{n} w_i \, \mathbf{1}[y_i \neq G_m(x_i)],\] <p>where:</p> \[W = \sum_{i=1}^{n} w_i.\] <ul> <li> <p><strong>Compute classifier weight</strong>:</p> <p>Assign a weight to the classifier based on its error:</p> \[\alpha_m = \ln\left( \frac{1 - \text{err}_m}{\text{err}_m} \right).\] </li> <li> <p><strong>Update example weights</strong>:</p> <p>Update the training example weights to emphasize misclassified examples:</p> \[w_i \leftarrow w_i \times \exp\left( \alpha_m \mathbf{1}[y_i \neq G_m(x_i)] \right).\] </li> </ul> </li> <li> <p><strong>Final classifier</strong>:</p> <p>After \(M\) rounds, return the final classifier:</p> \[G(x) = \text{sign}\left( \sum_{m=1}^{M} \alpha_m G_m(x) \right).\] </li> </ol> <p><strong>To put it shortly:</strong></p> <ul> <li><strong>Start</strong>: Treat every sample equally.</li> <li><strong>Learn</strong>: Focus the learner on samples that previous classifiers got wrong.</li> <li><strong>Combine</strong>: Build a strong final classifier by combining the weighted votes of all the base classifiers.</li> </ul> <p>Each \(\alpha_m\) ensures that <strong>better-performing classifiers get a stronger say</strong> in the final decision!</p> <hr/> <h5 id="weighted-error-vs-classifiers-true-error"><strong>Weighted Error vs. Classifier’s True Error</strong></h5> <p>In AdaBoost, the error \(\text{err}_m\) computed at each iteration is the <strong>weighted error</strong> based on the current distribution of sample weights, <strong>not</strong> the classifier’s true (unweighted) error rate over the data.</p> <p>This distinction is important: a classifier might have a low overall misclassification rate but could still have a <strong>high weighted error</strong> if it misclassifies examples that currently have large weights (i.e., harder or previously misclassified points).</p> <p>AdaBoost intentionally shifts focus toward difficult examples, so <strong>do not confuse the weighted empirical error used in boosting with the base learner’s standard classification error</strong>.</p> <h5 id="how-is-the-base-learner-optimized-at-each-iteration"><strong>How is the Base Learner Optimized at Each Iteration?</strong></h5> <p>At each iteration \(m\) of AdaBoost, the goal is to find the base classifier \(G_m(x)\) that <strong>minimizes the weighted empirical error</strong>:</p> \[\text{err}_m = \frac{1}{W} \sum_{i=1}^{n} w_i \, \mathbf{1}[y_i \neq G_m(x_i)].\] <p>Here’s the key idea:</p> <ul> <li>We <strong>don’t</strong> try to find a classifier that perfectly fits the original (unweighted) training data.</li> <li>Instead, we <strong>optimize for the current weighted dataset</strong> — meaning examples with larger weights influence the learning process more.</li> <li>The base learner is trained to focus on <strong>minimizing mistakes</strong> on the examples that have been <strong>harder to classify</strong> so far.</li> </ul> <p><strong>Typical Optimization Process</strong>:</p> <ul> <li>If using <strong>decision stumps</strong> (one-level decision trees), the learner searches for the split that minimizes the weighted classification error.</li> <li>In general, the base model uses the <strong>sample weights</strong> as importance scores to guide its fitting.</li> </ul> <p>Thus, at each step, AdaBoost adapts the learning problem to focus on what the previous classifiers struggled with, gradually building a strong ensemble.</p> <div class="row justify-content-center"> <div class="col-sm-4 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-6-480.webp 480w,/assets/img/ensemble-6-800.webp 800w,/assets/img/ensemble-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-7-480.webp 480w,/assets/img/ensemble-7-800.webp 800w,/assets/img/ensemble-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-8-480.webp 480w,/assets/img/ensemble-8-800.webp 800w,/assets/img/ensemble-8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-8" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> AdaBoost with Decision Stumps. (I)After 1 round, (II)After 3 rounds &amp; (III)After 120 rounds. Size of plus sign represents weight of example. Blackness represents preference for red class; whiteness represents preference for blue class. </div> <h5 id="does-adaboost-overfit"><strong>Does AdaBoost Overfit?</strong></h5> <p>While boosting generally performs well, it’s natural to ask: <strong>Does AdaBoost overfit with many rounds?</strong></p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-9-480.webp 480w,/assets/img/ensemble-9-800.webp 800w,/assets/img/ensemble-9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-9" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> General learning curves if we were overfitting </div> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-10-480.webp 480w,/assets/img/ensemble-10-800.webp 800w,/assets/img/ensemble-10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-10" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Learning Curves for AdaBoost </div> <p>The learning curves for AdaBoost typically show that the test error continues to decrease even after the training error reaches zero, which indicates that AdaBoost is <strong>resistant to overfitting</strong>. This is one of the reasons why AdaBoost is so powerful: it can maintain good generalization even with many weak learners.</p> <h5 id="adaboost-in-real-world-applications"><strong>AdaBoost in Real-World Applications</strong></h5> <p>A famous application of AdaBoost is <strong>face detection</strong>, as demonstrated in Viola &amp; Jones (2001). In this case, AdaBoost uses pre-defined weak classifiers and employs a smart way of doing real-time inference, even on hardware from 2001. This demonstrates the efficiency and applicability of AdaBoost in practical scenarios.</p> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>Boosting is an ensemble technique aimed at reducing bias by combining multiple weak learners. The sequential nature of boosting means that each learner focuses on errors made by previous ones, ultimately improving the model’s performance. <strong>AdaBoost</strong> is a specific and highly effective boosting algorithm that can be used with decision trees as weak learners to achieve powerful classification results.</p> <p><strong>Next Steps:</strong></p> <p>In the next section, we’ll explore the <strong>objective function</strong> of AdaBoost in more detail, along with some <strong>generalizations</strong> to other loss functions and the popular <strong>Gradient Boosting</strong> algorithm.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.]]></summary></entry><entry><title type="html">Random Forests</title><link href="https://monishver11.github.io/blog/2025/random-forest/" rel="alternate" type="text/html" title="Random Forests"/><published>2025-04-27T16:14:00+00:00</published><updated>2025-04-27T16:14:00+00:00</updated><id>https://monishver11.github.io/blog/2025/random-forest</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/random-forest/"><![CDATA[<p>After understanding the power of bagging to reduce variance by combining multiple models, a natural question arises: <strong>Can we make this idea even stronger?</strong></p> <p>Bagging helps, but when the individual models (like decision trees) are still correlated, variance reduction is not as efficient as it could be. This brings us to the motivation for a more advanced ensemble method — <strong>Random Forests</strong>.</p> <hr/> <h5 id="motivating-random-forests-handling-correlated-trees"><strong>Motivating Random Forests: Handling Correlated Trees</strong></h5> <p>Let’s revisit an important insight we learned from bagging.</p> <p>Suppose we have independent estimates \(\hat{\theta}_1, \dots, \hat{\theta}_n\) satisfying:</p> \[\mathbb{E}[\hat{\theta}_i] = \theta, \quad \text{Var}(\hat{\theta}_i) = \sigma^2\] <p>Then:</p> <ul> <li> <p>The mean of the estimates is:</p> \[\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n \hat{\theta}_i\right] = \theta\] </li> <li> <p>And the variance of the mean is:</p> \[\text{Var}\left(\frac{1}{n} \sum_{i=1}^n \hat{\theta}_i\right) = \frac{\sigma^2}{n}\] </li> </ul> <p>This shows that averaging independent estimators reduces variance effectively.</p> <p>However, in practice, if the estimators \(\hat{\theta}_i\) are <strong>correlated</strong>, the variance reduction is <strong>less effective</strong>. Why is that? Let’s break it down.</p> <p><strong>What Happens When Estimators are Correlated?</strong></p> <p>To understand this, consider that each estimator \(\hat{\theta}_i\) has some <strong>variance</strong> on its own. If these estimators are independent, averaging them reduces the total variance by a factor of \(\frac{1}{n}\) (as we saw earlier in bagging). This is because <strong>independence</strong> means that the errors or fluctuations in one model don’t affect the others.</p> <p>But, when estimators are <strong>correlated</strong>, this doesn’t hold true anymore. In fact, the variance reduction becomes less effective. The reason for this is that correlation introduces <strong>covariance terms</strong>.</p> <ul> <li><strong>Covariance</strong> is a measure of how much two random variables change together. If two estimators are correlated, the error in one model is likely to be reflected in the other. This reduces the benefit of averaging since both estimators will likely “make similar mistakes.”</li> </ul> <p><strong>A Simple Example</strong></p> <p>Imagine you’re trying to estimate the weight of an object. Suppose you have two different methods (estimators) to measure this weight:</p> <ol> <li><strong>Estimator 1</strong> measures the weight but has some error due to a systematic bias (e.g., the scale is always slightly off by 1kg).</li> <li><strong>Estimator 2</strong> is another scale that also has a bias, but it happens to be the same as Estimator 1.</li> </ol> <p>Now, if these two estimators have <strong>the same bias</strong>, they are correlated because they both tend to make the same mistake. Averaging the two estimations won’t help much because both estimators have the same error. So, the resulting variance after averaging will not reduce as much as we would expect with independent estimators.</p> <p><strong>Why Are Bootstrap Samples Not Fully Independent?</strong></p> <p>Now, let’s return to bagging, where we train multiple trees on <strong>bootstrap samples</strong>.</p> <ul> <li>A <strong>bootstrap sample</strong> is generated by randomly sampling with replacement from the original dataset. This means that each point in the original dataset has a chance of being included multiple times, and some points may not be included at all.</li> <li>These <strong>bootstrap samples</strong> are independent <strong>from each other</strong> in terms of how they are created, but they are <strong>not independent</strong> from the true population distribution \(P_{X \times Y}\).</li> </ul> <p>Why? Because all the bootstrap samples come from the <strong>same original dataset</strong>, which means they contain the same underlying distribution. If the original dataset has certain features that are particularly strong predictors, these features will often appear at the top of many decision trees across different bootstrap samples. This can make the individual trees more <strong>similar</strong> to each other than we would like.</p> <p>For example, consider a dataset where the feature “age” strongly predicts whether a customer will buy a product. If all the bootstrap samples include “age” as a key feature, the decision trees trained on these samples will end up making similar decisions based on “age”. This creates <strong>correlation</strong> between the predictions of the trees because they are all making similar splits based on the same strong features.</p> <p><strong>What Does This Mean for Bagging?</strong></p> <p>In bagging, since the trees are still correlated (due to the shared features across bootstrap samples), the reduction in variance is not as significant as we would expect if the trees were fully independent. This limits the effectiveness of bagging.</p> <p><strong>Can We Reduce This Correlation?</strong></p> <p>This is the key challenge addressed by <strong>Random Forests</strong>. By introducing additional randomness into the process — specifically, by randomly selecting a subset of features at each node when building each tree — we can reduce the correlation between the trees. This leads to a more diverse set of trees, improving the overall performance of the ensemble.</p> <p>Thus, reducing the correlation between trees is one of the main innovations of Random Forests that makes it more powerful than bagging alone.</p> <hr/> <p>To clearly understand how correlation between trees impacts variance reduction, let’s break down the two scenarios with a full mathematical setup.</p> <h5 id="setup"><strong>Setup:</strong></h5> <ul> <li>Suppose we have a dataset \(D = \{x_1, x_2, \dots, x_n\}\), where each \(x_i\) is a feature vector and each corresponding output \(y_i\) is from a true population distribution \(P_{X \times Y}\).</li> <li>We are training decision trees, and we want to understand how their predictions are affected by the sampling method.</li> <li>We’ll define two key scenarios: <ul> <li><strong>Independent sampling</strong> (ideal case): Each tree in the ensemble is trained on independently drawn samples from the true population \(P_{X \times Y}\).</li> <li><strong>Bootstrap sampling</strong>: Each tree is trained on a bootstrap sample, which is created by sampling <em>with replacement</em> from the original dataset \(D\).</li> </ul> </li> </ul> <p><strong>Case 1: Independent Sampling</strong></p> <p>Let’s assume that we have two estimators (trees) \(\hat{f}_1(x)\) and \(\hat{f}_2(x)\), both trained independently from the true population. Their predictions are unbiased, and their variances are \(\text{Var}(\hat{f}_1(x)) = \sigma_1^2\) and \(\text{Var}(\hat{f}_2(x)) = \sigma_2^2\). Since they are trained on independent samples, the covariance between their predictions is zero:</p> \[\text{Cov}(\hat{f}_1(x), \hat{f}_2(x)) = 0\] <p>This means that the two trees are completely independent of each other. When we combine their predictions (by averaging), we can reduce the overall variance of the ensemble:</p> \[\text{Var}(\hat{f}_{\text{avg}}(x)) = \frac{1}{2} \left( \text{Var}(\hat{f}_1(x)) + \text{Var}(\hat{f}_2(x)) \right) = \frac{\sigma_1^2 + \sigma_2^2}{2}\] <p>Since the predictions are independent, the variance reduces nicely without any issues.</p> <p><strong>Case 2: Bootstrap Sampling</strong></p> <p>Now, let’s look at the case of bootstrap sampling. Each decision tree is trained on a bootstrap sample of the original data, which means that the training samples are drawn <em>with replacement</em> from the dataset. This results in <strong>correlated trees</strong> because:</p> <ul> <li>Bootstrap samples are independent of each other (each sample is drawn from the dataset), but they are <strong>not independent</strong> of the true population distribution \(P_{X \times Y}\).</li> <li>As a result, the predictions from different trees \(\hat{f}_1(x)\) and \(\hat{f}_2(x)\) are <strong>correlated</strong> with each other.</li> </ul> <p>Let’s define the correlation coefficient between \(\hat{f}_1(x)\) and \(\hat{f}_2(x)\) as \(\rho\), where \(0 &lt; \rho &lt; 1\). This correlation arises because both trees are trained on slightly different subsets of the data, which means they will make similar predictions on the same inputs.</p> <p>Now, the <strong>variance of the ensemble</strong> will depend on both the variance of individual trees and the covariance between them:</p> \[\text{Var}(\hat{f}_{\text{avg}}(x)) = \frac{1}{2} \left( \text{Var}(\hat{f}_1(x)) + \text{Var}(\hat{f}_2(x)) \right) + \text{Cov}(\hat{f}_1(x), \hat{f}_2(x))\] <p>Since \(\text{Cov}(\hat{f}_1(x), \hat{f}_2(x)) = \rho \cdot \sigma_1 \cdot \sigma_2\), we get:</p> \[\text{Var}(\hat{f}_{\text{avg}}(x)) = \frac{\sigma_1^2 + \sigma_2^2}{2} + \rho \cdot \sigma_1 \cdot \sigma_2\] <p>Notice that the correlation \(\rho\) causes the variance to <strong>not reduce as effectively</strong> as in the independent case. The more correlated the trees are, the less variance reduction we achieve, and the ensemble may not perform as well as expected.</p> <p><strong>Key Differences and Conclusion:</strong></p> <ol> <li> <p><strong>Independent Sampling</strong>: When the trees are independent, we see <strong>maximum variance reduction</strong> because there is no covariance between the models. The variance of the average prediction is simply the average of the individual variances.</p> </li> <li> <p><strong>Bootstrap Sampling</strong>: When the trees are trained on bootstrap samples, the <strong>correlation</strong> between the trees reduces the potential for variance reduction. This is because the trees share a common structure due to being trained on similar data. The variance of the average prediction is larger because of the covariance term.</p> </li> </ol> <p>This setup clearly shows how correlation between trees in bootstrap sampling impacts the variance reduction in bagging. The next step to address this issue is through <strong>Random Forests</strong>, where we introduce further randomness to decorrelate the trees.</p> <hr/> <h5 id="random-forests"><strong>Random Forests</strong></h5> <p>Random Forests build upon the foundation of bagging decision trees, but introduce an extra layer of randomness to improve performance and reduce correlation between trees. Here’s how:</p> <ul> <li><strong>Grow trees independently</strong>, just as in bagging, by training each tree on a different bootstrap sample.</li> <li><strong>At each split</strong> in the tree, instead of considering all available features, <strong>randomly select a subset of \(m\) features</strong> and split based only on these.</li> </ul> <p><strong>What Does This Change Do?</strong></p> <p>This adjustment has a significant impact on the performance of the ensemble:</p> <ul> <li><strong>Reduces correlation between trees</strong>: By limiting the set of features considered at each split, trees become less likely to make the same decisions and thus become less correlated with each other.</li> <li><strong>Increases diversity among trees</strong>: Different features lead to different decision boundaries in each tree, creating a diverse set of models that are not overly similar to each other.</li> <li><strong>Improves ensembling effectiveness</strong>: With greater diversity, the ensemble as a whole becomes stronger. The averaged predictions from these less correlated trees lead to more robust and accurate results.</li> </ul> <p><strong>Typical Values for \(m\)</strong></p> <p>The parameter \(m\) determines the number of features considered at each split. The value of \(m\) is chosen based on the type of task:</p> <ul> <li> <p>For <strong>classification tasks</strong>, it is common to set:<br/> \(m \approx \sqrt{p}\)</p> </li> <li> <p>For <strong>regression tasks</strong>, we typically set:<br/> \(m \approx \frac{p}{3}\)</p> </li> </ul> <p>(where \(p\) is the total number of features.)</p> <p>These values help strike a balance between randomness and the amount of information available at each decision node.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-3-480.webp 480w,/assets/img/ensemble-3-800.webp 800w,/assets/img/ensemble-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Random Forests: Effect of Feature Subset Size (m) </div> <p><strong>Important Note:</strong></p> <p>If you set \(m = p\) (i.e., if each tree is allowed to use all the features at each split), then Random Forests will behave just like <strong>bagging</strong> — i.e., there will be no additional randomness, and the trees will be fully correlated.</p> <p>By introducing this random selection of features, <strong>Random Forests</strong> overcome one of the limitations of bagging (the correlation between trees) and unlock the full power of ensemble learning. This makes Random Forests one of the most powerful and widely used machine learning techniques today.</p> <hr/> <h5 id="review-recap-of-key-concepts"><strong>Review: Recap of Key Concepts</strong></h5> <p>In summary, here’s a quick review of the key points we’ve covered:</p> <ul> <li> <p><strong>Deep decision trees</strong> generally have <strong>low bias</strong> (they can fit the training data very well) but <strong>high variance</strong> (small changes in the data can lead to significant changes in the tree structure).</p> </li> <li> <p><strong>Ensembling</strong> multiple models helps <strong>reduce variance</strong>. The rationale behind this is that the <strong>mean of i.i.d. estimates</strong> tends to have <strong>smaller variance</strong> than a single estimate.</p> </li> <li> <p><strong>Bootstrap sampling</strong> allows us to simulate many different datasets from a single training set, which is the foundation of <strong>bagging</strong>.</p> </li> </ul> <p>However, while bagging uses <strong>bootstrap samples</strong> to train individual models, these models (the decision trees) are <strong>correlated</strong>, which limits the reduction in variance.</p> <ul> <li><strong>Random Forests</strong> address this by <strong>increasing the diversity</strong> of the ensemble. They achieve this by selecting a <strong>random subset of features</strong> at each split of the decision trees, reducing correlation and enhancing performance.</li> </ul> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>To wrap it up, Random Forests combine the strengths of bagging (reduced variance) with the added benefit of increased diversity among trees. By introducing randomness in feature selection, they make each tree in the ensemble more independent, leading to a <strong>stronger, more robust model</strong>.</p> <p>Random Forests stand out as one of the most powerful and widely used techniques in machine learning, thanks to their ability to handle complex data patterns while mitigating overfitting through ensembling and randomness.</p> <p>Next, we’ll explore <strong>Boosting</strong> - another powerful ensemble technique that builds models sequentially to improve accuracy by focusing on the mistakes made by previous models.</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.]]></summary></entry><entry><title type="html">Bagging - Bootstrap Aggregation</title><link href="https://monishver11.github.io/blog/2025/bagging/" rel="alternate" type="text/html" title="Bagging - Bootstrap Aggregation"/><published>2025-04-27T00:10:00+00:00</published><updated>2025-04-27T00:10:00+00:00</updated><id>https://monishver11.github.io/blog/2025/bagging</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/bagging/"><![CDATA[<p>Previously, we saw how ensemble methods, like averaging independent models or using bootstrap sampling, can reduce variance and improve prediction stability.</p> <p>Today, we dive deeper into one powerful technique: <strong>Bagging</strong> — Bootstrap Aggregating, and explore how it helps build more stable and accurate models.</p> <hr/> <h5 id="bagging-bootstrap-aggregation"><strong>Bagging: Bootstrap Aggregation</strong></h5> <p>Bagging, short for <strong>Bootstrap Aggregation</strong>, is a general-purpose method for reducing variance.</p> <p>The idea is simple:</p> <ul> <li>Draw \(B\) bootstrap samples \(D_1, D_2, \dots, D_B\) from the original dataset \(D\).</li> <li> <p>Train a model on each bootstrap sample to get prediction functions:</p> \[\hat{f}_1, \hat{f}_2, \dots, \hat{f}_B\] </li> <li> <p>Combine their predictions. The bagged prediction function is:</p> \[\hat{f}_{\text{avg}}(x) = \text{Combine}\left( \hat{f}_1(x), \hat{f}_2(x), \dots, \hat{f}_B(x) \right)\] </li> </ul> <p>Depending on the task:</p> <ul> <li>For <strong>regression</strong>, we typically average predictions.</li> <li>For <strong>classification</strong>, we usually take a <strong>majority vote</strong>.</li> </ul> <p><strong>Key Point: Why Bagging Doesn’t Overfit</strong></p> <p>One of the most powerful aspects of bagging is that <strong>increasing the number of trees does not cause overfitting</strong>.<br/> In fact, adding more trees generally <strong>improves performance</strong> by further reducing the variance of the model.<br/> Since each tree is trained on a slightly different bootstrap sample, their errors tend to average out, making the overall prediction more stable and reliable.<br/> Thus, more trees usually help — at worst, performance plateaus, but it rarely gets worse with additional trees.</p> <p><strong>Downside: Loss of Interpretability</strong></p> <p>However, this variance reduction comes at a cost. A <strong>single decision tree</strong> is often easy to visualize and interpret: you can follow a clear, logical path from the root to a prediction.<br/> With <strong>hundreds or thousands of trees</strong> combined in a bagged ensemble, the resulting model becomes a black box — it is much harder (or nearly impossible) to trace a single prediction back through all contributing trees. In other words, bagging sacrifices <strong>interpretability</strong> for <strong>predictive performance</strong>.</p> <hr/> <h5 id="out-of-bag-oob-predictions-and-error-estimation"><strong>Out-of-Bag (OOB) Predictions and Error Estimation</strong></h5> <p>Bagging also provides a natural way to estimate test error without needing a separate validation set!</p> <p>Recall:</p> <ul> <li>Each bagged model is trained on roughly <strong>63%</strong> of the original data.</li> <li>The remaining <strong>37%</strong> — the observations not included in the bootstrap sample — are called <strong>out-of-bag (OOB) observations</strong>.</li> </ul> <p>For the \(i\)-th training point:</p> <ul> <li> <p>Define the set of bootstrap samples <strong>that did not include</strong> \(x_i\) as:</p> \[S_i = \{b \,\vert\, D_b \text{ does not contain } x_i\}\] </li> <li> <p>The <strong>Out-of-Bag (OOB) prediction</strong> for \(x_i\) is then the average prediction across all trees where \(x_i\) was not used in training:</p> \[\hat{f}_{\text{OOB}}(x_i) = \frac{1}{|S_i|} \sum_{b \in S_i} \hat{f}_b(x_i)\] </li> </ul> <p>Here, \(\vert S_i \vert\) is the number of trees that did not see \(x_i\) during training.</p> <p>Once we have OOB predictions for all training points, we can compute the <strong>OOB error</strong> by comparing these predictions against the true labels.</p> <p>The OOB error serves as a reliable estimate of the model’s test error — <strong>similar to cross-validation</strong>, but <strong>without needing to split the data</strong> or perform multiple rounds of training. This makes OOB evaluation very efficient, especially for large datasets.</p> <h5 id="applying-bagging-to-classification-trees"><strong>Applying Bagging to Classification Trees</strong></h5> <p>Let’s look at a practical example:</p> <ul> <li>Input space: \(X = \mathbb{R}^5\)</li> <li>Output space: \(Y = \{-1, 1\}\)</li> <li>Sample size: \(n = 30\)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-2-480.webp 480w,/assets/img/ensemble-2-800.webp 800w,/assets/img/ensemble-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When we train decision trees on different bootstrap samples, the trees we get are often <strong>very different</strong> from one another.</p> <ul> <li>Each bootstrap sample is a slightly different version of the original dataset.</li> <li>As a result, the <strong>splitting variables</strong> chosen at the root (and deeper nodes) can change significantly across trees.</li> <li>This leads to trees with <strong>different structures</strong>, <strong>different decision boundaries</strong>, and ultimately, <strong>different predictions</strong>.</li> </ul> <p>This behavior highlights a key property of decision trees: they are <strong>high variance models</strong>. Even small perturbations or changes in the training data can cause a decision tree to change substantially.</p> <p>However, instead of seeing this as a disadvantage, we <strong>leverage</strong> this property through <strong>bagging</strong> (Bootstrap Aggregation). By training multiple high-variance trees on different bootstrap samples and <strong>averaging</strong> their predictions, we are able to:</p> <ul> <li><strong>Reduce variance</strong> significantly</li> <li><strong>Stabilize predictions</strong></li> <li><strong>Improve overall model performance</strong> without increasing bias</li> </ul> <p>Thus, bagging turns the natural instability of decision trees into a <strong>strength</strong>, leading to a more robust and accurate ensemble model.</p> <h5 id="why-decision-trees-are-ideal-for-bagging"><strong>Why Decision Trees Are Ideal for Bagging</strong></h5> <p>Bagging is particularly powerful when the base models are <strong>relatively unbiased</strong> but have <strong>high variance</strong> — and decision trees fit this description perfectly.</p> <ul> <li>Deep, unpruned decision trees tend to have <strong>low bias</strong>, as they can closely fit the training data and capture complex patterns.</li> <li>However, they also suffer from <strong>high variance</strong>: small changes or perturbations in the training data can lead to drastically different tree structures and predictions.</li> <li>By training many such high-variance trees on different bootstrap samples and <strong>averaging</strong> their outputs, bagging <strong>reduces the overall variance</strong> without significantly affecting the bias.</li> </ul> <p>The result is a <strong>more stable, robust, and accurate ensemble model</strong> compared to relying on a single decision tree.</p> <p>Thus, decision trees are an ideal candidate for bagging — their natural instability becomes an advantage when combined through this technique.</p> <hr/> <p>With this understanding in place, we are now ready to move toward an even more powerful idea: <strong>Random Forests</strong> - where we take bagging one step further by adding an extra layer of randomness to make our ensemble even stronger!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.]]></summary></entry><entry><title type="html">Introduction to Ensemble Methods</title><link href="https://monishver11.github.io/blog/2025/intro-to-ensemble-methods/" rel="alternate" type="text/html" title="Introduction to Ensemble Methods"/><published>2025-04-25T00:24:00+00:00</published><updated>2025-04-25T00:24:00+00:00</updated><id>https://monishver11.github.io/blog/2025/intro-to-ensemble-methods</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/intro-to-ensemble-methods/"><![CDATA[<p>Ensemble methods are a powerful set of techniques in machine learning that aim to improve prediction performance by combining the outputs of multiple models. Before diving into ensemble strategies, let’s revisit some foundational concepts that lead us to the rationale behind ensemble methods.</p> <hr/> <h5 id="review-decision-trees"><strong>Review: Decision Trees</strong></h5> <ul> <li><strong>Non-linear</strong>, <strong>non-metric</strong>, and <strong>non-parametric</strong> models</li> <li>Capable of <strong>regression</strong> or <strong>classification</strong></li> <li><strong>Interpretable</strong>, especially when shallow</li> <li>Constructed using a <strong>greedy algorithm</strong> that seeks to maximize the <strong>purity of nodes</strong></li> <li>Prone to <strong>overfitting</strong>, unless properly regularized</li> </ul> <p>These models serve as the building blocks for many ensemble techniques like Random Forests.</p> <h5 id="recap-statistics-and-point-estimators"><strong>Recap: Statistics and Point Estimators</strong></h5> <p>We begin with data:</p> \[D = (x_1, x_2, \dots, x_n)\] <p>sampled i.i.d. from a parametric distribution</p> \[p(\cdot \mid \theta)\] <p>A <strong>statistic</strong> is any function of the data, e.g.,</p> <ul> <li>Sample mean</li> <li>Sample variance</li> <li>Histogram</li> <li>Empirical distribution</li> </ul> <p>A <strong>point estimator</strong> is a statistic used to estimate a parameter:</p> \[\hat{\theta} = \hat{\theta}(D) \approx \theta\] <p><strong>Example:</strong> Suppose we’re estimating the average height \(\theta\) of a population. We collect a sample of \(n\) people and compute the sample mean:</p> \[\hat{\theta}(D) = \frac{1}{n} \sum_{i=1}^n x_i\] <p>This sample mean is a <strong>point estimator</strong> for the true average height \(\theta\) of the entire population.</p> <h5 id="recap-bias-and-variance-of-an-estimator"><strong>Recap: Bias and Variance of an Estimator</strong></h5> <p>Since statistics are derived from random samples, they themselves are <strong>random variables</strong>. The distribution of a statistic across different random samples is called the <strong>sampling distribution</strong>.</p> <p>Understanding the <strong>bias</strong> and <strong>variance</strong> of an estimator helps us evaluate how good the estimator is.</p> <ul> <li> <p><strong>Bias</strong> measures the systematic error — how far, on average, the estimator is from the true parameter:</p> \[\text{Bias}(\hat{\theta}) \overset{\text{def}}{=} \mathbb{E}[\hat{\theta}] - \theta\] </li> <li> <p><strong>Variance</strong> measures the variability of the estimator due to sampling randomness:</p> \[\text{Var}(\hat{\theta}) \overset{\text{def}}{=} \mathbb{E}[\hat{\theta}^2] - \left(\mathbb{E}[\hat{\theta}]\right)^2\] </li> </ul> <p>Intuitively:</p> <ul> <li><strong>Low bias</strong> means the estimator is <em>accurate</em> on average.</li> <li><strong>Low variance</strong> means the estimator is <em>stable</em> across different samples.</li> </ul> <p>Even an <strong>unbiased estimator</strong> can be <strong>unreliable</strong> if its variance is high. That is, it may give wildly different results on different samples, even if the average over many samples is correct.</p> <p><strong>Example:</strong> Suppose we are trying to estimate the <strong>true mean</strong> \(\theta\) of a population — for example, the average height of all adults in a city. We collect a sample of size \(n\):</p> \[D = (x_1, x_2, \dots, x_n)\] <p>where each \(x_i\) is drawn i.i.d. from a distribution with <strong>mean</strong> \(\theta\) and some unknown variance \(\sigma^2\).</p> <p>Consider two different estimators of the population mean:</p> <ol> <li>\(\hat{\theta}_1(D) = x_1\) — just the first point in the sample</li> <li>\(\hat{\theta}_2(D) = \frac{1}{n} \sum_{i=1}^n x_i\) — the sample mean</li> </ol> <p>We say an estimator \(\hat{\theta}\) is <strong>unbiased</strong> if, on average, it correctly estimates the true value of the parameter \(\theta\):</p> \[\mathbb{E}[\hat{\theta}] = \theta\] <p>In this case:</p> <ul> <li> <p>For \(\hat{\theta}_1\), since \(x_1\) is sampled from the distribution with mean \(\theta\), we have:</p> \[\mathbb{E}[x_1] = \theta \Rightarrow \mathbb{E}[\hat{\theta}_1] = \theta\] </li> <li> <p>For \(\hat{\theta}_2\), because of the linearity of expectation:</p> \[\mathbb{E}[\hat{\theta}_2] = \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n x_i\right] = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[x_i] = \frac{1}{n} \cdot n \cdot \theta = \theta\] </li> </ul> <p>Thus, <strong>both estimators are unbiased</strong> — their expected value equals the true mean \(\theta\).</p> <p>However, they differ in <strong>variance</strong>:</p> <ul> <li>\(\hat{\theta}_1\) uses only one data point, so its value can fluctuate greatly between different samples — it has <strong>high variance</strong>.</li> <li>\(\hat{\theta}_2\) averages over all \(n\) data points, which helps cancel out individual fluctuations — it has <strong>lower variance</strong>.</li> </ul> <p><strong>Key takeaway:</strong> Although both estimators are unbiased, the sample mean \(\hat{\theta}_2\) is <strong>more reliable</strong> due to its lower variance. This highlights the importance of considering both <strong>bias</strong> and <strong>variance</strong> when evaluating an estimator — a foundational idea for ensemble methods.</p> <h5 id="variance-of-a-mean"><strong>Variance of a Mean</strong></h5> <p>Suppose we have an unbiased estimator \(\hat{\theta}\) with variance \(\sigma^2\):</p> \[\mathbb{E}[\hat{\theta}] = \theta, \quad \text{Var}(\hat{\theta}) = \sigma^2\] <p>Now imagine we have \(n\) independent copies of this estimator — say, from different data samples or different random seeds — denoted by:</p> \[\hat{\theta}_1, \hat{\theta}_2, \dots, \hat{\theta}_n\] <p>We form a new estimator by averaging them:</p> \[\hat{\theta}_{\text{avg}} = \frac{1}{n} \sum_{i=1}^n \hat{\theta}_i\] <p>This average is still an unbiased estimator of \(\theta\), because the expectation of a sum is the sum of expectations:</p> \[\mathbb{E}[\hat{\theta}_{\text{avg}}] = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[\hat{\theta}_i] = \frac{1}{n} \cdot n \cdot \theta = \theta\] <p>But here’s the key insight: its <strong>variance is smaller</strong>. Since the \(\hat{\theta}_i\) are independent and each has variance \(\sigma^2\), we get:</p> \[\text{Var}(\hat{\theta}_{\text{avg}}) = \text{Var} \left( \frac{1}{n} \sum_{i=1}^n \hat{\theta}_i \right) = \frac{1}{n^2} \sum_{i=1}^n \text{Var}(\hat{\theta}_i) = \frac{n \cdot \sigma^2}{n^2} = \frac{\sigma^2}{n}\] <p>So, by averaging multiple estimators, we <strong>preserve unbiasedness</strong> while <strong>reducing variance</strong>.</p> <p>This simple statistical property is the backbone of many ensemble methods in machine learning — especially those like bagging and random forests, where we average multiple models to get a more stable and reliable prediction.</p> <hr/> <h5 id="averaging-independent-prediction-functions"><strong>Averaging Independent Prediction Functions</strong></h5> <p>Let’s now connect the earlier statistical insight to machine learning.</p> <p>Suppose we train \(B\) models <strong>independently</strong> on \(B\) different training sets, each drawn from the same underlying data distribution. This gives us a set of prediction functions:</p> \[\hat{f}_1(x),\ \hat{f}_2(x),\ \dots,\ \hat{f}_B(x)\] <p>We define their <strong>average prediction function</strong> as:</p> \[\hat{f}_{\text{avg}}(x) \overset{\text{def}}{=} \frac{1}{B} \sum_{b=1}^B \hat{f}_b(x)\] <p>At any specific input point \(x_0\):</p> <ul> <li>Each model’s prediction \(\hat{f}_b(x_0)\) is a random variable (since it depends on the randomly drawn training set), but they all share the same <strong>expected prediction</strong>.</li> <li> <p>The average prediction:</p> \[\hat{f}_{\text{avg}}(x_0) = \frac{1}{B} \sum_{b=1}^B \hat{f}_b(x_0)\] <p>has the <strong>same expected value</strong> as each individual \(\hat{f}_b(x_0)\), but with a <strong>reduced variance</strong>:</p> \[\text{Var}(\hat{f}_{\text{avg}}(x_0)) = \frac{1}{B} \cdot \text{Var}(\hat{f}_1(x_0))\] </li> </ul> <p>This means that by averaging multiple independent models, we can achieve a <strong>more stable and less noisy prediction</strong>, without increasing bias.</p> <p>However, here’s the challenge:</p> <blockquote> <p>In practice, we don’t have access to \(B\) truly independent training sets — we usually only have <strong>one dataset</strong>.</p> </blockquote> <h5 id="the-bootstrap-sample"><strong>The Bootstrap Sample</strong></h5> <p>So, how do we simulate multiple datasets when we only have <strong>one</strong>?</p> <p>The answer is: use <strong>bootstrap sampling</strong> — a clever statistical trick to mimic sampling variability.</p> <p>A <strong>bootstrap sample</strong> is formed by sampling <strong>with replacement</strong> from the original dataset:</p> \[D_n = (x_1, x_2, \dots, x_n)\] <p>We draw \(n\) points <em>with replacement</em> from \(D_n\), forming a new dataset (also of size \(n\)). Because sampling is done with replacement, some data points will appear multiple times, while others might not appear at all.</p> <p>What’s the chance a particular data point \(x_i\) is <strong>not</strong> selected in one draw?<br/> That’s \(1 - \frac{1}{n}\).</p> <p>The chance it’s not selected <strong>in any of the \(n\) draws</strong> is:</p> \[\left(1 - \frac{1}{n} \right)^n \approx \frac{1}{e} \approx 0.368\] <p>So, on average, <strong>about 36.8%</strong> of the data points are <strong>not</strong> included in a given bootstrap sample. This also means:</p> <blockquote> <p>Around <strong>63.2%</strong> of the original data points are expected to appear <strong>at least once</strong> in each bootstrap sample.</p> </blockquote> <h5 id="the-bootstrap-method"><strong>The Bootstrap Method</strong></h5> <p>Bootstrap gives us a way to <strong>simulate variability</strong> and generate multiple pseudo-datasets — without requiring any new data.</p> <p>Here’s how it works:</p> <ol> <li> <p>From the original dataset \(D_n\), generate \(B\) bootstrap samples:</p> \[D_n^1, D_n^2, \dots, D_n^B\] </li> <li> <p>Compute some function of interest — such as a statistic or a trained model — on each bootstrap sample:</p> \[\phi(D_n^1), \phi(D_n^2), \dots, \phi(D_n^B)\] </li> </ol> <p>These values behave almost like they were computed from <strong>\(B\) independent samples</strong> drawn from the original population distribution.</p> <p><strong>Why?</strong> Because each bootstrap sample is a randomized resampling of the original dataset — introducing enough variability to approximate the natural randomness we’d expect from drawing entirely new datasets from the true distribution.</p> <p>Although bootstrap samples are not truly independent, the statistical properties of estimators (like variance and confidence intervals) computed using bootstrapping tend to closely mirror those from actual independent samples.</p> <blockquote> <p>This makes bootstrap an incredibly useful tool for simulating sampling distributions, especially when acquiring more data is costly or impossible.</p> </blockquote> <h5 id="independent-samples-vs-bootstrap-samples"><strong>Independent Samples vs. Bootstrap Samples</strong></h5> <p>Let’s say we want to estimate a parameter \(\alpha\) using a point estimator:</p> \[\hat{\alpha} = \hat{\alpha}(D_{100})\] <p>Now, consider two scenarios:</p> <ul> <li><strong>Case 1:</strong> You collect <strong>1000 independent samples</strong>, each of size 100 (<strong>Left</strong>)</li> <li><strong>Case 2:</strong> You generate <strong>1000 bootstrap samples</strong> from a <strong>single dataset</strong> of size 100 (<strong>Right</strong>)</li> </ul> <p>If you compute \(\hat{\alpha}\) for each sample and plot the resulting histograms, you’ll notice something powerful:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-1-480.webp 480w,/assets/img/ensemble-1-800.webp 800w,/assets/img/ensemble-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ensemble-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <blockquote> <p>The distribution of estimates from bootstrap samples closely resembles that from truly independent samples.</p> </blockquote> <p>While not exact, the <strong>bootstrap approximation</strong> to the sampling distribution is often <strong>good enough</strong> for practical applications — especially when collecting new data is expensive or infeasible.</p> <hr/> <h5 id="ensemble-methods"><strong>Ensemble Methods</strong></h5> <p>This naturally leads us to the concept of <strong>ensemble learning</strong> — a powerful technique in modern machine learning.</p> <p><strong>Core idea:</strong> Combine multiple <strong>weak models</strong> into a <strong>strong, robust predictor</strong>.</p> <p>Why ensemble methods work:</p> <ul> <li>Averaging predictions from i.i.d. models <strong>reduces variance</strong> without increasing bias</li> <li>Bootstrap lets us <strong>simulate multiple training sets</strong>, even from just one dataset</li> </ul> <p>There are two primary flavors of ensemble methods:</p> <ul> <li> <p><strong>Parallel Ensembles</strong> (e.g., <strong>Bagging</strong>):<br/> Models are trained <strong>independently</strong> on different subsets of data</p> </li> <li> <p><strong>Sequential Ensembles</strong> (e.g., <strong>Boosting</strong>):<br/> Models are trained <strong>sequentially</strong>, with each new model <strong>focusing on the errors</strong> made by previous ones</p> </li> </ul> <p>By leveraging these strategies, ensemble methods often achieve <strong>greater accuracy</strong>, <strong>stability</strong>, and <strong>generalization</strong> than any single model could alone.</p> <hr/> <p>Stay tuned as we dive deeper into specific ensemble techniques — starting with <strong>Bagging</strong> and the incredibly popular <strong>Random Forests</strong> — and see how these ideas come to life in practice!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A beginner's guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.]]></summary></entry></feed>