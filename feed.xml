<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-22T15:05:13+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Understanding the Basics of Probability Theory for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/probability-1/" rel="alternate" type="text/html" title="Understanding the Basics of Probability Theory for Machine Learning"/><published>2024-12-22T11:55:00+00:00</published><updated>2024-12-22T11:55:00+00:00</updated><id>https://monishver11.github.io/blog/2024/probability-1</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/probability-1/"><![CDATA[<p>Probability theory forms the foundation of machine learning by enabling models to quantify uncertainty and make evidence-based predictions. This article delves into core probability concepts, providing explanations, examples, and analogies designed for clarity and practical sense.</p> <hr/> <h3 id="probability-definition-interpretation-and-basic-axioms"><strong>Probability: Definition, Interpretation, and Basic Axioms</strong></h3> <p>In simple terms, <strong>probability</strong> is a measure of the likelihood of an event occurring. It quantifies uncertainty by assigning a number between 0 and 1, where:</p> <ul> <li>A probability of <strong>0</strong> means the event is impossible.</li> <li>A probability of <strong>1</strong> means the event is certain.</li> <li>Values between 0 and 1 represent the likelihood of an event, with higher values indicating greater likelihood.</li> </ul> <p>Mathematically, probability is defined as a function:\(P: \mathcal{F} \to [0, 1]\) where \(\mathcal{F}\) is a collection of events, and \(P\) assigns a value to each event representing its likelihood.</p> <h4 id="interpretation-of-probability"><strong>Interpretation of Probability</strong></h4> <ol> <li><strong>Frequentist Interpretation</strong>: Probability is the long-run relative frequency of an event occurring after repeated trials. Example: In flipping a fair coin many times, the probability of getting heads is \(0.5\), as heads appear in roughly 50% of the flips.</li> <li><strong>Bayesian Interpretation</strong>: Probability reflects a degree of belief or confidence in an event occurring, updated as new evidence becomes available. Example: If the chance of rain tomorrow is initially assigned as \(0.7\), this reflects 70% confidence based on current information.</li> </ol> <h4 id="basic-axioms-of-probability"><strong>Basic Axioms of Probability</strong></h4> <p>The three fundamental axioms govern how probabilities are assigned:</p> <ol> <li><strong>Non-negativity</strong>: \(P(E) \geq 0 \quad \text{for all events } E\). Probabilities cannot be negative.</li> <li><strong>Normalization</strong>: \(P(S) = 1\). Here, \(S\) is the <strong>sample space</strong> (all possible outcomes). The probability of \(S\) is 1, as one outcome must occur. Example: For a die roll, \(S = \{1, 2, 3, 4, 5, 6\}\), and \(P(S) = 1\).</li> <li><strong>Additivity</strong>:</li> <li> \[P(E_1 \cup E_2) = P(E_1) + P(E_2) \quad \text{if } E_1 \text{ and } E_2 \text{ are mutually exclusive}\] <p>For mutually exclusive events \(E_1\) and \(E_2\), the probability of either occurring is the sum of their individual probabilities. <strong>Example</strong>: For a die roll, let \(E_1 = \{1\}\) and \(E_2 = \{2\}\):\(P(E_1 \cup E_2) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3}\)</p> </li> </ol> <h4 id="consequences-of-the-axioms"><strong>Consequences of the Axioms</strong></h4> <ol> <li><strong>Complementary Rule:</strong> \(P(E^c) = 1 - P(E)\). The probability of the complement of \(E\) (event not occurring) equals \(1\) minus \(P(E)\). Example: If \(P(\text{rain}) = 0.8\), then \(P(\text{no rain}) = 1 - 0.8 = 0.2\).</li> <li> <p><strong>Addition Rule for Non-Mutually Exclusive Events:</strong></p> \[P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)\] <p>For events that are not mutually exclusive, subtract the overlap probability. Example: Overlapping probabilities in surveys or data categorization.</p> </li> <li> <p><strong>Multiplication Rule for Independent Events:</strong></p> \[P(E_1 \cap E_2) = P(E_1) \cdot P(E_2) \quad \text{if } E_1 \text{ and } E_2 \text{ are independent}\] <p>Example: Probability of flipping two heads with two coins:\(P(\text{heads on coin 1} \cap \text{heads on coin 2}) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\)</p> </li> <li> <p><strong>Conditional Probability:</strong></p> \[P(E_1 \mid E_2) = \frac{P(E_1 \cap E_2)}{P(E_2)} \quad \text{if } P(E_2) &gt; 0\] <p>Conditional probability calculates \(E_1\)’s likelihood given \(E_2\) has occurred. Example: Used in models like <strong>Naive Bayes</strong> for predictions under given conditions.</p> </li> </ol> <h4 id="why-is-probability-important-in-machine-learning"><strong>Why is Probability Important in Machine Learning?</strong></h4> <p>Probability theory plays a critical role in machine learning by enabling:</p> <ol> <li><strong>Modeling Uncertainty</strong>: Essential in probabilistic models like Bayesian networks and Hidden Markov Models.</li> <li><strong>Decision Making</strong>: Used in reinforcement learning for action selection under uncertainty.</li> <li><strong>Risk Assessment and Confidence</strong>: Provides confidence intervals and helps quantify prediction risks.</li> <li><strong>Bayesian Inference</strong>: Updates beliefs about parameters or predictions using new data.</li> </ol> <hr/> <h3 id="random-variables-discrete-and-continuous"><strong>Random Variables: Discrete and Continuous</strong></h3> <p>Building upon the foundational axioms of probability, understanding <strong>random variables</strong> is the next critical step. Random variables bridge the gap between abstract probabilistic events and numerical representation, enabling a deeper connection between data and mathematical models.</p> <p>A <strong>random variable</strong> is a variable whose possible values are outcomes of a random process or experiment. It maps outcomes from a probabilistic event to real numbers, playing a central role in probability theory and machine learning. Random variables are typically categorized into two types: <strong>discrete</strong> and <strong>continuous</strong>.</p> <p><strong>Intuition for Random Variables</strong> Think of a random variable as a “number generator” that transforms outcomes of a random process into numbers. For instance:</p> <ul> <li>In a dice roll, the outcome (e.g., rolling a 4) is translated to the random variable \(X = 4\).</li> <li>In measuring rainfall, the amount (e.g., 12.5 mm) is assigned to the random variable \(X = 12.5\). This abstraction helps in applying mathematical operations and deriving distributions.</li> </ul> <h4 id="discrete-random-variables"><strong>Discrete Random Variables</strong></h4> <p>A <strong>discrete random variable</strong> takes on a countable number of distinct values. These values are often integers or counts, representing outcomes that are distinct and separate from one another. For example, the number of heads obtained when flipping a coin multiple times is a discrete random variable.</p> <p><strong>Characteristics</strong>:</p> <ul> <li><strong>Countable Outcomes</strong>: The possible values can be listed, even if the list is infinite (e.g., the number of calls received by a call center).</li> <li><strong>Probability Mass Function (PMF)</strong>: The probability distribution of a discrete random variable is described by a probability mass function (PMF), which assigns probabilities to each possible value. The PMF satisfies:</li> </ul> \[P(X = x) \geq 0 \quad \text{for all } x\] \[\sum_{x} P(X = x) = 1\] <p>Here, the sum is over all possible values \(x\) that the random variable can take.</p> <p><strong>Examples of Discrete Random Variables</strong>:</p> <ol> <li> <p><strong>Number of heads in coin flips</strong>: Let \(X\) be the number of heads in 3 flips of a fair coin. The possible values of \(X\) are \(0, 1, 2,\) and \(3\). The probabilities for each outcome can be computed using the binomial distribution.</p> </li> <li> <p><strong>Number of customers arriving at a store</strong>: Let \(X\) represent the number of customers who arrive at a store during a 1-hour period. If customers arrive independently with an average rate of 3 per hour, \(X\) could follow a <strong>Poisson distribution</strong>.</p> </li> </ol> <h4 id="continuous-random-variables"><strong>Continuous Random Variables</strong></h4> <p>A <strong>continuous random variable</strong> can take on an infinite number of possible values within a given range. These values are not countable but form a continuum. For example, the height of a person is a continuous random variable because it can take any value within a range, such as \(5.6\) feet, \(5.65\) feet, \(5.654\) feet, and so on.</p> <p><strong>Characteristics</strong>:</p> <ul> <li><strong>Uncountably Infinite Outcomes</strong>: The possible values form a continuum, often represented by intervals on the real number line.</li> <li><strong>Probability Density Function (PDF)</strong>: The probability distribution of a continuous random variable is described by a probability density function (PDF). Unlike a PMF, the PDF does not give the probability of any specific outcome but rather the probability of the random variable falling within a certain range. The total area under the PDF curve is equal to 1, and the probability of the variable falling in an interval \([a, b]\) is given by:</li> </ul> \[P(a \leq X \leq b) = \int_{a}^{b} f_X(x) \, dx\] <p>where \(f_X(x)\) is the PDF of \(X\).</p> <p><strong>Examples of Continuous Random Variables</strong>:</p> <ol> <li><strong>Height of a person</strong>: The height \(X\) of a person can take any value within a realistic range (e.g., between 4 and 7 feet). The exact value is not countable, and it is typically modeled by a normal distribution.</li> <li><strong>Time taken to complete a task</strong>: If you measure the time \(X\) taken by someone to complete a task, this can take any value (e.g., 2.5 minutes, 2.55 minutes, etc.). The distribution could be modeled using an exponential or normal distribution, depending on the scenario.</li> </ol> <h4 id="why-are-random-variables-important-in-machine-learning"><strong>Why are Random Variables Important in Machine Learning?</strong></h4> <ol> <li><strong>Modeling Uncertainty</strong>: In machine learning, random variables allow us to model uncertainty in data and predictions. For instance, in regression models, the target variable is often modeled as a random variable with some uncertainty, usually represented as a continuous distribution.</li> <li><strong>Bayesian Inference</strong>: In Bayesian models, parameters are treated as random variables, and their distributions are updated with new data. This allows for probabilistic reasoning and uncertainty quantification in predictions.</li> <li><strong>Stochastic Processes</strong>: Many machine learning algorithms, such as those in reinforcement learning or Monte Carlo simulations, involve <strong>stochastic processes</strong>, where future states or outcomes are modeled as random variables with given distributions.</li> </ol> <hr/> <h3 id="more-on-probability-distribution-and-types"><strong>More on Probability Distribution and Types</strong></h3> <p>A probability distribution describes how probabilities are assigned to different possible outcomes of a random variable. It provides a mathematical function that represents the likelihood of each possible value the random variable can take. In simpler terms, it tells us how likely each outcome of an experiment or process is.</p> <p>Formally, A <strong>probability distribution</strong> of a random variable \(X\) is a function that provides the probabilities of occurrence of different possible outcomes for the random variable. Depending on the nature of the random variable, the probability distribution can take different forms:</p> <ol> <li><strong>Discrete Probability Distribution</strong>: This applies when the random variable can only take on a finite or countably infinite number of values (e.g., the number of heads in a coin flip). The distribution is described by a <strong>probability mass function (PMF)</strong>.</li> <li><strong>Continuous Probability Distribution</strong>: This applies when the random variable can take on an infinite number of values within a range (e.g., the height of a person). The distribution is described by a <strong>probability density function (PDF)</strong>.</li> </ol> <p>For both types, the total probability across all possible outcomes must sum (or integrate) to 1:</p> <ul> <li> <p>For discrete distributions:</p> \[\sum_{x} P(X = x) = 1\] </li> <li> <p>For continuous distributions:</p> \[\int_{-\infty}^{\infty} f_X(x) \, dx = 1\] </li> </ul> <p>where \(f_X(x)\) is the probability density function.</p> <h5 id="1-discrete-probability-distributions"><strong>1. Discrete Probability Distributions</strong></h5> <p>Discrete distributions are used when the random variable can take a countable number of distinct values. Some common discrete probability distributions are:</p> <ul> <li><strong>Bernoulli Distribution</strong>: Models a binary outcome (success/failure, \(1/0\)) of a single trial. The probability of success is \(p\), and the probability of failure is \(1 - p\). The PMF is: \(P(X = 1) = p, \quad P(X = 0) = 1 - p\) <ul> <li><strong>Example</strong>: The outcome of a coin flip (Heads = 1, Tails = 0).</li> </ul> </li> <li> <p><strong>Binomial Distribution</strong>: Describes the number of successes in a fixed number of independent Bernoulli trials. The random variable \(X\) counts the number of successes. The PMF is:</p> \[P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}\] </li> </ul> <p>where \(n\) is the number of trials, \(p\) is the probability of success, and \(k\) is the number of successes. - <strong>Example</strong>: The number of heads in 10 coin flips.</p> <ul> <li> <p><strong>Poisson Distribution</strong>: Models the number of events occurring in a fixed interval of time or space, given a constant average rate of occurrence. The PMF is:</p> \[P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}\] </li> </ul> <p>where \(\lambda\) is the average rate of occurrence (mean), and \(k\) is the number of events. - <strong>Example</strong>: The number of phone calls received by a call center in an hour.</p> <h5 id="2-continuous-probability-distributions">2. <strong>Continuous Probability Distributions</strong></h5> <p>Continuous distributions are used when the random variable can take any value within a given range or interval. These distributions are described by probability density functions (PDFs), where the probability of any single point is zero, and probabilities are calculated over intervals. Some common continuous probability distributions include:</p> <ul> <li> <p><strong>Uniform Distribution</strong>: A continuous distribution where all values within a given interval are equally likely. The PDF is:</p> \[f_X(x) = \frac{1}{b-a} \quad \text{for} \ a \leq x \leq b\] <ul> <li><strong>Example</strong>: The time it takes for a bus to arrive, uniformly distributed between 5 and 15 minutes.</li> </ul> </li> <li> <p><strong>Normal (Gaussian) Distribution</strong>: A continuous distribution that is symmetric and bell-shaped. It is fully described by its mean \(\mu\) and standard deviation \(\sigma\). The PDF is:</p> \[f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\] <ul> <li><strong>Example</strong>: The distribution of heights in a population.</li> </ul> </li> <li><strong>Exponential Distribution</strong>: A continuous distribution often used to model the time between events in a Poisson process (events happening at a constant rate). The PDF is: \(f_X(x) = \lambda e^{-\lambda x} \quad \text{for} \ x \geq 0\) <ul> <li><strong>Example</strong>: The time between arrivals of customers at a service station.</li> </ul> </li> <li> <p><strong>Gamma Distribution</strong>: A generalization of the exponential distribution, used for modeling the sum of multiple exponentially distributed random variables. Its PDF is:</p> \[f_X(x) = \frac{x^{k-1} e^{-x/\theta}}{\Gamma(k) \theta^k} \quad \text{for} \ x \geq 0\] <ul> <li><strong>Example</strong>: The waiting time until a certain number of events occur in a Poisson process.</li> </ul> </li> <li> <p><strong>Beta Distribution</strong>: A continuous distribution on the interval \([0, 1]\), often used to model probabilities and proportions. Its PDF is:</p> \[f_X(x) = \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha, \beta)}\] <ul> <li><strong>Example</strong>: Modeling the proportion of customers who prefer a certain product in a market research study.</li> </ul> </li> </ul> <h4 id="why-are-probability-distributions-important-in-machine-learning"><strong>Why are Probability Distributions Important in Machine Learning?</strong></h4> <ol> <li><strong>Modeling Uncertainty</strong>: Many machine learning models assume that the data follows a certain probability distribution (e.g., Gaussian distribution in linear regression).</li> <li><strong>Inference and Prediction</strong>: In probabilistic models, such as Bayesian inference or Hidden Markov Models, understanding the probability distributions of variables allows for reasoning about uncertainty and making predictions based on observed data.</li> <li><strong>Risk Analysis</strong>: Distributions help quantify the risk or uncertainty in machine learning predictions. For example, a model’s output might be a probability distribution over potential outcomes, providing insights into the confidence of predictions.</li> </ol> <h3 id="cumulative-distribution-function-cdf"><strong>Cumulative Distribution Function (CDF)</strong></h3> <p>CDF is closely related to the <strong>Probability Density Function (PDF)</strong> in the case of continuous random variables and the <strong>Probability Mass Function (PMF)</strong> for discrete variables. The CDF gives the cumulative probability that a random variable takes a value less than or equal to a particular point.</p> <p>For a random variable \(X\), the <strong>Cumulative Distribution Function (CDF)</strong>, denoted by \(F_X(x)\), is defined as the probability that the random variable \(X\) takes a value less than or equal to \(x\). Formally:</p> \[F_X(x) = P(X \leq x)\] <p>The CDF is a function that provides the cumulative probability up to a point \(x\) and is computed by integrating (for continuous variables) or summing (for discrete variables) the corresponding probability distributions.</p> <h4 id="properties-of-the-cdf"><strong>Properties of the CDF</strong></h4> <ol> <li> <p><strong>Non-decreasing</strong>: The CDF is a non-decreasing function, meaning that the probability increases as \(x\) increases:</p> \[F_X(x_1) \leq F_X(x_2) \quad \text{if} \ x_1 \leq x_2\] </li> <li> <p><strong>Range</strong>: The CDF always lies within the range \([0, 1]\) : \(0 \leq F_X(x) \leq 1 \quad \text{for all} \ x\)</p> </li> <li><strong>Limits</strong>: <ul> <li>As \(x \to -\infty\), the CDF tends to 0: \(\lim_{x \to -\infty} F_X(x) = 0\)</li> <li>As \(x \to \infty\), the CDF tends to 1: \(\lim_{x \to \infty} F_X(x) = 1\)</li> </ul> </li> <li><strong>Continuity</strong>: <ul> <li>For <strong>continuous random variables</strong>, the CDF is continuous and smooth.</li> <li>For <strong>discrete random variables</strong>, the CDF is a step function, with jumps corresponding to the probabilities at specific points.</li> </ul> </li> </ol> <h4 id="cdf-for-discrete-random-variables"><strong>CDF for Discrete Random Variables</strong></h4> <p>For a <strong>discrete random variable</strong> \(X\), the CDF is computed by summing the probabilities given by the PMF. If the possible values of \(X\) are \(x_1, x_2, \dots\), the CDF is:</p> \[F_X(x) = P(X \leq x) = \sum_{x_i \leq x} P(X = x_i)\] <p><strong>Example</strong> Consider a discrete random variable \(X\) that represents the outcome of a fair 6-sided die. The possible values for \(X\) are \(1, 2, 3, 4, 5, 6\), each with a probability of \(\frac{1}{6}\). The CDF for \(X\) is:</p> \[F_X(x) = \begin{cases} 0 &amp; \text{for} \ x &lt; 1 \\ \frac{1}{6} &amp; \text{for} \ 1 \leq x &lt; 2 \\ \frac{2}{6} &amp; \text{for} \ 2 \leq x &lt; 3 \\ \frac{3}{6} &amp; \text{for} \ 3 \leq x &lt; 4 \\ \frac{4}{6} &amp; \text{for} \ 4 \leq x &lt; 5 \\ \frac{5}{6} &amp; \text{for} \ 5 \leq x &lt; 6 \\ 1 &amp; \text{for} \ x \geq 6 \end{cases}\] <h4 id="cdf-for-continuous-random-variables"><strong>CDF for Continuous Random Variables</strong></h4> <p>For a <strong>continuous random variable</strong> \(X\), the CDF is obtained by integrating the PDF:</p> \[F_X(x) = P(X \leq x) = \int_{-\infty}^{x} f_X(t) \, d t\] <p><strong>Example</strong> For a continuous random variable \(X\) that follows a <strong>uniform distribution</strong> on the interval \([0, 1]\), the PDF is:</p> \[f_X(x) = \begin{cases} 1 &amp; \text{for} \ 0 \leq x \leq 1 \\ 0 &amp; \text{otherwise} \end{cases}\] <p>The CDF is:</p> \[F_X(x) = \begin{cases} 0 &amp; \text{for} \ x &lt; 0 \\ x &amp; \text{for} \ 0 \leq x \leq 1 \\ 1 &amp; \text{for} \ x &gt; 1 \end{cases}\] <p>This shows that for values of \(x\) between 0 and 1, the probability increases linearly from 0 to 1.</p> <h4 id="relationship-between-pdf-and-cdf"><strong>Relationship Between PDF and CDF</strong></h4> <p>For a continuous random variable \(X\), the PDF is the derivative of the CDF:</p> \[f_X(x) = \frac{d}{dx} F_X(x)\] <p>Conversely, the CDF can be obtained by integrating the PDF:</p> \[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\] <h4 id="why-is-the-cdf-important-in-machine-learning"><strong>Why is the CDF Important in Machine Learning?</strong></h4> <ol> <li><strong>Data Interpretation</strong>: The CDF provides a clear interpretation of the distribution of data and is useful for understanding the likelihood of a random variable being less than or equal to a specific value.</li> <li><strong>Probabilistic Decision Making</strong>: The CDF helps integrate outcomes for decision-making in models like <strong>Naive Bayes</strong> or <strong>Bayesian networks</strong>.</li> </ol> <h4 id="how-pmf-pdf-and-cdf-interrelate"><strong>How PMF, PDF, and CDF Interrelate</strong></h4> <ul> <li><strong>Discrete Variables</strong>: <ul> <li>PMF: \(P(X = x_i)\)</li> <li>CDF:</li> </ul> \[F_X(x) = \sum_{x_i \leq x} P(X = x_i)\] </li> <li><strong>Continuous Variables</strong>: <ul> <li>PDF: \(f_X(x)\)</li> <li>CDF:</li> </ul> \[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\] <ul> <li>PDF from CDF:</li> </ul> \[f_X(x) = \frac{d}{dx} F_X(x)\] </li> </ul> <h3 id="choosing-the-right-distribution-in-ml"><strong>Choosing the Right Distribution in ML</strong></h3> <p>Choosing the appropriate probability distribution for a given machine learning (ML) problem is crucial to making accurate predictions. Each probability distribution captures a unique set of characteristics regarding the randomness and uncertainty in the data. A proper understanding of these distributions can directly influence the performance and efficiency of your model. Here’s a more detailed look at the various distributions commonly used in ML:</p> <ul> <li> <p><strong>Bernoulli/Binomial Distributions:</strong> These are useful for binary or count outcomes. The <strong>Bernoulli distribution</strong> models a single trial with two possible outcomes, often labeled as success (1) or failure (0). The <strong>Binomial distribution</strong> generalizes this by modeling the number of successes in a fixed number of independent Bernoulli trials. These distributions are especially useful in classification tasks, such as predicting whether an email is spam or not.</p> </li> <li> <p><strong>Gaussian (Normal) Distribution:</strong> The Gaussian or <strong>normal distribution</strong> is one of the most widely used distributions in statistics and machine learning, especially when the data exhibits natural variability. It is characterized by its symmetric bell-shaped curve, defined by its mean and standard deviation. It’s particularly useful when you have continuous data that tends to cluster around a central value, such as the distribution of heights in a population or the error terms in regression models. Many machine learning algorithms, such as <strong>linear regression</strong> and <strong>k-nearest neighbors (KNN)</strong>, assume that the underlying data follows a Gaussian distribution.</p> </li> <li> <p><strong>Poisson/Exponential Distributions:</strong> These distributions model events that occur over time or space, where the events happen at a constant average rate. The <strong>Poisson distribution</strong> models the number of events happening in a fixed interval of time or space (e.g., the number of customer arrivals at a service station). The <strong>Exponential distribution</strong> is often used to model the time between events in a Poisson process. Both distributions are important in scenarios involving queues or event-based systems, like predicting the time between customer purchases or server failures in network systems.</p> </li> </ul> <h4 id="why-does-choosing-the-right-distribution-matter">Why Does Choosing the Right Distribution Matter?</h4> <ol> <li><strong>Tailoring Models to Data:</strong> Understanding the underlying distribution of the data helps in selecting the right model for your problem. For example, if the data is normally distributed, using models like <strong>linear regression</strong> (which assumes normality of residuals) can result in more accurate predictions. On the other hand, when data is binary (e.g., yes/no outcomes), a <strong>logistic regression</strong> or <strong>Bernoulli distribution</strong> approach would be more appropriate.</li> <li><strong>Improving Model Efficiency:</strong> When we align the assumptions of a machine learning algorithm with the real-world distribution of the data, models tend to be more efficient and require less computation. For instance, algorithms that work with Gaussian-distributed data can be optimized to take advantage of the symmetry of the distribution, leading to faster convergence in training.</li> <li><strong>Quantifying Uncertainty:</strong> Different distributions provide unique ways to handle uncertainty in predictions. For example, when working with <strong>Poisson distributions</strong>, we can predict the expected number of events in a fixed period with a known variance. In contrast, the <strong>Exponential distribution</strong> models waiting times, making it suitable for applications like survival analysis or reliability engineering.</li> </ol> <h4 id="further-exploration"><strong>Further Exploration</strong></h4> <p>While this overview has touched on the most commonly used distributions, the world of probability distributions in machine learning is vast. As you dive deeper into various ML topics, you’ll encounter additional distributions tailored to specific data types and problems. Understanding how these distributions behave allows you to refine your models for more accurate, effective predictions.</p> <p>If you’re eager to explore further, we will be diving deeper into these distributions as we continue our series on probability theory. The next section will introduce even more important concepts, so stay tuned for that! <strong>See you in the next post!</strong></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog explores essential probability concepts and their significance in machine learning.]]></summary></entry><entry><title type="html">Multivariate Calculus - Prerequisites for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/multivariate-calculus/" rel="alternate" type="text/html" title="Multivariate Calculus - Prerequisites for Machine Learning"/><published>2024-12-20T00:10:00+00:00</published><updated>2024-12-20T00:10:00+00:00</updated><id>https://monishver11.github.io/blog/2024/multivariate-calculus</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/multivariate-calculus/"><![CDATA[<p>Before we dive into the math-heavy world of machine learning, it’s crucial to build a solid foundation in multivariate calculus. This blog post covers the essentials you’ll need to understand and work with the mathematical concepts underpinning ML.</p> <p>If you’re already familiar with undergraduate-level topics like functions and vectors, this should feel like a natural progression. If not, I recommend revisiting those fundamentals first—they’ll make this much easier to grasp. While this list isn’t exhaustive, it’s sufficient to get started and serves as a reference as we build on these ideas.</p> <p>Let’s dive in!</p> <hr/> <h3 id="what-is-calculus"><strong>What is Calculus?</strong></h3> <p>Calculus is the study of continuous change, centered on two main concepts:</p> <ol> <li><strong>Differentiation</strong>: The process of finding rates of change or slopes of curves. It helps analyze how quantities change over time or in response to other variables.</li> <li><strong>Integration</strong>: The reverse of differentiation, used to calculate areas under curves and accumulate quantities.</li> </ol> <p>These tools are applied in fields like physics, engineering, economics, and computer science to solve real-world problems involving continuous change.</p> <hr/> <h3 id="what-is-multivariate-calculus"><strong>What is Multivariate Calculus?</strong></h3> <p>Multivariate calculus, or multivariable calculus, extends single-variable calculus to functions with multiple variables. Key components include:</p> <ul> <li><strong>Partial Derivatives</strong>: Measure how a function changes with respect to one variable while keeping others constant.</li> <li><strong>Multiple Integration</strong>: Used to calculate volumes and higher-dimensional quantities for functions of multiple variables.</li> <li><strong>Vector Calculus</strong>: Explores vector fields, gradients, and theorems like Green’s and Stokes’ theorems.</li> </ul> <h4 id="why-multivariate-calculus-matters"><strong>Why Multivariate Calculus Matters</strong></h4> <p>Multivariate calculus is indispensable for:</p> <ul> <li>Analyzing systems with multiple inputs.</li> <li>Solving optimization problems in higher dimensions.</li> <li>Modeling physical phenomena in 3D or more dimensions.</li> <li>Understanding advanced concepts in physics, engineering, and data science.</li> </ul> <p>By extending the tools of calculus to multidimensional problems, multivariate calculus equips us to tackle complex systems and optimize machine learning models.</p> <hr/> <h3 id="derivatives-a-fundamental-concept"><strong>Derivatives: A Fundamental Concept</strong></h3> <p>Derivatives are the backbone of calculus, measuring how a function’s output changes in response to its input. They play a critical role in understanding rates of change and optimizing functions.</p> <p>Here are some analogies to grasp the concept of derivatives:</p> <ol> <li><strong>The Speedometer Analogy</strong>:<br/> Imagine driving a car—the speedometer shows your instantaneous speed at any given moment. Similarly, a derivative tells you how fast a function is changing at a specific point.</li> <li><strong>Slope of a Tangent Line</strong>:<br/> Visually, the derivative represents the slope of the tangent line at a point on a graph. Think of it as placing a ruler on a curve to measure its tilt.</li> <li><strong>Sensitivity Measure</strong>:<br/> A derivative reveals how sensitive a function’s output is to small changes in its input. For example, it’s like determining how much the water level in a bathtub rises when you add a cup of water.</li> </ol> <h3 id="partial-derivatives-and-their-role-in-ml"><strong>Partial Derivatives and Their Role in ML</strong></h3> <p>Expanding on the concept of derivatives, <strong>partial derivatives</strong> allow us to analyze how functions of multiple variables change with respect to a single variable while keeping others constant. This is a cornerstone of multivariate calculus and a fundamental tool in machine learning.</p> <h4 id="what-are-partial-derivatives"><strong>What are Partial Derivatives?</strong></h4> <p>A partial derivative extends the idea of a derivative to multivariable functions. It isolates the effect of one variable while treating the others as fixed.</p> <p>Here are a couple of analogies to help visualize partial derivatives:</p> <ol> <li><strong>Mountain Climbing</strong>:<br/> Imagine standing on a mountain. A partial derivative measures the steepness in one specific direction (e.g., north-south or east-west), while you stay fixed in the same spot.</li> <li><strong>Baking a Cake</strong>:<br/> Consider a recipe where the outcome (taste) depends on multiple ingredients (variables). A partial derivative tells you how the taste changes when you adjust one ingredient (e.g., sugar) while keeping all others constant.</li> </ol> <h4 id="applications-in-machine-learning"><strong>Applications in Machine Learning</strong></h4> <p>Partial derivatives are indispensable in machine learning, especially when working with models that involve multiple parameters.</p> <ol> <li><strong>Gradient Descent</strong>: <ul> <li>The <strong>gradient vector</strong>, composed of partial derivatives, points in the direction of the steepest increase of a function.</li> <li>Gradient descent uses this information to move in the opposite direction, minimizing the loss function and optimizing model parameters.</li> </ul> </li> <li><strong>Back-propagation in Neural Networks</strong>: <ul> <li>Partial derivatives calculate how each weight in a neural network contributes to the overall error, enabling efficient training through backpropagation.</li> </ul> </li> <li><strong>Optimization of Complex Loss Functions</strong>: <ul> <li>In high-dimensional parameter spaces, partial derivatives help navigate the loss landscape to find optimal solutions.</li> </ul> </li> <li><strong>Sensitivity Analysis</strong>: <ul> <li>They provide insights into how sensitive a model’s output is to changes in individual input variables, aiding interpretability and robustness.</li> </ul> </li> <li><strong>Second-Order Optimization</strong>: <ul> <li>Techniques like Newton’s method use the <strong>Hessian matrix</strong> (second-order partial derivatives) to achieve faster convergence.</li> </ul> </li> </ol> <p>By employing partial derivatives, machine learning algorithms can effectively optimize performance across multiple parameters and gain insights into intricate relationships within the data.</p> <hr/> <h3 id="gradient-vectors-a-multivariable-power-tool"><strong>Gradient Vectors: A Multivariable Power Tool</strong></h3> <p>The <strong>gradient vector</strong> combines partial derivatives to form a powerful tool for analyzing multivariable functions. It indicates both the direction and magnitude of the steepest ascent at any given point.</p> <h4 id="what-is-a-gradient-vector"><strong>What is a Gradient Vector?</strong></h4> <p>The gradient vector generalizes the derivative to functions with multiple variables, providing a way to “sense” the terrain of the function.</p> <p>Here are some analogies to conceptualize gradient vectors:</p> <ol> <li><strong>Mountain Climber’s Compass</strong>:<br/> Imagine you’re on a mountain, and you have a compass that always points uphill in the steepest direction. That’s the gradient vector—it guides you to the quickest ascent.</li> <li><strong>Water Flow on a Surface</strong>:<br/> Think of water droplets on a curved surface. The gradient vector at any point shows the direction water would flow—the steepest descent.</li> <li><strong>Heat-Seeking Missile</strong>:<br/> The gradient works like a heat-seeking missile’s guidance system, constantly recalibrating to move toward the function’s maximum.</li> </ol> <h4 id="applications-in-machine-learning-1"><strong>Applications in Machine Learning</strong></h4> <p>Gradient vectors are pivotal in optimization and training algorithms:</p> <ol> <li><strong>Gradient Descent</strong>: <ul> <li>The gradient vector points toward the steepest ascent, so moving in the opposite direction minimizes the loss function.</li> </ul> </li> <li><strong>Adaptive Learning Rate Methods</strong>: <ul> <li>Advanced algorithms like AdaGrad and Adam utilize the gradient vector to adjust learning rates dynamically for each parameter.</li> </ul> </li> <li><strong>Local Linear Approximation</strong>: <ul> <li>The gradient provides a local linear estimate of the function, helping algorithms make informed adjustments to parameters.</li> </ul> </li> </ol> <p>By leveraging the gradient vector, machine learning algorithms efficiently navigate complex parameter spaces, optimize models, and adapt to data characteristics. The gradient not only informs about the loss landscape but also helps shape strategies to improve performance.</p> <p>You might be wondering why the gradient points in the direction of steepest ascent—see the references below for more details.</p> <hr/> <h3 id="hessian-and-jacobian-higher-order-tools"><strong>Hessian and Jacobian: Higher-Order Tools</strong></h3> <p>Building on partial derivatives and gradient vectors, <strong>Hessian</strong> and <strong>Jacobian matrices</strong> offer even deeper insights into multivariable functions. These higher-order constructs are essential for advanced optimization techniques and will be explored in detail next down.</p> <h4 id="jacobian-matrix"><strong>Jacobian Matrix</strong></h4> <p>The Jacobian matrix generalizes the gradient vector for vector-valued functions, capturing how changes in multiple inputs affect multiple outputs.</p> <h5 id="definition"><strong>Definition</strong></h5> \[\text{For a function } \mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m, \text{ the Jacobian matrix } \mathbf{J} \text{ is an } m \times n \text{ matrix defined as:}\] \[\mathbf{J} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n} \end{bmatrix}\] \[\text{The entries of the Jacobian matrix are partial derivatives of the component functions } f_i\] \[\text{ with respect to the input variables } x_j.\] <h4 id="hessian-matrix"><strong>Hessian Matrix</strong></h4> <p>The Hessian matrix contains all second-order partial derivatives of a scalar-valued function. It provides information about the curvature of the function, making it essential for understanding the landscape of optimization problems.</p> <h5 id="definition-1"><strong>Definition</strong></h5> \[\text{For a function } f: \mathbb{R}^n \to \mathbb{R}, \text{ the Hessian matrix } \mathbf{H} \text{ is an } n \times n \text{ symmetric matrix defined as:}\] \[\mathbf{H} = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}\] \[\text{The Hessian provides information about the curvature of the function in all directions.}\] <p>Both the Jacobian and Hessian matrices extend the concept of partial derivatives to offer deeper insights into multivariable functions. In machine learning, these tools enhance the ability to analyze and optimize models by providing detailed information about variable interdependencies and the curvature of the function’s landscape. The Jacobian is particularly useful for understanding transformations and sensitivities in vector-valued functions, while the Hessian aids in second-order optimization by characterizing curvature and guiding efficient convergence. Together, they enable sophisticated analysis and optimization techniques, making it possible to tackle the complexities of high-dimensional spaces typical in machine learning tasks.</p> <hr/> <h3 id="taylor-series"><strong>Taylor Series</strong></h3> <p>The Taylor series is a powerful tool that approximates complex functions using simpler polynomial expressions. This approximation is widely used in optimization and machine learning.</p> <h4 id="definition-2"><strong>Definition</strong></h4> \[\text{For a function } f(x) \text{ that is infinitely differentiable at a point } a,\] \[\text{ the Taylor series is given by:}\] \[f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \cdots\] \[\text{And more generally the Taylor series is given by:}\] \[f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x - a)^n\] <p>Quick analogies to idealize it:</p> <ol> <li> <p><strong>Function Microscope</strong>:<br/> The Taylor series “zooms in” on a specific point of a function, describing its local behavior with increasing precision.</p> </li> <li> <p><strong>Prediction Machine</strong>:<br/> Think of it as a step-by-step refinement of a guess about the function’s behavior. It starts with a constant, then linear, quadratic, cubic, and so on, improving accuracy at each step.</p> </li> </ol> <h4 id="applications-in-machine-learning-2"><strong>Applications in Machine Learning</strong></h4> <ol> <li><strong>Function Approximation</strong>:<br/> Simplifies complex functions into polynomials that are computationally easier to work with.</li> <li><strong>Optimization Algorithms</strong>:<br/> Techniques like Newton’s method use Taylor approximations to estimate minima or maxima.</li> <li><strong>Gradient Estimation</strong>:<br/> When direct computation of gradients is challenging, Taylor series can approximate them.</li> </ol> <p>The Taylor series provides a detailed local understanding of functions, facilitating optimization, gradient estimation, and model interpretation. This makes it a valuable tool for bridging the gap between continuous mathematical phenomena and practical computational algorithms, playing a crucial role in solving complex problems and enhancing the efficiency of machine learning workflows.</p> <p>This foundational knowledge from multivariate calculus sets the stage for deeper exploration. If you’ve made it this far, congratulations! You’ve taken an important step in understanding and internalizing key concepts in machine learning. Take a moment to reflect, and when you’re ready, let’s move forward to the next topic. See you there!</p> <h3 id="references"><strong>References</strong></h3> <ul> <li><a href="https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent" target="_blank">Why is gradient the direction of steepest ascent?</a></li> <li><a href="https://betterexplained.com/articles/calculus-building-intuition-for-the-derivative/" target="_blank">Calculus: Building Intuition for the Derivative – BetterExplained</a></li> <li><a href="https://photomath.com/articles/what-is-calculus-definition-applications-and-concepts/" target="_blank">What is Calculus? Definition, Applications, and Concepts – Photomath</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.]]></summary></entry><entry><title type="html">Linear Algebra - Prerequisites for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/linear-algebra/" rel="alternate" type="text/html" title="Linear Algebra - Prerequisites for Machine Learning"/><published>2024-12-20T00:10:00+00:00</published><updated>2024-12-20T00:10:00+00:00</updated><id>https://monishver11.github.io/blog/2024/linear-algebra</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/linear-algebra/"><![CDATA[<p>Linear algebra forms the backbone of modern machine learning. As a branch of mathematics, it deals with vector spaces and the linear transformations between them. This area of study allows for the manipulation and efficient computation of datasets, making it fundamental to various machine learning algorithms. Whether you’re working with deep learning, regression models, or optimization techniques, a solid understanding of linear algebra will be crucial to mastering machine learning.</p> <hr/> <h2 id="core-components-of-linear-algebra"><strong>Core Components of Linear Algebra</strong></h2> <h3 id="vectors-and-matrices"><strong>Vectors and Matrices</strong></h3> <h4 id="1-vectors"><strong>1. Vectors</strong></h4> <p>A <strong>vector</strong> is a fundamental concept in linear algebra and is essentially a one-dimensional array of numbers. In machine learning, vectors can represent different elements, including features, weights, or data points.</p> <ul> <li><strong>Definition:</strong> A vector is a set of numbers arranged in a specific order, and it can be represented either as a <strong>row vector</strong> or a <strong>column vector</strong>. <ul> <li>Row vector: \(\mathbf{v} = [v_1, v_2, \dots, v_n]\)</li> <li>Column vector: \(\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n\end{bmatrix}\)</li> </ul> </li> <li> <p><strong>Properties:</strong></p> <ul> <li><strong>Magnitude (Norm):</strong> The magnitude of a vector, often referred to as its norm, measures the vector’s length. The most common norms used are the L2 norm (Euclidean norm) and L1 norm.</li> </ul> \[\|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2} \ \ ; \quad \|\mathbf{v}\|_1 = \sum_{i=1}^{n} |v_i|\] <ul> <li><strong>Dot Product:</strong> The dot product of two vectors measures their similarity. The dot product between two vectors \(\mathbf{v}_1​\) and \(\mathbf{v}_2\)​ is computed as:</li> </ul> \[\mathbf{v}_1 \cdot \mathbf{v}_2 = \sum_{i=1}^{n} v_{1i} v_{2i}\] <ul> <li><strong>Distance:</strong> The Euclidean distance is a common way to measure the difference between two vectors:</li> </ul> \[d(\mathbf{v}_1, \mathbf{v}_2) = \sqrt{\sum_{i=1}^{n} (v_{1i} - v_{2i})^2}\] </li> <li><strong>Operations on Vectors:</strong> <ul> <li><strong>Addition:</strong> Vectors of the same size can be added element-wise.</li> <li><strong>Scalar Multiplication:</strong> Multiplying each element of a vector by a scalar.</li> <li><strong>Dot Product:</strong> A fundamental operation for determining the similarity between two vectors.</li> </ul> </li> </ul> <h4 id="2-matrices"><strong>2. Matrices</strong></h4> <p>A <strong>matrix</strong> is a two-dimensional array of numbers, and it is widely used in machine learning for data storage, transformations, and solving systems of equations.</p> <ul> <li> <p><strong>Definition:</strong> A matrix consists of rows and columns and is denoted as \(A\), where \(A_{ij}\)​ represents the element in the i-th row and j-th column.</p> \[A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn} \end{bmatrix}\] </li> <li><strong>Properties of Matrices:</strong> <ul> <li><strong>Rank:</strong> The rank of a matrix is the maximum number of linearly independent rows or columns, indicating the number of independent dimensions in the matrix.</li> <li><strong>Trace:</strong> The trace is the sum of the diagonal elements of a square matrix. It is often involved in optimization problems.</li> <li><strong>Determinant:</strong> The determinant helps in determining whether a matrix is invertible. A non-zero determinant implies that the matrix is invertible.</li> <li><strong>Invertibility:</strong> A matrix \(A\) is invertible if it has full rank and a non-zero determinant. The inverse of a matrix \(A\) is denoted by \(A^{-1}\), and it satisfies the equation: \(A A^{-1} = I\) where \(I\) is the identity matrix.</li> </ul> </li> <li><strong>Operations on Matrices:</strong> <ul> <li><strong>Matrix Addition/Subtraction:</strong> Matrices of the same dimension can be added or subtracted element-wise.</li> <li><strong>Matrix Multiplication:</strong> Matrix multiplication is the dot product of rows and columns between two matrices. This operation is central to machine learning algorithms.</li> <li><strong>Transpose:</strong> The transpose of a matrix \(A\) is denoted as \(A^T\) and involves flipping its rows and columns.</li> <li><strong>Inverse:</strong> If a matrix is invertible, its inverse can be used to solve systems of linear equations.</li> </ul> </li> </ul> <h3 id="vectors-and-matrices-in-ml"><strong>Vectors and Matrices in ML</strong></h3> <p>Vectors and matrices play a pivotal role in representing both data and models in machine learning.</p> <p><strong>1. Data Representation</strong></p> <ul> <li>In supervised learning, each data point is typically represented as a feature vector. For example, if a dataset has \(m\) samples and \(n\) features, it can be represented as an \(m \times n\) matrix, where each row corresponds to a feature vector for a data point. The corresponding labels or target values are often stored in a vector.</li> </ul> <p><strong>2. Model Representation</strong></p> <ul> <li>In models like linear regression and neural networks, the weights that transform input data are stored in vectors or matrices. For example, in linear regression, the model is defined as: \(\hat{y} = Xw + b\) where \(X\) is the data matrix, \(w\) is the weight vector, and \(b\) is the bias term.</li> </ul> <p><strong>3. Operations in Machine Learning Algorithms</strong></p> <ul> <li><strong>Linear Regression:</strong> In linear regression, matrix operations are used to solve for the optimal weights. The normal equation for linear regression is:</li> </ul> \[w=(X^TX)^{−1}X^Ty\] <p>where \(X\) is the matrix of input features and \(y\) is the vector of target values.</p> <ul> <li><strong>Neural Networks:</strong> Each layer of a neural network applies a linear transformation to its input, which is represented by matrix multiplication:</li> </ul> \[y=XW+b\] <p>where \(X\) is the input matrix, \(W\) is the weight matrix, and \(b\) is the bias vector.</p> <ul> <li><strong>Gradient Descent:</strong> The gradient descent optimization algorithm frequently uses vector and matrix operations to update model parameters iteratively. In deep learning, the gradient of the loss function with respect to the weights and biases is calculated using matrix operations during back-propagation.</li> </ul> <p><strong>4. Dimensionality Reduction</strong></p> <ul> <li>Principal Component Analysis (PCA) is a popular technique for dimensionality reduction. It involves finding the eigenvectors and eigenvalues of the covariance matrix of the data. Eigenvalues and Eigenvectors are explained down below.</li> </ul> <hr/> <h3 id="eigenvalues-and-eigenvectors"><strong>Eigenvalues and Eigenvectors</strong></h3> <h5 id="definition"><strong>Definition:</strong></h5> <ul> <li> <p><strong>Eigenvector:</strong><br/> An eigenvector of a square matrix \(\mathbf{A}\) is a non-zero vector \(\mathbf{v}\) that, when the matrix \(\mathbf{A}\) is applied to it, only scales the vector without changing its direction:</p> \[\mathbf{A} \mathbf{v} = \lambda \mathbf{v}\] <p>where:</p> <ul> <li>\(\mathbf{v}\) is the eigenvector,</li> <li>\(\lambda\) is the eigenvalue, the scalar that represents how much the eigenvector is scaled by the transformation.</li> </ul> </li> <li> <p><strong>Eigenvalue:</strong><br/> The eigenvalue \(\lambda\) is the factor by which the eigenvector is scaled when the matrix \(\mathbf{A}\) acts on it.</p> </li> </ul> <p><strong>To build intuition, consider this analogy:</strong></p> <p>Imagine a squishy sheet of rubber (the matrix) and a point in space (the vector). If you apply a transformation (like stretching, rotating, or shearing) to the point using the rubber sheet, most points move to new locations. However, some special points, called <strong>eigenvectors</strong>, only get <strong>stretched</strong> or <strong>compressed</strong> but <strong>stay in the same direction</strong>. The amount of stretching or compression is determined by the <strong>eigenvalue</strong>.</p> <p><strong>Mathematical Properties of Eigenvalues and Eigenvectors</strong></p> <ul> <li>Eigenvectors must be <strong>non-zero vectors</strong>.</li> <li>Eigenvalues can be <strong>real</strong> or <strong>complex</strong> (but are often real in machine learning applications).</li> <li>A matrix can have multiple eigenvectors corresponding to the <strong>same eigenvalue</strong> (if it is <strong>degenerate</strong>) or distinct eigenvalues corresponding to distinct eigenvectors.</li> </ul> <h4 id="why-are-eigenvalues-and-eigenvectors-important-in-machine-learning"><strong>Why Are Eigenvalues and Eigenvectors Important in Machine Learning?</strong></h4> <p><strong>PCA</strong> is a widely used technique for <strong>dimensionality reduction</strong> in machine learning. It reduces the number of features while retaining the most important information in the dataset.</p> <ul> <li><strong>Covariance Matrix:</strong> PCA begins by computing the covariance matrix to capture relationships between features.</li> <li><strong>Eigenvectors of Covariance Matrix:</strong> The eigenvectors represent the directions of maximum variance in the data—these are the <strong>principal components</strong>.</li> <li><strong>Eigenvalues:</strong> The corresponding eigenvalues indicate the magnitude of variance in each direction.</li> </ul> <p>Key steps in PCA:</p> <ul> <li>Sort eigenvectors in decreasing order of their eigenvalues.</li> <li>Select the top <strong>k</strong> eigenvectors to reduce dimensionality while preserving most of the variance.</li> </ul> <h4 id="key-takeaways-of-eigenvalues-and-eigenvectors-in-ml"><strong>Key takeaways of Eigenvalues and Eigenvectors in ML</strong></h4> <ul> <li><strong>Diagonalizability:</strong><br/> A matrix is diagonalizable if it has enough eigenvectors to form a full basis. This property is essential in PCA and <strong>Singular Value Decomposition (SVD)</strong>, enabling efficient computation and interpretation.</li> <li><strong>Magnitude of Eigenvalues:</strong><br/> The magnitude of eigenvalues corresponds to the <strong>variance captured</strong> by the associated eigenvectors (principal components). Larger eigenvalues imply more variance explained.</li> <li><strong>Orthogonality of Eigenvectors (Symmetric Matrices):</strong><br/> For <strong>symmetric matrices</strong>, eigenvectors are <strong>orthogonal</strong>. This is critical in PCA, where the principal components are orthogonal, ensuring that reduced dimensions remain <strong>uncorrelated</strong>.</li> </ul> <hr/> <h3 id="a-few-more-key-matrices-types-relevant-to-ml"><strong>A Few More Key Matrices Types Relevant to ML</strong></h3> <h4 id="symmetric-matrix"><strong>Symmetric Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix \(A\) is <strong>symmetric</strong> if \(A = A^T\), meaning it is equal to its transpose.</li> <li><strong>Properties:</strong> <ul> <li>A symmetric matrix always has real eigenvalues and orthogonal eigenvectors.</li> <li>If \(A\) is symmetric, it is always diagonalizable.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Covariance Matrices:</strong> Covariance matrices are always symmetric because the covariance between two features is the same regardless of the order.</li> <li><strong>Optimization Problems:</strong> Many optimization problems in machine learning involve symmetric matrices (e.g., in second-order optimization methods like Newton’s method or in regularization).</li> </ul> </li> </ul> <h4 id="orthogonal-matrix"><strong>Orthogonal Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix \(A\) is <strong>orthogonal</strong> if \(A^T A = I\), where \(I\) is the identity matrix.</li> <li><strong>Properties:</strong> <ul> <li>The rows and columns of an orthogonal matrix are orthonormal (i.e., they are both orthogonal and of unit length).</li> <li>The inverse of an orthogonal matrix is equal to its transpose \((A^{-1} = A^T)\).</li> <li>The determinant of an orthogonal matrix is either +1 or -1.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Rotation and Transformation:</strong> Orthogonal matrices are used in certain machine learning algorithms for transformations that preserve distances and angles. For example, in PCA, orthogonal transformation is used to create new orthogonal basis vectors.</li> </ul> </li> </ul> <h4 id="positive-definite-matrix-pd"><strong>Positive Definite Matrix (PD):</strong></h4> <ul> <li><strong>Definition:</strong> A square matrix \(A\) is <strong>positive definite</strong> if for any non-zero vector \(\mathbf{v}\), \(\mathbf{v}^T A \mathbf{v} &gt; 0\). In simpler terms, it means that the matrix has strictly positive eigenvalues.</li> <li><strong>Properties:</strong> <ul> <li>All eigenvalues are positive.</li> <li>The matrix is invertible (non-singular).</li> <li>It implies that the quadratic form is always positive.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Optimization Problems:</strong> In convex optimization, the Hessian matrix of a convex function is often positive definite. This ensures that a function has a unique local minimum, making optimization well-posed.</li> <li><strong>Covariance Matrices:</strong> The covariance matrix of any dataset with multiple features is positive semi-definite. In special cases (e.g., full rank), it can be positive definite.</li> </ul> </li> </ul> <h4 id="positive-semi-definite-matrix-psd"><strong>Positive Semi-Definite Matrix (PSD):</strong></h4> <ul> <li><strong>Definition:</strong> A matrix \(A\) is <strong>positive semi-definite</strong> if for any vector \(\mathbf{v}\), \(\mathbf{v}^T A \mathbf{v} \geq 0\). In other words, all eigenvalues are non-negative (i.e., zero or positive).</li> <li><strong>Properties:</strong> <ul> <li>Eigenvalues are non-negative \((\lambda_i \geq 0)\).</li> <li>The matrix may not be invertible if it has zero eigenvalues.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Covariance Matrices:</strong> As mentioned, the covariance matrix of a dataset is positive semi-definite. This is essential because covariance cannot be negative and the matrix represents the relationship between features.</li> <li><strong>Kernel Matrices (in SVMs, Gaussian Processes, etc.):</strong> The kernel matrix in algorithms like SVM and kernel PCA is always positive semi-definite. It measures similarity between data points in a transformed feature space.</li> </ul> </li> </ul> <h4 id="covariance-matrix"><strong>Covariance Matrix:</strong></h4> <ul> <li> <p><strong>Definition:</strong> A <strong>covariance matrix</strong> is a square matrix that contains the covariances between pairs of features in a dataset. If a dataset has \(n\) features, the covariance matrix will be an \(n \times n\) matrix, where each entry represents the covariance between two features.</p> </li> <li> <p><strong>Covariance of two variables X and Y:</strong></p> \[\text{Cov}(X, Y) = \frac{1}{m} \sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})\] <p>where \(m\) is the number of data points, and \(\bar{x}\) and \(\bar{y}\)​ are the means of the features \(X\) and \(Y\), respectively.</p> </li> <li> <p><strong>Covariance Matrix Definition:</strong> For a dataset with \(n\) features, the covariance matrix \(\Sigma\) is an \(n \times n\) matrix where each entry is:</p> \[\Sigma_{ij} = \text{Cov}(X_i, X_j)\] <p>where \(X_i\)​ and \(X_j\)​ are the \(i\)-th and \(j\)-th features, respectively.</p> </li> <li><strong>Properties:</strong> <ul> <li><strong>Symmetry:</strong> The covariance matrix is always symmetric, i.e., \(\Sigma_{ij} = \Sigma_{ji}\)</li> <li><strong>Positive Semi-Definiteness (PSD):</strong> The covariance matrix is always positive semi-definite, meaning for any vector \(\mathbf{v}\), \(\mathbf{v}^T \Sigma \mathbf{v} \geq 0\).</li> <li><strong>Diagonal Entries (Variance):</strong> The diagonal entries represent the variance of individual features.</li> <li><strong>Off-Diagonal Entries (Covariance):</strong> The off-diagonal entries represent the covariance between different features. Positive covariance indicates that the features increase or decrease together, while negative covariance suggests they move inversely.</li> <li><strong>Eigenvalues and Eigenvectors:</strong> The eigenvectors of the covariance matrix represent the directions of maximum variance, while the eigenvalues represent the magnitude of variance along these directions.</li> <li><strong>Rank:</strong> The rank of the covariance matrix corresponds to the number of <strong>independent</strong> features. If the matrix is rank-deficient, it indicates linearly dependent features.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>PCA:</strong> In PCA, the covariance matrix is used to identify the directions (eigenvectors) of maximum variance in the dataset. Eigenvalues indicate how much variance is explained by each principal component. This helps in dimensionality reduction by selecting the most important components.</li> <li><strong>Multivariate Gaussian Distribution:</strong> In probabilistic models like <strong>Gaussian Mixture Models (GMM)</strong>, the covariance matrix defines the shape of the data distribution. It is used to model the distribution of features in a multi-dimensional space.</li> <li><strong>Feature Selection:</strong> Covariance matrices help identify correlated features. Features that show high covariance (i.e., strong correlation) can be dropped or combined to improve model performance and reduce dimensionality.</li> </ul> </li> </ul> <h4 id="full-rank-matrix"><strong>Full Rank Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix is <strong>full rank</strong> if its rank is equal to the smallest of its number of rows or columns. In other words, all rows (or columns) are linearly independent.</li> <li><strong>Properties:</strong> <ul> <li>A matrix \(A\) with full rank has no redundant or dependent rows or columns.</li> <li>If \(A\) is an \(m \times n\) matrix, and \(\text{rank}(A) = \min(m, n)\), the matrix is full rank.</li> <li>A full rank matrix is <strong>invertible</strong> if it is square (i.e., if \(m = n\)).</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Linear Regression:</strong> In linear regression, the design matrix \(X\) must be full rank to ensure a unique solution. If \(X\) is not full rank, the matrix \(X^T X\) is singular and cannot be inverted.</li> </ul> </li> </ul> <h4 id="singular-matrix"><strong>Singular Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix is <strong>singular</strong> if it is not invertible, meaning its determinant is zero. A singular matrix has linearly dependent rows or columns.</li> <li><strong>Properties:</strong> <ul> <li>The determinant of a singular matrix is 0.</li> <li>The matrix has at least one eigenvalue equal to 0.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Linear Dependence:</strong> If the feature matrix \(X\) in a linear model is singular, some features are perfectly correlated, and this leads to instability in training and difficulties in solving for the model parameters.</li> </ul> </li> </ul> <hr/> <p>That wraps up the key linear algebra concepts for machine learning. This post is designed as a quick reference rather than an exhaustive guide. Don’t stress about memorizing everything—focus instead on understanding the concepts and knowing when to revisit them if needed.</p> <p>Math is a language, and like any language, it’s more about learning to use it than memorizing rules. Treat this as a foundation to build on, and come back to refresh your knowledge whenever necessary.</p> <p>Up next, we’ll explore the prerequisites of <strong>Probability Theory</strong> for machine learning. Since probability can often feel trickier, we’ll focus more on “what,” “why,” and “how” questions to make the concepts intuitive and approachable.</p> <p>See you in the next one!</p> <h3 id="references"><strong>References:</strong></h3> ]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog post covers the key linear algebra concepts and their applications in machine learning.]]></summary></entry><entry><title type="html">Introduction to Machine Learning(ML)</title><link href="https://monishver11.github.io/blog/2024/intro-to-ml/" rel="alternate" type="text/html" title="Introduction to Machine Learning(ML)"/><published>2024-12-19T19:44:00+00:00</published><updated>2024-12-19T19:44:00+00:00</updated><id>https://monishver11.github.io/blog/2024/intro-to-ml</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/intro-to-ml/"><![CDATA[<p>What does it mean to learn? The Merriam-Webster dictionary defines learning as “the activity or process of gaining knowledge or skill by studying, practicing, being taught, or experiencing something.” Tom Mitchell, a pioneer in machine learning, extends this concept to machines:</p> <blockquote> <p>“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”</p> </blockquote> <p>Machine learning (ML) is about teaching machines to learn from data and solve problems. Let’s dive into what this means, how it works, and why it’s transforming the way we use technology.</p> <h4 id="what-is-machine-learning"><strong>What is Machine Learning?</strong></h4> <p>At its core, machine learning is like <strong>meta-programming</strong>—programming a system to program itself. For tasks like recognizing faces or understanding speech, coding specific rules is impractical. Instead, ML lets machines learn directly from data to solve problems such as:</p> <ul> <li>Predicting whether an email is spam.</li> <li>Diagnosing diseases from symptoms.</li> <li>Forecasting stock prices.</li> </ul> <p>The goal? Take an input x and predict an output y—it can be a label, a number, or a decision.</p> <hr/> <h4 id="why-use-machine-learning"><strong>Why Use Machine Learning?</strong></h4> <p>Machine learning is particularly valuable when:</p> <ul> <li><strong>Rules are too complex to define manually</strong>: Recognizing a face or interpreting speech involves subtleties that are hard to encode in rules.</li> <li><strong>The system needs to adapt</strong>: For instance, spam filters must evolve as spammers devise new tricks.</li> <li><strong>It outperforms human-built solutions</strong>: Algorithms can detect patterns or nuances humans might miss.</li> <li><strong>Fairness and privacy are critical</strong>: For example, ranking search results or filtering harmful content.</li> </ul> <h3 id="canonical-examples-of-machine-learning"><strong>Canonical Examples of Machine Learning</strong></h3> <ol> <li><strong>Spam Detection</strong> <ul> <li><strong>Input</strong>: Incoming email</li> <li><strong>Output</strong>: “SPAM” or “NOT SPAM”</li> <li>Problem Type: Binary Classification</li> </ul> </li> <li><strong>Medical Diagnosis</strong> <ul> <li><strong>Input</strong>: Patient symptoms (e.g., fever, cough)</li> <li><strong>Output</strong>: Diagnosis (e.g., flu, pneumonia)</li> <li>Problem Type: Multiclass Classification</li> <li>Probabilistic Classification: Uncertainty is expressed as probabilities, e.g., P(pneumonia)=0.7</li> </ul> </li> <li><strong>Stock Price Prediction</strong> <ul> <li><strong>Input</strong>: Historical stock prices</li> <li><strong>Output</strong>: Price at the close of the next day</li> <li>Problem Type: Regression (continuous outputs)</li> </ul> </li> </ol> <h4 id="ml-vs-rule-based-systems"><strong>ML vs. Rule-Based Systems</strong></h4> <p>Before ML, many problems were solved with <strong>rule-based systems</strong> (or expert systems). For instance, medical diagnosis might involve encoding expert knowledge into rules that map symptoms to diseases.</p> <p><strong>Strengths of Rule-Based Systems</strong></p> <ul> <li>Leverage domain expertise.</li> <li>Interpretable and explainable.</li> <li>Reliable for known scenarios.</li> </ul> <p><strong>Weaknesses of Rule-Based Systems</strong></p> <ul> <li>Labor-intensive to build and maintain.</li> <li>Poor generalization to new or unseen scenarios.</li> <li>Struggle with uncertainty and probabilistic reasoning.</li> </ul> <h4 id="how-ml-overcomes-these-weaknesses"><strong>How ML Overcomes These Weaknesses</strong></h4> <p>Instead of encoding rules, ML systems learn directly from <strong>training data</strong>—examples of input-output pairs. For instance:</p> <ul> <li>Input: Emails</li> <li>Output: SPAM or NOT SPAM<br/> This approach, called <strong>supervised learning</strong>, involves learning from labeled examples of input-output pairs.</li> </ul> <h4 id="key-concepts-in-ml"><strong>Key Concepts in ML</strong></h4> <ul> <li><strong>Common Problem Types</strong>: <ul> <li>Classification (binary, multi-class)</li> <li>Regression (continuous prediction)</li> </ul> </li> <li><strong>Core Elements</strong>: <ul> <li>Prediction function: Maps x (input) to y (output).</li> <li>Training data: A collection of input-output pairs for the model to learn from.</li> <li>Algorithms: Methods to produce the best prediction function from data.</li> </ul> </li> <li><strong>Beyond Supervised Learning</strong>: <ul> <li><strong>Unsupervised Learning</strong>: Discovering patterns, like clustering similar users.</li> <li><strong>Reinforcement Learning</strong>: Optimizing long-term objectives or learning through rewards, e.g., chess and other gameplays.</li> <li><strong>Representation Learning</strong>: Automatically discovering useful features, e.g., learning word embeddings.</li> </ul> </li> </ul> <h4 id="core-questions-in-machine-learning"><strong>Core Questions in Machine Learning</strong></h4> <ol> <li><strong>Modeling</strong>: What kinds of prediction functions should we consider?</li> <li><strong>Learning</strong>: How do we find the best prediction function from training data?</li> <li><strong>Inference</strong>: How do we compute predictions for new inputs?</li> </ol> <h2 id="well-tackle-each-of-these-questions-as-we-move-forward-so-stick-around">We’ll tackle each of these questions as we move forward, so stick around!</h2> <h4 id="ml-vs-statistics-key-differences"><strong>ML vs. Statistics: Key Differences</strong></h4> <p>While both fields use mathematical tools like calculus, probability, and linear algebra, they differ in focus:</p> <ul> <li><strong>Statistics</strong>: Emphasizes interpretability and aiding human decision-making.</li> <li><strong>ML</strong>: Prioritizes scalability, automation, and predictive performance.</li> </ul> <h4 id="ml-in-ai-and-human-learning"><strong>ML in AI and Human Learning</strong></h4> <p><strong>Relation to AI:</strong> Machine learning is a crucial subset of artificial intelligence (AI). While AI encompasses a broad range of approaches to simulate human-like intelligence, machine learning focuses specifically on learning patterns from data to make predictions or decisions.</p> <p><strong>Relation to Human Learning:</strong> Though inspired by human cognition, ML differs significantly:</p> <ul> <li>We humans are highly efficient with limited data.</li> <li>Machines require large datasets but excel at specific tasks.</li> </ul> <p>ML systems, like neural networks, borrow ideas from biology but don’t aim to replicate human learning entirely. Instead, their focus remains on solving specialized problems effectively.</p> <h4 id="conclusion"><strong>Conclusion</strong></h4> <p>Machine learning bridges the gap between raw data and intelligent decision-making. By enabling systems to learn and adapt, ML transforms how we approach problems in healthcare, finance, and beyond. With its foundations rooted in mathematics and its applications spanning diverse domains, ML continues to redefine the boundaries of what technology can achieve.</p> <p>That’s it for this, see you in the next post! 👋</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[An easy guide to machine learning, its applications, and how it connects to AI and human learning.]]></summary></entry><entry><title type="html">Preface &amp;amp; Introduction</title><link href="https://monishver11.github.io/blog/2024/preface-ml/" rel="alternate" type="text/html" title="Preface &amp;amp; Introduction"/><published>2024-12-18T17:43:00+00:00</published><updated>2024-12-18T17:43:00+00:00</updated><id>https://monishver11.github.io/blog/2024/preface-ml</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/preface-ml/"><![CDATA[<h4 id="welcome-to-my-machine-learning-blog-series"><strong>Welcome to My Machine Learning Blog Series!</strong></h4> <p>This blog series draws inspiration from my machine learning course at NYU—a graduate-level adventure designed to explore concepts deeply and build a strong foundation from scratch. A solid grasp of the fundamentals is essential for mastering advanced topics and making meaningful progress. The course was initially designed by <a href="https://scholar.google.com/citations?user=YsHFgSAAAAAJ&amp;hl=en" target="_blank">Prof. David Rosenburg</a> and later adapted by <strong>Prof. He He</strong>, <strong>Tal Linzen</strong>, and others. I had the privilege of learning under <a href="https://mengyeren.com/" target="_blank">Prof. Mengye Ren</a>, whose teaching and structured content have greatly influenced this blog series.</p> <p>A heartfelt thank you to everyone who has supported me, continues to support me, or will support me in the future. I’m incredibly grateful for all the experiences that have brought me to this point in my journey.</p> <h4 id="what-to-expect-in-this-blog-series"><strong>What to Expect in This Blog Series</strong></h4> <p>The focus here is on the <strong>theoretical aspects</strong> of machine learning rather than programming. Why? Because understanding theory forms the critical intuition that separates a beginner randomly trying things from an expert who knows exactly where to focus for impactful results. While I consider myself a beginner, I’m determined to keep learning, growing, and sharing insights along the way.</p> <p>Since <strong>mathematics forms the backbone</strong> of machine learning and much of computer science, I’ll ensure no critical details are overlooked. For challenging or abstract concepts, I’ll include <strong>analogies</strong> to help you remember them more easily. For key topics, expect to see discussions about their real-world applications, <strong>industry relevance</strong>, and <strong>tips and tricks</strong> that can significantly boost performance, along with the reasoning behind why they work.</p> <p>While the focus is theoretical, I won’t leave you hanging! I’ll include <strong>code snippets</strong> and <strong>programming references</strong> for topics where information isn’t easily accessible online or through tools like ChatGPT</p> <p>If you’re someone with undergraduate-level knowledge in mathematics and programming, you’ll find this series accessible. While topics like Convex optimization or Lagrangians might not be covered in standard undergraduate curricula, don’t worry—I’ll explain them in detail as we go along.</p> <h4 id="attention-to-detail-and-collaboration"><strong>Attention to Detail and Collaboration</strong></h4> <p>Every blog post will include a list of <strong>references</strong> I’ve read or used while preparing the content. I plan to thoroughly review each post multiple times before publishing, incorporating feedback from classmates and my professor to ensure accuracy. If you spot any mistakes, please don’t hesitate to reach out—I’d be thrilled to correct them and keep this content as reliable as possible.</p> <h4 id="a-lifelong-learning-project"><strong>A Lifelong Learning Project</strong></h4> <p>This blog series is more than just a project—it’s a lifelong commitment. My goal is to gradually evolve this resource as I publish posts one by one. I’ll strive to remain consistent and ensure this series not only serves as a learning resource but also inspires more people to dive into machine learning and contribute to the community.</p> <p>It’s time to get to work—fingers crossed!</p> <p>Thank you for being part of this journey. See you in the next post! 👋</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[First blog post—more of a preface, setting the stage for the journey ahead.]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://monishver11.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://monishver11.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://monishver11.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://monishver11.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://monishver11.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://monishver11.github.io/blog/2024/tabs</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="7c5ac8fa-cf5d-401a-8ba4-f34163d3fadf" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="7c5ac8fa-cf5d-401a-8ba4-f34163d3fadf" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="ed2abe02-1f11-4244-9270-761ca350f5b3" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="ed2abe02-1f11-4244-9270-761ca350f5b3" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="91dd316c-40e6-42d1-a428-0b2451b20b30" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="91dd316c-40e6-42d1-a428-0b2451b20b30" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://monishver11.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://monishver11.github.io/blog/2024/typograms</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://monishver11.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://monishver11.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry></feed>