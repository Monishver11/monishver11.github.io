<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-07T03:02:43+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Understanding the Maximum Margin Classifier</title><link href="https://monishver11.github.io/blog/2025/max-margin-classifier/" rel="alternate" type="text/html" title="Understanding the Maximum Margin Classifier"/><published>2025-01-06T05:57:00+00:00</published><updated>2025-01-06T05:57:00+00:00</updated><id>https://monishver11.github.io/blog/2025/max-margin-classifier</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/max-margin-classifier/"><![CDATA[<h4 id="linearly-separable-data"><strong>Linearly Separable Data</strong></h4> <p>Let’s start with the simplest case: linearly separable data. Imagine a dataset where we can draw a straight line (or more generally, a hyperplane in higher dimensions) to perfectly separate two classes of points. Formally, for a dataset \(D\) with points \((x_i, y_i)\), we seek a hyperplane that satisfies the following conditions:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_1-480.webp 480w,/assets/img/Max_Margin_Classifier_1-800.webp 800w,/assets/img/Max_Margin_Classifier_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>\(w^T x_i &gt; 0\) for all \(x_i\) where \(y_i = +1\),</li> <li>\(w^T x_i &lt; 0\) for all \(x_i\) where \(y_i = -1\).</li> </ul> <p>This hyperplane is defined by a weight vector \(w\) and a bias \(b\), and our goal is to find \(w\) and \(b\) such that all points are correctly classified.</p> <p>But how do we design a learning algorithm to find such a hyperplane? This brings us to the <strong>Perceptron Algorithm</strong>.</p> <h4 id="the-perceptron-algorithm"><strong>The Perceptron Algorithm</strong></h4> <p>The perceptron is one of the earliest learning algorithms developed to find a separating hyperplane. Here’s how it works: we start with an initial guess for \(w\) (usually a zero vector) and iteratively adjust it based on misclassified examples.</p> <p>Each time we encounter a point \((x_i, y_i)\) that is misclassified (i.e., \(y_i w^T x_i &lt; 0\)), we update the weight vector as follows:</p> \[w \gets w + y_i x_i.\] <p>This update rule ensures that the algorithm moves the hyperplane towards misclassified positive examples and away from misclassified negative examples.</p> <p>The perceptron algorithm has a remarkable property: if the data is linearly separable, it will converge to a solution with zero classification error in a finite number of steps.</p> <p>In terms of loss functions, the perceptron can be viewed as minimizing the <strong>hinge loss</strong>:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_2-480.webp 480w,/assets/img/Max_Margin_Classifier_2-800.webp 800w,/assets/img/Max_Margin_Classifier_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> \[\ell(x, y, w) = \max(0, -y w^T x).\] <p>However, while the perceptron guarantees a solution, it doesn’t always find the best one. This brings us to the concept of <strong>maximum-margin classifiers</strong>. But before exploring that, let’s take a deeper look at why the this update rule works.</p> <h5 id="understanding-why-the-perceptron-update-rule-works"><strong>Understanding why the Perceptron Update Rule works?</strong></h5> <p>The <strong>perceptron update rule</strong> shifts the hyperplane differently depending on whether the misclassified point belongs to the positive class (\(y_i = 1\)) or the negative class (\(y_i = -1\)). Let’s take the two cases:</p> <h6 id="positive-case-y_i--1"><strong>Positive Case (\(y_i = 1\))</strong></h6> <ul> <li> <p><strong>Condition for misclassification:</strong> \(w^T x_i &lt; 0.\)<br/> This means the point \(x_i\) is on the wrong side of the hyperplane or too far from the correct side.</p> </li> <li> <p><strong>Update rule:</strong></p> \[w \gets w + x_i.\] </li> <li> <p><strong>Effect of the update:</strong></p> <ul> <li>Adding \(x_i\) to \(w\) increases the dot product \(w^T x_i\) because \(w\) is now pointing more in the direction of \(x_i\).</li> <li>This adjustment shifts the hyperplane towards \(x_i\), ensuring \(x_i\) is more likely to be correctly classified in the next iteration.</li> </ul> </li> </ul> <h6 id="negative-case-y_i---1"><strong>Negative Case (\(y_i = -1\))</strong></h6> <ul> <li> <p><strong>Condition for misclassification:</strong> \(w^T x_i &gt; 0.\)<br/> This means the point \(x_i\) is either incorrectly classified as positive or too close to the positive side.</p> </li> <li> <p><strong>Update rule:</strong></p> \[w \gets w - x_i.\] </li> <li> <p><strong>Effect of the update:</strong></p> <ul> <li>Subtracting \(x_i\) from \(w\) decreases the dot product \(w^T x_i\) because \(w\) is now pointing less in the direction of \(x_i\).</li> <li>This adjustment shifts the hyperplane away from \(x_i\), making it more likely to correctly classify \(x_i\) as negative in subsequent iterations.</li> </ul> </li> </ul> <p><strong>Geometric Interpretation</strong>: The perceptron update ensures that the weight vector \(w\) aligns more closely with the correctly classified side.</p> <hr/> <h4 id="maximum-margin-separating-hyperplane"><strong>Maximum-Margin Separating Hyperplane</strong></h4> <p>When the data is linearly separable, there are infinitely many hyperplanes that can separate the classes. The perceptron algorithm, for instance, might return any one of these. But not all hyperplanes are equally desirable.</p> <p>We prefer a hyperplane that is farthest from both classes of points. This idea leads to the concept of the <strong>maximum-margin classifier</strong>, which finds the hyperplane that maximizes the smallest distance between the hyperplane and the data points.</p> <h5 id="geometric-margin"><strong>Geometric Margin</strong></h5> <p>The <strong>geometric margin</strong> of a hyperplane is defined as the smallest distance between the hyperplane and any data point. For a hyperplane defined by \(w\) and \(b\), this margin can be expressed as:</p> \[\gamma = \min_i \frac{y_i (w^T x_i + b)}{\|w\|_2}.\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_3-480.webp 480w,/assets/img/Max_Margin_Classifier_3-800.webp 800w,/assets/img/Max_Margin_Classifier_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Maximizing this geometric margin provides a hyperplane that is robust to small perturbations in the data, making it a desirable choice.</p> <h5 id="distance-between-a-point-and-a-hyperplane"><strong>Distance Between a Point and a Hyperplane</strong></h5> <p>To understand the geometric margin more concretely, let’s calculate the distance from a point \(x'\) to a hyperplane \(H: w^T v + b = 0\). This derivation involves the following steps:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_4-480.webp 480w,/assets/img/Max_Margin_Classifier_4-800.webp 800w,/assets/img/Max_Margin_Classifier_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h6 id="step-1-perpendicular-distance-from-a-point-to-a-hyperplane"><strong>Step 1: Perpendicular Distance from a Point to a Hyperplane</strong></h6> <p>The distance from a point \(x'\) to the hyperplane is defined as the shortest (perpendicular) distance between the point and the hyperplane. The equation of the hyperplane is:</p> \[w^T v + b = 0,\] <p>where:</p> <ul> <li>\(w\) is the normal vector to the hyperplane.</li> <li>\(b\) is the bias term.</li> <li>\(v\) represents any point on the hyperplane.</li> </ul> <h6 id="step-2-projecting-the-point-onto-the-normal-vector"><strong>Step 2: Projecting the Point onto the Normal Vector</strong></h6> <p>The perpendicular distance is proportional to the projection of the point \(x'\) onto the normal vector \(w\). Mathematically, the projection of \(x'\) onto \(w\), denoted \(\text{Proj}_{w}(x')\), is given by:</p> \[\text{Proj}_{w}(x') = \frac{x' \cdot w}{w \cdot w} w = \left( \frac{w^T x'}{\|w\|_2^2} \right) w\] <p>For the hyperplane \(H: w^T v + b = 0\), the bias term \(b\) shifts the hyperplane as the hyperplane is not always centered at the origin Incorporating this into the projection formula, the signed distance becomes:</p> \[d(x', H) = \frac{w^T x' + b}{\|w\|_2}.\] <h6 id="step-3-accounting-for-the-label-y"><strong>Step 3: Accounting for the Label \(y\)</strong></h6> <p>The label \(y\) of the point \(x'\) determines whether the point is on the positive or negative side of the hyperplane:</p> <ul> <li>For correctly classified points, \(y (w^T x' + b) &gt; 0\).</li> <li>For misclassified points, \(y (w^T x' + b) &lt; 0\).</li> </ul> <p>Including the label ensures that the signed distance is positive for correctly classified points and negative for misclassified points. Thus, the signed distance becomes:</p> \[d(x', H) = \frac{y (w^T x' + b)}{\|w\|_2}.\] <hr/> <h5 id="maximizing-the-margin"><strong>Maximizing the Margin</strong></h5> <p>To maximize the margin, we solve the following optimization problem:</p> \[\max \min_i \frac{y_i (w^T x_i + b)}{\|w\|_2}.\] <p>To simplify, let \(M = \min_i \frac{y_i (w^T x_i + b)}{\|w\|_2}\). The problem becomes:</p> \[\max M, \quad \text{subject to } \frac{y_i (w^T x_i + b)}{\|w\|_2} \geq M, \; \forall i.\] <p><strong>This means:</strong> We want to maximize \(M\), which corresponds to maximizing the smallest margin across all data points.</p> <p>The constraint: \(\frac{y_i (w^T x_i + b)}{\|w\|_2} \geq M\) ensures that for every data point \(x_i\), the margin is at least \(M\), i.e., the data point lies on the correct side of the margin boundary.</p> <p><strong>Another way to put this is:</strong> Since \(M\) is the smallest margin, the constraint \(\frac{y_i (w^T x_i + b)}{\|w\|_2} \geq M\) ensures that every data point has a margin at least as large as \(M\) and this condition is enforced for every data point \(i\).</p> <p>Next, by fixing \(\|w\|_2 = \frac{1}{M}\), we reformulate it as:</p> \[\min \frac{1}{2} \|w\|_2^2, \quad \text{subject to } y_i (w^T x_i + b) \geq 1, \; \forall i.\] <p>This is the optimization problem solved by a <strong>hard margin support vector machine (SVM)</strong>.</p> <p><strong>Note:</strong> Maximizing the margin \(M\) is equivalent to minimizing the inverse, \(\frac{1}{2} \|w\|_2^2\), since the margin is inversely proportional to the norm of \(w\).</p> <h5 id="what-if-the-data-is-not-linearly-separable"><strong>What If the Data Is Not Linearly Separable?</strong></h5> <p>In real-world scenarios, data is often not perfectly linearly separable. For any \(w\), there might be points with negative margins. To handle such cases, we introduce <strong>slack variables</strong> \(\xi_i\), which allow some margin violations.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_5-480.webp 480w,/assets/img/Max_Margin_Classifier_5-800.webp 800w,/assets/img/Max_Margin_Classifier_5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h4 id="soft-margin-svm"><strong>Soft Margin SVM</strong></h4> <p>The optimization problem for a soft margin SVM is:</p> \[\min \frac{1}{2} \|w\|_2^2 + C \sum_{i=1}^n \xi_i,\] <p>subject to:</p> \[y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \; \forall i.\] <h5 id="breaking-this-down"><strong>Breaking this down:</strong></h5> <ul> <li> <p><strong>Regularization Term</strong>:</p> \[\frac{1}{2} \|w\|_2^2\] <p>This term is the <strong>regularization</strong> component of the objective function. It penalizes large values of \(w\), which corresponds to smaller margins. By minimizing this term, we aim to <strong>maximize the margin</strong> between the two classes. A larger margin typically leads to better generalization and lower overfitting.</p> </li> <li> <p><strong>Penalty Term</strong>:</p> \[C \sum_{i=1}^n \xi_i\] <p>This term introduces <strong>penalties</strong> for margin violations. The \(\xi_i\) are the <strong>slack variables</strong> that measure how much each data point violates the margin. The parameter \(C\) controls the trade-off between <strong>maximizing the margin</strong> (by minimizing \(\|w\|_2^2\)) and <strong>minimizing the violations</strong> (the sum of the slack variables).</p> <ul> <li>A <strong>larger value of \(C\)</strong> places more emphasis on minimizing violations, which results in a stricter margin but could lead to overfitting if \(C\) is too large.</li> <li>A <strong>smaller value of \(C\)</strong> allows for more margin violations, potentially leading to a <strong>wider margin</strong> and better generalization.</li> </ul> </li> <li> <p><strong>Margin Constraint</strong>:</p> \[y_i (w^T x_i + b) \geq 1 - \xi_i\] <p>This constraint ensures that the data points are correctly classified with a margin of at least 1, unless there is a violation. If a data point violates the margin (i.e., it lies inside the margin or on the wrong side of the hyperplane), the slack variable \(\xi_i\) becomes positive. The value of \(\xi_i\) measures how much the margin is violated for the data point \(x_i\).</p> <ul> <li> <p>When \(\xi_i = 0\), the data point \(x_i\) satisfies the margin condition:</p> \[y_i (w^T x_i + b) \geq 1\] <p>This represents the ideal case where the point lies correctly outside or on the margin.</p> </li> <li> <p>When \(\xi_i &gt; 0\), the point <strong>violates the margin</strong>. The larger the value of \(\xi_i\), the greater the violation. For example, if \(\xi_i = 0.5\), the point lies inside the margin or is misclassified by 0.5 units.</p> </li> </ul> </li> <li> <p><strong>Non-Negativity of Slack Variables</strong>:</p> \[\xi_i \geq 0\] <p>This ensures that the slack variables \(\xi_i\) are always non-negative, as they represent the <strong>degree of violation</strong> of the margin. Since it’s not possible to have a negative violation, this constraint enforces that \(\xi_i\) cannot be less than zero.</p> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Max_Margin_Classifier_6-480.webp 480w,/assets/img/Max_Margin_Classifier_6-800.webp 800w,/assets/img/Max_Margin_Classifier_6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Max_Margin_Classifier_6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Max_Margin_Classifier_6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h6 id="wrapping-up"><strong>Wrapping Up</strong></h6> <p>The maximum-margin classifier forms the foundation of modern support vector machines. For non-linearly separable data, the introduction of slack variables allows SVMs to adapt while maintaining their core principle of maximizing the margin.</p> <p>In the next post, we’ll dive deeper into the world of SVMs, explore how they work under the hood, and work through this optimization problem to solve it. Stay tuned!</p> <h6 id="references"><strong>References:</strong></h6> <ul> <li><a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html">Lecture 9: SVM</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.]]></summary></entry><entry><title type="html">L1 and L2 Regularization - Nuanced Details</title><link href="https://monishver11.github.io/blog/2025/l1-l2-reg-indepth/" rel="alternate" type="text/html" title="L1 and L2 Regularization - Nuanced Details"/><published>2025-01-05T17:50:00+00:00</published><updated>2025-01-05T17:50:00+00:00</updated><id>https://monishver11.github.io/blog/2025/l1-l2-reg-indepth</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/l1-l2-reg-indepth/"><![CDATA[<p>Regularization is a cornerstone in machine learning, providing a mechanism to prevent overfitting while controlling model complexity. Among the most popular techniques are <strong>L1</strong> and <strong>L2 regularization</strong>, which serve different purposes but share a common goal of improving model generalization. In this post, we will delve deep into the theory, mathematics, and practical implications of these regularization methods.</p> <p>Let’s set the stage with linear regression. For a dataset</p> \[D_n = \{(x_1, y_1), \dots, (x_n, y_n)\},\] <p>the objective in ordinary least squares is to minimize the mean squared error:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2.\] <p>While effective, this approach can overfit when the number of features \(d\) is large compared to the number of samples \(n\). For example, in natural language processing, it is common to have millions of features but only thousands of documents.</p> <h5 id="addressing-overfitting-with-regularization"><strong>Addressing Overfitting with Regularization</strong></h5> <p>To mitigate overfitting, <strong>\(L_2\) regularization</strong> (also known as <strong>ridge regression</strong>) adds a penalty term proportional to the \(L_2\) norm of the weights:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2 + \lambda \|w\|_2^2,\] <p>where:</p> \[\|w\|_2^2 = w_1^2 + w_2^2 + \dots + w_d^2.\] <p>This penalty term discourages large weight values, effectively shrinking them toward zero. When \(\lambda = 0\), the solution reduces to ordinary least squares. As \(\lambda\) increases, the penalty grows, favoring simpler models with smaller weights.</p> <h5 id="understanding-l_2-regularization"><strong>Understanding \(L_2\) Regularization</strong></h5> <p>L2 regularization is particularly effective at reducing sensitivity to fluctuations in the input data. To understand this, consider a simple linear function:</p> \[\hat{f}(x) = \hat{w}^\top x.\] <p>The function \(\hat{f}(x)\) is said to be <strong>Lipschitz continuous</strong>, with a Lipschitz constant defined as:</p> \[L = \|\hat{w}\|_2.\] <p>This implies that when the input changes from \(x\) to \(x + h\), the function’s output change is bounded by \(L\|h\|_2\). In simpler terms, \(L_2\) regularization controls the rate of change of \(\hat{f}(x)\), making the model less sensitive to variations in the input data.</p> <h6 id="mathematical-proof-of-lipschitz-continuity"><strong>Mathematical Proof of Lipschitz Continuity</strong></h6> <p>To formalize this property, let’s derive the Lipschitz bound:</p> \[|\hat{f}(x + h) - \hat{f}(x)| = |\hat{w}^\top (x + h) - \hat{w}^\top x| = |\hat{w}^\top h|.\] <p>Using the <strong>Cauchy-Schwarz inequality</strong>, this can be bounded as:</p> \[|\hat{w}^\top h| \leq \|\hat{w}\|_2 \|h\|_2.\] <p>Thus, the Lipschitz constant \(L = \|\hat{w}\|_2\) quantifies the maximum rate of change for the function \(\hat{f}(x)\).</p> <h5 id="generalization-to-other-norms"><strong>Generalization to Other Norms</strong></h5> <p>The generalization to other norms comes from the equivalence of norms in finite-dimensional vector spaces. Here’s the reasoning:</p> <p><strong>Norm Equivalence:</strong></p> <p>In finite-dimensional spaces (e.g., \(\mathbb{R}^d\)), all norms are equivalent. This means there exist constants \(C_1, C_2 &gt; 0\) such that for any vector \(\mathbf{w} \in \mathbb{R}^d\):</p> \[C_1 \| \mathbf{w} \|_p \leq \| \mathbf{w} \|_q \leq C_2 \| \mathbf{w} \|_p\] <p>For example, the \(L_1\), \(L_2\), and \(L_\infty\) norms can all bound one another with appropriate scaling constants.</p> <p><strong>Lipschitz Continuity:</strong></p> <p>The Lipschitz constant for \(\hat{f}(\mathbf{x}) = \mathbf{w}^\top \mathbf{x}\) depends on the norm of \(\mathbf{w}\) because the bound for the rate of change involves the norm of \(\mathbf{w}\). When using a different norm \(\| \cdot \|_p\) to regularize, the Lipschitz constant adapts to that norm.</p> <p>Specifically, for the \(L_p\) norm:</p> \[| \hat{f}(\mathbf{x} + \mathbf{h}) - \hat{f}(\mathbf{x}) | \leq \| \mathbf{w} \|_p \| \mathbf{h} \|_q\] <p>where \(p\) and \(q\) satisfies:</p> \[\frac{1}{p} + \frac{1}{q} = 1\] <p><strong>Key Insight:</strong></p> <p>This shows that the idea of controlling the sensitivity of the model (through the Lipschitz constant) extends naturally to any norm. The choice of norm alters how the regularization penalizes weights but retains the fundamental property of bounding the function’s rate of change.</p> <h6 id="an-analogy-to-internalize-this"><strong>An analogy to internalize this:</strong></h6> <p>Think of \(L_2\) regularization as a bungee cord attached to a daring rock climber. The climber represents the model trying to navigate a complex landscape (data). Without the cord (regularization), they might venture too far and fall into overfitting. The cord adds just enough tension (penalty) to keep the climber balanced and safe, ensuring they explore the terrain without taking reckless leaps. Similarly, regularization helps the model stay grounded, generalizing well without succumbing to overfitting.</p> <p>Now, imagine different types of bungee cords for different norms. The \(L_2\) regularization bungee cord is like a standard elastic cord, providing a smooth and consistent tension, ensuring the climber doesn’t over-extend but can still make significant progress.</p> <p>For \(L_1\) regularization, the bungee cord is more rigid and less forgiving, preventing large movements in any direction. It forces the climber to stick to fewer, more significant paths, like sparsity in feature selection — only the most important features remain.</p> <p>In the case of \(L_\infty\) regularization, the bungee cord has a fixed maximum stretch. No matter how hard the climber tries to move, they cannot go beyond a certain point, ensuring the model remains under tight control, limiting the complexity of each individual parameter.</p> <p>In each case, the regularization (the cord) helps the climber (the model) stay within safe bounds, preventing them from falling into overfitting while ensuring they can still navigate the data effectively.</p> <hr/> <h4 id="linear-vs-ridge-regression"><strong>Linear vs. Ridge Regression</strong></h4> <p>The inclusion of L2 regularization modifies the optimization objective, as illustrated by the difference between <strong>linear regression</strong> and <strong>ridge regression</strong>.</p> <p>In <strong>linear regression</strong>, the goal is to minimize the sum of squared residuals, expressed as:</p> \[L(w) = \frac{1}{2} \|Xw - y\|_2^2\] <p>In contrast, <strong>ridge regression</strong> introduces an additional penalty term proportional to the L2 norm of the weights:</p> \[L(w) = \frac{1}{2} \|Xw - y\|_2^2 + \frac{\lambda}{2} \|w\|_2^2\] <p>This additional term penalizes large weights, helping to control model complexity and reduce overfitting.</p> <h6 id="gradients-of-the-objective"><strong>Gradients of the Objective:</strong></h6> <p>The inclusion of the regularization term affects the gradient of the loss function. For linear regression, the gradient is:</p> \[\nabla L(w) = X^T (Xw - y)\] <p>For ridge regression, the gradient becomes:</p> \[\nabla L(w) = X^T (Xw - y) + \lambda w\] <p>The regularization term \(\lambda w\) biases the solution toward smaller weights, thereby stabilizing the optimization. By adding this term, the model is less sensitive to small changes in the data, especially in cases where multicollinearity exists, i.e., when features are highly correlated.</p> <h6 id="closed-form-solutions"><strong>Closed-form Solutions:</strong></h6> <p>Both linear regression and ridge regression admit closed-form solutions. For linear regression, the weights are given by:</p> \[w = (X^T X)^{-1} X^T y\] <p>For ridge regression, the solution is slightly modified:</p> \[w = (X^T X + \lambda I)^{-1} X^T y\] <p>The addition of \(\lambda I\) ensures that \(X^T X + \lambda I\) is always invertible, addressing potential issues of singularity in the design matrix. In linear regression, if the matrix \(X^T X\) is singular or nearly singular (which can occur when features are linearly dependent or when there are more features than samples), the inverse may not exist or be unstable. By adding \(\lambda I\), where \(I\) is the identity matrix, we effectively shift the eigenvalues of \(X^T X\), making the matrix non-singular and ensuring a stable solution.</p> <hr/> <h4 id="a-constrained-optimization-perspective"><strong>A Constrained Optimization Perspective</strong></h4> <p>L2 regularization can also be understood through the lens of constrained optimization. In this perspective, the ridge regression objective is expressed in <strong>Tikhonov regularization</strong> form as:</p> \[w^* = \arg\min_w \left( \frac{1}{2} \|Xw - y\|_2^2 + \frac{\lambda}{2} \|w\|_2^2 \right)\] <p>The <strong>Ivanov form</strong> is another perspective where the objective is similarly constrained, but the constraint is typically applied in a more specific way, usually in the context of ill-posed problems or regularization approaches in functional analysis. It focuses on minimizing the error while controlling the solution’s smoothness or complexity. While this form is less commonly used directly in machine learning, it is foundational in understanding regularization in more theoretical settings. We mention this now because both forms will appear later in the discussion of other concepts, and it’s helpful to have a brief overview before we revisit them in more depth.</p> <p>Alternatively, using <strong>Lagrangian theory</strong>, we can reframe ridge regression as a constrained optimization problem. The objective is to minimize the residual sum of squares subject to a constraint on the L2 norm of the weights:</p> \[w^* = \arg\min_{w : \|w\|_2^2 \leq r} \frac{1}{2} \|Xw - y\|_2^2\] <p>Here, \(r\) represents the maximum allowed value for the squared norm of the weights, effectively placing a limit on their size. The Lagrange multiplier adjusts the importance of the constraint during optimization. This form emphasizes the constraint on model complexity, ensuring that the weights don’t grow too large.</p> <p>At the optimal solution, the gradients of the objective function and the constraint term balance each other, providing a geometric interpretation of how regularization controls the model complexity.</p> <p><strong>Note:</strong> The Lagrangian theory will be explored further when we discuss Support Vector Machines (SVMs), where this approach plays a central role in optimization.</p> <hr/> <h4 id="lasso-regression-and-l_1-regularization"><strong>Lasso Regression and \(L_1\) Regularization</strong></h4> <p>While L2 regularization minimizes the sum of squared weights, <strong>L1 regularization</strong> (used in Lasso regression) minimizes the sum of absolute weights. This is expressed as:</p> \[w^* = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n (\hat{w}^T x_i - y_i)^2 + \lambda \|w\|_1\] <p>Here, the L1 norm</p> \[\|w\|_1 = |w_1| + |w_2| + \dots + |w_d|\] <p>encourages sparsity in the weight vector, setting some coefficients exactly to zero. <strong>But what’s behind this, really?</strong> Keep reading!</p> <h5 id="ridge-vs-lasso-regression"><strong>Ridge vs. Lasso Regression</strong></h5> <p>The key difference between ridge and lasso regression lies in their impact on the weights. Ridge regression tends to shrink all coefficients toward zero but does not eliminate any of them. In contrast, lasso regression produces sparse solutions, where some coefficients are exactly zero. <strong>We’ll dive into this next.</strong></p> <p>This sparsity has significant practical advantages. By zeroing out irrelevant features, lasso regression simplifies the model, making it:</p> <ul> <li><strong>Faster</strong> to compute, as fewer features need to be processed.</li> <li><strong>Cheaper</strong> to store and deploy, especially on resource-constrained devices.</li> <li><strong>More interpretable</strong>, as it highlights the most important features.</li> <li><strong>Less prone to overfitting</strong>, since the reduced complexity often leads to better generalization.</li> </ul> <hr/> <h4 id="why-does-l_1-regularization-lead-to-sparsity"><strong>Why Does \(L_1\) Regularization Lead to Sparsity?</strong></h4> <p>A distinctive property of <strong>L1 regularization</strong> is its ability to produce sparse solutions, where some weights are exactly zero. This characteristic makes L1 regularization particularly useful for feature selection, as it effectively identifies the most important features by eliminating irrelevant ones. To understand this better, let’s explore the theoretical underpinnings and geometric intuition behind this phenomenon.</p> <h6 id="revisiting-lasso-regression"><strong>Revisiting Lasso Regression:</strong></h6> <p>Lasso regression penalizes the <strong>L1 norm</strong> of the weights. The objective function, also known as the <strong>Tikhonov form</strong>, is given by:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \big(w^T x_i - y_i\big)^2 + \lambda \|w\|_1\] <p>Here, the L1 norm is defined as:</p> \[\|w\|_1 = |w_1| + |w_2| + \dots + |w_d|\] <p>This formulation encourages sparsity by applying a uniform penalty across all weights, effectively “pushing” some weights to zero when they contribute minimally to the prediction.</p> <h6 id="regularization-as-constrained-empirical-risk-minimization-erm"><strong>Regularization as Constrained Empirical Risk Minimization (ERM)</strong></h6> <p>Regularization can also be viewed through the lens of <strong>constrained ERM</strong>. For a given complexity measure \(\Omega\) and a fixed threshold \(r \geq 0\), the optimization problem is expressed as:</p> \[\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i) \quad \text{s.t.} \quad \Omega(f) \leq r\] <p>In the case of Lasso regression, this is equivalent to the <strong>Ivanov form</strong>:</p> \[\hat{w} = \arg\min_{\|w\|_1 \leq r} \frac{1}{n} \sum_{i=1}^n \big(w^T x_i - y_i\big)^2\] <p>Here, \(r\) plays the same role as the regularization parameter \(\lambda\) in the penalized ERM (Tikhonov) form. The choice between these forms depends on whether the complexity is penalized directly or constrained explicitly.</p> <h5 id="the-ℓ1-and-ℓ2-norm-constraints"><strong>The ℓ1 and ℓ2 Norm Constraints</strong></h5> <p>To understand why L1 regularization promotes sparsity, consider a simple hypothesis space \(\mathcal{F} = \{f(x) = w_1x_1 + w_2x_2\}\). Each function can be represented as a point \((w_1, w_2)\) in \(\mathbb{R}^2\). The regularization constraints can be visualized as follows:</p> <ul> <li><strong>L2 norm constraint:</strong> \(w_1^2 + w_2^2 \leq r\), which is a <strong>circle</strong> in \(\mathbb{R}^2\).</li> <li><strong>L1 norm constraint:</strong> \(|w_1| + |w_2| \leq r\), which forms a <strong>diamond</strong> in \(\mathbb{R}^2\).</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_1-480.webp 480w,/assets/img/L1_Reg_1-800.webp 800w,/assets/img/L1_Reg_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><code class="language-plaintext highlighter-rouge">Note</code>: The sparse solutions correspond to the vertices of the diamond, where at least one weight is zero.</p> <p><strong>To build intuition</strong>, let’s analyze the geometry of the optimization:</p> <ol> <li>The <strong>blue region</strong> represents the feasible space defined by the regularization constraint (e.g., \(w_1^2 + w_2^2 \leq r\) for L2, or \(|w_1| + |w_2| \leq r\) for L1).</li> <li> <p>The <strong>red contours</strong> represent the level sets of the empirical risk function:</p> \[\hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \big(w^T x_i - y_i\big)^2\] </li> </ol> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_2_1-480.webp 480w,/assets/img/L1_Reg_2_1-800.webp 800w,/assets/img/L1_Reg_2_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_2_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_2_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_2_2-480.webp 480w,/assets/img/L1_Reg_2_2-800.webp 800w,/assets/img/L1_Reg_2_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_2_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_2_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The optimal solution is found where the smallest contour intersects the feasible region. For L1 regularization, this intersection tends to occur at the corners of the diamond, where one or more weights are exactly zero.</p> <p>Suppose the loss contours grow as perfect circles (or spheres in higher dimensions). When these contours intersect the diamond-shaped feasible region of L1 regularization, the corners of the diamond are more likely to be touched. These corners correspond to solutions where at least one weight is zero.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_3_2-480.webp 480w,/assets/img/L1_Reg_3_2-800.webp 800w,/assets/img/L1_Reg_3_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_3_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_3_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_3_1-480.webp 480w,/assets/img/L1_Reg_3_1-800.webp 800w,/assets/img/L1_Reg_3_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_3_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_3_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In contrast, for L2 regularization, the feasible region is a circle (or sphere), and the intersection is equally likely to occur in any direction. This results in small, but non-zero, weights across all features, rather than sparse solutions.</p> <h6 id="optimization-perspective"><strong>Optimization Perspective:</strong></h6> <p>From an optimization viewpoint, the difference between L1 and L2 regularization lies in how the penalty affects the gradient:</p> <ul> <li>For <strong>L2 regularization</strong>, as a weight \(w_i\) becomes smaller, the penalty \(\lambda w_i^2\) decreases more rapidly. However, the gradient of the penalty also diminishes, providing less incentive to shrink the weight to exactly zero.</li> <li>For <strong>L1 regularization</strong>, the penalty \(\lambda |w_i|\) decreases linearly, and its gradient remains constant regardless of the weight’s size. This consistent gradient drives small weights to zero, promoting sparsity.</li> </ul> <p><strong>Consider the following idea:</strong> Imagine you’re packing items into a small rectangular box, and you have two kinds of items: rigid boxes (representing \(L_1\) regularization) and pebbles (representing \(L_2\) regularization).</p> <p>The rigid boxes are shaped with sharp corners and don’t squish or deform. When you try to fit them into the small box, they naturally stack at the edges or corners of the space. This means some of the rigid boxes might not fit at all, so you leave them out—just like \(L_1\) regularization pushing weights to zero.</p> <p>The pebbles, on the other hand, are smooth and can be squished slightly. When you pack them into the box, they distribute evenly, filling in gaps without leaving any pebbles completely outside. This is like \(L_2\) regularization, where weights are reduced but not exactly zero.</p> <p>So, that’s why \(L_1\) regularization creates sparse solutions (only the most critical items get packed) while \(L_2\) regularization spreads the influence across all features (everything gets included, but smaller).</p> <h5 id="generalizing-to-ell_q-regularization"><strong>Generalizing to \(\ell_q\) Regularization</strong></h5> <p>\(\ell_1\) and \(\ell_2\) regularization are specific cases of the more general \(\ell_q\) regularization, defined as:</p> \[\|w\|_q^q = |w_1|^q + |w_2|^q + \dots + |w_d|^q\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/L1_Reg_4-480.webp 480w,/assets/img/L1_Reg_4-800.webp 800w,/assets/img/L1_Reg_4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/L1_Reg_4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="L1_Reg_4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here are some notable cases:</p> <ul> <li>For \(q \geq 1\), \(\|w\|_q\) is a valid norm.</li> <li>For \(0 &lt; q &lt; 1\), the constraint becomes non-convex, making optimization challenging. While \(\ell_q\) regularization with \(q &lt; 1\) can induce even sparser solutions than L1, it is often impractical in real-world scenarios. For instance when \(q=0.5\), the regularization takes the form of a square root function, which is non-convex.</li> <li>The \(\ell_0\) norm, defined as the number of non-zero weights, corresponds to <strong>subset selection</strong> but is computationally infeasible due to its combinatorial nature.</li> </ul> <p><strong>Note:</strong> \(L_n\)and \(\ell_n\) represent the same concept, so don’t let the difference in notation confuse you.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>\(L_1\) regularization’s sparsity-inducing property makes it an indispensable tool in feature selection and high-dimensional problems. Its optimization characteristics and ability to simplify models while retaining interpretability set it apart from \(L_2\) regularization.</p> <p>Next, we’ll talk about the <strong>maximum margin classifier &amp; SVM</strong>. Stay tuned, as moving on, it’s going to get a little intense, but don’t worry—we’ll get through it together!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models">why-l1-norm-for-sparse-models</a></li> <li><a href="https://blog.mlreview.com/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a">L1 Norm Regularization and Sparsity Explained for Dummies</a></li> <li><a href="https://math.stackexchange.com/questions/1904767/why-small-l1-norm-means-sparsity">why-small-l1-norm-means-sparsity</a></li> <li><a href="https://medium.com/analytics-vidhya/regularization-path-using-lasso-regression-c450eea9321e">Regularization path using Lasso regression</a></li> <li>Image Credits: Mairal et al.’s Sparse Modeling for Image and Vision Processing Fig 1.6, KPM Fig. 13</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.]]></summary></entry><entry><title type="html">Regularization - Balancing Model Complexity and Overfitting</title><link href="https://monishver11.github.io/blog/2025/regularization/" rel="alternate" type="text/html" title="Regularization - Balancing Model Complexity and Overfitting"/><published>2025-01-03T16:39:00+00:00</published><updated>2025-01-03T16:39:00+00:00</updated><id>https://monishver11.github.io/blog/2025/regularization</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/regularization/"><![CDATA[<p>When building machine learning models, one of the core challenges is finding the right balance between <strong>approximation error</strong> and <strong>estimation error</strong>. The trade-off can be understood in terms of the size and complexity of the hypothesis space, denoted by \(F\).</p> <p>On the one hand, a <strong>larger hypothesis space</strong> allows the model to better approximate the true underlying function. However, this flexibility comes at a cost: the risk of overfitting the training data, especially if the dataset is small. On the other hand, a <strong>smaller hypothesis space</strong> is less prone to overfitting, but it may lack the expressiveness needed to capture the true relationship between inputs and outputs, leading to higher approximation error.</p> <p>To control this trade-off, we need a way to quantify and limit the complexity of \(F\). This can be done in various ways, such as limiting the number of variables or restricting the degree of polynomials in a model.</p> <hr/> <h4 id="how-to-control-model-complexity"><strong>How to Control Model Complexity</strong></h4> <p>A common strategy to manage complexity involves learning a sequence of models with increasing levels of sophistication. Mathematically, this sequence can be represented as:</p> \[F_1 \subset F_2 \subset \dots \subset F_n \subset F\] <p>where each subsequent space, \(F_i\), is a superset of the previous one, representing models of greater complexity.</p> <p>For example, consider polynomial regression. The full hypothesis space, \(F\), includes all polynomial functions, while \(F_d\) is restricted to polynomials of degree \(\leq d\). By increasing \(d\), we explore more complex models within the same overarching hypothesis space.</p> <p>Once this sequence of models is defined, we evaluate them using a scoring metric, such as validation error, to identify the one that best balances complexity and accuracy. This approach ensures a systematic way to control overfitting while retaining sufficient expressive power.</p> <hr/> <h4 id="feature-selection-in-linear-regression"><strong>Feature Selection in Linear Regression</strong></h4> <p>In linear regression, the concept of nested hypothesis spaces is closely tied to <strong>feature selection</strong>. The idea is to construct a series of models using progressively fewer features:</p> \[F_1 \subset F_2 \subset \dots \subset F_n \subset F\] <p>where \(F\) represents models that use all available features, and \(F_d\) contains models using fewer than \(d\) features.</p> <p>For example, if we have two features, \(\{X_1, X_2\}\), we can train models using the subsets \(\{\}\), \(\{X_1\}\), \(\{X_2\}\), and \(\{X_1, X_2\}\). Each subset corresponds to a different hypothesis space, and the goal is to select the one that performs best according to a validation score.</p> <p>However, this approach quickly becomes computationally infeasible as the number of features grows. Exhaustively searching through all subsets of features leads to a combinatorial explosion, making it impractical for datasets with many features.</p> <h5 id="greedy-feature-selection-methods"><strong>Greedy Feature Selection Methods</strong></h5> <p>To overcome the inefficiency of exhaustive search, greedy algorithms such as forward selection and backward selection are commonly used.</p> <h6 id="forward-selection"><strong>Forward Selection</strong></h6> <p>Forward selection begins with an empty set of features and incrementally adds the most promising feature at each step. Initially, the model contains no features, represented as \(S = \{\}\). At each iteration:</p> <ol> <li> <p>For every feature not in the current set \(S\), a model is trained using the combined set \(S \cup \{i\}\).</p> </li> <li> <p>The performance of the model is evaluated, and a score, \(\alpha_i\), is assigned to each feature.</p> </li> <li> <p>The feature \(j\) with the highest score is added to the set, provided it improves the model’s performance.</p> </li> <li> <p>This process repeats until adding more features no longer improves the score.</p> </li> </ol> <h6 id="backward-selection"><strong>Backward Selection</strong></h6> <p>Backward selection starts at the opposite end of the spectrum. Instead of beginning with an empty set, it starts with all available features, \(S = \{X_1, X_2, \dots, X_p\}\). At each step, the feature that contributes the least to the model’s performance is removed. This process continues until no further removals improve the model’s score.</p> <h6 id="reflections-on-feature-selection"><strong>Reflections on Feature Selection</strong></h6> <p>Feature selection provides a natural way to control the complexity of a linear prediction function by limiting the number of features. The overarching goal is to strike a balance between minimizing training error and controlling model complexity, often through a scoring metric that incorporates both factors.</p> <p>While forward and backward selection methods are intuitive and computationally efficient, they have their limitations. For one, they do not guarantee finding the optimal subset of features. Additionally, the subsets selected by the two methods may differ, as the process is sensitive to the order in which features are evaluated.</p> <p>This brings us to an important question:</p> <blockquote> <p>Can feature selection be framed as a consistent optimization problem, leading to more robust and reliable solutions?</p> </blockquote> <p>In the next section, we explore how <strong>regularization</strong> offers a principled way to tackle this problem, providing a unified framework to balance model complexity and performance.</p> <hr/> <h4 id="l_1-and-l_2-regularization"><strong>\(L_1\) and \(L_2\) Regularization</strong></h4> <p>In the previous section, we discussed feature selection as a means to control model complexity. While effective, these methods are often computationally expensive and can lack consistency. Regularization offers a more systematic approach by introducing a <strong>complexity penalty</strong> directly into the objective function. This allows us to balance prediction performance with model simplicity in a principled manner.</p> <h5 id="complexity-penalty-balancing-simplicity-and-accuracy"><strong>Complexity Penalty: Balancing Simplicity and Accuracy</strong></h5> <p>The idea behind regularization is to augment the loss function with a penalty term that discourages overly complex models. For example, a scoring function for feature selection can be expressed as:</p> \[\text{score}(S) = \text{training_loss}(S) + \lambda |S|\] <p>where \(|S|\) is the number of selected features, and \(\lambda\) is a hyperparameter that controls the trade-off between training loss and complexity.</p> <p>A larger \(\lambda\) imposes a heavier penalty on complexity, meaning that adding an extra feature is only justified if it significantly improves the training loss—by at least \(\lambda\). This approach discourages the inclusion of unnecessary features, effectively shrinking the hypothesis space \(F\).</p> <p>However, directly using the number of features as a complexity measure is non-differentiable, making it hard to optimize. This limitation motivates the use of alternative measures, such as norms on the model weights, which provide a differentiable and computationally efficient framework.</p> <p><strong>Consider it like this:</strong>: Think of choosing ingredients for a dish. The training loss is like the flavor of the dish, and the penalty term is like the cost of adding ingredients. If you add too many ingredients (features), the cost goes up, and the dish may become overcomplicated or unbalanced. By introducing a penalty (regularization), you’re essentially saying, “Only add more ingredients if they significantly improve the flavor.” The larger the penalty (larger \(\lambda\)), the more careful you have to be about adding new ingredients, encouraging simplicity and preventing the dish from becoming too cluttered. This approach keeps the recipe (model) balanced and prevents unnecessary complexity.</p> <h6 id="soft-selection-through-weight-shrinkage"><strong>Soft Selection Through Weight Shrinkage</strong></h6> <p>Instead of hard feature selection, regularization encourages <strong>soft selection</strong> by penalizing the magnitude of the model weights. Consider a linear regression model:</p> \[f(x) = w^\top x\] <p>where \(w_i\) represents the weight for the \(i\)-th feature. If \(w_i\) is zero or close to zero, it effectively excludes the corresponding feature from the model.</p> <h6 id="why-shrink-weights"><strong>Why Shrink Weights?</strong></h6> <p>Intuitively, smaller weights make the model more stable. A regression line with a smaller slope produces smaller changes in the output for a given change in the input. This stability has two key benefits:</p> <ol> <li> <p><strong>Reduced Sensitivity to Noise:</strong> Smaller weights make the model less prone to overfitting, as predictions are less sensitive to fluctuations in the training data.</p> </li> <li> <p><strong>Better Generalization:</strong> By pushing weights toward zero, the model becomes less sensitive to variations in new datasets, improving its robustness.</p> </li> </ol> <h6 id="weight-shrinkage-in-polynomial-regression"><strong>Weight Shrinkage in Polynomial Regression</strong></h6> <p>In polynomial regression, where the \(n\)-th feature corresponds to the \(n\)-th power of \(x\), weight shrinkage plays a crucial role in preventing overfitting. For instance, consider two polynomial models:</p> \[\hat{y} = 0.001x^7 + 0.003x^3 + 1, \quad \text{and} \quad \hat{y} = 1000x^7 + 500x^3 + 1\] <p>The second model has large coefficients, making the curve “wiggle” excessively to fit the training data, a hallmark of overfitting. In contrast, the first model—with smaller weights—is smoother and less prone to overfitting.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Polynomial_Regression_Plot-480.webp 480w,/assets/img/Polynomial_Regression_Plot-800.webp 800w,/assets/img/Polynomial_Regression_Plot-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Polynomial_Regression_Plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Polynomial_Regression_Plot" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Function Plots in Desmos </div> <p><strong>Think of it this way</strong>: Imagine you’re driving a car down a winding road. A car with a sensitive steering wheel (large weights) will make sharp turns with every slight variation in the road, making the ride bumpy and unpredictable. In contrast, a car with a more stable, less sensitive steering wheel (smaller weights) will handle the same road with smoother, more controlled movements, reducing the impact of small bumps and ensuring a more stable journey. Similarly, in regression, smaller weights lead to smoother, more stable models that are less prone to overfitting and better at handling new data.</p> <h5 id="linear-regression-with-l_2-regularization"><strong>Linear Regression with \(L_2\) Regularization</strong></h5> <p>Let’s formalize this idea using linear regression. For a dataset \(D_n = \{(x_1, y_1), \dots, (x_n, y_n)\}\), the objective in ordinary least squares is to minimize the mean squared error:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2.\] <p>While effective, this approach can overfit when the number of features \(d\) is large compared to the number of samples \(n\). For example, in natural language processing, it’s common to have millions of features but only thousands of documents.</p> <p>To address this, <strong>\(L_2\) regularization</strong> (also known as <strong>ridge regression</strong>) adds a penalty on the \(L_2\) norm of the weights:</p> \[\hat{w} = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \left( w^\top x_i - y_i \right)^2 + \lambda \|w\|_2^2,\] <p>where:</p> \[\|w\|_2^2 = w_1^2 + w_2^2 + \dots + w_d^2\] <p>This additional term penalizes large weights, shrinking them toward zero. When \(\lambda = 0\), the solution reduces to ordinary least squares. As \(\lambda\) increases, the penalty grows, favoring simpler models with smaller weights.</p> <p><strong>Intuition</strong>: Think of fitting a suit to someone. In ordinary least squares, you would tailor the suit to fit perfectly according to every measurement. However, if the person has an unusual body shape or you have limited data, the suit might end up being too tight in some areas, causing discomfort. With \(L_2\) regularization, it’s like adding some flexibility to the design, allowing for slight adjustments to ensure the suit is comfortable and fits well, even if the measurements aren’t perfect. This prevents overfitting and makes the model more robust, much like a well-tailored suit that remains comfortable under different conditions.</p> <h6 id="generalization-to-other-models"><strong>Generalization to Other Models</strong></h6> <p>Although we’ve illustrated \(L_2\) regularization with linear regression, the concept extends naturally to other models, including neural networks. By penalizing the magnitude of weights, \(L_2\) regularization helps improve generalization across a wide range of machine learning tasks.</p> <hr/> <h6 id="closing-thoughts"><strong>Closing Thoughts</strong></h6> <p>Regularization, whether through weight shrinkage or complexity penalties, provides a robust mechanism to balance model expressiveness and generalization. In the next section, we’ll explore <strong>\(L_1\) regularization</strong>, its sparsity-inducing properties, and how it differs from \(L_2\) regularization.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.]]></summary></entry><entry><title type="html">Loss Functions - Regression and Classification</title><link href="https://monishver11.github.io/blog/2025/loss-functions/" rel="alternate" type="text/html" title="Loss Functions - Regression and Classification"/><published>2025-01-02T21:28:00+00:00</published><updated>2025-01-02T21:28:00+00:00</updated><id>https://monishver11.github.io/blog/2025/loss-functions</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/loss-functions/"><![CDATA[<p>Loss functions are of critical importance to machine learning, guiding models to minimize errors and improve predictions. They quantify how far off a model’s predictions are from the actual outcomes and serve as the basis for optimization. In this post, we’ll explore loss functions for <strong>regression</strong> and <strong>classification</strong> problems, breaking down their mathematical foundations and building intuitive understanding along the way. We will then transition our focus to logistic regression, examining its relationship with loss functions in classification tasks.</p> <hr/> <h4 id="loss-functions-for-regression"><strong>Loss Functions for Regression</strong></h4> <p>Regression tasks focus on predicting continuous values. Think about forecasting stock prices, estimating medical costs based on patient details, or predicting someone’s age from their photograph. These problems share a common requirement: accurately measuring how close the predicted values are to the true values.</p> <h6 id="setting-the-stage-notation"><strong>Setting the Stage: Notation</strong></h6> <p>Before diving in, let’s clarify the notation:</p> <ul> <li>\(\hat{y}\) represents the predicted value (the model’s output).</li> <li>\(y\) denotes the actual observed value (the ground truth).</li> </ul> <p>A <strong>loss function</strong> for regression maps the predicted and actual values to a real number: \(\ell(\hat{y}, y) \in \mathbb{R}.\) Most regression losses are based on the <strong>residual</strong>, defined as:</p> \[r = y - \hat{y}\] <p>The residual captures the difference between the true value and the prediction.</p> <h6 id="what-makes-a-loss-function-distance-based"><strong>What Makes a Loss Function Distance-Based?</strong></h6> <p>A loss function is <strong>distance-based</strong> if it meets two criteria:</p> <ol> <li>It depends solely on the residual: \(\ell(\hat{y}, y) = \psi(y - \hat{y}),\) where \(\psi: \mathbb{R} \to \mathbb{R}.\)</li> <li>It equals zero when the residual is zero: \(\psi(0) = 0.\)</li> </ol> <p>Such loss functions are <strong>translation-invariant</strong>, meaning they remain unaffected if both the prediction and the actual value are shifted by the same amount: \(\ell(\hat{y} + b, y + b) = \ell(\hat{y}, y), \quad \forall b \in \mathbb{R}.\)</p> <p>However, in some scenarios, translation invariance may not be desirable. For example, using the <strong>relative error</strong>: \(\text{Relative error} = \frac{\hat{y} - y}{y},\) provides a loss function better suited to cases where proportional differences matter.</p> <p>For instance:</p> <ul> <li>If the actual stock price is $100, and your model predicts $110, the absolute error is $10, but the relative error is 10%.</li> <li>But, if the actual stock price is $10, and your model predicts $11, the absolute error is still $1, but the relative error is 10%.</li> </ul> <h5 id="exploring-common-loss-functions-for-regression"><strong>Exploring Common Loss Functions for Regression</strong></h5> <h6 id="1-squared-loss-l2-loss"><strong>1. Squared Loss (L2 Loss)</strong></h6> <p>Squared loss is one of the most widely used loss functions:</p> \[\ell(r) = r^2 = (y - \hat{y})^2\] <p>This loss penalizes large residuals more heavily, making it sensitive to outliers. Its simplicity and differentiability make it popular in linear regression and similar models.</p> <h6 id="2-absolute-loss-l1-loss"><strong>2. Absolute Loss (L1 Loss)</strong></h6> <p>Absolute loss measures the magnitude of the residual:</p> \[\ell(r) = |r| = |y - \hat{y}|\] <p>Unlike squared loss, absolute loss is robust to outliers but lacks smooth differentiability.</p> <p><strong>Think of it this way</strong>: Imagine predicting house prices based on size. If one house in the dataset has an extremely high price (an outlier), using absolute loss will make the model focus more on the typical pricing pattern of most houses and ignore the outlier. In contrast, least squares regression would try to minimize the error caused by that outlier, potentially distorting the model.</p> <h6 id="3-huber-loss"><strong>3. Huber Loss</strong></h6> <p>The Huber loss combines the best of both worlds:</p> \[\ell(r) = \begin{cases} \frac{1}{2}r^2 &amp; \text{if } |r| \leq \delta, \\ \delta |r| - \frac{1}{2}\delta^2 &amp; \text{if } |r| &gt; \delta. \end{cases}\] <p>For small residuals, it behaves like squared loss, while for large residuals, it switches to absolute loss, providing robustness without sacrificing differentiability. <strong>Note</strong>: Equal values and slopes at \((r = \delta)\).</p> <p><strong>Understanding Robustness</strong>: It describes a loss function’s resistance to the influence of outliers.</p> <ul> <li><strong>Squared loss</strong> is highly sensitive to outliers.</li> <li><strong>Absolute loss</strong> is much more robust.</li> <li><strong>Huber loss</strong> strikes a balance between sensitivity and robustness. Meaning, it is sensitive enough to provide a useful gradient for smaller errors (via L2), but becomes more robust to large residuals, preventing them from disproportionately influencing the model (via L1).</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Regression_Losses-480.webp 480w,/assets/img/Regression_Losses-800.webp 800w,/assets/img/Regression_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Regression_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Regression_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Regression Loss Functions </div> <hr/> <h4 id="loss-functions-for-classification"><strong>Loss Functions for Classification</strong></h4> <p>Classification tasks involve predicting discrete labels. For instance, we might want to decide whether an email is spam or if an image contains a cat. The challenge lies in guiding the model to make accurate predictions while quantifying the degree of correctness.</p> <h6 id="the-role-of-the-score-function"><strong>The Role of the Score Function</strong></h6> <p>In binary classification, the model predicts a score, \(f(x)\), for each input \(x\):</p> <ul> <li>If \(f(x) &gt; 0\), the model predicts the label \(1\).</li> <li>If \(f(x) &lt; 0\), the model predicts the label \(-1\).</li> </ul> <p>This score represents the model’s confidence, and its magnitude indicates how certain the prediction is.</p> <h6 id="what-is-the-margin"><strong>What is the Margin?</strong></h6> <p>The <strong>margin</strong> captures the relationship between the predicted score and the true label:</p> \[m = y\hat{y}\] <p>or equivalently:</p> \[m = yf(x)\] <p>The margin measures correctness:</p> <ul> <li><strong>Positive margin</strong>: The prediction is correct.</li> <li><strong>Negative margin</strong>: The prediction is incorrect.</li> </ul> <p>The goal of many classification tasks is to maximize this margin, ensuring confident and accurate predictions.</p> <h5 id="common-loss-functions-for-classification"><strong>Common Loss Functions for Classification</strong></h5> <h6 id="1-0-1-loss"><strong>1. 0-1 Loss</strong></h6> <p>The 0-1 loss is a simple yet impractical loss function:</p> \[\ell(y, \hat{y}) = \begin{cases} 0 &amp; \text{if } y = \hat{y} \\ 1 &amp; \text{if } y \neq \hat{y} \end{cases}\] <p>Alternatively,</p> \[\ell_{0-1}(f(x), y) = \mathbf{1}[yf(x) \leq 0]\] <p>Here, \(\mathbf{1}\) is the indicator function, which equals 1 if the condition is true and 0 otherwise.</p> <p>Although intuitive, the 0-1 loss is:</p> <ul> <li><strong>Non-convex</strong>, making optimization difficult, because its value is either 0 or 1, which creates a step-like behavior.</li> <li><strong>Non-differentiable</strong>, rendering gradient-based methods inapplicable. For instance, if \(\hat{y} = 0.5\), the loss could change abruptly from 0 to 1 depending on whether the true label \(y\) is 0 or 1, leading to no gradient at this boundary.</li> </ul> <h6 id="2-hinge-loss"><strong>2. Hinge Loss</strong></h6> <p>Hinge loss, commonly used in Support Vector Machines (SVMs), addresses the limitations of 0-1 loss:</p> \[\ell_{\text{Hinge}}(m) = \max(1 - m, 0)\] <p>It is a convex, upper bound on 0-1 loss and encourages a positive margin. However, it is not differentiable at \(m = 1\).</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Classification_Losses-480.webp 480w,/assets/img/Classification_Losses-800.webp 800w,/assets/img/Classification_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Classification_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classification_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Classification Loss Functions </div> <hr/> <h4 id="diving-deeper-logistic-regression"><strong>Diving Deeper: Logistic Regression</strong></h4> <p>In our exploration above, we’ve covered the basics of regression and classification losses. Now, let’s shift our focus to <strong>logistic regression</strong> and its corresponding loss functions, which are pivotal in classification problems. We’ll also touch on why square loss isn’t typically used for classification.</p> <p>Despite its name, <strong>logistic regression</strong> is not actually a regression algorithm—it’s a <strong>linear classification</strong> method. Logistic regression predicts probabilities, making it well-suited for binary classification problems.</p> <p>The predictions are modeled using the <strong>sigmoid function</strong>, denoted by \(\sigma(z)\), where:</p> \[\sigma(z) = \frac{1}{1 + \exp(-z)}\] <p>and \(z = f(x) = w^\top x\) is the score computed from the input features and weights.</p> <h5 id="logistic-regression-with-labels-as-0-or-1"><strong>Logistic Regression with Labels as 0 or 1</strong></h5> <p>When the labels are in \(\{0, 1\}\):</p> <ul> <li> <p>The predicted probability is: \(\hat{y} = \sigma(z)\)</p> </li> <li> <p>The loss function for logistic regression in this case is the <strong>binary cross-entropy loss</strong>:</p> \[\ell_{\text{Logistic}} = -y \log(\hat{y}) - (1 - y) \log(1 - \hat{y})\] </li> </ul> <p>Here’s how it works based on different predicted values of \(\hat{y}\):</p> <ul> <li><strong>If \(y = 1\) (True label is 1)</strong>: <ul> <li> <p>The loss is:</p> \[\ell_{\text{Logistic}} = -\log(\hat{y})\] <p>This means if the predicted probability \(\hat{y}\) is close to 1 (i.e., the model is confident that the class is 1), the loss will be very small (approaching 0). On the other hand, if \(\hat{y}\) is close to 0, the loss becomes large, penalizing the model for being very wrong.</p> </li> </ul> </li> <li><strong>If \(y = 0\) (True label is 0)</strong>: <ul> <li> <p>The loss is:</p> \[\ell_{\text{Logistic}} = -\log(1 - \hat{y})\] <p>In this case, if the predicted probability \(\hat{y}\) is close to 0 (i.e., the model correctly predicts the class as 0), the loss will be very small (approaching 0). However, if \(\hat{y}\) is close to 1, the loss becomes large, penalizing the model for incorrectly predicting class 1.</p> </li> </ul> </li> </ul> <h6 id="example-of-different-predicted-values">Example of Different Predicted Values:</h6> <ol> <li><strong>For a true label \(y = 1\):</strong> <ul> <li> <p>If \(\hat{y} = 0.9\): \(\ell_{\text{Logistic}} = -\log(0.9) \approx 0.105\) This is a small loss, since the model predicted a high probability for class 1, which is correct.</p> </li> <li> <p>If \(\hat{y} = 0.1\): \(\ell_{\text{Logistic}} = -\log(0.1) \approx 2.302\) This is a large loss, since the model predicted a low probability for class 1, which is incorrect.</p> </li> </ul> </li> <li><strong>For a true label \(y = 0\):</strong> <ul> <li> <p>If \(\hat{y} = 0.1\): \(\ell_{\text{Logistic}} = -\log(1 - 0.1) \approx 0.105\) This is a small loss, since the model predicted a low probability for class 1, which is correct.</p> </li> <li> <p>If \(\hat{y} = 0.9\): \(\ell_{\text{Logistic}} = -\log(1 - 0.9) \approx 2.302\) This is a large loss, since the model predicted a high probability for class 1, which is incorrect.</p> </li> </ul> </li> </ol> <h6 id="key-points">Key Points:</h6> <ul> <li>The <strong>negative sign</strong> in the loss function ensures that when the model predicts correctly (i.e., \(\hat{y}\) is close to the true label), the loss is minimized (approaching 0).</li> <li>The loss grows as the predicted probability \(\hat{y}\) moves away from the true label \(y\), and it grows more rapidly as the predicted probability becomes more confident but incorrect.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Binary_Cross_Entropy_Loss-480.webp 480w,/assets/img/Binary_Cross_Entropy_Loss-800.webp 800w,/assets/img/Binary_Cross_Entropy_Loss-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Binary_Cross_Entropy_Loss.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Binary_Cross_Entropy_Loss" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Binary Cross Entropy Loss Function(https://www.desmos.com/calculator/ygciza1leg) </div> <h5 id="logistic-regression-with-labels-as--1-or-1"><strong>Logistic Regression with Labels as -1 or 1</strong></h5> <p>When the labels are in \(\{-1, 1\}\), the sigmoid function simplifies using the property: \(1 - \sigma(z) = \sigma(-z).\)</p> <p>This allows us to express the loss equivalently as:</p> \[\ell_{\text{Logistic}} = \begin{cases} -\log(\sigma(z)) &amp; \text{if } y = 1, \\ -\log(\sigma(-z)) &amp; \text{if } y = -1. \end{cases}\] <p>Simplifying further:</p> \[\ell_{\text{Logistic}} = -\log(\sigma(yz)) = -\log\left(\frac{1}{1 + e^{-yz}}\right) = \log(1 + e^{-m})\] <p>where \(m = yz\) is the margin.</p> <h6 id="key-insights-of-logistic-loss"><strong>Key Insights of Logistic loss</strong>:</h6> <ul> <li>Is differentiable, enabling gradient-based optimization.</li> <li>Always rewards larger margins, encouraging more confident predictions.</li> <li>Never becomes zero, ensuring continuous optimization pressure.</li> </ul> <h6 id="what-about-square-loss-for-classification"><strong>What About Square Loss for Classification?</strong></h6> <p>Square loss, while effective for regression, is rarely used for classification. Let’s break it down:</p> \[\ell(f(x), y) = (f(x) - y)^2\] <p>For binary classification where \(y \in \{-1, 1\}\), we can rewrite this in terms of the margin:</p> \[\ell(f(x), y) = (f(x) - y)^2 = f^2(x) - 2f(x)y + y^2.\] <p>Using the fact that \(y^2 = 1\):</p> \[\ell(f(x), y) = f^2(x) - 2f(x)y + 1 = (1 - f(x)y)^2 = (1 - m)^2.\] <h6 id="why-not-use-square-loss"><strong>Why Not Use Square Loss?</strong></h6> <p>Square loss heavily penalizes outliers, such as mislabeled examples, making it unsuitable for classification tasks where robust performance on noisy data is crucial.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Logistic_Regression_Losses-480.webp 480w,/assets/img/Logistic_Regression_Losses-800.webp 800w,/assets/img/Logistic_Regression_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Logistic_Regression_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Logistic_Regression_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Logistic Regression Loss Functions </div> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>Loss functions form the backbone of machine learning, providing a mathematical framework for optimization. A quick recap:</p> <ul> <li><strong>Regression Losses</strong>: <ul> <li>Squared (L2) loss: Sensitive to outliers.</li> <li>Absolute (L1) loss: Robust but non-differentiable.</li> <li>Huber loss: Balances robustness and smoothness.</li> </ul> </li> <li><strong>Classification Losses</strong>: <ul> <li>Hinge loss: Encourages a large positive margin (used in SVMs).</li> <li>Logistic loss: Differentiable and rewards confidence.</li> </ul> </li> </ul> <p>These concepts tie back to critical components of machine learning workflows, such as <strong>gradient descent</strong>, which relies on the properties of loss functions to update model parameters effectively.</p> <p>Up next, we’ll dive into <strong>Regularization</strong>, focusing on how it combats overfitting and improves model performance. Stay tuned!</p> ]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.]]></summary></entry><entry><title type="html">Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training</title><link href="https://monishver11.github.io/blog/2025/sgd-tips/" rel="alternate" type="text/html" title="Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training"/><published>2025-01-01T19:18:00+00:00</published><updated>2025-01-01T19:18:00+00:00</updated><id>https://monishver11.github.io/blog/2025/sgd-tips</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/sgd-tips/"><![CDATA[<p>This is a continuation of the previous blog, and the content presented here consists of notes extracted from <a href="https://leon.bottou.org/">L´eon Bottou’s</a> <a href="https://leon.bottou.org/publications/pdf/tricks-2012.pdf">Stochastic Gradient Descent Tricks</a>. If you haven’t read my previous blog, I recommend taking a look, as the notations used here are introduced there. Alternatively, if you’re comfortable with the notations, you can jump straight into the content. So, let’s get started!</p> <h5 id="stochastic-gradient-descent-sgd"><strong>Stochastic Gradient Descent (SGD)</strong></h5> <p>Stochastic Gradient Descent (SGD) is a simplified version of the standard gradient descent algorithm. Rather than computing the exact gradient of the entire cost function \(E_n(f_w)\), each iteration of SGD estimates this gradient based on a <strong>single randomly chosen example</strong> \(z_t\). The update rule for the weights \(w\) at each iteration is:</p> \[w_{t+1} = w_t - \gamma_t \nabla_w Q(z_t, w_t)\] <p>where \(\gamma_t\) is the learning rate, and \(Q(z_t, w_t)\) represents the cost function evaluated at the current weights \(w_t\) for the randomly selected example \(z_t\).</p> <h6 id="key-features-of-sgd"><strong>Key Features of SGD:</strong></h6> <ul> <li> <p><strong>Randomness</strong>: The algorithm’s stochastic nature means that the updates depend on the examples randomly picked at each iteration. This randomness introduces some noise into the optimization process, but it is hoped that the algorithm behaves like its expectation despite this noise.</p> </li> <li> <p><strong>On-the-Fly Computation</strong>: Since SGD does not need to store information about the examples visited in previous iterations, it can process examples one by one. This makes it suitable for online learning or deployed systems, where data can arrive sequentially, and the model is updated in real-time.</p> </li> <li> <p><strong>Expected Risk Optimization</strong>: In a deployed system, where examples are drawn randomly from the ground truth distribution, SGD directly optimizes the expected risk, which is the expected value of the loss function over all possible examples. So, to put in simply - Each loss computed during SGD updates serves as an approximation of the expected loss over the true data distribution, and with more examples, it gradually optimizes the expected risk.</p> </li> </ul> <h6 id="convergence-of-sgd"><strong>Convergence of SGD:</strong></h6> <p>The convergence of SGD has been studied extensively in the stochastic approximation literature. Convergence typically requires that the learning rates satisfy the conditions:</p> \[\sum_{t=1}^{\infty} \gamma_t^2 &lt; \infty \quad\] \[\text{(It means the total sum of learning rates must go to infinity over time, ensuring enough updates for convergence)}\] \[\text{and} \quad \sum_{t=1}^{\infty} \gamma_t = \infty\] \[\text{(This mean the sum of squared learning rates must remain finite, ensuring the updates become smaller and smaller as the algorithm proceeds)}\] <p>These conditions help strike the balance between making large enough updates early on to explore the parameter space, but small enough updates later on to fine-tune the model and avoid overshooting the optimum.</p> <p>The <strong>Robbins-Siegmund theorem</strong> offers a formal proof that, under the right conditions—such as appropriate decreasing learning rates—<strong>SGD will converge almost surely</strong>, even when the loss function is non-smooth. This includes cases where the loss function has discontinuities or sharp gradients, making SGD a robust optimization method.</p> <h6 id="convergence-speed"><strong>Convergence Speed:</strong></h6> <p>The speed of convergence in SGD is ultimately limited by the noisy gradient approximations. Several factors impact the rate at which the algorithm converges:</p> <ul> <li><strong>Learning Rate Decay</strong>: <ul> <li>If the learning rates decrease too slowly, the variance of the parameter estimates \(w_t\) decreases at a similarly slow rate. <strong>Why?</strong> If the learning rate decreases too slowly, updates remain large for too long, causing high variance in parameter estimates and preventing the algorithm from stabilizing near the optimum.</li> <li>If the learning rates decay too quickly, the parameter estimates \(w_t\) take a long time to approach the optimum. <strong>Why?</strong> If the learning rate decreases too quickly, updates become too small early on, leading to insufficient exploration of the parameter space and slow convergence to the optimum.</li> </ul> </li> <li><strong>Optimal Convergence Speed</strong>: <ul> <li>When the <strong>Hessian matrix</strong> of the cost function at the optimum is <strong>strictly positive definite</strong>, the best convergence rate is achieved using learning rates of the form \(\gamma_t \sim t^{-1}\). In this case, the expectation of the residual error \(\rho\) decreases at the same rate, i.e., \(E(\rho) \sim t^{-1}\). This rate is commonly observed in practice.</li> </ul> </li> <li><strong>Relaxed Assumptions</strong>: <ul> <li>When these regularity assumptions(like a positive definite hessian, smoothness and strong convexity) are relaxed, the convergence rate slows down. The theoretical convergence rate in such cases is typically \(E(\rho) \sim t^{-1/2}\). However, in practice, this slower convergence tends to only manifest during the final stages of the optimization process. Often, optimization is stopped before this stage is reached, making the slower convergence less significant.</li> </ul> </li> </ul> <p>In summary, while the convergence of SGD can be slow due to its noisy nature, proper management of the learning rate and understanding of the problem’s characteristics can ensure good performance in practice.</p> <hr/> <h5 id="second-order-stochastic-gradient-descent-2sgd"><strong>Second-Order Stochastic Gradient Descent (2SGD)</strong></h5> <p>Second-Order Stochastic Gradient Descent (2SGD) extends stochastic gradient descent by incorporating curvature information through a positive definite matrix \(\Gamma_t\), which approximates the inverse of the Hessian matrix. The update rule for 2SGD is:</p> \[w_{t+1} = w_t - \gamma_t \Gamma_t \nabla_w Q(z_t, w_t),\] <p>where:</p> <ul> <li>\(w_t\): Current weights at iteration \(t\).</li> <li>\(\gamma_t\): Learning rate (step size), which may vary over iterations.</li> <li>\(\Gamma_t\): A positive definite matrix that approximates the inverse of the Hessian.</li> <li>\(\nabla_w Q(z_t, w_t)\): Gradient of the loss function \(Q\) with respect to \(w_t\) for the stochastic sample \(z_t\).</li> </ul> <h6 id="key-advantages-of-2sgd"><strong>Key Advantages of 2SGD</strong></h6> <ol> <li><strong>Curvature Awareness</strong>: <ul> <li>The inclusion of \(\Gamma_t\) enables the algorithm to account for the curvature of the loss surface.</li> <li>This adaptation improves convergence by rescaling updates to balance faster progress in flat directions and slower progress in steep directions.</li> </ul> </li> <li><strong>Improved Constants</strong>: <ul> <li>The scaling introduced by \(\Gamma_t\) can reduce the condition number of the problem. <strong>What?</strong> The condition number is the ratio of the largest to smallest eigenvalue of the Hessian, reflecting the curvature’s uniformity. A high condition number implies uneven curvature, slowing convergence.</li> <li>2SGD addresses this by scaling the parameter space to reduce the condition number, making the optimization landscape more uniform and this leads to faster convergence in terms of iteration efficiency when compared to standard SGD.</li> </ul> </li> </ol> <h6 id="challenges-in-2sgd"><strong>Challenges in 2SGD</strong></h6> <p>Despite the advantages, 2SGD has significant limitations:</p> <ol> <li><strong>Stochastic Noise</strong>: <ul> <li>The introduction of \(\Gamma_t\) does not address the stochastic noise inherent in gradient estimates.</li> <li>As a result, the variance in the weights \(w_t\) remains high, which limits its convergence benefits.</li> </ul> </li> <li><strong>Asymptotic Behavior</strong>: <ul> <li>The expected residual error decreases at a rate of \(\mathbb{E}[\rho] \sim t^{-1}\) at best.</li> <li>While constants(i.e., step size) are improved, the convergence rate remains fundamentally constrained by the stochastic nature of the gradients.</li> </ul> </li> </ol> <h6 id="comparison-with-batch-algorithms"><strong>Comparison with Batch Algorithms</strong></h6> <p><strong>Batch Algorithms</strong>:</p> <ul> <li>Batch methods utilize the full dataset to compute gradients at each iteration.</li> <li>They achieve better asymptotic performance with convergence rates that often scale as \(t^{-2}\) or better, depending on the algorithm.</li> </ul> <p><strong>2SGD</strong>:</p> <ul> <li>2SGD operates on a per-sample basis, which limits its ability to achieve higher convergence rates in expectation.</li> <li>The variance introduced by stochastic gradients limits its asymptotic efficiency compared to batch methods.</li> </ul> <h6 id="the-bigger-picture"><strong>The Bigger Picture</strong></h6> <p>Despite being asymptotically slower than batch algorithms, 2SGD remains highly relevant in modern machine learning for many reasons:</p> <ol> <li><strong>Efficiency in Large Datasets</strong>: <ul> <li>When datasets are too large to process as a batch, 2SGD provides an efficient alternative.</li> <li>It avoids the computational and memory overhead of storing and processing the entire dataset.</li> </ul> </li> <li><strong>Online Learning</strong>: <ul> <li>In online learning scenarios, where data arrives sequentially, 2SGD offers a practical approach to updating models in real time.</li> </ul> </li> </ol> <h6 id="summary-of-convergence-behavior"><strong>Summary of Convergence Behavior</strong></h6> <hr/> <table> <thead> <tr> <th><strong>Algorithm</strong></th> <th><strong>Error Decay</strong></th> <th><strong>Asymptotic Behavior</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gradient Descent (GD)</strong></td> <td>\(|w_t - w^*| \sim \rho^t\)</td> <td>Linear convergence: \(\mathcal{O}(t^{-1})\)</td> </tr> <tr> <td><strong>Stochastic Gradient Descent (SGD)</strong></td> <td>\(\mathbb{E}[|w_t - w^*|] \sim t^{-1}\)</td> <td>Asymptotic rate: \(t^{-1}\)</td> </tr> <tr> <td><strong>Second-Order Stochastic GD (2SGD)</strong></td> <td>\(\mathbb{E}[|w_t - w^*|] \sim t^{-1}\)</td> <td>Same as SGD, but with improved constants</td> </tr> </tbody> </table> <hr/> <p>Note:</p> <ul> <li> <p><strong>Linear Convergence</strong> (\(\mathcal{O}(t^{-1})\)): Implies an exponential decay of the error over time, with the error shrinking by a constant factor at each step.</p> </li> <li> <p><strong>Asymptotic Rate</strong> (\(t^{-1}\)): Describes the long-term error decay rate, indicating a polynomial decay (slower than exponential) where the error decreases inversely with time.</p> </li> </ul> <p>By incorporating second-order information through \(\Gamma_t\), 2SGD makes more informed updates. However, its performance is ultimately limited by the stochastic noise in gradient estimates. In practice, 2SGD is a compromise between computational efficiency and convergence speed, making it suitable for large-scale and online learning tasks.</p> <h4 id="when-to-use-stochastic-gradient-descent-sgd"><strong>When to Use Stochastic Gradient Descent (SGD)</strong></h4> <p>Stochastic Gradient Descent (SGD) is particularly well-suited when <strong>training time is the bottleneck</strong>. It is an effective choice in scenarios where computational efficiency and scalability are critical, such as in large-scale machine learning tasks.</p> <h6 id="key-insights-from-table"><strong>Key Insights from Table</strong></h6> <p>The table below summarizes the asymptotic behavior of four optimization algorithms:</p> <ul> <li><strong>Gradient Descent (GD)</strong>: Standard first-order method.</li> <li><strong>Second-Order Gradient Descent (2GD)</strong>: Incorporates curvature information.</li> <li><strong>Stochastic Gradient Descent (SGD)</strong>: A stochastic variant of GD.</li> <li><strong>Second-Order Stochastic Gradient Descent (2SGD)</strong>: Combines stochastic updates with curvature adaptation.</li> </ul> <hr/> <table> <thead> <tr> <th><strong>Algorithm</strong></th> <th><strong>Time per Iteration</strong></th> <th><strong>Iterations to Accuracy (\(\rho\))</strong></th> <th><strong>Time to Accuracy (\(\rho\))</strong></th> <th><strong>Time to Excess Error \(\epsilon\)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gradient Descent (GD)</strong></td> <td>\(n\)</td> <td>\(\log(1 / \rho)\)</td> <td>\(n \log(1 / \rho)\)</td> <td>\(\frac{1}{\epsilon^{1/\alpha}} \log(1 / \epsilon)\)</td> </tr> <tr> <td><strong>Second-Order Gradient Descent (2GD)</strong></td> <td>\(n\)</td> <td>\(\log \log(1 / \rho)\)</td> <td>\(n \log \log(1 / \rho)\)</td> <td>\(\frac{1}{\epsilon^{1/\alpha}} \log(1 / \epsilon) \log \log(1 / \epsilon)\)</td> </tr> <tr> <td><strong>Stochastic Gradient Descent (SGD)</strong></td> <td>\(1\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \epsilon\)</td> </tr> <tr> <td><strong>Second-Order Stochastic GD (2SGD)</strong></td> <td>\(1\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \rho\)</td> <td>\(1 / \epsilon\)</td> </tr> </tbody> </table> <hr/> <h6 id="discussion"><strong>Discussion</strong></h6> <ol> <li><strong>Per-Iteration Cost</strong>: <ul> <li><strong>GD and 2GD</strong>: Both require \(\mathcal{O}(n)\) time per iteration due to full-batch gradient computations.</li> <li><strong>SGD and 2SGD</strong>: Require \(\mathcal{O}(1)\) time per iteration, making them computationally inexpensive for large datasets.</li> </ul> </li> <li><strong>Convergence Speed</strong>: <ul> <li>GD and 2GD converge faster in terms of the number of iterations but incur higher computational costs because of full-batch updates.</li> <li>SGD and 2SGD require more iterations to converge but compensate with lower per-iteration costs.</li> </ul> </li> <li><strong>Asymptotic Performance</strong>: <ul> <li>While SGD and 2SGD have worse optimization noise, they require significantly less time to achieve a predefined expected risk \(\epsilon\) due to their reduced computational overhead.</li> <li>In large-scale settings where computation time is the limiting factor, <strong>stochastic learning algorithms are asymptotically better</strong>.</li> </ul> </li> </ol> <h6 id="key-takeaways"><strong>Key Takeaways</strong></h6> <ul> <li>Use <strong>SGD</strong> when: <ul> <li>Dataset size is large, and full-batch methods become computationally infeasible.</li> <li>Real-time or online learning scenarios require frequent updates with minimal latency.</li> <li>Memory efficiency is a concern, as SGD processes one sample at a time.</li> </ul> </li> <li>Despite higher variance in updates, <strong>SGD and 2SGD</strong> are preferred in large-scale setups due to their faster convergence to the expected risk with minimal computational resources.</li> </ul> <p>In conclusion, while SGD and 2SGD might appear less efficient in small-scale setups, their practical advantages in high-dimensional, data-intensive tasks make them highly favorable in modern machine learning applications.</p> <hr/> <h4 id="general-recommendations-for-stochastic-gradient-descent-sgd"><strong>General Recommendations for Stochastic Gradient Descent (SGD)</strong></h4> <p>The following is a series of recommendations for using stochastic gradient algorithms. Though seemingly trivial, the author’s experience highlights how easily they can be overlooked.</p> <h5 id="1-randomly-shuffle-the-training-examples"><strong>1. Randomly Shuffle the Training Examples</strong></h5> <p>Although the theory behind Stochastic Gradient Descent (SGD) calls for picking examples randomly, it is often tempting to process them sequentially through the training set. While sequentially passing through the examples may seem like an optimization, it can be problematic when the data is structured in a way that affects training performance.</p> <h6 id="key-points"><strong>Key Points:</strong></h6> <ul> <li><strong>Class Grouping and Order</strong>: If training examples are grouped by class or presented in a particular order, processing them in sequence can lead to biases in the gradient updates.</li> <li><strong>The Importance of Randomization</strong>: Randomizing the order helps break any inherent structure or patterns in the dataset that may skew the learning process. This ensures that each update is less dependent on the order of the examples, promoting better convergence.</li> </ul> <h6 id="analogy"><strong>Analogy:</strong></h6> <p>Think of SGD like a person learning to navigate a maze. If they always follow the same path (training examples in order), they may become “stuck” in a loop. However, if they randomly choose different routes (randomized examples), they are more likely to explore and discover the optimal path.</p> <h5 id="2-use-preconditioning-techniques"><strong>2. Use Preconditioning Techniques</strong></h5> <p>Stochastic Gradient Descent (SGD) is a first-order optimization algorithm, meaning it only uses the first derivatives (gradients) to guide the updates. However, this can lead to significant issues when the optimization process encounters areas where the <strong>Hessian</strong> (the matrix of second derivatives) is ill-conditioned. In such regions, the gradients may not provide efficient updates, slowing down convergence or leading to poor results.</p> <p>Fortunately, <strong>preconditioning techniques</strong> like Adagrad or Adam, adjust the learning rates based on past gradients, helping optimize in ill-conditioned regions for faster and more stable convergence.</p> <h6 id="key-points-1"><strong>Key Points:</strong></h6> <ul> <li><strong>Ill-conditioned regions</strong>: Areas where the curvature (second derivatives) of the cost function varies dramatically, making it hard for SGD to make efficient progress.</li> <li><strong>Improved convergence</strong>: Preconditioning techniques can rescale the gradients to make the learning process more stable and faster, improving convergence even in difficult regions.</li> </ul> <h6 id="analogy-1"><strong>Analogy:</strong></h6> <p>Imagine trying to push a boulder up a steep hill (representing optimization in ill-conditioned areas). Without a proper approach, the effort may be inefficient or lead you off-course. Preconditioning techniques act like a ramp, providing a smoother path and making it easier to move the boulder in the right direction.</p> <h5 id="3-monitor-both-the-training-cost-and-the-validation-error"><strong>3. Monitor Both the Training Cost and the Validation Error</strong></h5> <p>To effectively gauge the performance of your model during training, it is crucial to monitor both the <strong>training cost</strong> and the <strong>validation error</strong>. A simple yet effective approach involves repeating the following steps:</p> <h6 id="key-steps"><strong>Key Steps:</strong></h6> <ol> <li> <p><strong>Stochastic Gradient Descent (SGD) Update</strong>: Process once through the shuffled training set and perform the SGD updates. This helps adjust the model’s parameters based on the current data.</p> </li> <li> <p><strong>Compute Training Cost</strong>: After the updates, run another loop over the training set to compute the <strong>training cost</strong>. This cost represents the criterion (such as the loss function) the algorithm is optimizing. Monitoring the training cost provides insight into how well the model is minimizing the objective.</p> </li> <li> <p><strong>Compute Validation Error</strong>: With another loop, calculate the <strong>validation error</strong> using the validation set. This error is the performance measure of interest (such as classification error, accuracy, etc.). The validation error helps track how well the model generalizes to unseen data.</p> </li> </ol> <p>Although these steps require additional computational effort, including extra passes over both the training and validation datasets, they provide critical feedback and prevent training in isolation, avoiding the risk of overfitting or diverging from the optimal solution.</p> <h6 id="analogy-2"><strong>Analogy:</strong></h6> <p>Think of training a model like tuning a musical instrument. The training cost is like checking the sound of the instrument while you play (adjusting and fine-tuning as you go), while the validation error is like getting feedback from a concert audience (seeing how the performance holds up in a real-world scenario). Without both, you might end up with a well-tuned instrument that doesn’t sound good in a performance.</p> <h5 id="4-check-the-gradients-using-finite-differences"><strong>4. Check the Gradients Using Finite Differences</strong></h5> <p>When the computation of gradients is slightly incorrect, Stochastic Gradient Descent (SGD) tends to behave slowly and erratically. This often leads to the misconception that such behavior is the normal operation of the algorithm.</p> <p>Over the years, many practitioners have sought advice on how to set the learning rates \(\gamma_t\) for a rebellious SGD program. However, the best advice is often to <strong>forget about the learning rates</strong> and ensure that the gradients are being computed correctly. Once gradients are correctly computed, setting small enough learning rates becomes easy. Those who struggle with tuning learning rates often have faulty gradients in their computations.</p> <h6 id="how-to-check-gradients-using-finite-differences"><strong>How to Check Gradients Using Finite Differences:</strong></h6> <p>Rather than manually checking each line of the gradient computation code, use finite differences to verify the accuracy of the gradients.</p> <h6 id="steps"><strong>Steps:</strong></h6> <ol> <li> <p><strong>Pick an Example</strong>: Choose a training example \(z\) from the dataset.</p> </li> <li> <p><strong>Compute the Loss</strong>: Calculate the loss function \(Q(z, w)\) for the current weights \(w\).</p> </li> <li> <p><strong>Compute the Gradient</strong>: Calculate the gradient of the loss with respect to the weights: \(g = \nabla_w Q(z, w)\)</p> </li> <li><strong>Apply a Perturbation</strong>: Slightly perturb the weights by changing them. This can be done by either: <ul> <li>Changing a single weight by a small increment: \(w' = w + \delta\)</li> <li>Perturbing the weights using the gradient: \(w' = w - \gamma g\), where \(\gamma\) is small enough.</li> </ul> </li> <li> <p><strong>Compute the New Loss</strong>: After applying the perturbation, compute the new loss \(Q(z, w')\).</p> </li> <li><strong>Verify the Approximation</strong>: Ensure that the new loss approximates the original loss plus the perturbation multiplied by the gradient: \(Q(z, w') \approx Q(z, w) + \delta g\)</li> </ol> <p><strong>Example</strong>, consider the MSE loss function:</p> \[Q(z, w) = (w - z)^2\] <ol> <li><strong>Pick an Example</strong>: Let ( z = 5 ), ( w = 4 ).</li> <li><strong>Compute the Loss</strong>: \(Q(z, w) = (4 - 5)^2 = 1\)</li> <li><strong>Compute the Gradient</strong>: \(g = \nabla_w Q(z, w) = 2(w - z) = -2\)</li> <li><strong>Apply Perturbation</strong>: \(w' = w + 0.01 = 4.01\)</li> <li><strong>Compute the New Loss</strong>: \(Q(z, w') = (4.01 - 5)^2 = 0.9801\)</li> <li><strong>Verify</strong>: \(Q(z, w') \approx Q(z, w) + 0.01 \cdot (-2) = 0.98\)</li> </ol> <h6 id="automating-the-process"><strong>Automating the Process:</strong></h6> <p>This process can be automated and should be repeated for many examples \(z\), many perturbations \(\delta\), and many initial weights \(w\). Often, flaws in the gradient computation only appear under peculiar conditions, and it’s not uncommon to discover such bugs in SGD code that has been used for years without issue.</p> <h6 id="analogy-3"><strong>Analogy:</strong></h6> <p>Think of gradient checking like testing the brakes of a car. If the brakes (gradients) are faulty, the car (SGD) might not stop properly, leading to erratic behavior. Instead of repeatedly adjusting the speed (learning rate), you test the brakes by applying a small perturbation to the system (finite differences). If the brakes are working well, the car will stop smoothly at the right place (convergence).</p> <h5 id="5-experiment-with-the-learning-rates-gamma_t-using-a-small-sample-of-the-training-set"><strong>5. Experiment with the Learning Rates \(\gamma_t\) Using a Small Sample of the Training Set</strong></h5> <p>The mathematics behind Stochastic Gradient Descent (SGD) are surprisingly independent of the training set size. Specifically, the asymptotic convergence rates of SGD are not influenced by the sample size. This means that once you’ve ensured the gradients are correct, the most effective way to determine appropriate learning rates is to experiment with a <strong>small, but representative</strong> sample of the training set.</p> <h6 id="key-steps-1"><strong>Key Steps:</strong></h6> <ol> <li> <p><strong>Use a Small Sample</strong>: Select a small subset of the training data that still reflects the diversity of the full dataset. The small size allows you to test different learning rates quickly without incurring the computational cost of working with the entire dataset.</p> </li> <li> <p><strong>Traditional Optimization Methods</strong>: Since the sample is small, you can apply traditional optimization techniques (e.g., gradient descent or other optimization algorithms) to find a reference point and set the training cost target. This provides a useful benchmark for SGD.</p> </li> <li> <p><strong>Refining Learning Rates</strong>: Experiment with various learning rates on this small dataset to find a value that minimizes the training cost efficiently. Once you identify a good learning rate, it’s likely to work well on the full dataset.</p> </li> <li> <p><strong>Scale to Full Dataset</strong>: Once the learning rates are set based on the small sample, use the same rates on the full training set. Keep in mind that the performance on the validation set is expected to plateau after a number of epochs. The number of epochs required to reach this plateau should be roughly the same as what was needed on the small dataset.</p> </li> </ol> <h6 id="analogy-4"><strong>Analogy:</strong></h6> <p>Think of this like testing the settings of a new recipe. Instead of preparing a full meal, you start with a small portion of ingredients (a small sample). Once you find the perfect amount of seasoning (learning rates), you can apply it to the full dish (the entire training set). While the small sample may not capture every nuance of the full dish, it gives you a good starting point without wasting resources.</p> <hr/> <p>This concludes the key points related to Stochastic Gradient Descent (SGD). After iterating through Gradient Descent (GD) and SGD multiple times, I hope the concepts are now firmly imprinted, even if briefly. In the upcoming blog posts, we will delve into loss functions and regression, so stay tuned!</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.]]></summary></entry><entry><title type="html">Gradient Descent Convergence - Prerequisites and Detailed Derivation</title><link href="https://monishver11.github.io/blog/2024/gd-convergence/" rel="alternate" type="text/html" title="Gradient Descent Convergence - Prerequisites and Detailed Derivation"/><published>2024-12-29T01:44:00+00:00</published><updated>2024-12-29T01:44:00+00:00</updated><id>https://monishver11.github.io/blog/2024/gd-convergence</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/gd-convergence/"><![CDATA[<p>To understand the <strong>Convergence Theorem for Fixed Step Size</strong>, it is essential to grasp a few foundational concepts like <strong>Lipschitz continuity</strong> and <strong>convexity</strong>. This section introduces these concepts and establishes the necessary prerequisites.</p> <p><strong>Quick note:</strong> If you find yourself struggling with any part or step, don’t worry—just copy and paste it into ChatGPT or Perplexity for an explanation. In most cases, you’ll be able to grasp the concept and move forward. If you’re still stuck, feel free to ask for help. The key is not to let small obstacles slow you down—keep going and seek assistance when needed!</p> <hr/> <h4 id="lipschitz-continuity"><strong>Lipschitz Continuity?</strong></h4> <p>At its core, Lipschitz continuity imposes a <strong>limit on how fast a function can change</strong>. Mathematically, a function \(g : \mathbb{R}^d \to \mathbb{R}\) is said to be <strong>Lipschitz continuous</strong> if there exists a constant \(L &gt; 0\) such that:</p> \[\|g(x) - g(x')\| \leq L \|x - x'\|, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This means the function’s rate of change is bounded by \(L\). For differentiable functions, Lipschitz continuity is often applied to the gradient. If \(\nabla f(x)\) is Lipschitz continuous with constant \(L &gt; 0\), then:</p> \[\|\nabla f(x) - \nabla f(x')\| \leq L \|x - x'\|, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This ensures the gradient does not change too rapidly, which is crucial for the convergence of optimization algorithms like gradient descent.</p> <h6 id="intuition-behind-lipschitz-continuity"><strong>Intuition Behind Lipschitz Continuity</strong></h6> <ol> <li><strong>Bounding the Slope</strong>: Lipschitz continuity ensures that the slope of the function (or the steepness of the graph) is bounded by \(L\). You can think of it as saying, “No part of the function can change too steeply.”</li> <li><strong>Gradient Smoothness</strong>: For \(\nabla f(x)\), Lipschitz continuity means the gradient varies smoothly between nearby points. This avoids abrupt jumps or erratic behavior in the optimization landscape.</li> </ol> <h6 id="visual-way-to-think-about-it"><strong>Visual Way to Think About It</strong></h6> <p>Imagine walking along a path represented by the graph of \(f(x)\). Lipschitz continuity guarantees:</p> <ul> <li>No sudden steep hills or cliffs.</li> <li>A smooth path where the steepness (gradient) is capped.</li> </ul> <p>Alternatively, picture a <strong>rubber band stretched smoothly over some pegs</strong>. The tension in the rubber band ensures there are no sharp kinks, making the graph smooth and predictable.</p> <h6 id="examples-of-lipschitz-continuous-functions"><strong>Examples of Lipschitz Continuous Functions</strong></h6> <ol> <li><strong>Linear Function</strong>: \(f(x) = mx + b\) is Lipschitz continuous because the slope \(m\) is constant, and \(|f'(x)| = |m|\) is bounded.</li> <li><strong>Quadratic Function</strong>: \(f(x) = x^2\) is \(L\)-smooth with \(L = 2\). Its gradient \(f'(x) = 2x\) satisfies:</li> </ol> \[|f'(x) - f'(x')| = |2x - 2x'| = 2|x - x'|.\] <ol> <li><strong>Non-Lipschitz Example</strong>: \(f(x) = \sqrt{x}\) (for \(x &gt; 0\)) is <strong>not Lipschitz continuous</strong> at \(x = 0\) because the slope becomes infinitely steep as \(x \to 0\). (If you’re not getting this, just plot \(\sqrt{x}\) function in <a href="https://www.desmos.com/">Desmos</a> and you’ll get it.)</li> </ol> <h6 id="why-does-lipschitz-continuity-matter"><strong>Why Does Lipschitz Continuity Matter?</strong></h6> <ol> <li><strong>Predictability</strong>: Lipschitz continuity ensures that a function behaves predictably, without sudden spikes or erratic changes.</li> <li><strong>Gradient Descent</strong>: If \(\nabla f(x)\) is Lipschitz continuous, we can choose a step size \(\eta \leq \frac{1}{L}\) to ensure gradient descent converges smoothly without overshooting the minimum.</li> </ol> <p>But Why? We’ll see that in the Convergence Theorem down below. For now, lets equip ourselves with the next important concept needed.</p> <hr/> <h4 id="2-convex-functions-and-convexity-condition"><strong>2. Convex Functions and Convexity Condition</strong></h4> <p>A function \(f : \mathbb{R}^d \to \mathbb{R}\) is <strong>convex</strong> if for any \(x, x' \in \mathbb{R}^d\) and \(\alpha \in [0, 1]\):</p> \[f(\alpha x + (1 - \alpha)x') \leq \alpha f(x) + (1 - \alpha)f(x').\] <p>Intuitively, the line segment between any two points on the graph of \(f\) lies above the graph itself.</p> <h6 id="convexity-condition-using-gradients"><strong>Convexity Condition Using Gradients</strong></h6> <p>If \(f\) is differentiable, convexity is equivalent to the following condition:</p> \[f(x') \geq f(x) + \langle \nabla f(x), x' - x \rangle, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This means that the function lies above its tangent plane at any point.</p> <hr/> <h4 id="3-l-smoothness"><strong>3. \(L\)-Smoothness</strong></h4> <p>A function \(f\) is said to be \(L\)-smooth if its gradient is Lipschitz continuous. This implies the following inequality:</p> \[f(x') \leq f(x) + \langle \nabla f(x), x' - x \rangle + \frac{L}{2} \|x' - x\|^2.\] <p>This property bounds the change in the function value using the gradient and the distance between \(x\) and \(x'\).</p> <hr/> <h4 id="4-optimality-conditions-for-convex-functions"><strong>4. Optimality Conditions for Convex Functions</strong></h4> <p>For convex functions, the following is true:</p> <ul> <li>If \(x^*\) is a minimizer of \(f\), then:</li> </ul> \[\nabla f(x^*) = 0.\] <ul> <li>For any \(x\), the difference between \(f(x)\) and \(f(x^*)\) can be bounded using the gradient:</li> </ul> \[f(x) - f(x^*) \leq \langle \nabla f(x), x - x^* \rangle.\] <p>These conditions help in deriving the convergence results for gradient descent.</p> <hr/> <p><strong>To quickly summarize, before we proceed further:</strong></p> <ol> <li><strong>Lipschitz continuity</strong> ensures the gradient does not change too rapidly.</li> <li><strong>Convexity</strong> guarantees that the function behaves well, with no local minima other than the global minimum.</li> <li><strong>\(L\)-smoothness</strong> combines convexity and Lipschitz continuity to bound the function’s behavior using gradients.</li> </ol> <hr/> <p>With these concepts in place, we can now proceed to derive the <strong>Convergence Theorem for Fixed Step Size</strong>.</p> <h4 id="convergence-of-gradient-descent-with-fixed-step-size"><strong>Convergence of Gradient Descent with Fixed Step Size</strong></h4> <h5 id="theorem"><strong>Theorem:</strong></h5> <p>Suppose the function \(f : \mathbb{R}^n \to \mathbb{R}\) is convex and differentiable, and its gradient is Lipschitz continuous with constant \(L &gt; 0\), i.e.,</p> \[\|\nabla f(x) - \nabla f(y)\|_2 \leq L \|x - y\|_2 \quad \text{for any} \quad x, y.\] <p>Then, if we run gradient descent for \(k\) iterations with a fixed step size \(t \leq \frac{1}{L}\), the solution \(x^{(k)}\) satisfies:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t k},\] <p>where \(f(x^*)\) is the optimal value.</p> <h5 id="proof"><strong>Proof:</strong></h5> <h6 id="step-1-lipschitz-continuity-and-smoothness"><strong>Step 1: Lipschitz Continuity and Smoothness</strong></h6> <p>From the Lipschitz continuity of \(\nabla f\), the function \(f\) satisfies the following inequality for any \(x, y \in \mathbb{R}^n\):</p> \[f(y) \leq f(x) + \nabla f(x)^T (y - x) + \frac{L}{2} \|y - x\|_2^2.\] <p>This inequality allows us to bound how the function \(f\) changes as we move from \(x\) to \(y\), given the Lipschitz constant \(L\).</p> <h6 id="step-2-gradient-descent-update"><strong>Step 2: Gradient Descent Update</strong></h6> <p>The gradient descent update step is defined as:</p> \[x^{+} = x - t \nabla f(x),\] <p>where \(t\) is the step size. Letting \(y = x^+\) in the smoothness inequality gives:</p> \[f(x^+) \leq f(x) + \nabla f(x)^T (x^+ - x) + \frac{L}{2} \|x^+ - x\|_2^2.\] <h6 id="step-3-substituting-the-update-rule"><strong>Step 3: Substituting the Update Rule</strong></h6> <p>Substituting \(x^+ - x = -t \nabla f(x)\), we get:</p> \[f(x^+) \leq f(x) + \nabla f(x)^T (-t \nabla f(x)) + \frac{L}{2} \| -t \nabla f(x)\|_2^2.\] <p>Simplifying each term:</p> <ul> <li>The second term simplifies to:</li> </ul> \[\nabla f(x)^T (-t \nabla f(x)) = -t \|\nabla f(x)\|_2^2.\] <ul> <li>The third term simplifies to:</li> </ul> \[\frac{L}{2} \| -t \nabla f(x)\|_2^2 = \frac{L t^2}{2} \|\nabla f(x)\|_2^2.\] <p>Combining these, we have:</p> \[f(x^+) \leq f(x) - t \|\nabla f(x)\|_2^2 + \frac{L t^2}{2} \|\nabla f(x)\|_2^2.\] <p>Factoring out \(\|\nabla f(x)\|_2^2\):</p> \[f(x^+) \leq f(x) - \left( t - \frac{L t^2}{2} \right) \|\nabla f(x)\|_2^2.\] <h6 id="step-4-ensuring-decrease-in-fx"><strong>Step 4: Ensuring Decrease in \(f(x)\)</strong></h6> <p>To ensure that the function value decreases at each iteration, the coefficient \(t - \frac{L t^2}{2}\) must be non-negative. This holds when \(t \leq \frac{1}{L}\). Substituting \(t = \frac{1}{L}\), we verify:</p> \[t - \frac{L t^2}{2} = \frac{1}{L} - \frac{L}{2} \cdot \frac{1}{L^2} = \frac{1}{L} - \frac{1}{2L} = \frac{1}{2L}.\] <p>Thus, with \(t \leq \frac{1}{L}\), the function value strictly decreases:</p> \[f(x^+) \leq f(x) - \frac{t}{2} \|\nabla f(x)\|_2^2.\] <h6 id="step-5-bounding-fx---fx"><strong>Step 5: Bounding \(f(x^+) - f(x^*)\)</strong></h6> <p>From the convexity of \(f\), we know:</p> \[f(x^*) \geq f(x) + \nabla f(x)^T (x^* - x).\] <p>Rearranging: \(f(x) \leq f(x^*) + \nabla f(x)^T (x - x^*).\)</p> <p>Substituting this into the inequality for \(f(x^+)\):</p> \[f(x^+) \leq f(x^*) + \nabla f(x)^T (x - x^*) - \frac{t}{2} \|\nabla f(x)\|_2^2.\] <p>Rearranging terms:</p> \[f(x^+) - f(x^*) \leq \frac{1}{2t} \left( \|x - x^*\|_2^2 - \|x^+ - x^*\|_2^2 \right).\] <p>This shows how the objective value at \(x^+\) is related to the distance between \(x\) and the optimal solution \(x^*\).</p> <h6 id="step-6-summing-over-k-iterations"><strong>Step 6: Summing Over \(k\) Iterations</strong></h6> <p>Let \(x^{(i)}\) denote the iterate after \(i\) steps. Applying the inequality iteratively, we have:</p> \[f(x^{(i)}) - f(x^*) \leq \frac{1}{2t} \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right).\] <p>Summing over \(i = 1, 2, \dots, k\):</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \sum_{i=1}^k \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right).\] <h6 id="step-7-telescoping-sum"><strong>Step 7: Telescoping Sum</strong></h6> <p>The terms on the right-hand side form a telescoping sum:</p> \[\sum_{i=1}^k \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right) = \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2.\] <p>Thus, we have:</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Since \(f(x^{(i)})\) is decreasing with each iteration, the largest term dominates the average:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right).\] <p>But, why is the above inequality right? Let’s find out:</p> <p>The inequality</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right)\] <p>is derived based on the property that \(f(x^{(i)})\) is <strong>monotonically decreasing</strong> during gradient descent. Let’s break it down step by step.</p> <ul> <li><strong>Key Property: Monotonic Decrease</strong>: In gradient descent, the function value decreases with each iteration due to the fixed step size \(t \leq \frac{1}{L}\). This means:</li> </ul> \[f(x^{(1)}) \geq f(x^{(2)}) \geq \cdots \geq f(x^{(k)}).\] <p>Thus, the latest value \(f(x^{(k)})\) is the smallest among all iterations.</p> <ul> <li><strong>Averaging the Function Values</strong>: The sum of the differences \(f(x^{(i)}) - f(x^*)\) over all \(k\) iterations can be written as:</li> </ul> \[\frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right),\] <p>which represents the average difference between the function values at each iteration and the optimal value \(f(x^*)\).</p> <ul> <li><strong>Bounding the Smallest Term by the Average</strong>: Since \(f(x^{(k)})\) is the smallest value (due to monotonic decrease), it cannot exceed the average value. In mathematical terms:</li> </ul> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right).\] <ul> <li> <p><strong>Intuition Behind the Inequality</strong>: This inequality reflects a simple fact: the smallest value in a decreasing sequence of numbers is less than or equal to their average. For example, if we have values \(10, 8, 7, 6\), the smallest value (6) will always be less than or equal to the average of these values.</p> </li> <li> <p><strong>Significance in Gradient Descent</strong>: This inequality is important because it allows us to bound the final iterate \(f(x^{(k)})\) using the sum of all previous iterations.</p> </li> </ul> <h6 id="step-8-final-substitution-to-derive-the-convergence-result"><strong>Step 8: Final Substitution to Derive the Convergence Result</strong></h6> <p>From the telescoping sum, we have:</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Using the inequality:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right),\] <p>we substitute the bound on the sum into this expression:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \cdot \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Since \(\|x^{(k)} - x^*\|_2^2 \geq 0\), we drop this term to get the worst-case bound:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2tk}.\] <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>We have derived the convergence guarantee for gradient descent with a fixed step size \(t \leq \frac{1}{L}\). The final result:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t k},\] <p>shows that the function value \(f(x^{(k)})\) decreases towards the optimal value \(f(x^*)\) at a rate proportional to \(O(1/k)\). This rate depends on the step size \(t\) and the initial distance \(\|x^{(0)} - x^*\|_2^2\).</p> <p>The result highlights that gradient descent converges reliably under the conditions of convexity, differentiability, and Lipschitz continuity of the gradient. As \(k \to \infty\), the function value approaches the optimal value, demonstrating the effectiveness of gradient descent for optimization problems with these properties.</p> <hr/> <p>Next,</p> <ul> <li>Convergence of gradient descent with adaptive step size</li> <li>Strongly convex - “linear convergence” rate</li> </ul> <h4 id="convergence-of-gradient-descent-with-adaptive-step-size"><strong>Convergence of gradient descent with adaptive step size</strong></h4> <p>In the above section, we derived the convergence rate for gradient descent with a <strong>fixed step size</strong>. In this part, we extend this analysis to the case where the step size is chosen adaptively using a <strong>backtracking line search</strong>. This method ensures that the step size decreases as necessary to guarantee sufficient decrease in the objective function at each iteration.</p> <h6 id="step-1-setup-and-assumptions"><strong>Step 1: Setup and Assumptions</strong></h6> <p>Consider a differentiable convex function \(f: \mathbb{R}^n \to \mathbb{R}\) with a <strong>Lipschitz continuous gradient</strong>. That is, for any two points \(x, y \in \mathbb{R}^n\),</p> \[\|\nabla f(x) - \nabla f(y)\|_2 \leq L \|x - y\|_2,\] <p>where \(L\) is the <strong>Lipschitz constant</strong> of the gradient.</p> <p>Let \(x^*\) be the minimizer of \(f\), and let \(x^{(i)}\) represent the iterates of gradient descent. The update rule for gradient descent with backtracking line search is:</p> \[x^{(i+1)} = x^{(i)} - t_i \nabla f(x^{(i)}),\] <p>where \(t_i\) is the step size at iteration \(i\), chosen adaptively using the backtracking procedure.</p> <h6 id="step-2-descent-lemma"><strong>Step 2: Descent Lemma</strong></h6> <p>In the case of gradient descent with a <strong>fixed step size</strong> \(t\), we know from the <strong>descent lemma</strong> (for smooth convex functions) that:</p> \[f(x^{(i+1)}) \leq f(x^{(i)}) - t \|\nabla f(x^{(i)})\|_2^2 + \frac{L}{2} t^2 \|\nabla f(x^{(i)})\|_2^2.\] <p>This inequality states that at each iteration, the function value decreases by a term proportional to the gradient’s squared norm, and this decrease depends on the step size \(t\).</p> <h6 id="step-3-backtracking-line-search"><strong>Step 3: Backtracking Line Search</strong></h6> <p>With <strong>backtracking line search</strong>, the step size \(t_i\) is chosen at each iteration to ensure sufficient decrease in the function value. Specifically, the step size is selected such that:</p> \[f(x^{(i+1)}) \leq f(x^{(i)}) + \alpha t_i \nabla f(x^{(i)})^T \nabla f(x^{(i)}),\] <p>where \(0 &lt; \alpha &lt; 1\) is a constant. The backtracking line search ensures that \(t_i\) satisfies the condition:</p> \[t_i \leq \frac{1}{L}.\] <p>Thus, the step size at each iteration is bounded by \(\frac{1}{L}\), which prevents the gradient from changing too rapidly and ensures that the update does not overshoot the optimal point.</p> <p><strong>Why “Adaptive”?</strong></p> <p>The step size is called <strong>adaptive</strong> because it changes at each iteration depending on the function’s behavior. If the function is steep or the gradient is large, the backtracking line search may choose a smaller step size to avoid overshooting. If the function is shallow or the gradient is small, it might allow a larger step size. This adaptive process uses a parameter \(\beta\) to control how the step size is reduced when the decrease condition is not met.</p> <h6 id="step-4-backtracking-process-and-beta"><strong>Step 4: Backtracking Process and \(\beta\)</strong></h6> <p>The process of backtracking works as follows:</p> <ul> <li> <p><strong>Initial Step Size</strong>: Start with an initial guess for the step size, typically \(t_0 = 1\).</p> </li> <li> <p><strong>Condition Check</strong>: Check whether the condition</p> </li> </ul> \[f(x^{(i+1)}) \leq f(x^{(i)}) + \alpha t_i \nabla f(x^{(i)})^T \nabla f(x^{(i)})\] <p>holds. If it does, accept \(t_i\); if not, reduce the step size.</p> <ul> <li><strong>Reduce Step Size</strong>: If the condition is not satisfied, reduce the step size \(t_i\) by a factor \(\beta\):</li> </ul> \[t_{i+1} = \beta t_i,\] <p>where \(\beta\) is a constant between 0 and 1 (usually around 0.5 or 0.8). This step size reduction continues until the condition is met.</p> <ul> <li><strong>Accept the Step Size</strong>: Once the condition is satisfied, the current \(t_i\) is accepted for the update.</li> </ul> <p>The use of \(\beta\) helps to ensure that the step size does not become too large, allowing the algorithm to converge smoothly without overshooting.</p> <h6 id="step-5-bounding-the-convergence"><strong>Step 5: Bounding the Convergence</strong></h6> <p>Now, let’s derive the convergence bound for gradient descent with backtracking line search. From the descent lemma, the change in the function value at each iteration can be bounded as:</p> \[f(x^{(i+1)}) - f(x^{(i)}) \leq - t_i \|\nabla f(x^{(i)})\|_2^2 \left( 1 - \frac{L}{2} t_i \right).\] <p>Because the backtracking line search ensures that \(t_i \leq t_{\text{min}} = \min\left( 1, \frac{\beta}{L} \right)\), we can bound the function value decrease as:</p> \[f(x^{(i+1)}) - f(x^{(i)}) \leq - t_{\text{min}} \|\nabla f(x^{(i)})\|_2^2 \left( 1 - \frac{L}{2} t_{\text{min}} \right).\] <p>This shows that the function value decreases at each iteration, with the step size \(t_{\text{min}}\) controlling the rate of decrease.</p> <p>Now, if you observe carefully, the equation above closely resembles the one we encountered in the fixed step size proof. The only minor difference is that \(t\) has been replaced with \(t_{\text{min}}\). Therefore, we can follow the same steps as in the fixed step size case and eventually arrive at the following result:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t_{\text{min}} k}.\] <p>This shows that by adaptively choosing the step size, we can achieve a convergence rate similar to that of the fixed step size approach, but without needing to manually set a fixed value for ( t ).</p> <p><strong>Quick Note:</strong> I’m still not completely satisfied with the proof for Adaptive Step Size. I’ll be working on refining the explanation further and will update you with any improvements.</p> <h5 id="and-finally"><strong>And finally…</strong></h5> <p>We’ve reached the end of this blog post! A huge kudos to you for making it all the way through and sticking with me. The reason we went through all of this is that understanding such proofs will lay the foundation for exploring the intricate details that drive machine learning and produce its remarkable results. To truly dive into ML research, we need to immerse ourselves in these depths and make it happen.</p> <p>So, take a well-deserved break, and in the next post, we’ll delve into the tips and tricks of SGD that are widely practiced in the industry. Until then, take care and see you soon!</p> <h5 id="references"><strong>References:</strong></h5> <ul> <li><a href="https://nyu-cs2565.github.io/mlcourse-public/2024-fall/lectures/lec02/gradient_descent_converge.pdf"> Gradient Descent: Convergence Analysis - Ryan Tibshirani</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.]]></summary></entry><entry><title type="html">Gradient Descent and Second-Order Optimization - A Thorough Comparison</title><link href="https://monishver11.github.io/blog/2024/gd-tips/" rel="alternate" type="text/html" title="Gradient Descent and Second-Order Optimization - A Thorough Comparison"/><published>2024-12-29T01:44:00+00:00</published><updated>2024-12-29T01:44:00+00:00</updated><id>https://monishver11.github.io/blog/2024/gd-tips</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/gd-tips/"><![CDATA[ <p>The concept discussed below may already be familiar to you, but it might appear a bit different in this blog. This content is adapted from the reference material - <a href="https://leon.bottou.org/publications/pdf/tricks-2012.pdf">Stochastic Gradient Descent Tricks</a> we received for Gradient Descent (GD) and Stochastic Gradient Descent (SGD) tips, written by <a href="https://leon.bottou.org/">L´eon Bottou</a>. It’s a well-written piece with many theoretical aspects that are often overlooked when applying GD in machine learning. To be honest, I still don’t fully grasp all of it, but I hope that as I continue on this learning journey, I’ll come to understand most of it and be able to make sense of it.</p> <p>This blog post, along with the next one, will serve as my personal notes on the material. Another reason for sharing this content is to familiarize ourselves with the different notations commonly used in machine learning research. Before diving directly into the SGD tips and tricks from the material, I felt it was important to revisit Gradient Descent (GD) as described in the text. In machine learning, the same concepts are often presented using various notations, which can be confusing if you’re not prepared.</p> <p>Think of it this way: just as there is a base language with many different slangs or colloquialisms, in math, the core concepts remain the same, but the notations can vary depending on who’s explaining them or for what purpose. So, consider this as the same concept, expressed in a different notation—another perspective on the same idea. That’s it—let’s get started!</p> <hr/> <h4 id="1-gradient-descent-gd"><strong>1. Gradient Descent (GD)</strong></h4> <h6 id="objective"><strong>Objective</strong></h6> <p>Minimize the empirical risk \(E_n(f_w)\).</p> <h6 id="update-rule"><strong>Update Rule</strong></h6> \[w_{t+1} = w_t - \gamma \frac{1}{n} \sum_{i=1}^n \nabla_w Q(z_i, w_t),\] <p>where:</p> <ul> <li>\(w_t\): Current weights at iteration \(t\).</li> <li>\(\gamma\): Learning rate (a small positive scalar).</li> <li>\(\nabla_w Q(z_i, w_t)\): Gradient of the loss function \(Q\) with respect to \(w_t\) for data point \(z_i\).</li> </ul> <p><strong>Convergence Requirements</strong>:</p> <ul> <li>\(w_0\) (initial weights) close to the optimum.</li> <li>\(\gamma\) small enough.</li> </ul> <p><strong>Performance</strong>:</p> <ul> <li>Achieves <strong>linear convergence</strong>, meaning the error decreases exponentially with iterations. The convergence rate is denoted as \(\rho\), so:</li> </ul> \[-\log \rho \sim t,\] <p>where \(\rho\) represents the residual error.</p> <hr/> <h4 id="2-second-order-gradient-descent-2gd"><strong>2. Second-Order Gradient Descent (2GD)</strong></h4> <h6 id="improvement"><strong>Improvement</strong></h6> <p>Instead of using a scalar learning rate \(\gamma\), introduce a positive definite matrix \(\Gamma_t\):</p> \[w_{t+1} = w_t - \Gamma_t \frac{1}{n} \sum_{i=1}^n \nabla_w Q(z_i, w_t).\] <ul> <li>\(\Gamma_t\): Approximates the inverse of the Hessian matrix of the cost function at the optimum.</li> <li>The Hessian is the second derivative of the cost function, capturing curvature information.</li> </ul> <h6 id="advantages"><strong>Advantages</strong></h6> <ul> <li>The algorithm accounts for the curvature of the cost function, leading to more informed updates.</li> <li>When \(\Gamma_t\) is exactly the inverse of the Hessian:</li> <li><strong>Convergence is quadratic</strong>, meaning:</li> </ul> \[-\log \log \rho \sim t,\] <p>where the error decreases much faster than linear convergence.</p> <ul> <li>If the cost function is quadratic and the scaling matrix \(\Gamma_t\) is exact, the optimum is reached in <strong>one iteration</strong>.</li> </ul> <h6 id="assumptions-for-quadratic-convergence"><strong>Assumptions for Quadratic Convergence</strong></h6> <ul> <li>Smoothness of the cost function.</li> <li>\(w_0\) close enough to the optimum.</li> </ul> <h6 id="intuition-behind-quadratic-convergence"><strong>Intuition Behind Quadratic Convergence</strong></h6> <ul> <li>In GD, the learning rate \(\gamma\) is fixed and doesn’t adapt to the problem’s geometry, leading to slower convergence in certain directions.</li> <li>In 2GD, the matrix \(\Gamma_t\) adapts to the curvature of the cost function:</li> <li>Allows larger steps in flat directions.</li> <li>Takes smaller steps in steep directions.</li> <li>This results in significantly faster convergence.</li> </ul> <hr/> <h4 id="follow-up-gradient-descent-and-second-order-gradient-descent"><strong>Follow-Up: Gradient Descent and Second-Order Gradient Descent</strong></h4> <p>This below section answers follow-up questions about the convergence behavior of Gradient Descent (GD) and the role of the inverse Hessian in Second-Order Gradient Descent (2GD).</p> <h5 id="1-how-does-linear-convergence-in-gd-lead-to-exponential-error-reduction"><strong>1. How does linear convergence in GD lead to exponential error reduction?</strong></h5> <h6 id="recap-of-linear-convergence"><strong>Recap of Linear Convergence</strong></h6> <p>Linear convergence means that the error at iteration \(t\) is proportional to the error at iteration \(t-1\), scaled by a constant \(\rho\):</p> \[\|w_t - w^*\| \leq \rho \|w_{t-1} - w^*\|, \quad \text{where } 0 &lt; \rho &lt; 1.\] <h6 id="derivation-of-exponential-error-reduction">Derivation of Exponential Error Reduction</h6> <p>Let’s derive how the error becomes proportional to \(\rho^t\) after \(t\) iterations:</p> <ul> <li>From the recurrence relation:</li> </ul> \[\|w_t - w^*\| \leq \rho \|w_{t-1} - w^*\|.\] <p>Expanding this iteratively:</p> \[\|w_t - w^*\| \leq \rho (\|w_{t-2} - w^*\|) \leq \rho^2 \|w_{t-2} - w^*\|.\] <ul> <li>Generalizing this pattern:</li> </ul> \[\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|,\] <p>where \(\|w_0 - w^*\|\) is the initial error at \(t = 0\).</p> <ul> <li>Since \(\rho &lt; 1\), \(\rho^t\) decreases exponentially as \(t\) increases. This shows the error reduces at an exponential rate in terms of the number of iterations.</li> </ul> <hr/> <h5 id="2-what-is-the-inverse-of-the-hessian-matrix"><strong>2. What is the inverse of the Hessian matrix?</strong></h5> <h6 id="hessian-matrix"><strong>Hessian Matrix</strong></h6> <p>The Hessian matrix is a second-order derivative matrix of the cost function \(Q(w)\), defined as:</p> \[H = \nabla^2_w Q(w),\] <p>where each entry \(H_{ij} = \frac{\partial^2 Q(w)}{\partial w_i \partial w_j}\) captures how the gradient changes with respect to each pair of weights.</p> <h6 id="inverse-of-the-hessian"><strong>Inverse of the Hessian</strong></h6> <p>The inverse of the Hessian, \(H^{-1}\), rescales the gradient updates based on the curvature of the cost function:</p> <ul> <li>In directions where the curvature is steep, \(H^{-1}\) reduces the step size.</li> <li>In flatter directions, \(H^{-1}\) increases the step size.</li> </ul> <p>This adjustment improves convergence by adapting the optimization step to the geometry of the cost function.</p> <hr/> <h5 id="3-how-does-using-the-inverse-hessian-converge-faster"><strong>3. How does using the inverse Hessian converge faster?</strong></h5> <h6 id="faster-convergence-with-2gd"><strong>Faster Convergence with 2GD</strong></h6> <p>In <strong>Second-Order Gradient Descent (2GD)</strong>:</p> \[w_{t+1} = w_t - H^{-1} \nabla_w Q(w_t).\] <p>This update accounts for the curvature of the cost function.</p> <h6 id="quadratic-convergence"><strong>Quadratic Convergence</strong></h6> <ul> <li>Near the optimum \(w^*\), the cost function can be locally approximated as quadratic:</li> </ul> \[Q(w) \approx \frac{1}{2}(w - w^*)^T H (w - w^*),\] <p>where \(H\) is the Hessian at \(w^*\).</p> <ul> <li>The gradient of the cost is:</li> </ul> \[\nabla_w Q(w) = H (w - w^*).\] <ul> <li>Substituting this gradient into the 2GD update:</li> </ul> \[w_{t+1} = w_t - H^{-1} H (w_t - w^*).\] <ul> <li>Simplifies to:</li> </ul> \[w_{t+1} = w^*.\] <p>This shows that in the best case (when the cost is exactly quadratic and \(H^{-1}\) is exact), the algorithm converges in <strong>one iteration</strong>.</p> <hr/> <h5 id="4-summary-of-convergence-behavior"><strong>4. Summary of Convergence Behavior</strong></h5> <ul> <li><strong>Gradient Descent (GD)</strong>: <ul> <li>Linear convergence: \(\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|\).</li> <li>Error decreases exponentially at a rate \(\rho\), where \(\rho\) depends on the learning rate and the condition number of the Hessian.</li> </ul> </li> <li><strong>Second-Order Gradient Descent (2GD)</strong>: <ul> <li>Quadratic convergence: \(\|w_t - w^*\| \sim (\text{error})^2\) at each iteration.</li> <li>When the cost is quadratic and \(H^{-1}\) is exact, the algorithm converges in one step.</li> </ul> </li> </ul> <hr/> <h4 id="more-details-linear-vs-quadratic-convergence-in-optimization">More Details: Linear vs. Quadratic Convergence in Optimization</h4> <h5 id="1-linear-convergence-in-gradient-descent"><strong>1. Linear Convergence in Gradient Descent</strong></h5> <h6 id="key-idea"><strong>Key Idea:</strong></h6> <p>Gradient Descent (GD) decreases the error at a fixed proportion \(\rho\) per iteration:</p> \[\|w_t - w^*\| \leq \rho \|w_{t-1} - w^*\|, \quad \text{where } 0 &lt; \rho &lt; 1.\] <h6 id="step-by-step-derivation"><strong>Step-by-Step Derivation:</strong></h6> <ul> <li><strong>Iterative Expansion</strong>: Expanding the recurrence:</li> </ul> \[\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|,\] <p>where \(\|w_0 - w^*\|\) is the initial error.</p> <ul> <li><strong>Take the Logarithm</strong>: Apply the natural logarithm to both sides:</li> </ul> \[\log \|w_t - w^*\| \leq \log (\rho^t \|w_0 - w^*\|).\] <ul> <li><strong>Simplify Using Logarithm Rules</strong>: Using \(\log (ab) = \log a + \log b\) and \(\log (\rho^t) = t \log \rho\), we get:</li> </ul> \[\log \|w_t - w^*\| \leq t \log \rho + \log \|w_0 - w^*\|.\] <ul> <li><strong>Why Does \(t \log \rho\) Decrease Linearly?</strong> <ul> <li>The parameter \(\rho\) satisfies \(0 &lt; \rho &lt; 1\), so \(\log \rho &lt; 0\).</li> <li>As \(t\) increases, \(t \log \rho\) becomes a larger negative number, reducing the value of \(\log \|w_t - w^*\|\).</li> <li>Since \(\log \rho\) is a constant, the term \(t \log \rho\) depends <strong>linearly on \(t\)</strong>:</li> </ul> \[t \log \rho = (\text{constant}) \cdot t, \quad \text{where constant} = \log \rho.\] </li> <li><strong>Interpretation of Convergence Rate</strong>: <ul> <li>From the error bound \(\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|\), we see exponential error decay with \(t\).</li> <li>Taking the logarithm leads to a linear relationship in \(t\):</li> </ul> </li> </ul> \[\log \|w_t - w^*\| \sim t \log \rho.\] <ul> <li>This behavior is summarized as:</li> </ul> \[-\log \rho \sim t.\] <h5 id="2-quadratic-convergence-in-second-order-gradient-descent"><strong>2. Quadratic Convergence in Second-Order Gradient Descent</strong></h5> <h6 id="key-idea-1"><strong>Key Idea:</strong></h6> <p>In Second-Order Gradient Descent (2GD), the error at each step is proportional to the <strong>square</strong> of the error at the previous step:</p> \[\|w_t - w^*\| \sim (\|w_{t-1} - w^*\|)^2.\] <h6 id="step-by-step-derivation-1"><strong>Step-by-Step Derivation:</strong></h6> <ul> <li><strong>Iterative Expansion</strong>: Rewriting the error at step \(t\) in terms of the initial error \(\|w_0 - w^*\|\):</li> </ul> \[\|w_t - w^*\| \sim (\|w_{t-1} - w^*\|)^2 \sim \left((\|w_{t-2} - w^*\|)^2\right)^2 \sim \dots \sim (\|w_0 - w^*\|)^{2^t}.\] <p>Thus:</p> \[\|w_t - w^*\| \sim (\|w_0 - w^*\|)^{2^t}.\] <ul> <li><strong>Take the Logarithm</strong>: Apply the natural logarithm:</li> </ul> \[\log \|w_t - w^*\| \sim 2^t \log \|w_0 - w^*\|.\] <ul> <li><strong>Take Another Logarithm</strong>: To analyze the rate of convergence, take the logarithm again:</li> </ul> \[\log \log \|w_t - w^*\| \sim \log (2^t) + \log \log \|w_0 - w^*\|.\] <p>Using \(\log (2^t) = t \log 2\), we simplify:</p> \[\log \log \|w_t - w^*\| \sim t + \log \log \|w_0 - w^*\|.\] <ul> <li><strong>Interpretation of Convergence Rate</strong>: <ul> <li>From the error bound \(\|w_t - w^*\| \sim (\|w_0 - w^*\|)^{2^t}\), we see super-exponential error decay.</li> <li>Taking the logarithm of the logarithm shows linear growth in \(t\):</li> </ul> </li> </ul> \[\log \log \|w_t - w^*\| \sim t.\] <ul> <li>This behavior is expressed as:</li> </ul> \[-\log \log \rho \sim t.\] <hr/> <h6 id="summary-of-convergence-behavior"><strong>Summary of Convergence Behavior</strong></h6> <table> <thead> <tr> <th><strong>Convergence Type</strong></th> <th><strong>Error Decay</strong></th> <th><strong>Logarithmic Analysis</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gradient Descent (GD)</strong></td> <td>\(|w_t - w^*| \sim \rho^t\)</td> <td>\(\log |w_t - w^*| \sim t\)</td> </tr> <tr> <td><strong>Second-Order Gradient Descent (2GD)</strong></td> <td>\(|w_t - w^*| \sim (|w_0 - w^*|)^{2^t}\)</td> <td>\(\log \log |w_t - w^*| \sim t\)</td> </tr> </tbody> </table> <hr/> <p>The next blog continues from this one, where we’ll explore SGD (Stochastic Gradient Descent) along with some helpful tips and analogies. We will use the same notations as those introduced in this blog. Keep learning, and head on to the next post!</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.]]></summary></entry><entry><title type="html">Understanding Stochastic Gradient Descent (SGD)</title><link href="https://monishver11.github.io/blog/2024/SGD/" rel="alternate" type="text/html" title="Understanding Stochastic Gradient Descent (SGD)"/><published>2024-12-28T03:42:00+00:00</published><updated>2024-12-28T03:42:00+00:00</updated><id>https://monishver11.github.io/blog/2024/SGD</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/SGD/"><![CDATA[<p>In the last blog, we introduced <strong>Stochastic Gradient Descent (SGD)</strong> as a computationally efficient optimization method. In this post, we’ll dive deeper into the mechanics of SGD, exploring its nuances, trade-offs, and how it compares to other gradient descent variants. Let’s unravel the details and gain a comprehensive understanding of these optimization techniques.</p> <h4 id="noisy-gradient-descent"><strong>“Noisy” Gradient Descent</strong></h4> <p>Instead of computing the exact gradient at every step, <strong>noisy gradient descent</strong> estimates the gradient using random subsamples. Surprisingly, this approximation often works well.</p> <p><strong>Why Does It Work?</strong></p> <p>Gradient descent is inherently iterative, meaning it has the chance to recover from previous missteps at each step. Leveraging noisy estimates can speed up the process without significantly impacting the final results.</p> <hr/> <h4 id="mini-batch-gradient-descent"><strong>Mini-batch Gradient Descent</strong></h4> <p>The <strong>full gradient</strong> for a dataset \(D_n = (x_1, y_1), \dots, (x_n, y_n)\) is given by:</p> \[\nabla \hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \nabla_w \ell(f_w(x_i), y_i)\] <p>This requires the entire dataset, which can be computationally expensive. To mitigate this, we use a <strong>mini-batch</strong> of size \(N\), a random subset of the data:</p> \[\nabla \hat{R}_N(w) = \frac{1}{N} \sum_{i=1}^N \nabla_w \ell(f_w(x_{m_i}), y_{m_i})\] <p>Here, \((x_{m_1}, y_{m_1}), \dots, (x_{m_N}, y_{m_N})\) is the mini-batch.</p> <hr/> <h4 id="batch-vs-stochastic-methods"><strong>Batch vs. Stochastic Methods</strong></h4> <p><strong>Rule of Thumb:</strong></p> <ul> <li><strong>Stochastic methods</strong> perform well far from the optimum but struggle as we approach it.</li> <li><strong>Batch methods</strong> excel near the optimum due to more precise gradient calculations.</li> </ul> <h4 id="mini-batch-gradient-properties"><strong>Mini-batch Gradient Properties</strong></h4> <ul> <li>The mini-batch gradient is an <strong>unbiased estimator</strong> of the full gradient, meaning on average, the gradient computed using a minibatch (a small, random subset of the dataset) gives the same direction of descent as the gradient computed using the entire dataset.</li> </ul> \[\mathbb{E}[\nabla \hat{R}_N(w)] = \nabla \hat{R}_n(w)\] <ul> <li> <p>This implies that while individual minibatch gradients may vary due to the randomness of the sample, their expected value matches the full batch gradient. This property allows Stochastic Gradient Descent (SGD) to make consistent progress toward the optimum without requiring computation over the entire dataset in each iteration.</p> </li> <li> <p>Larger mini-batches result in better estimates but are slower to compute:</p> </li> </ul> \[\text{Var}[\nabla \hat{R}_N(w)] = \frac{1}{N} \text{Var}[\nabla \hat{R}_i(w)]\] <ul> <li>This is because averaging over more samples reduces randomness. Specifically, the variance is scaled by \(1/𝑁\), meaning larger minibatches produce more accurate and stable gradient estimates, closer to the full batch gradient.</li> </ul> <p><strong>Tradeoffs of minibatch size:</strong></p> <ul> <li><strong>Larger \(N\):</strong> Better gradient estimate, slower computation.</li> <li><strong>Smaller \(N\):</strong> Faster computation, noisier gradient estimates.</li> </ul> <hr/> <h4 id="convergence-of-sgd"><strong>Convergence of SGD</strong></h4> <p>To ensure convergence, <strong>diminishing step sizes</strong> like \(\eta_k = 1/k\) are often used. While gradient descent (GD) theoretically converges faster than SGD:</p> <ul> <li><strong>GD</strong> is efficient near the minimum due to higher accuracy.</li> <li><strong>SGD</strong> is more practical for large-scale problems where high accuracy is unnecessary.</li> </ul> <p>In practice, SGD with <strong>fixed step sizes</strong> works well and can be adjusted using techniques like <strong>staircase decay</strong> or <strong>inverse time decay</strong> (\(1/t\)).</p> <h6 id="sgd-algorithm-with-mini-batches"><strong>SGD Algorithm with Mini-batches</strong></h6> <ol> <li>Initialize \(w = 0\).</li> <li>Repeat: <ul> <li>Randomly sample \(N\) points from \(D_n\): \({(x_i, y_i)}_{i=1}^N\).</li> <li>Update weights:</li> </ul> \[w \leftarrow w - \eta \left( \frac{1}{N} \sum_{i=1}^N \nabla_w \ell(f_w(x_i), y_i) \right)\] </li> </ol> <hr/> <h5 id="why-diminishing-step-sizes-theoretical-aspects"><strong>Why Diminishing Step Sizes? (Theoretical Aspects)</strong></h5> <p>If \(f\) is \(L\)-smooth and convex, and the variance of \(\nabla f(x^{(k)})\) is bounded</p> \[\text{Var}(\nabla f(x^{(k)})) \leq \sigma^2\] <p>, then SGD with step size</p> \[\eta \leq \frac{1}{L}\] <p>satisfies:</p> \[\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2] \leq \frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k} + \frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\] <p><strong>Breaking it Down:</strong></p> <ol> <li><strong>L-Smooth and Convex Function</strong>: <ul> <li>A function ( f ) is <strong>smooth</strong> if its gradient doesn’t change too rapidly. Specifically, an \(L\)-smooth function means that the gradient’s rate of change is bounded by a constant \(L\).</li> <li>A <strong>convex</strong> function means that it has a single global minimum, making optimization easier because we don’t have to worry about getting stuck in local minima.</li> </ul> </li> <li><strong>Variance of Gradient</strong>: <ul> <li>The gradient at each step of SGD might not be exact. The variance \(\text{Var}(\nabla f(x^{(k)}))\) measures the “noise” or fluctuations in the gradient estimate. A smaller variance means the gradient is more stable.</li> </ul> </li> </ol> <p><strong>What Does the Formula Mean?</strong></p> <p>The formula provides an upper bound on the expected squared magnitude of the gradient:</p> \[\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2] \leq \frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k} + \frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\] <ul> <li> <p><strong>Left Side</strong>: \(\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2]\) represents the minimum expected squared gradient magnitude. A smaller value indicates that the gradient is approaching zero, meaning we’re getting closer to the optimal solution.</p> </li> <li> <p><strong>Right Side</strong>:</p> <ul> <li>The first term \(\frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k}\) reflects how the gap between the initial point \(x^{(0)}\) and the optimal solution \(x^*\) decreases over time. The more steps we take (i.e., the larger the sum of the step sizes \(\eta_k\)), the smaller the gap becomes.</li> <li>The second term \(\frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\) accounts for the variance in the gradient. If the step size doesn’t decrease over time, this variance term grows, which can destabilize the optimization process. The numerator \(\sum_k \eta_k^2\) grows faster than the denominator \(\sum_k \eta_k\), so increasing step sizes overall increases the second term. So, this term will dominate if the step size does not decrease.</li> </ul> </li> </ul> <p><strong>Intuition Behind Diminishing Step Sizes:</strong></p> <ul> <li> <p><strong>Without diminishing step sizes</strong> - If you keep taking large steps, especially when close to the minimum, you risk overshooting the optimal solution. Large gradients or noisy estimates can lead to erratic behavior.</p> </li> <li> <p><strong>With diminishing step sizes</strong> - As we get closer to the minimum, reducing the step size helps take smaller, more controlled steps. This reduces the variance (noise) in the gradient and makes the convergence process smoother and more stable.</p> </li> </ul> <p><strong>So, now why diminish step sizes?</strong></p> <p>Diminishing step sizes are important because:</p> <ul> <li>Early on, larger steps help explore the solution space and make significant progress.</li> <li>As you approach the optimal solution, smaller steps are needed to fine-tune the result and avoid overshooting. This balance helps the optimization process converge efficiently while maintaining stability.</li> </ul> <p>More on the mathematical details of convergence will be covered in a separate blog post. For now, the key intuition to keep in mind is that diminishing step sizes help strike a balance between exploration (larger steps) and stability (smaller steps), leading to smoother convergence.</p> <hr/> <h4 id="summary"><strong>Summary</strong></h4> <p>Gradient descent variants provide trade-offs in speed, accuracy, and computational cost:</p> <ul> <li><strong>Full-batch gradient descent:</strong> Uses the entire dataset for gradient computation, yielding precise updates but high computational cost.</li> <li><strong>Mini-batch gradient descent:</strong> Balances computational efficiency and gradient accuracy by using subsets of data.</li> <li><strong>Stochastic gradient descent (SGD):</strong> Uses a single data point (\(N = 1\)) for updates, making it highly efficient but noisy.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GD_Variations.webp" sizes="95vw"/> <img src="/assets/img/GD_Variations.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GD_Variations" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Batch Vs Stochastic Vs Mini-Batch GD </div> <p>When referring to SGD, always clarify the batch size to avoid ambiguity. Modern machine learning heavily relies on SGD due to its time and memory efficiency, especially for large-scale problems.</p> <hr/> <h5 id="example-logistic-regression-with-ell_2-regularization"><strong>Example: Logistic Regression with \(\ell_2\)-Regularization</strong></h5> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_1-480.webp 480w,/assets/img/SGD_Comp_1-800.webp 800w,/assets/img/SGD_Comp_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SGD_Comp_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_2-480.webp 480w,/assets/img/SGD_Comp_2-800.webp 800w,/assets/img/SGD_Comp_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SGD_Comp_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_3-480.webp 480w,/assets/img/SGD_Comp_3-800.webp 800w,/assets/img/SGD_Comp_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SGD_Comp_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Convergence Speed(1), Computational Efficiency(2) and Rate at Near Optimum(3) of different GD approaches </div> <ul> <li><strong>Batch methods:</strong> Converge faster near the optimum.</li> <li><strong>Stochastic methods:</strong> Are computationally efficient, especially for large datasets.</li> </ul> <p>Understanding these trade-offs helps in choosing the right approach for different scenarios.</p> <hr/> <p>In the next blog, we’ll explore <strong>Gradient Descent Convergence Theorems</strong> and how to intuitively make sense out of it! See you.</p> <h6 id="image-credits"><strong>Image Credits:</strong></h6> <ul> <li><a href="https://alwaysai.co/blog/what-is-gradient-descent">Batch Vs Stochastic Vs Mini-Batch GD</a></li> <li><a href="https://www.stat.berkeley.edu/~ryantibs/">Example from Ryan Tibshirani</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).]]></summary></entry><entry><title type="html">Gradient Descent - A Detailed Walkthrough</title><link href="https://monishver11.github.io/blog/2024/gradient-descent/" rel="alternate" type="text/html" title="Gradient Descent - A Detailed Walkthrough"/><published>2024-12-25T19:01:00+00:00</published><updated>2024-12-25T19:01:00+00:00</updated><id>https://monishver11.github.io/blog/2024/gradient-descent</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/gradient-descent/"><![CDATA[<p>In our last blog post, we discussed Empirical Risk Minimization (ERM). Let’s build on that foundation by exploring a concrete example: <strong>Linear Least Squares Regression</strong>. This will help us understand how gradient descent fits into the optimization landscape.</p> <h4 id="linear-least-squares-regression"><strong>Linear Least Squares Regression</strong></h4> <p><strong>Problem Setup</strong></p> <p>We aim to minimize the empirical risk using linear regression. Here’s the setup:</p> <ul> <li><strong>Loss function</strong>: \(\ell(\hat{y}, y) = (\hat{y} - y)^2\)</li> <li><strong>Hypothesis space</strong>: \(\mathcal{F} = \{ f : \mathbb{R}^d \to \mathbb{R} \mid f(x) = w^\top x, \, w \in \mathbb{R}^d \}\)</li> <li><strong>Data set</strong>: \(\mathcal{D}_n = \{(x_1, y_1), \dots, (x_n, y_n)\}\) Our goal is to find the ERM solution: \(\hat{f} \in \mathcal{F}\). <strong>Objective Function</strong></li> </ul> <p>We want to find the function in \(\mathcal{F}\), parametrized by \(w \in \mathbb{R}^d\), that minimizes the empirical risk:</p> \[\hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n (w^\top x_i - y_i)^2\] <p>This leads to the optimization problem: \(\min_{w \in \mathbb{R}^d} \hat{R}_n(w)\)</p> <p>Although ordinary least squares (OLS) offers a closed-form solution (<strong>refer to the provided resource to understand OLS</strong>), gradient descent proves more versatile, particularly when closed-form solutions are not feasible.</p> <hr/> <h3 id="gradient-descent"><strong>Gradient Descent</strong></h3> <p>Gradient descent is a powerful optimization technique for unconstrained problems.</p> <h4 id="unconstrained-optimization-setting"><strong>Unconstrained Optimization Setting</strong></h4> <p>We assume the objective function \(f : \mathbb{R}^d \to \mathbb{R}\) is <strong>differentiable</strong>, and we aim to find:</p> \[x^* = \arg \min_{x \in \mathbb{R}^d} f(x)\] <h4 id="the-gradient"><strong>The Gradient</strong></h4> <p>The gradient is a fundamental concept in optimization. For a differentiable function \(f : \mathbb{R}^d \to \mathbb{R}\), the gradient at a point \(x_0 \in \mathbb{R}^d\) is denoted by \(\nabla f(x_0)\). It represents the vector of partial derivatives of \(f\) with respect to each dimension of \(x\):</p> \[\nabla f(x_0) = \left[ \frac{\partial f}{\partial x_1}(x_0), \frac{\partial f}{\partial x_2}(x_0), \dots, \frac{\partial f}{\partial x_d}(x_0) \right]\] <h6 id="key-points"><strong>Key points:</strong></h6> <ul> <li>The gradient points in the direction of the steepest <strong>increase</strong> of the function \(f(x)\) starting from \(x_0\).</li> <li>The <strong>magnitude</strong> of the gradient indicates how steep the slope is in that direction.</li> </ul> <p>For example, consider a 2D function \(f(x, y)\). The gradient at a point \((x_0, y_0)\) is:</p> \[\nabla f(x_0, y_0) = \left[ \frac{\partial f}{\partial x}(x_0, y_0), \frac{\partial f}{\partial y}(x_0, y_0) \right]\] <p>This tells us how \(f\) changes with respect to \(x\) and \(y\) near \((x_0, y_0)\).</p> <h6 id="importance-in-optimization"><strong>Importance in Optimization</strong></h6> <p>The gradient is crucial because it provides the direction in which the function \(f(x)\) increases most rapidly. To minimize \(f(x)\):</p> <ul> <li>We move in the <strong>opposite</strong> direction of the gradient, as this is where the function decreases most rapidly.</li> </ul> <h6 id="geometric-interpretation"><strong>Geometric Interpretation</strong></h6> <p>Imagine a 3D surface representing a function \(f(x, y)\). At any point on the surface:</p> <ul> <li>The gradient vector points <strong>uphill</strong>, perpendicular to the contour lines (or level curves) of the function.</li> <li>To find a minimum, we “descend” by moving in the <strong>opposite direction</strong> of the gradient vector.</li> </ul> <p>This understanding lays the foundation for applying gradient descent effectively in optimization problems.</p> <hr/> <h3 id="gradient-descent-algorithm"><strong>Gradient Descent Algorithm</strong></h3> <p>To iteratively minimize \(f(x)\), follow these steps:</p> <ol> <li><strong>Initialize</strong>: \(x \leftarrow 0\)</li> <li><strong>Repeat</strong>: \(x \leftarrow x - \eta \nabla f(x)\) <ul> <li>until a stopping criterion is met.</li> </ul> </li> </ol> <p>Here, \(\eta\) is the <strong>step size</strong> (or <strong>learning rate</strong>). Choosing \(\eta\) appropriately is critical to avoid divergence or slow convergence. “Step size” is also referred to as “learning rate” in neural networks literature.</p> <div align="center"> <img src="https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif" alt="Gradient Descent GIF" width="500"/> <p>Path of a Gradient Descent Algorithm</p> </div> <h4 id="insights-into-gradient-descent"><strong>Insights into Gradient Descent</strong></h4> <h6 id="step-size"><strong>Step Size</strong></h6> <ul> <li><strong>Fixed step size</strong>: Works if small enough.</li> <li>If \(\eta\) is too large, the process might diverge.</li> <li>Experimenting with multiple step sizes is often necessary.</li> <li>Big vs. Small Steps: <ul> <li><strong>Big steps</strong>: In flat regions where the gradient is small, larger steps accelerate convergence.</li> <li><strong>Small steps</strong>: In steep regions where the gradient is large, smaller steps ensure stability and prevent overshooting.</li> <li>Adaptive methods like Adam or RMSprop leverage this intuition by dynamically adjusting the step size based on the gradient’s magnitude or past behavior.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GD_Learning_Rate-480.webp 480w,/assets/img/GD_Learning_Rate-800.webp 800w,/assets/img/GD_Learning_Rate-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/GD_Learning_Rate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GD_Learning_Rate" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Step size variations. Image credit: https://medium.com/@yennhi95zz </div> <h6 id="convergence"><strong>Convergence</strong></h6> <p>Gradient descent converges to a stationary point (where the derivative is zero) for differentiable functions. These stationary points could be:</p> <ul> <li>Local minima</li> <li>Local maxima</li> <li>Saddle points</li> </ul> <p>For convex functions(Added Reference), gradient descent can converge to the global minimum.</p> <hr/> <p>The following theorems are often overlooked when learning about gradient descent. We’ll dive into them in detail in a separate blog, but for now, give them a quick read and continue.</p> <h4 id="theorem-convergence-of-gradient-descent-with-fixed-step-size"><strong>Theorem: Convergence of Gradient Descent with Fixed Step Size</strong></h4> <p>Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is convex and differentiable, and \(\nabla f\) is Lipschitz continuous with constant \(L &gt; 0\) (i.e., \(f\) is L-smooth). This means:</p> \[\|\nabla f(x) - \nabla f(x')\| \leq L \|x - x'\|\] <p>for any \(x, x' \in \mathbb{R}^d\).</p> <p><strong>Result:</strong></p> <p>If gradient descent uses a fixed step size \(\eta \leq \frac{1}{L}\), then:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|^2}{2\eta k}\] <p><strong>Implications:</strong></p> <ul> <li>Gradient descent is <strong>guaranteed to converge</strong> under these conditions.</li> <li>The convergence rate is \(O(1/k)\)</li> </ul> <hr/> <h4 id="strongly-convex-functions"><strong>Strongly Convex Functions</strong></h4> <p>A function \(f\) is \(\mu\)-strongly convex if:</p> \[f(x') \geq f(x) + \nabla f(x) \cdot (x' - x) + \frac{\mu}{2} \|x - x'\|^2\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Strongly_Convex-480.webp 480w,/assets/img/Strongly_Convex-800.webp 800w,/assets/img/Strongly_Convex-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Strongly_Convex.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Strongly_Convex" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Convex and Strongly Convex Curve </div> <h4 id="convergence-theorem-for-strongly-convex-functions"><strong>Convergence Theorem for Strongly Convex Functions</strong></h4> <p>If \(f\) is both \(L\)-smooth and \(\mu\)-strongly convex, with step size \(0 &lt; \eta \leq \frac{1}{L}\), then gradient descent achieves convergence with the following inequality:</p> \[\|x^{(k)} - x^*\|^2 \leq (1 - \eta \mu)^k \|x^{(0)} - x^*\|^2\] <p>This implies <strong>linear convergence</strong>, but it depends on \(\mu\). If the estimate of µ is bad then the rate is not great.</p> <hr/> <h4 id="stopping-criterion"><strong>Stopping Criterion</strong></h4> <ul> <li>Stop when \(\|\nabla f(x)\|_2 \leq \epsilon\), where \(\epsilon\) is a small threshold(of our choice). <strong>Why?</strong> At a local minimum, \(\nabla f(x) = 0\). If the gradient becomes small and plateaus, further updates are unlikely to significantly reduce the objective function, so we can stop the gradient updates.</li> <li>Early Stopping <ul> <li>Evaluate the loss on validation data (unseen held-out data) after each iteration.</li> <li>Stop when the loss no longer improves or starts to worsen.</li> </ul> </li> </ul> <hr/> <h3 id="quick-recap-gradient-descent-for-erm"><strong>Quick recap: Gradient Descent for ERM</strong></h3> <p>Given a hypothesis space \(F = \{f_w : X \to Y \mid w \in \mathbb{R}^d\}\), we aim to minimize:</p> \[\hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \ell(f_w(x_i), y_i)\] <p>Gradient descent is applicable if \(\ell(f_w(x_i), y_i)\) is differentiable with respect to \(w\).</p> <h6 id="scalability"><strong>Scalability</strong></h6> <p>At each iteration, we compute the gradient at the current \(w\) as:</p> \[\nabla \hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \nabla_w \ell(f_w(x_i), y_i)\] <p>This requires \(O(n)\) computation per step, as we have to iterate over all n training points to take a single step. To scale better, alternative methods like <strong>stochastic gradient descent (SGD)</strong> can be considered.</p> <hr/> <p>Gradient descent is an indispensable tool for optimization, especially in machine learning. By understanding its principles, convergence properties, and practical considerations, we can effectively tackle a variety of optimization problems.</p> <p>Stay tuned for the next post, where we’ll explore stochastic gradient descent and its variations for scalability!</p> <hr/> <h5 id="references--good-resources-for-visualizing-gradient-descent"><strong>References &amp; Good Resources for Visualizing Gradient Descent:</strong></h5> <ul> <li><a href="https://aero-learn.imperial.ac.uk/vis/Machine%20Learning/gradient_descent_3d.html" target="_blank">3D Gradient Descent</a></li> <li><a href="https://kaggle.com/code/trolukovich/animating-gradien-descent" target="_blank">Animating Gradient Descent</a></li> <li><a href="https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c" target="_blank">A Visual Explanation of Gradient Descent Methods (Momentum, AdaGrad, RMSProp, Adam) – Towards Data Science</a> (Variations of Gradient Descent)</li> <li><a href="https://medium.com/@yennhi95zz/4-a-beginners-guide-to-gradient-descent-in-machine-learning-773ba7cd3dfe" target="_blank">Yennhi95zz, 2023. #4. A Beginner’s Guide to Gradient Descent in Machine Learning. Medium</a></li> <li><a href="https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif">Path of a Gradient Descent Algorithm</a> (Image Credit)</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[An in-depth exploration of gradient descent, including its convergence and step size considerations.]]></summary></entry><entry><title type="html">Empirical Risk Minimization (ERM)</title><link href="https://monishver11.github.io/blog/2024/ERM/" rel="alternate" type="text/html" title="Empirical Risk Minimization (ERM)"/><published>2024-12-24T23:50:00+00:00</published><updated>2024-12-24T23:50:00+00:00</updated><id>https://monishver11.github.io/blog/2024/ERM</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/ERM/"><![CDATA[<p>Continuing from our discussion of supervised learning, we now dive into <strong>Empirical Risk Minimization (ERM)</strong>. While the ultimate goal is to minimize the true risk, ERM provides a practical way to approximate this goal using finite data. Let’s break it down.</p> <hr/> <h3 id="definition-empirical-risk-minimization"><strong>Definition: Empirical Risk Minimization</strong></h3> <p>A function \(\hat{f}\) is an <strong>empirical risk minimizer</strong> if:</p> \[\hat{f} \in \arg\min_f \hat{R}_n(f),\] <p>where \(\hat{R}_n(f)\) is the empirical risk, defined over a finite sample and the minimum is taken over all functions\(f: X \to Y\). In an ideal scenario, we would want to minimize the true risk \(R(f)\). This raises a critical question:</p> <p><strong>Is the empirical risk minimizer close enough to the true risk minimizer?</strong></p> <h4 id="example-of-erm-in-action"><strong>Example of ERM in Action</strong></h4> <p>Let’s consider a simple case:</p> <ul> <li>Let \(P_X = \text{Uniform}[0,1]\) and \(Y \equiv 1\) (i.e., \(Y\) is always 1).</li> <li>A proposed prediction function:</li> </ul> \[\hat{f}(x) = \begin{cases} 1 &amp; \text{if } x \in \{0.25, 0.5, 0.75\}, \\ 0 &amp; \text{otherwise.} \end{cases}\] <p><strong>Loss Analysis:</strong></p> <p>Under both the square loss and the 0-1 loss:</p> <ul> <li><strong>Empirical Risk</strong>: 0</li> <li><strong>True Risk</strong>: 1</li> </ul> <p><strong>Explanation</strong>:</p> <ul> <li>The <strong>empirical risk</strong> measures the loss on the training data. Since \(\hat{f}(x)\) perfectly predicts the labels for the training points \(x \in \{0.25, 0.5, 0.75\}\), the empirical risk is zero. There are no errors on the observed data points. - The <strong>true risk</strong>, however, measures the loss over the entire distribution of \(P_X\). For all \(x \notin \{0.25, 0.5, 0.75\}\), \(\hat{f}(x)\) incorrectly predicts 0 instead of the correct label 1, resulting in significant errors. Since \(P_X\) is uniform over \([0,1]\), this means \(\hat{f}(x)\) is incorrect for all others data points of the domain, leading to a true risk of 1.</li> </ul> <p>This illustrates a key problem: <strong>ERM can lead to overfitting by simply memorizing the training data.</strong></p> <hr/> <h3 id="generalization-improving-erm"><strong>Generalization: Improving ERM</strong></h3> <p>In the above example, ERM failed to generalize to unseen data. To improve generalization, we must “smooth things out”—a process that spreads and extrapolates information from observed parts of the input space \(X\) to unobserved parts.</p> <p>One solution is <strong>Constrained ERM</strong>: Instead of minimizing empirical risk over all possible functions, we restrict our search to a subset of functions, known as a <strong>hypothesis space</strong>.</p> <h4 id="hypothesis-spaces"><strong>Hypothesis Spaces</strong></h4> <p>A <strong>hypothesis space</strong> \(\mathcal{F}\) is a set of prediction functions mapping \(X \to Y\) that we consider when applying ERM.</p> <p><strong>Desirable Properties of a Hypothesis Space</strong></p> <ul> <li>Includes only functions with the desired “regularity” (e.g., smoothness or simplicity).</li> <li>Is computationally tractable (efficient algorithms exist for finding the best function in \(\mathcal{F}\)).</li> </ul> <p>In practice, much of machine learning involves designing appropriate hypothesis spaces for specific tasks.</p> <h4 id="constrained-erm"><strong>Constrained ERM</strong></h4> <p>Given a hypothesis space \(\mathcal{F}\), the empirical risk minimizer in \(\mathcal{F}\) is defined as:</p> \[\hat{f}_n \in \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i),\] <p>where \(\ell(f(x), y)\) is the loss function.</p> <p>Similarly, the true risk minimizer in \(\mathcal{F}\) is:</p> \[f^*_{\mathcal{F}} \in \arg\min_{f \in \mathcal{F}} \mathbb{E}[\ell(f(x), y)].\] <hr/> <h3 id="excess-risk-decomposition"><strong>Excess Risk Decomposition</strong></h3> <p>We analyze the performance of ERM through <strong>excess risk decomposition</strong>, which breaks down the gap between the true risk(e.g., the function returned by ERM) and the risk of the Bayes optimal function:</p> <p><strong>Again, Definitions</strong></p> <ul> <li> <p><strong>Bayes Optimal Function</strong>:</p> \[f^* = \arg\min_f \mathbb{E}[\ell(f(x), y)]\] </li> <li> <p><strong>Risk Minimizer in \(\mathcal{F}\)</strong>:</p> \[f_{\mathcal{F}} = \arg\min_{f \in \mathcal{F}} \mathbb{E}[\ell(f(x), y)]\] </li> <li> <p><strong>ERM Solution</strong>:</p> \[\hat{f}_n = \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)\] </li> </ul> <h4 id="excess-risk-decomposition-for-erm"><strong>Excess Risk Decomposition for ERM</strong></h4> <p><strong>Definition</strong></p> <p>The <strong>excess risk</strong> measures how much worse the risk of a function \(f\) is compared to the risk of the Bayes optimal function \(f^*\), which minimizes the true risk. Mathematically, it is defined as:</p> \[\text{Excess Risk}(f) = R(f) - R(f^*)\] <p>where:</p> <ul> <li>\(R(f)\) is the true risk of the function \(f\).</li> <li>\(R(f^*)\) is the Bayes risk, i.e., the lowest achievable risk.</li> </ul> <p><strong>Can Excess Risk Be Negative?</strong> No, the excess risk can never be negative because the Bayes optimal function \(f^*\) achieves the minimum possible risk by definition. For any other function \(f\), the risk \(R(f)\) will be equal to or greater than \(R(f^*)\).</p> <h5 id="decomposition-of-excess-risk-for-erm"><strong>Decomposition of Excess Risk for ERM</strong></h5> <p>For the empirical risk minimizer \(\hat{f}_n\), the excess risk can be decomposed as follows:</p> \[\text{Excess Risk}(\hat{f}_n) = R(\hat{f}_n) - R(f^*) = \underbrace{R(\hat{f}_n) - R(f_\mathcal{F})}_{\text{Estimation Error}} + \underbrace{R(f_\mathcal{F}) - R(f^*)}_{\text{Approximation Error}}\] <p>where:</p> <ul> <li>\(f_\mathcal{F}\) is the best function within the chosen hypothesis space \(\mathcal{F}\).</li> <li><strong>Estimation Error</strong>: This term captures the error due to estimating the best function \(f_\mathcal{F}\) using finite training data.</li> <li><strong>Approximation Error</strong>: This term reflects the penalty for restricting the search to the hypothesis space \(\mathcal{F}\) instead of considering all possible functions.</li> </ul> <p><strong>Key Insight: Tradeoff Between Errors</strong> There is always a <strong>tradeoff</strong> between approximation and estimation errors:</p> <ul> <li>A larger hypothesis space \(\mathcal{F}\) reduces approximation error but increases estimation error (due to greater model complexity).</li> <li>A smaller hypothesis space \(\mathcal{F}\) reduces estimation error but increases approximation error (due to limited flexibility).</li> </ul> <p>This tradeoff is crucial when designing models and choosing hypothesis spaces.</p> <hr/> <h3 id="erm-in-practice"><strong>ERM in Practice</strong></h3> <p>In real-world machine learning, finding the exact ERM solution is challenging. We often settle for an approximate solution:</p> <ul> <li>Let \(\tilde{f}_n\) be the function returned by an optimization algorithm.</li> <li>The <strong>optimization error</strong> is:</li> </ul> \[\text{Optimization Error} = R(\tilde{f}_n) - R(\hat{f}_n)\] <p>where:</p> <ul> <li>\(\tilde{f}_n\)is the function returned by the optimization method.</li> <li>\(\hat{f}_n\)is the empirical risk minimizer.</li> </ul> <h5 id="practical-decomposition"><strong>Practical Decomposition</strong></h5> <p>For \(\tilde{f}_n\), the excess risk can be further decomposed as:</p> \[\text{Excess Risk}(\tilde{f}_n) = \underbrace{R(\tilde{f}_n) - R(\hat{f}_n)}_{\text{Optimization Error}} + \underbrace{R(\hat{f}_n) - R(f_{\mathcal{F}})}_{\text{Estimation Error}} + \underbrace{R(f_{\mathcal{F}}) - R(f^*)}_{\text{Approximation Error}}.\] <hr/> <h3 id="summary-erm-overview"><strong>Summary: ERM Overview</strong></h3> <p>To apply ERM in practice:</p> <ol> <li><strong>Choose a loss function</strong> \(\ell(f(x), y)\).</li> <li><strong>Define a hypothesis space</strong> \(\mathcal{F}\) that balances approximation and estimation error.</li> <li><strong>Use an optimization method</strong> to find \(\hat{f}_n = \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)\) (or an approximate solution \(\tilde{f}_n\)).</li> </ol> <p>As the size of training data increases, we can use larger hypothesis spaces to reduce approximation error while keeping estimation error manageable.</p> <hr/> <h3 id="conclusion"><strong>Conclusion</strong></h3> <p>Empirical Risk Minimization (ERM) provides a foundational framework for supervised learning by optimizing a model’s performance on training data. However, achieving a balance between approximation, estimation, and optimization errors is key to building effective models. This naturally raises the question: <strong>How do we efficiently minimize empirical risk in practice, especially for complex models and large datasets?</strong></p> <p>In the next blog, we’ll dive into <strong>Gradient Descent</strong>, one of the most powerful and widely used optimization algorithms for minimizing risk, and explore how it enables us to tackle the challenges of ERM. Stay tuned and see you! 👋</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.]]></summary></entry></feed>