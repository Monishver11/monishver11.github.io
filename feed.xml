<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-16T05:00:25+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A space for journaling my learnings, career, thoughts, and experiences in this amazing ride of life. </subtitle><entry><title type="html">MapReduce Design Patterns</title><link href="https://monishver11.github.io/blog/2025/big-data-5-mr-dp/" rel="alternate" type="text/html" title="MapReduce Design Patterns"/><published>2025-12-15T22:40:00+00:00</published><updated>2025-12-15T22:40:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-5-mr-dp</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-5-mr-dp/"><![CDATA[<h4 id="serialization"><strong>Serialization</strong></h4> <ul> <li>Serialization is the process of turning structured objects into a byte stream for transmission over a network or for writing to persistent storage.</li> <li>Deserialization is the reverse process of turning a byte stream back into a series of structured objects.</li> <li>A good serialization format should be compact, fast, extensible and interoperable.</li> <li>Hadoop uses its own serialization format: Writable.</li> <li>Hadoop comes with a large selection of Writable classes, which are available in the org.apache.hadoop.io package.</li> <li>There are Writable wrappers for all the Java primitive types except char (which can be stored in an IntWritable).</li> <li>All Writable wrappers have a get() and set() method for retrieving and storing the wrapped value. Example: IntWritable count = new IntWritable(42).</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mr-dp1-480.webp 480w,/assets/img/mr-dp1-800.webp 800w,/assets/img/mr-dp1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mr-dp1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mr-dp-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Text is the Writable wrapper for mutable UTF-8 strings. Ex: Text word = new Text(“Hadoop”);</li> <li>BytesWritable is the Writable wrapper for byte[].</li> <li>NullWritable is a special type of Writable, which has zero-length serialization. No bytes are written to or read from the stream. It is used as a placeholder.</li> <li>For example, in MapReduce, a key or a value can be declared as a NullWritable when you don’t need to use that position, eﬀectively storing a constant empty value. It is an immutable singleton, and the instance can be retrieved by calling NullWritable.get(). Ex: NullWritable nullKey = NullWritable.get();</li> </ul> <h4 id="counters"><strong>Counters</strong></h4> <ul> <li>Counters are a useful channel for gathering statistics about the job: For quality control (Example: what’s the percentage of records that are invalid?), For application-level statistics (Example: how many users in the dataset are between the ages of 18—64?)</li> <li>MapReduce allows user code to define a set of counters, which are then incremented as desired in the mapper or reducer.</li> <li>Counters are defined by a Java enum, which serves to group related counters.</li> <li>A job may define any number of enums, each with any number of fields. The name of the enum is the group name. The enum’s fields are the counter names.</li> <li>Counters are global: the MapReduce framework aggregates them across all mappers and reducers to produce a grand total at the end of the job.</li> <li>Ex: enum Temperature { MISSING, MALFORMED}</li> <li>context.getCounter(Temperature.MALFORMED).increment(1);</li> <li>context.getCounter(Temperature.MISSING).increment(1);</li> <li>context.getCounter(“TemperatureQuality”, parser.getQuality()).increment(1); //dynamic counter, here “TemperatureQuality” is a manual group name (not an enum).</li> <li>Note: In Hadoop, counters are defined and incremented by the Mapper or Reducer, tracked locally by the NodeManager, aggregated by the Application Master, and finally reported to the client by the Resource Manager at job completion.</li> <li>Note: Hadoop also provides built-in counter groups such as FileSystemCounters (bytes read/written), TaskCounters (records processed, spilled data), and JobCounters (launched or failed tasks).</li> <li>Ex: Counter hdfsRead = context.getCounter(“FileSystemCounters”, “HDFS_BYTES_READ”); //access built-in FileSystem counter</li> </ul> <h4 id="mapreduce-design-patterns"><strong>MapReduce design patterns</strong></h4> <ul> <li>Summarization patterns <ul> <li>Numerical summarizations</li> <li>Inverted index summarizations</li> <li>Counting with counters</li> </ul> </li> <li>Filtering patterns <ul> <li>Filtering</li> <li>Bloom filtering</li> <li>Top ten</li> <li>Distinct</li> </ul> </li> <li>Data organization patterns <ul> <li>Structured to hierarchical</li> <li>Partitioning</li> <li>Binning</li> <li>Total order sorting</li> <li>Shuﬄing</li> </ul> </li> <li>Join patterns <ul> <li>Reduce-side join</li> <li>Replicated join</li> <li>Cartesian product</li> </ul> </li> <li>Metapatterns <ul> <li>Job chaining</li> <li>Chain folding</li> <li>Job merging</li> </ul> </li> <li>Input and output patterns</li> </ul> <p><strong>TODO:</strong> Add details, explanation and images to MapReduce design patterns;</p>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 5]]></summary></entry><entry><title type="html">Big Data Processing Concepts &amp;amp; MapReduce</title><link href="https://monishver11.github.io/blog/2025/big-data-4-mapreduce/" rel="alternate" type="text/html" title="Big Data Processing Concepts &amp;amp; MapReduce"/><published>2025-12-15T22:39:00+00:00</published><updated>2025-12-15T22:39:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-4-mapreduce</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-4-mapreduce/"><![CDATA[<h4 id="big-data-processing-concepts"><strong>Big data processing concepts</strong></h4> <ul> <li>Parallel data processing <ul> <li>Parallel data processing reduces the execution time by dividing a single large job into multiple smaller tasks that run concurrently. Ex: a single machine with multiple processors or cores.</li> </ul> </li> <li>Distributed data processing <ul> <li>Distributed data processing is achieved through physically separate machines that are networked together as a cluster.</li> </ul> </li> <li>Hadoop <ul> <li>Hadoop is an open-source framework for large-scale data storage and data processing that is compatible with commodity hardware.</li> <li>It can be used as an analytics engine for processing large amounts of structured, semi-structured and unstructured data.</li> <li>It implements the MapReduce processing framework.</li> </ul> </li> <li>Processing workloads <ul> <li>A processing workload in Big Data is defined as the amount and nature of data that is processed within a certain amount of time.</li> <li>Workloads are usually divided into two types: Batch processing and transactional processing.</li> <li>Batch processing: <ul> <li>(A.k.a. offline processing) involves processing data in batches and usually imposes delays, which results in high-latency responses.</li> <li>Batch workloads typically involve large quantities of data with sequential read/writes and comprise of groups of read or write queries.</li> <li>Queries can be complex and involve multiple joins.</li> <li>OLAP (online analytical processing) systems commonly process workloads in batches.</li> <li>Gist: In batch processing, data is read and written in large, continuous chunks on disk (sequentially), which is faster than random access. Instead of many small operations, the system groups multiple read/write queries—such as aggregations, joins, or filters—into a single large job (like MapReduce), processing them together for high throughput but with higher latency.</li> </ul> </li> <li>Transactional processing: <ul> <li>(A.k.a online processing) follows an approach whereby data is processed interactively without delay, resulting in low-latency responses.</li> <li>Transaction workloads involve small amounts of data with random reads and writes and fewer joins.</li> <li>OLTP (online transaction processing) system fall within this category.</li> </ul> </li> </ul> </li> </ul> <h4 id="mapreduce"><strong>MapReduce</strong></h4> <ul> <li>MapReduce is a widely used implementation of a batch processing framework.</li> <li>It is highly scalable and reliable and is based on the principle of divide-and-conquer, which provides built-in fault tolerance and redundancy.</li> <li>MapReduce does not require that the input data conform to any particular data model. Therefore, it can be used to process schema-less datasets.</li> <li>A dataset is broken down into multiple smaller parts, and operations are performed on each part independently and in parallel.</li> <li>The results from all operations are then summarized to arrive at the answer.</li> <li>Traditionally, data processing requires moving data from the storage node to the processing node that runs the data processing algorithm. This approach works fine for smaller datasets. However, with large datasets, moving data can incur more overhead than the actual processing of the data.</li> <li>With MapReduce, the data processing algorithm is instead moved to the nodes that store the data. The data processing algorithm executes in parallel on these nodes, thereby eliminating the need to move the data first. It saves network bandwidth and reduces processing time for large datasets.</li> <li>Terminology: <ul> <li>A MapReduce job is a unit of work that the client wants to be performed.</li> <li>Hadoop runs the job by dividing it into tasks: map tasks and reduce tasks.</li> <li>Hadoop divides the input to a MapReduce job into fixed-size splits.</li> <li>Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-1-480.webp 480w,/assets/img/mapreduce-1-800.webp 800w,/assets/img/mapreduce-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Map: <ul> <li>The dataset is divided into multiple smaller splits. Each split contains multiple key-value pairs.</li> <li>The map function (“mapper”) executes user-defined logic on each split: (K1, V1) -&gt; list(K2, V2)</li> <li>A mapper may generate zero(filtering) or more than one(demultiplexing) key-value pairs.</li> </ul> </li> <li>Combine: <ul> <li>With larger datasets, moving data between map and reduce stages is more expensive than the actual processing.</li> <li>The optional combiner function summarizes a mapper’s output before it gets processed by the reducer: (K2, list(V2)) -&gt; list(K2, V2)</li> </ul> </li> <li>Partition: <ul> <li>If more than one reducer is involved, a partitioning function (“partitioner”) divides the output from the mapper or combiner (if used) into partitions between reducer instances.</li> <li>Although each partition contains multiple key-value pairs, all records for a particular key are assigned to the same partition.</li> </ul> </li> <li>Shuffle &amp; Sort: <ul> <li>Output from all partitioners is copied across the network to the nodes running the reduce task.</li> <li>The key-value pairs are grouped and sorted according to the keys, list(K2, V2) -&gt; (K2, list(V2))</li> </ul> </li> <li>Reduce: <ul> <li>The reduce function (“reducer”) further summarizes its input: (K2, list(V2)) -&gt; list(K3, V3).</li> <li>The output of the reducer is then written as a separate file. One file per reducer.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-2-480.webp 480w,/assets/img/mapreduce-2-800.webp 800w,/assets/img/mapreduce-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>MapReduce in action (Ex: NCDC weather dataset) <ul> <li>Note: In Hadoop’s TextInputFormat, each line of a file is given a key representing its byte offset from the start of the entire file, not just the split. Offsets are used instead of line numbers because files are split and processed in parallel across multiple mappers, and byte offsets allow each mapper to locate its data efficiently without reading previous lines, ensuring scalability and consistency.</li> <li>Note: Line numbers can’t be used in Hadoop because the input files are split and processed in parallel by multiple mappers. Each mapper starts reading from a different byte position in the file, so it has no way to know how many lines came before its split without scanning the entire file sequentially. Byte offsets, on the other hand, can be determined directly from the file’s position on disk, making them independent, efficient, and uniquely identifiable across splits — perfect for distributed processing.</li> <li>The map function extracts the year and the air temperature, and emits them as its output.</li> <li>The output from the map function is processed by the MapReduce framework before being sent to the reduce function. This processing sorts and groups the key-value pairs by key.</li> <li>For each input, the reduce function iterates through the list and picks up the maximum reading ￼</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-3-480.webp 480w,/assets/img/mapreduce-3-800.webp 800w,/assets/img/mapreduce-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-4-480.webp 480w,/assets/img/mapreduce-4-800.webp 800w,/assets/img/mapreduce-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ ￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-5-480.webp 480w,/assets/img/mapreduce-5-800.webp 800w,/assets/img/mapreduce-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-6-480.webp 480w,/assets/img/mapreduce-6-800.webp 800w,/assets/img/mapreduce-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ ￼</p> <ul> <li>Data flow <ul> <li>Where to run the map task?</li> <li>Data locality: run the map task on a node where the input data resides in HDFS, because it doesn’t use valuable cluster bandwidth.</li> <li>If not possible, the job scheduler will look for a free map slot on a node in the same rack as one of the blocks, and the required data block is transferred over the rack’s local network.</li> <li>If still not possible, an oﬀ-rack node is used, which results in an inter-rack network transfer.</li> <li>So, what’s transferred is the actual HDFS block data required by the Map task to perform its computation. ￼</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-7-480.webp 480w,/assets/img/mapreduce-7-800.webp 800w,/assets/img/mapreduce-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ </p> <ul> <li>How large is an input split? <ul> <li>Map tasks process input splits in parallel. So the processing is better load balanced when the splits are small.</li> <li>However, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time.</li> <li>For most jobs, a good split size tends to be the size of an HDFS block, which is 128 MB by default. It is the largest size of input that can be guaranteed to be stored on a single node.</li> </ul> </li> <li>Where to store the map output? <ul> <li>Map tasks write their output to the local disk, not to HDFS. Why? Map output is intermediate output. It’s processed by reduce tasks to produce the final output. Once the job is complete, the map output can be thrown away.</li> <li>Storing it in HDFS with replication would be overkill.</li> <li>What if the node running the map task fails before the map output has been consumed by the reduce task? If a node fails before the reduce phase reads its map output, Hadoop simply re-runs the failed map task on another node. Since the map output is intermediate and deterministic, it can be regenerated from the original input data stored safely in HDFS.</li> </ul> </li> <li>Why doesn’t Hadoop move the map task to another node that holds the same HDFS block instead of transferring the data? Why move data instead of computation? <ul> <li>A: Hadoop’s scheduler tries first to move computation (the map task) to where the data already resides — this is called data locality, and it’s the preferred option. However, if all nodes holding that block are busy (no free map slots), the scheduler can’t wait indefinitely because it would delay the job. In that case, it assigns the task to another available node and transfers the required data block over the network. So, Hadoop only moves data as a fallback when moving computation isn’t immediately possible, balancing performance and cluster utilization.</li> </ul> </li> <li>Do reduce tasks enjoy data locality?</li> <li>The input to each reduce task is normally the output from all mappers. ￼</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-8-480.webp 480w,/assets/img/mapreduce-8-800.webp 800w,/assets/img/mapreduce-8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-8" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Minimizing the data transferred between map and reduce tasks <ul> <li>The combiner function is an optimization that runs on the map output to reduce the amount of data transferred to the reducers by performing local aggregation. Hadoop does not provide a guarantee of how many times it will call it for a particular map output record. Calling the combiner function zero, one, or many times should produce the same output from the reducer.</li> <li>It works best for associative and commutative operations, where partial results can be safely combined.</li> <li>Which of the following data processing can benefit from a combiner function? <ul> <li>Count the number of occurrences. YES</li> <li>Find the maximum value. YES</li> <li>Find the average value. NO, since the combiner may process subsets differently, leading to incorrect results unless additional logic (like tracking sums and counts separately) is used.</li> <li>Filter values based on a predicate. YES</li> </ul> </li> </ul> </li> </ul> <h4 id="architecture"><strong>Architecture</strong></h4> <p><strong>YARN (Yet Another Resource Negotiator)</strong></p> <ul> <li>YARN is Hadoop’s cluster resource management system.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-9-480.webp 480w,/assets/img/mapreduce-9-800.webp 800w,/assets/img/mapreduce-9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-9" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ ￼</p> <ul> <li>YARN provides its core services via two types of long-running daemons.</li> <li>Resource manager (only one in the cluster): manage the use of resources across the cluster.</li> <li>Node managers (on each node): launch and monitor containers. A container executes an application-specific process with a constrained set of resources (memory, CPU, …). ￼</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-10-480.webp 480w,/assets/img/mapreduce-10-800.webp 800w,/assets/img/mapreduce-10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-10" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Running an application: A client contacts the resource manager and asks it to run an application master process. The resource manager finds a node manager that can launch the application master in a container. The application master may request more containers from the resource manager. The application master use them to run a distributed computation (e.g., MapReduce).</li> <li>Types of YARN scheduler: (FIFO, Capacity, FAIR) scheduler.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-11-480.webp 480w,/assets/img/mapreduce-11-800.webp 800w,/assets/img/mapreduce-11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-11" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <p><strong>Hadoop</strong></p> <ul> <li>Running a MapReduce job:</li> <li>Client: submit the MapReduce job.</li> <li>YARN resource manager: coordinate the allocation of compute resources in cluster.</li> <li>YARN node managers: launch and monitor the containers on machines in the cluster.</li> <li>MapReduce application master: coordinate the tasks running the MR job. The application master and the MapReduce tasks run in containers scheduled by the resource manager and managed by the node managers.</li> <li>HDFS: share job files between the other entities.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-12-480.webp 480w,/assets/img/mapreduce-12-800.webp 800w,/assets/img/mapreduce-12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-12" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <ul> <li>Progress &amp; status updates:</li> <li>A job and each of its tasks have a status.</li> <li>The state of the job or task (e.g., running, successfully completed, failed).</li> <li>The progress of maps and reduces.</li> <li>The values of the job’s counters.</li> <li>A status message or description.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-13-480.webp 480w,/assets/img/mapreduce-13-800.webp 800w,/assets/img/mapreduce-13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-13" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <ul> <li>Q: Does each node hold HDFS blocks and containers for task execution, managed by the NodeManager?</li> <li>A: Yes. Each node stores blocks as part of HDFS and can also run containers, which execute tasks (Map or Reduce) scheduled by YARN. The NodeManager on each node handles launching, monitoring, and reporting on these containers, while the ResourceManager coordinates cluster-wide resource allocation.</li> <li>Q: What does the YARN scheduler do? Does it schedule Map and Reduce tasks, and how is memory/CPU utilization handled?</li> <li>A: The YARN scheduler (FIFO, Capacity, or FAIR) manages the allocation of resources—CPU, memory, and containers—across the cluster. It decides which nodes get containers for tasks. While it schedules containers for Map and Reduce tasks indirectly via the ApplicationMaster, the actual memory and CPU usage is constrained per container on individual nodes as specified by the scheduler.</li> <li>Q: Does the ResourceManager use HDFS block information to improve data locality and distributed efficiency?</li> <li>A: Yes. The ResourceManager, through the ApplicationMaster, tries to schedule tasks on nodes that already hold the input HDFS blocks to exploit data locality, reducing network transfer and improving performance. If local nodes aren’t available, it may schedule tasks on the same rack or another node as a fallback.</li> <li>Q: What are job counters in Hadoop, and what do they mean?</li> <li>A: Counters are metrics collected during job execution. They track things like the number of bytes read/written, number of records processed, map/reduce task attempts, and custom user-defined counters. Counters provide insight into job progress, efficiency, and can help debug or optimize jobs.</li> <li>Q: What is a status message, and how is it used?</li> <li>A: A status message is a short description of the current state of a job or task (e.g., “Reading input”, “Merging outputs”, “Task failed”). It helps users and administrators monitor progress, understand failures, and debug issues during job execution.</li> <li>Gist: In Hadoop with YARN, each node stores HDFS blocks and also runs containers for executing tasks, with the NodeManager handling container lifecycle and reporting. The ResourceManager coordinates cluster-wide resource allocation, while the YARN scheduler (FIFO, Capacity, or FAIR) decides which nodes get containers, controlling CPU and memory usage per container. To maximize efficiency, tasks are ideally scheduled on nodes holding the relevant HDFS blocks, leveraging data locality; if unavailable, tasks may run on the same rack or another node. During execution, job counters track metrics like bytes read/written, records processed, and task attempts, providing insight for monitoring and optimization. Status messages report the current state of jobs or tasks, helping users and administrators monitor progress and debug issues.</li> </ul> <p><strong>Resilience</strong></p> <ul> <li>Where can a failure happen? It can happen in 4 places: A MapReduce task, the MapReduce application master, YARN node manager or the YARN resource manager.</li> <li>Task failure: <ul> <li>Possible causes: Mapper/reducer bug, JVM bug, Hanging tasks: timeout (default: 10 min) exceeded without a progress update.</li> <li>The application master will reschedule the task on another node manager.</li> <li>If a task fails too many times (default: 4), it will not be retried again, and the whole job will fail.</li> </ul> </li> <li>Application master failure: <ul> <li>An application master sends periodic heartbeats to the resource manager.</li> <li>In the event of application master failure, the resource manager will detect the failure and start a new instance of the master running in a new container (managed by a node manager).</li> <li>In the case of the MapReduce application master, it’ll use the job history to recover the state of the tasks that were already run by the (failed) application, so they don’t have to be rerun.</li> <li>If a MapReduce application master fails too many times (default: 2), it will not be retried again, and the whole job will fail.</li> <li>Q: Where is the MapReduce job history stored and retrieved if the application master fails?</li> <li>A: The job history is persisted in HDFS, not in the application master’s memory. When a new application master is started after a failure, it reads the job history from HDFS to recover the state of already completed or partially completed tasks, so they don’t have to be rerun.</li> </ul> </li> <li>Node manager failure: <ul> <li>The resource manager will notice a node manager that has stopped sending heartbeats (default: 10 min) and remove it from its pool of nodes to schedule containers on.</li> <li>Q: If a NodeManager fails, do we need to rerun completed Map tasks?</li> <li>Yes, because the intermediate results are only stored in the node’s disk and not in hdfs. So, we’ve to rerun, so as to collect all the intermediate results from all mappers before the reducer part.</li> <li>Q: If a NodeManager fails, do we need to rerun completed Reduce tasks?</li> <li>A: No. Completed Reduce tasks write their output to HDFS, which is replicated and durable. Only in-progress Reduce tasks on the failed node need to be rescheduled on another node.</li> </ul> </li> <li>Resource manager failure: <ul> <li>Failure of the resource manager is serious, because without it, neither jobs nor task containers can be launched.</li> <li>In the default configuration, the resource manager is a single point of failure, since in the (unlikely) event of machine failure, all running jobs fails, and can’t be recovered.</li> <li>For high availability (HA), we need to configure a standby resource manager.</li> <li>Q: In ResourceManager failure, do we need to store information about all running applications?A: Yes. To recover after a ResourceManager failure, information about all running applications—including job metadata and application master states—needs to be persisted(not in HDFS). In HA setups, a standby ResourceManager keeps this information in ZooKeeper or shared storage(NFS) to resume operations without losing job information.</li> <li>Q: In ResourceManager failure, do we need to store NodeManager information?A: No. It can be reconstructed back again from the NodeManager’s heartbeats.</li> <li>Q: In ResourceManager failure, what about task information?A: Task information (status, progress, container allocation) is primarily tracked by the ApplicationMaster. The ResourceManager coordinates resources but doesn’t store detailed task outputs. In HA setups, the standby ResourceManager works with running ApplicationMasters to continue scheduling containers and managing tasks without losing track of progress.</li> </ul> </li> <li>Shuffle and sort (Most important part) <ul> <li>MapReduce guarantees that the input to every reducer is sorted by key.</li> <li>Flow: In MapReduce, each map task processes an input split, which contains multiple records. The map function runs on each record and generates intermediate key-value pairs, which are initially buffered in memory rather than written to disk immediately. This in-memory buffering allows the system to perform local partitioning, which determines which reducer each key belongs to, as well as optional combining, which reduces the number of intermediate records by aggregating data locally before transfer. Additionally, the intermediate data is sorted in memory by key to facilitate efficient merging later. Once the buffer reaches a configured threshold (default 100 MB), it is spilled to disk as a temporary file. Large map outputs may generate multiple spill files, which are later merged using a merge sort into a single sorted file per partition. During the shuffle phase, reducers fetch the intermediate data from all mappers, ensuring that each reducer receives all data corresponding to its assigned partition. Because the map output is already sorted locally, reducers perform a multi-way merge rather than a full sort, which improves efficiency. The final merged data is then streamed directly to the reduce function without writing it back to disk, minimizing I/O. If the intermediate data is too large to fit in memory, MapReduce applies external sorting, reading chunks of data from disk, merging them in memory, and writing back to disk. After merging, the reducer processes the records using the reduce function and writes the final output to HDFS. Throughout this process, combiners may optionally reduce the volume of data transferred, merges are optimized with multi-way strategies, and copying of map outputs can begin even before all map tasks finish, though the reduce function only runs after all intermediate data for that partition has been fetched.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-14-480.webp 480w,/assets/img/mapreduce-14-800.webp 800w,/assets/img/mapreduce-14-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-14" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <p><strong>Speculative execution</strong></p> <ul> <li>The job execution time is sensitive to slow-running tasks (“stragglers”).</li> <li>Hadoop doesn’t try to diagnose and fix slow-running tasks; instead, it tries to detect when a task is running slower than expected and launches another equivalent task as a backup.</li> <li>The scheduler tracks the progress of all tasks of the same type (map and reduce) in a job, and only launches speculative duplicates for the small portion that are running significantly slower than the average.</li> <li>When a task completes successfully, any duplicate tasks that are running are killed since they are no longer needed.</li> <li>Q: Which part of Hadoop handles speculative execution?</li> <li>A: Speculative execution is managed by the Hadoop MapReduce framework itself, specifically by the ApplicationMaster in YARN (Hadoop 2+).</li> <li>Q: Is the YARN scheduler part of the ApplicationMaster, and how do they interact?</li> <li>A: No, the YARN scheduler is part of the ResourceManager and operates cluster-wide, deciding which nodes get containers based on available resources and scheduling policies (FIFO, Capacity, FAIR). It does not manage task-level details like progress or stragglers. The ApplicationMaster, on the other hand, is job-specific. It requests containers from the ResourceManager (which allocates them via the scheduler), monitors task progress, handles failures, and manages speculative execution. Essentially, the ApplicationMaster depends on the scheduler for container allocation, and once allocated, it schedules and manages tasks within those containers, coordinating retries, merges, and data flow. The scheduler handles resource allocation, while the ApplicationMaster handles job and task management.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4]]></summary></entry><entry><title type="html">Hadoop Distributed File System (HDFS)</title><link href="https://monishver11.github.io/blog/2025/big-data-3-hdfs/" rel="alternate" type="text/html" title="Hadoop Distributed File System (HDFS)"/><published>2025-12-15T22:35:00+00:00</published><updated>2025-12-15T22:35:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-3-hdfs</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-3-hdfs/"><![CDATA[<ul> <li>The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware.</li> <li>It provides an interface similar to a POSIX file system (files and directories), but with relaxed requirements.</li> <li>It scales up to 100+ PB of storage and thousands of servers, supporting close to a billion files and blocks.</li> <li>Small gist about POSIX: POSIX (Portable Operating System Interface) is a standard that defines how file systems and operating systems should behave, ensuring consistency across Unix-like systems such as Linux and macOS. In file systems, POSIX specifies that every write must be immediately visible to all readers, files can be modified at any position (supporting random writes), and operations like open, close, read, write, delete, and rename must appear atomic and consistent. It also enforces strict locking and consistency rules for concurrent access, ensuring multiple processes can safely read and write files simultaneously without data corruption.</li> </ul> <h4 id="assumptions-and-goals"><strong>Assumptions and goals:</strong></h4> <ul> <li>Commodity hardware: Hardware failure is the norm rather than the exception. An HDFS instance may consist of thousands of servers. Each component has a non-trivial probability of failure. As a result, some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS.</li> <li>Streaming data access: HDFS is designed more for batch processing (e.g., MapReduce) rather than interactive use by users. The emphasis is on high throughput of data access rather than low latency of data access. Therefore, some POSIX semantics has been relaxed to increase data throughput rates.</li> <li>Large datasets: A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. It should provide high aggregate data bandwidth. It should scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.</li> <li>Simple coherency model: HDFS applications need a write-once-read-many (WORM) access model for files. Files cannot be modified except for appends and truncates. This design avoids complex consistency and synchronization problems that arise from random writes or concurrent updates. By restricting how data can change, HDFS simplifies data coherency management across replicas and achieves higher throughput for large-scale, sequential data access. Ex: a MapReduce application or a web crawler application fits perfectly with this model.</li> <li>Moving computation is cheaper than moving data: A computation requested by an application is much more efficient if it is executed near the data it operates on, especially when the size of the dataset is huge. This minimizes network congestion and increases the overall throughput of the system. So, it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. HDFS provides interfaces for applications to move themselves closer to where the data is located.</li> <li> <p>Portability across heterogeneous hardware &amp; software platforms: HDFS has been designed to be easily portable from on platform to another. It is implemented as a user-level filesystem in Java. Because it’s written in Java, it runs anywhere a Java Virtual Machine (JVM) is available, such as Linux, Windows, or macOS. The JVM acts as a layer between the Java program and the underlying operating system, translating Java instructions into native instructions the OS can execute. And unlike traditional file systems built directly into the operating system kernel, HDFS is a user-level file system, meaning it operates as a regular application process instead of requiring kernel-level changes. This makes it easier to install, update, and move between environments. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.</p> </li> <li>A gist on how Java program is executed from code to instructions in device: When a Java program like HDFS is executed, the process starts with writing source code in .java files, which is then compiled by the Java compiler (javac) into bytecode stored in .class files. These class files, along with necessary resources and configuration, are often bundled into a JAR (Java ARchive) file for easy distribution and deployment. On the target machine, the Java Virtual Machine (JVM) loads the bytecode from the class or JAR files and interprets or Just-In-Time (JIT) compiles it into native machine instructions that the operating system and hardware can execute. This abstraction provided by the JVM allows the same Java program or JAR file to run on any device or OS with a JVM installed, making Java programs, including HDFS, portable across heterogeneous platforms.</li> </ul> <h4 id="architecture"><strong>Architecture</strong></h4> <ul> <li>HDFS has a master-workers architecture.</li> <li>One NameNode: Manages the file system metadata.</li> <li>Many DataNodes: Stores the actual data blocks</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-1-480.webp 480w,/assets/img/hdfs-1-800.webp 800w,/assets/img/hdfs-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The HDFS namespace is a hierarchy of files and dirs. It supports user quotas and access permissions.</li> <li>The file content is split into large blocks (typically 128MB). Large blocks can minimize seek time. A file smaller than a block does not occupy a full block’s worth of storage.</li> <li>Clarification: Disk seek time is the delay caused by moving the read/write head to the data location. By using large blocks, HDFS allows more data to be read sequentially once the head is positioned, reducing the number of seeks needed. Fewer, larger blocks overall improve throughput for reading large files, which is especially beneficial for batch processing like MapReduce.</li> <li>Benefits of blocks as primitive unit: <ul> <li>Support very large files, even larger than any single disk in the network.</li> <li>Simplify storage management and decouple metadata from blocks.</li> <li>Blocks can be replicated for fault tolerance and availability.</li> </ul> </li> <li>NameNode: <ul> <li>The NameNode manages the HDFS namespace.</li> <li>It maintains the file system tree and the metadata for all the files and directories. This information is persisted on disk. The NameNode loads the entire namespace image into memory at startup.</li> <li>The NameNode also knows where every block is located. This information is in memory only, not persisted on disk. It can be reconstructed from DataNodes when the system starts (via heartbeats).</li> <li>Without the NameNode, the file system cannot be used.</li> </ul> </li> <li>DataNode: <ul> <li>DataNodes are the workhorses of HDFS.</li> <li>Each block is independently replicated at multiple DataNodes. An application can specify the number of replicas of a file that should be maintained by HDFS. It is called the replication factor of that file (typically 3).</li> <li>DataNodes store and retrieve blocks when asked by clients or the NameNode.</li> <li>DataNodes sends heartbeats to the NameNode (typically every 3 seconds).</li> <li>DataNodes also report to the NameNode periodically with the lists of blocks they are storing (at startup and every hour).</li> </ul> </li> <li>HDFS client: <ul> <li>The HDFS client is a library that exports the HDFS file system interface.</li> <li>Applications access the file system using the HDFS client.</li> <li>The user application does not need to know that the filesystem metadata and storage are on different servers, and that blocks have multiple replicas.</li> <li>However, the block locations are exposed to the client, so that applications like MapReduce can schedule tasks to where the data are located.</li> </ul> </li> <li>Reading a file <ul> <li>First, the HDFS clients asks the NameNode for the list of DataNodes that host replicas of the blocks of the file.</li> <li>The list is sorted by the network topology distance from the client.</li> <li>Then the client contacts a DataNode directly and requests the transfer of the desired block.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-2-480.webp 480w,/assets/img/hdfs-2-800.webp 800w,/assets/img/hdfs-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Network topology <ul> <li>The distance between two nodes is the sum of their distances to their closest common ancestor.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-3-480.webp 480w,/assets/img/hdfs-3-800.webp 800w,/assets/img/hdfs-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Writing a file <ul> <li>First, the HDFS client asks the NameNode to choose DataNodes to host replicas of the first block of the file.</li> <li>The client organizes a pipeline from node to node and sends the data.</li> <li>When the first block is filled, the client requests new DataNodes to be chosen to host replicas of the next block. A new pipeline is organized, and the client sends the further bytes of the file.</li> <li>Choice of DataNodes for each block is likely to be different.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-4-480.webp 480w,/assets/img/hdfs-4-800.webp 800w,/assets/img/hdfs-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Block placement <ul> <li>Trade-oﬀ between minimizing the write cost, and maximizing data reliability, availability and aggregate read bandwidth. <ul> <li>#1: same node as the client.</li> <li>#2: diﬀerent rack from #1.</li> <li>#3: same rack as #2, but diﬀerent node.</li> <li>This default strategy gives a good balance among: <ul> <li>Reliability: blocks are stored on two racks.</li> <li>Write bandwidth: writes only have to traverse a single network switch.</li> <li>Read performance: choice of two racks to read from.</li> <li>Block distribution across the cluster: clients only write a single block on the local rack.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-5-480.webp 480w,/assets/img/hdfs-5-800.webp 800w,/assets/img/hdfs-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The single-writer, multiple-reader model <ul> <li>The HDFS client that opens a file for writing is granted a lease (lock) for the file; no other client can write to the file. The writer’s lease doesn’t prevent other clients from reading the file; a file may have many concurrent readers.</li> <li>The writing client periodically renews the lease by sending a heartbeat to the NameNode.</li> <li>When the file is closed, the lease is revoked.</li> </ul> </li> <li>Coherency model <ul> <li>A coherency model for a filesystem describes the data visibility of reads and writes for a file.</li> <li>HDFS trades off some POSIX semantics for performance.</li> <li>After creating a file, it is visible in the filesystem namespace. However, the last block’s content may not be visible until the file is closed.</li> <li>If the client needs the visibility guarantee, it can call hflush() explicitly. It guarantees that the data written up to that point in the file has reached all the DataNodes in the write pipeline and is visible to all new readers. However, the data may be in the DataNodes’ memory only.</li> <li>To guarantee that the DataNodes have written the data to disk, call hsync().</li> <li>So, in short: HDFS’s coherency model is tied to its block structure - a file is visible immediately, but the last block may be partially written. Visibility and durability of that block are controlled explicitly via hflush() (memory) and hsync() (disk).</li> </ul> </li> </ul> <h4 id="resilience"><strong>Resilience</strong></h4> <ul> <li>NameNode <ul> <li>NameNode persists checkpoint + journal on disk.</li> <li>Checkpoint: the file system tree and metadata at the specific point in time.</li> <li>Journal (“edit log”): All changes to HDFS since the last checkpoint.</li> <li>CheckpointNode periodically combines the existing checkpoint and journal into a new checkpoint and sends it back to NameNode, which truncates the journal.</li> <li>BackupNode maintains a read-only, synchronized namespace state of the NameNode without block locations.</li> </ul> </li> <li>DataNode <ul> <li>When writing a file, the client computes the checksum for each data block. DataNodes store the checksums locally in a metadata file.</li> <li>When reading a file, the client verifies the checksum. If a block is corrupted, the client notifies the NameNode and fetches another replica of the block.</li> <li>If a DataNode fails or a block is corrupted: <ul> <li>Data can be retrieved from another DataNode storing the block replica.</li> <li>The NameNode marks the replica as unavailable/corrupt.</li> <li>The NameNode schedules creation of new replicas on other DataNodes.</li> </ul> </li> </ul> </li> <li>A small gist about checksum: A checksum is a small value computed from a block of data to verify its integrity. It is generated by applying a hash function (e.g., CRC32) to a block of data. The resulting number changes if even a single bit of the data changes. In HDFS, when data is written, a checksum is generated and stored alongside it. During reads, the checksum is recalculated and compared with the stored value to detect any corruption. If they don’t match, HDFS identifies the block as damaged and retrieves a healthy replica from another DataNode. This ensures reliable and consistent data storage across the system.</li> </ul> <h4 id="optimizations"><strong>Optimizations</strong></h4> <ul> <li>Block caching <ul> <li>Frequently accessed blocks may be explicitly cached in DataNode’s memory.</li> <li>By default, a block is cached in only “one” DataNode’s memory. This # of DataNode’s is configurable on a per-file basis.</li> <li>Users or applications tell the NameNode which files to cache, and for how long.</li> <li>Applications (e.g., MapReduce) can schedule tasks on the DataNode where a block is cached, for increased read performance.</li> <li>The NameNode maintains which DataNodes holds the cache of a specific block, along with other bookkeeping information.</li> </ul> </li> <li>HDFS federation <ul> <li>On a very large clusters with many files, the NameNode’s memory becomes the limiting factor for scaling. This is because the NameNode keeps a reference to every file and block in the filesystem in memory.</li> <li>HDFS federation allows a cluster to scale by adding NameNodes. Each NameNode manages a portion of the filesystem namespace(its own part of the directory tree).</li> <li>Under federation, each NameNode manages a namespace volume, which contains the metadata for the namespace.</li> <li>Namespace volumes are independent of each other. NameNodes do not communicate with one another. If one NameNode fails, the availability of the namespaces managed by other NameNodes will not be affected.</li> <li>Under federation, each NameNode also manages a block pool, which contains all the blocks for the files in the namespace.</li> <li>Block pool storage is not partitioned among DataNodes. DataNodes register with each NameNode in the cluster and store blocks from multiple block pools.</li> <li>The conceptual flow: In very large HDFS clusters, the single NameNode becomes a scalability bottleneck because it must keep metadata for every file and block in memory. HDFS Federation solves this by allowing multiple independent NameNodes, each managing its own namespace volume—a separate portion of the filesystem’s directory structure. Along with the namespace, each NameNode also manages a corresponding block pool, which contains all the physical data blocks belonging to files in that namespace. The DataNodes in the cluster are shared among all NameNodes and register with each of them, storing blocks from multiple block pools simultaneously. Note that the DataNode storage is shared physically, but logically partitioned into multiple block pools - one per NameNode. This design separates metadata management from physical storage, allowing the system to scale horizontally, isolate faults (a failure in one NameNode doesn’t affect others), and support multiple namespaces while still using the same underlying DataNodes for efficient shared storage.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-6-480.webp 480w,/assets/img/hdfs-6-800.webp 800w,/assets/img/hdfs-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-7-480.webp 480w,/assets/img/hdfs-7-800.webp 800w,/assets/img/hdfs-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>HDFS High Availability (HA) <ul> <li>Although checkpoints of the NameNode protect against data loss, they don’t provide high availability of the filesystem.</li> <li>The NameNode is still a single point of failure (SPOF).</li> <li>If the NameNode fails or performs routine maintenance, the entire Hadoop ecosystem becomes out of service until a new NameNode is brought online. Manual intervention is required. On large clusters with many files and blocks, starting a new NameNode can take 30 minutes or more.</li> <li>With HDFS high availability (HA), there are a pair of NameNodes in an active-standby configuration.</li> <li>If the Active NameNode fails, the Standby NameNode takes over its duties to continue servicing client requests without a significant interruption (~1 minute).</li> <li>Even if the Standby NameNode is down when the Active NameNode fails, the sysadmin can still start the Standby NameNode from cold (same as non-HA).</li> <li>HDFS High Availability (HA) requires a few architectural changes: <ul> <li>The NameNodes must use highly available shared storage (e.g., NFS or the Quorum Journal Manager) to share the journal.</li> <li>DataNodes must send block reports to both NameNodes because the block mappings are stored in a NameNode’s memory, not on disk.</li> <li>Clients must be configured to handle NameNode failover transparently.</li> <li>Checkpoint/BackupNode’s role is subsumed by the Standby NameNode, which takes periodic checkpoints of the Active NameNode’s namespace.</li> </ul> </li> <li>The transition from the Active NameNode to the Standby NameNode is managed by a failover controller. By default, it uses ZooKeeper to ensure that only one NameNode is active.</li> <li>Each NameNode runs a lightweight failover controller process, which monitors its NameNode for failures (using a simple heartbeating mechanism) and triggers a failover should a NameNode fail.</li> <li>Failover may also be initiated manually (e.g., for routine maintenance). This is known as a graceful failover, since the failover controller arranges an orderly transition for both NameNodes to switch roles.</li> <li>However, in the case of an ungraceful failover, it’s impossible to be sure that the failed NameNode has stopped running.</li> <li>For example, a slow network or a network partition can trigger a failover transition, even though the previous Active NameNode is still running and thinks it’s still the Active NameNode.</li> <li>The HA implementation employs fencing methods to ensure that the previous Active NameNode is prevented from doing any damage or causing corruption.</li> <li>Have you heard of STONITH (shoot the other node in the head)?</li> <li>Q: How does each NameNode monitor failures in HDFS High Availability? A: Each NameNode runs a lightweight Failover Controller (ZKFC) that monitors the health of its own NameNode using a heartbeat mechanism. Both ZKFCs communicate with ZooKeeper, which coordinates and ensures that only one NameNode is Active at any time. If the Active NameNode fails, ZooKeeper triggers the Standby’s ZKFC to take over and become Active, ensuring continuous service.</li> <li>Q: What are fencing methods and why are they needed? A: Fencing is a safety mechanism used during failover to prevent the old Active NameNode from making changes after a Standby takes over. This is crucial because the old Active might still be running but disconnected (e.g., due to a network partition). Fencing isolates or disables the old node, commonly by killing its process, revoking access to shared storage, or even powering off the machine, ensuring that only one NameNode can write to the filesystem metadata, preventing corruption.</li> <li>Q: What is STONITH and how is it related? A: STONITH (Shoot The Other Node In The Head) is a type of fencing where the old node is forcibly shut down before the Standby takes over. In HDFS HA, it guarantees that the previous Active NameNode cannot interfere with the new Active, providing a strong safeguard against simultaneous writes and ensuring data consistency.</li> </ul> </li> <li>Balancer <ul> <li>Over time, the distribution of blocks across DataNodes can become unbalanced. An unbalanced cluster can aﬀect locality for applications (e.g., MapReduce), and it puts a greater strain on the highly utilized DataNodes.</li> <li>The balancer program is a daemon… <ul> <li>It redistributes blocks by moving them from overutilized DataNodes to underutilized DataNodes.</li> <li>It adheres to the block replica placement policy that makes data loss unlikely by placing block replicas on diﬀerent racks.</li> <li>It minimizes inter-rack data copying in the balancing process.</li> </ul> </li> <li>Q: What is the HDFS Balancer daemon and where does it run? A: The Balancer is a separate user-level daemon that runs on the Hadoop cluster, typically launched from a client node or NameNode host. It is not part of the NameNode or DataNode processes.</li> <li>Q: How does the Balancer work? A: It communicates with the NameNode to get cluster metadata, identifies overutilized and underutilized DataNodes, and moves blocks accordingly. It follows replica placement rules to avoid data loss and minimize inter-rack transfers.</li> <li>Q: What happens if there are failovers or crashes? A: If the Active NameNode fails, the Balancer reconnects to the new Active NameNode. If a DataNode fails, block moves to that node are paused or rescheduled. If the Balancer itself crashes, it can be restarted safely, resuming block moves without affecting cluster integrity.</li> <li>Q: If the Balancer runs outside HDFS, how does it access block information? A: The Balancer accesses block information through the HDFS client interface. It communicates with the NameNode using HDFS RPC APIs to retrieve metadata about all blocks, including which DataNodes store them and their disk utilization.</li> <li>Q: How does the Balancer move blocks between DataNodes? A: After obtaining metadata from the NameNode, the Balancer schedules block transfers between DataNodes. The actual data movement happens directly between DataNodes, while the Balancer only coordinates the process based on the metadata it received.</li> <li>Q: Why can the Balancer manage blocks even though it’s outside HDFS processes? A: Because it uses the official HDFS client APIs, the Balancer can read cluster metadata, monitor utilization, and orchestrate block redistribution without being part of the NameNode or DataNode processes.</li> </ul> </li> <li>Block scanner <ul> <li>Every DataNode runs a block scanner, which periodically verifies all the blocks stored on the DataNode.</li> <li>This allows bad blocks to be detected and fixed before they are read by clients.</li> <li>The scanner maintains a list of blocks to verify and scans them one by one for checksum errors.</li> <li>It also employs a throttling mechanism to preserve disk bandwidth on the DataNode. This throttling mechanism limits the speed or resource usage of the block scanner so it does not overwhelm the DataNode’s disk or network.</li> </ul> </li> </ul> <h4 id="usage"><strong>Usage</strong></h4> <ul> <li>The Hadoop FS shell provides commands that directly interact with HDFS.</li> <li>Ex: hadoop fs -command <args></args></li> </ul> <p><strong>Doubt: Is HDFS C+P or A+P under CAP theorem?</strong></p> <ul> <li>From the sources, I feel it falls under C+P. One of the answer/argument that makes sense is below.</li> <li>The cluster is consistent as long as the primary namenode is available. when a namenode becomes unavailable, the secondary namenode gets queried, however writes can become delayed / rejected. so despite not being an explicit “single point of failure”, it will still affect the “perceived availability and/or consistency of data” to external interfaces. At the end of the day, the primary namenode handles data consistency, and is synced to the secondary namenode (i.e. it does not require consensus). This is only “partially available” in practice.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3]]></summary></entry><entry><title type="html">Reading Notes from Aleksa Gordic’s GPU BlogPost</title><link href="https://monishver11.github.io/blog/2025/aleksagordic-gpu-blog-notes/" rel="alternate" type="text/html" title="Reading Notes from Aleksa Gordic’s GPU BlogPost"/><published>2025-12-15T13:28:00+00:00</published><updated>2025-12-15T13:28:00+00:00</updated><id>https://monishver11.github.io/blog/2025/aleksagordic-gpu-blog-notes</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/aleksagordic-gpu-blog-notes/"><![CDATA[<p><a href="https://www.aleksagordic.com/blog/matmul">https://www.aleksagordic.com/blog/matmul</a></p> <p><strong>Fundamentals of NVIDIA GPU architecture(on H100):</strong></p> <ul> <li>Tensor cores: wgmma instrutions needed to fully exploit it.</li> <li>CUDA cores: arithmetic instructions usually have a latency of ~4-15 cycles.</li> <li>(Tensor cores, CUDA cores, warp scheduler, LD/ST and register file(16k 32 bit)) x4 of these per SM, aka quadrants.</li> <li>TMA(Tensor memory accelerator): For small requests, TMA loads have higher latency than regular async copies (due to address generation overhead). It handles load/store transfers between GMEM and SMEM (with swizzling).</li> <li>1KiB of SMEM(Shared memory) goes for system use per block, so effectively we have: 228 - numblocks * 1kB kBs.</li> <li>Shared mem is faster than L1, as there is no need to tag stage comparisons for hit/miss.</li> <li>L1 cache line = 128B (=threads in warp fetching 4B floats)</li> <li>L1 <em>can</em> be used for register spill-over when register pressure is high</li> <li>Distributed shared memory(DSMEM): pooled shared memories (SMEM) of a physically close group of SMs (a GPC). Worse bandwidth/latency compared to shared mem, but better than L2.</li> <li>No. of SM’s: N=144(on the die), N=132(SXM5) and N=114(PCIe)</li> <li>L2: <ul> <li>We can set the granularity of the data fetch size to 32, 64 or 128B using cudaDeviceSetLimit.</li> <li>It is physically partitioned into two parts; each SM connects directly to only one partition and indirectly to the other through the crossbar.</li> <li>Residency control: we can set a part of L2 cache for persistent data accesses and map it to a chunk of GMEM</li> <li>It’s possible to redirect power from L2 to SMs (demonstrated in MLPerf 2024)</li> <li>L2 cache line = 128B, 4 sectors (sector==32B), same as L1</li> <li>Contains data compression circuitry and does global atomics</li> <li>60 MiB on the die (50MiB for SXM/PCIe)</li> <li>real read BW = 12-14 TB/s (near), far is significantly slower. Latency ~200 cycles.</li> </ul> </li> <li>GPC: Graphics processing clusters. Each GPC contains 18 SMs, so there are 8 GPCs on the GPU. Four GPCs connect directly to one L2 partition and the other four to the second partition.</li> <li>VRAM/device memory (80 GB, common form factor): <ul> <li>GMEM - 32, 64, 128B mem transactions granularity</li> <li>constant memory - very small ~64KiB</li> <li>local memory - 512 KiB/thread (register spill space)</li> </ul> </li> <li>To connect to other GPUs - nvlink v4. Bi-BW = 900 GB/s, Uni-BW = 450 GB/s (18 links with 25GB/s)</li> <li>To connect to x86 CPU, DPUs etc - PCIe Gen 5. Bi-BW = 128 GB/s and Uni-BW = 64 GB/s</li> <li> <p>Note: There are a few other smaller caches for instructions.</p> </li> <li>The memory system in a GPU is highly hierarchical, much like in CPU architectures.</li> <li>This hierarchy is dictated by physics and circuit design: SRAM cells are faster but larger (the control circuitry that enables their speed also increases their area), while DRAM cells are smaller/denser but slower. The result is that faster memory is lower capacity and expensive, while slower memory can be provided in much larger quantities.</li> <li>This trade-off between capacity and latency is exactly why cache hierarchies exist.</li> <li>Moving from device memory down to registers (levels 1-5), you see a clear trend: bandwidth increases by orders of magnitude, while both latency and capacity decrease by similar orders of magnitude.</li> <li>A few immediate implications follow: <ul> <li>Keep the most frequently accessed data as close as possible to the compute units.</li> <li>Minimize accesses to the lower levels of the hierarchy, especially device memory (GMEM).</li> </ul> </li> <li>One additional component worth noting is the Tensor Memory Accelerator (TMA), introduced with Hopper. TMA enables asynchronous data transfers between global memory and shared memory, as well as across shared memories within a cluster. It also supports swizzling to reduce bank conflicts.</li> </ul> <p>Compute:</p> <ul> <li>The fundamental unit is the streaming multiprocessor (SM). Hopper H100 (SXM5) integrates 132 SMs in total.</li> <li>SMs are grouped into graphics processing clusters (GPCs): each GPC contains 18 SMs, and there are 8 GPCs on the GPU. Four GPCs connect directly to one L2 partition, and the other four to the second partition.</li> <li>Tensor Cores: Specialized units that execute matrix multiplications on small tiles (e.g., 64x16 @ 16x256) at high throughput. Large matrix multiplications are decomposed into many such tile operations, so leveraging them effectively is critical for reaching peak performance.</li> <li>CUDA cores and SFUs: The so-called “CUDA cores” (marketing speech) execute standard floating-point operations such as FMA (fused multiply-add: c = a * b + c). Special Function Units (SFUs) handle transcendental functions such as sin, cos, exp, log, but also algebraic functions such as sqrt, rsqrt, etc.</li> <li>Load/Store (LD/ST) units: Circuits that service load and store instructions, complementary to the TMA engine.</li> <li>Warp schedulers: Each SM contains schedulers that issue instructions for groups of 32 threads (called warps in CUDA). A warp scheduler can issue one warp instruction per cycle.</li> <li>Each SM is physically divided into four quadrants, each housing a subset of the compute units described above.</li> <li>Parallelism vs Concurrency (Imp.) <ul> <li>An SM can issue instructions from at most four warps simultaneously (i.e., 128 threads in true parallel execution at a given cycle).</li> <li>However, an SM can host up to 2048 concurrent threads (64 warps). These warps are resident and scheduled in and out over time, allowing the hardware to hide memory/pipeline latency.</li> <li>In other words, instruction parallelism (how many threads start executing an instruction on a given cycle) is limited to 128 threads per SM at once (4 32-wide warp instructions), while concurrency (how many threads are tracked in the scheduler and eligible to run) extends to 2048 threads.</li> </ul> </li> <li>What is the ceiling—the maximum compute throughput of a GPU? This is often referred to as the “speed of light” (SoL) performance: the upper bound dictated by the physical characteristics of the chip.</li> <li>There are multiple ceilings depending on the data type. In LLM training workloads, bfloat16 (bf16) has been the dominant format in recent years, though fp8 and 4-bit formats are becoming increasingly important (for inference fp8 is fairly standard).</li> <li>The peak throughput is calculated as: <code class="language-plaintext highlighter-rouge">perf = freq_clk_max * num_tc * flop_per_tc_per_clk</code> or in words: maximum clock frequency × number of tensor cores × FLOPs per tensor core per cycle.</li> <li>The “speed of light” is not actually constant.</li> <li>In practice, the peak throughput depends on the actual clock frequency, which can vary under power or thermal throttling. If the GPU clock drops, so does the effective speed of light.</li> <li>Normally on H100 SXM the max clock freq is 1830 MHz =&gt; clock cycle takes ~0.55 ns</li> <li>But GPU might experience power throttling causing it to automatically drop the clock freq in order to reduce the transistor switching power.</li> </ul> <p>Doubts:</p> <ul> <li>In cache what is k-way set associative cache?</li> <li>What is transistor switching power and how its related to clock freq and power throttling?</li> </ul> <p>Further reading: Horace He went into this phenomenon in more depth in <a href="https://www.thonking.ai/p/strangely-matrix-multiplications">his blog post (3)</a>.</p>]]></content><author><name></name></author><category term="GPU"/><summary type="html"><![CDATA[Reading notes for my reference from Aleksa Gordic's GPU BlogPost]]></summary></entry><entry><title type="html">MLP Standard Derivatives Derivation</title><link href="https://monishver11.github.io/blog/2025/mlp-derivatives/" rel="alternate" type="text/html" title="MLP Standard Derivatives Derivation"/><published>2025-12-13T13:14:00+00:00</published><updated>2025-12-13T13:14:00+00:00</updated><id>https://monishver11.github.io/blog/2025/mlp-derivatives</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/mlp-derivatives/"><![CDATA[<p><strong>Softmax Definition</strong></p> \[y_i = \mathrm{softmax}(a)_i = \frac{e^{a_i}}{\sum_k e^{a_k}} \qquad\text{let } S = \sum_k e^{a_k}.\] <p><strong>Softmax Jacobian:</strong> \(\frac{\partial y_i}{\partial a_j}\)</p> <p>Start from:</p> \[y_i = \frac{e^{a_i}}{S}.\] <p>Differentiate w.r.t. \(a_j\) using the quotient rule:</p> \[\frac{\partial y_i}{\partial a_j} = \frac{S\cdot\frac{\partial}{\partial a_j}e^{a_i} \;-\; e^{a_i}\cdot\frac{\partial S}{\partial a_j}}{S^2}.\] <p>Compute derivatives:</p> <ul> <li> \[\frac{\partial}{\partial a_j} e^{a_i} = e^{a_i}\delta_{ij}\] </li> <li> \[\frac{\partial S}{\partial a_j} = e^{a_j}\] </li> </ul> <p>Substitute:</p> \[\frac{\partial y_i}{\partial a_j} = \frac{S(e^{a_i}\delta_{ij}) - e^{a_i}e^{a_j}}{S^2} = \frac{e^{a_i}}{S^2}(S\delta_{ij} - e^{a_j}).\] <p>Use softmax definitions:</p> \[y_i = \frac{e^{a_i}}{S}, \qquad y_j = \frac{e^{a_j}}{S}.\] <p>Final form:</p> \[\frac{\partial y_i}{\partial a_j} = y_i\delta_{ij} - y_i y_j = y_i (\delta_{ij} - y_j).\] <p><strong>Cross-Entropy Loss</strong></p> <p>For one-hot target \(t\):</p> \[L = -\sum_i t_i \log y_i.\] <p>Differentiate w.r.t. \(a_j\):</p> \[\frac{\partial L}{\partial a_j} = \sum_i \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial a_j} = \sum_i \left( -\frac{t_i}{y_i} \right) y_i(\delta_{ij} - y_j).\] <p>Simplify:</p> \[= \sum_i -t_i(\delta_{ij} - y_j) = -\sum_i t_i\delta_{ij} + \sum_i t_i y_j.\] <p>Use:</p> <ul> <li> \[\sum_i t_i\delta_{ij} = t_j\] </li> <li>\(\sum_i t_i = 1\) for one-hot \(t\)</li> </ul> <p>Thus:</p> \[\frac{\partial L}{\partial a_j} = -t_j + y_j = y_j - t_j.\] \[\boxed{ \frac{\partial L}{\partial a_j} = y_j - t_j }\] <p><strong>Note:</strong><br/> \(\delta_{ij}\) is the <strong>Kronecker delta</strong>, defined as</p> \[\delta_{ij} = \begin{cases} 1, &amp; i = j \\ 0, &amp; i \ne j \end{cases}\] <p>It acts as an index selector in summations.<br/> For example:</p> \[\sum_i a_i \delta_{ij} = a_j.\] <p>In the softmax Jacobian derivation, \(\delta_{ij}\) appears because</p> \[\frac{\partial e^{a_i}}{\partial a_j} = e^{a_i} \delta_{ij},\] <p>meaning only the term with matching indices contributes to the derivative.</p> <p><strong>Sigmoid Derivative (Element-wise)</strong></p> <p>The sigmoid function is defined as:</p> \[\sigma(x) = \frac{1}{1 + e^{-x}}.\] <p>Let the activation be applied element-wise: \(y_i = \sigma(a_i).\)</p> <p><strong>Derivative of Sigmoid</strong></p> <p>Differentiate w.r.t. \(a_i\):</p> \[\frac{d\sigma(x)}{dx} = \frac{d}{dx}\left( \frac{1}{1 + e^{-x}} \right) = \frac{e^{-x}}{(1 + e^{-x})^2}.\] <p>Rewrite in terms of \(\sigma(x)\):</p> \[\sigma(x) = \frac{1}{1 + e^{-x}}, \qquad 1 - \sigma(x) = \frac{e^{-x}}{1 + e^{-x}}.\] <p>Thus:</p> \[\frac{d\sigma(x)}{dx} = \sigma(x)\bigl(1 - \sigma(x)\bigr).\] <p><strong>Jacobian Form</strong></p> <p>Since sigmoid acts independently on each component,</p> \[\frac{\partial y_i}{\partial a_j} = \sigma(a_i)\bigl(1 - \sigma(a_i)\bigr)\delta_{ij}.\] <p>The Jacobian is diagonal.</p> <p><strong>Derivative of Matrix Multiplication</strong></p> <p>Let:</p> \[Y = A X\] <p>where:</p> <ul> <li> \[A \in \mathbb{R}^{m \times n}\] </li> <li> \[X \in \mathbb{R}^{n \times p}\] </li> <li> \[Y \in \mathbb{R}^{m \times p}\] </li> </ul> <p><strong>Gradient with Respect to \(A\)</strong></p> \[\frac{\partial L}{\partial A} = \frac{\partial L}{\partial Y} \, X^\top\] <p><strong>Gradient with Respect to \(X\)</strong></p> \[\frac{\partial L}{\partial X} = A^\top \frac{\partial L}{\partial Y}\] <p><strong>Matrix Multiplication Gradients in <code class="language-plaintext highlighter-rouge">einsum</code> Form</strong></p> <p>Let:</p> \[Y = A X\] <p>with components:</p> \[Y_{ij} = \sum_k A_{ik} X_{kj}.\] <p>Let the upstream gradient be:</p> \[G_{ij} = \frac{\partial L}{\partial Y_{ij}}.\] <p><strong>Gradient with Respect to (A)</strong></p> <p>Using the chain rule:</p> \[\frac{\partial L}{\partial A_{ik}} = \sum_j \frac{\partial L}{\partial Y_{ij}} \frac{\partial Y_{ij}}{\partial A_{ik}} = \sum_j G_{ij} X_{kj}.\] <p>In Einstein summation notation:</p> \[\frac{\partial L}{\partial A_{ik}} = G_{ij} X_{kj}.\] <p><strong><code class="language-plaintext highlighter-rouge">einsum</code> implementation:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">ik,kj-&gt;ij</span><span class="sh">"</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>


<span class="n">dLdX</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">ik,ij-&gt;kj</span><span class="sh">"</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">G</span><span class="p">)</span>
<span class="n">dLdA</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">ij,kj-&gt;ik</span><span class="sh">"</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="ML"/><summary type="html"><![CDATA[MLP Standard Derivatives Derivation]]></summary></entry><entry><title type="html">Simple MLP - Forward and Backward Pass (With Einsum) Derivation</title><link href="https://monishver11.github.io/blog/2025/mlp-fw-bwd/" rel="alternate" type="text/html" title="Simple MLP - Forward and Backward Pass (With Einsum) Derivation"/><published>2025-12-13T12:37:00+00:00</published><updated>2025-12-13T12:37:00+00:00</updated><id>https://monishver11.github.io/blog/2025/mlp-fw-bwd</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/mlp-fw-bwd/"><![CDATA[<h4 id="neural-network-forward-and-backward-pass"><strong>Neural Network Forward and Backward Pass</strong></h4> <p><strong>Notation</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">h = Sigmoid(wx + b)</code></li> <li><code class="language-plaintext highlighter-rouge">w = Nh × Ni</code></li> <li><code class="language-plaintext highlighter-rouge">V = No × Nh</code></li> <li><code class="language-plaintext highlighter-rouge">y' = Softmax(Vh + c)</code></li> <li><code class="language-plaintext highlighter-rouge">b = Nh</code></li> <li><code class="language-plaintext highlighter-rouge">c = No</code></li> <li><code class="language-plaintext highlighter-rouge">x = Ni × 1</code> (1 training sample)</li> </ul> <p><strong>Forward Pass</strong></p> \[\begin{aligned} h_a &amp;= W @ x + b \quad &amp;&amp; (Nh \times Ni) @ (Ni \times 1) + (Nh \times 1) \\ h &amp;= \text{Sigmoid}(h_a) \quad &amp;&amp; (Nh \times 1) \\ y_a &amp;= V @ h + c \quad &amp;&amp; (No \times Nh) \times (Nh \times 1) + (No \times 1) \\ y &amp;= \text{Softmax}(y_a) \quad &amp;&amp; (No \times 1) \end{aligned}\] <p>Cross-entropy loss: \(L = -\sum_{i=0}^{n} t \log y_i \quad \text{(scalar)} \rightarrow -t \log y \text{(vector)} \quad (No \times 1)\)</p> <p><strong>Backward Pass</strong></p> <p>Gradients w.r.t. loss:</p> \[\frac{\partial L}{\partial L} = 1 ; \quad \frac{\partial L}{\partial y} = -\frac{t}{y}\] \[\frac{\partial L}{\partial y_a} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial y_a} = y - t \quad (No \times 1)\] <p><strong>Note:</strong> Output shape must match the variable shape w.r.t. which derivative is taken.</p> <p>Derivatives:</p> \[\frac{\partial L}{\partial V} = \frac{\partial L}{\partial y_a} \cdot \frac{\partial y_a}{\partial V} = (y - t) \cdot h^\top \quad \Rightarrow \quad (No \times 1) @ (1 \times Nh) = (No \times Nh)\] \[\frac{\partial L}{\partial h} = \frac{\partial L}{\partial y_a} \cdot \frac{\partial y_a}{\partial h} = V^\top \cdot (y - t) \quad \Rightarrow \quad (Nh \times No) @ (No \times 1) = (Nh \times 1)\] \[\frac{\partial L}{\partial c} = \frac{\partial L}{\partial y_a} \cdot \frac{\partial y_a}{\partial c} = (y - t) \cdot 1 \quad \Rightarrow \quad (No \times 1)\] \[\frac{\partial L}{\partial h_a} = \frac{\partial L}{\partial h} \cdot \frac{\partial h}{\partial h_a} = \frac{\partial L}{\partial h} \left[ \sigma(h_a) \cdot (1 - \sigma(h_a)) \right] \quad \Rightarrow \quad (Nh \times 1) \cdot (Nh \times 1) = (Nh \times 1)\] \[\frac{\partial L}{\partial w} = \frac{\partial L}{\partial h_a} \cdot \frac{\partial h_a}{\partial w} = \frac{\partial L}{\partial h_a} \cdot x^\top = (Nh \times 1) @ (1 \times Ni) = (Nh \times Ni)\] \[\frac{\partial L}{\partial b} = \frac{\partial L}{\partial h_a} \cdot \frac{\partial h_a}{\partial b} = \frac{\partial L}{\partial h_a} \cdot 1 = (Nh \times 1)\] <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mlp-fw-bwd-480.webp 480w,/assets/img/mlp-fw-bwd-800.webp 800w,/assets/img/mlp-fw-bwd-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mlp-fw-bwd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bd-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="forward-and-backward-pass-using-einsum-single-sample-mlp"><strong>Forward and Backward Pass Using <code class="language-plaintext highlighter-rouge">einsum</code> (Single-Sample MLP)</strong></h4> <p><strong>Model</strong></p> <ul> <li>Hidden layer: \(h = \sigma(Wx + b)\)</li> <li>Output layer: \(y = \text{softmax}(Vh + c)\)</li> </ul> <p><strong>Index Convention</strong></p> <ul> <li>\(i\): input dimension index, \(i = 1 \dots N_i\)</li> <li>\(h\): hidden dimension index, \(h = 1 \dots N_h\)</li> <li>\(o\): output dimension index, \(o = 1 \dots N_o\)</li> </ul> <h5 id="forward-pass"><strong>Forward Pass</strong></h5> <p><strong>Hidden pre-activation</strong></p> \[h_{a,h} = \sum_i W_{h i} x_i + b_h\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">hi,i-&gt;h</span><span class="sh">"</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div></div> <p><strong>Hidden activation (sigmoid)</strong></p> \[h_h = \sigma(h_{a,h})\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">h</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">ha</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output pre-activation</strong></p> \[y_{a,o} = \sum_h V_{o h} h_h + c_o\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ya</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">oh,h-&gt;o</span><span class="sh">"</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>
</code></pre></div></div> <p><strong>Output activation (softmax)</strong></p> \[y_o = \frac{e^{y_{a,o}}}{\sum_{o'} e^{y_{a,o'}}}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">ya</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Loss (Cross-Entropy)</strong></p> \[L = -\sum_o t_o \log y_o\] <h5 id="backward-pass"><strong>Backward Pass</strong></h5> <p><strong>Gradient w.r.t. output pre-activation</strong></p> \[\frac{\partial L}{\partial y_{a,o}} = y_o - t_o\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdya</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">t</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. output weights \(V\)</strong></p> \[\frac{\partial L}{\partial V_{o h}} = (y_o - t_o) h_h\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdV</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">o,h-&gt;oh</span><span class="sh">"</span><span class="p">,</span> <span class="n">dLdya</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. output bias \(c\)</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdc</span> <span class="o">=</span> <span class="n">dLdya</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. hidden activations</strong></p> \[\frac{\partial L}{\partial h_h} = \sum_o V_{o h} (y_o - t_o)\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdh</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">oh,o-&gt;h</span><span class="sh">"</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dLdya</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. hidden pre-activations</strong></p> \[\frac{\partial L}{\partial h_{a,h}} = \frac{\partial L}{\partial h_h} \cdot \sigma(h_{a,h})(1 - \sigma(h_{a,h}))\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdha</span> <span class="o">=</span> <span class="n">dLdh</span> <span class="o">*</span> <span class="nf">sigmoidgrad</span><span class="p">(</span><span class="n">ha</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. input weights \(W\)</strong></p> \[\frac{\partial L}{\partial W_{h i}} = \frac{\partial L}{\partial h_{a,h}} x_i\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">h,i-&gt;hi</span><span class="sh">"</span><span class="p">,</span> <span class="n">dLdha</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Gradient w.r.t. hidden bias \(b\)</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dLdb</span> <span class="o">=</span> <span class="n">dLdha</span>
</code></pre></div></div> <h5 id="summary"><strong>Summary;</strong></h5> <ul> <li>All operations are tensor contractions.</li> <li><code class="language-plaintext highlighter-rouge">einsum</code> makes index flow explicit.</li> <li>Backprop through linear layers reduces to transposes and outer products.</li> </ul> <h4 id="code"><strong>Code:</strong></h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Dimensions
</span><span class="n">Ni</span> <span class="o">=</span> <span class="mi">784</span>  <span class="c1"># Input dimension (e.g., MNIST)
</span><span class="n">Nh</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># Hidden dimension
</span><span class="n">No</span> <span class="o">=</span> <span class="mi">10</span>   <span class="c1"># Output dimension (e.g., 10 classes)
</span>
<span class="c1"># Initialize parameters
</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">Nh</span><span class="p">,</span> <span class="n">Ni</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">Nh</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">No</span><span class="p">,</span> <span class="n">Nh</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">No</span><span class="p">)</span>

<span class="c1"># Single training example
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">Ni</span><span class="p">)</span>    <span class="c1"># Input vector
</span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">No</span><span class="p">)</span>           <span class="c1"># One-hot target
</span><span class="n">t</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>                   <span class="c1"># Example: class 3
</span>
<span class="c1"># --- Forward Pass ---
</span><span class="n">ha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">hi,i-&gt;h</span><span class="sh">"</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>        <span class="c1"># (Nh,)
</span><span class="n">h</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">ha</span><span class="p">))</span>                  <span class="c1"># (Nh,)
</span><span class="n">ya</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">oh,h-&gt;o</span><span class="sh">"</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>        <span class="c1"># (No,)
# Softmax with numerical stability
</span><span class="n">ya_max</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">ya</span><span class="p">)</span>
<span class="n">exp_ya</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">ya</span> <span class="o">-</span> <span class="n">ya_max</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">exp_ya</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_ya</span><span class="p">)</span>                <span class="c1"># (No,)
</span>
<span class="c1"># Loss
</span><span class="n">L</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>

<span class="c1"># --- Backward Pass ---
</span><span class="n">dL_dya</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">t</span>                             <span class="c1"># (No,)
</span><span class="n">dL_dV</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">o,h-&gt;oh</span><span class="sh">"</span><span class="p">,</span> <span class="n">dL_dya</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>    <span class="c1"># (No, Nh)
</span><span class="n">dL_dc</span> <span class="o">=</span> <span class="n">dL_dya</span>                             <span class="c1"># (No,)
</span><span class="n">dL_dh</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">oh,o-&gt;h</span><span class="sh">"</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dL_dya</span><span class="p">)</span>    <span class="c1"># (Nh,)
</span><span class="n">sigmoid_grad</span> <span class="o">=</span> <span class="n">h</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)</span>                 <span class="c1"># (Nh,)
</span><span class="n">dL_dha</span> <span class="o">=</span> <span class="n">dL_dh</span> <span class="o">*</span> <span class="n">sigmoid_grad</span>              <span class="c1"># (Nh,)
</span><span class="n">dL_dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">h,i-&gt;hi</span><span class="sh">"</span><span class="p">,</span> <span class="n">dL_dha</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>    <span class="c1"># (Nh, Ni)
</span><span class="n">dL_db</span> <span class="o">=</span> <span class="n">dL_dha</span>                             <span class="c1"># (Nh,)
</span>
<span class="c1"># --- Update parameters (gradient descent) ---
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">W</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dL_dW</span>
<span class="n">b</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dL_db</span>
<span class="n">V</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dL_dV</span>
<span class="n">c</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dL_dc</span>
</code></pre></div></div> <p><strong>Question</strong></p> <p>When computing<br/> \(\frac{\partial L}{\partial h} = V^\top \frac{\partial L}{\partial y_a},\) using <code class="language-plaintext highlighter-rouge">einsum</code>, does <code class="language-plaintext highlighter-rouge">einsum</code> implicitly transpose tensors?<br/> Should I change the index order (e.g. use <code class="language-plaintext highlighter-rouge">"ho"</code> instead of <code class="language-plaintext highlighter-rouge">"oh"</code>) to represent the transpose?</p> <p>**Answer **</p> <p>No. <strong><code class="language-plaintext highlighter-rouge">einsum</code> never performs implicit transposes.</strong><br/> All transposes must be expressed <strong>explicitly through index labels</strong>, not by rearranging axes incorrectly.</p> <p>In this example, the weight matrix is defined as:</p> \[V \in \mathbb{R}^{N_o \times N_h}, \quad \text{i.e. } V_{o h}.\] <p>The correct derivative is:</p> \[\frac{\partial L}{\partial h_h} = \sum_o V_{o h} \frac{\partial L}{\partial y_{a,o}}.\] <p>This maps <strong>directly</strong> to the following <code class="language-plaintext highlighter-rouge">einsum</code>:</p> <p>```python dL_dh = np.einsum(“oh, o-&gt;h”, V, dL_dya)</p>]]></content><author><name></name></author><category term="ML"/><summary type="html"><![CDATA[Simple MLP - Forward and Backward Pass Derivation]]></summary></entry><entry><title type="html">GPU Notes</title><link href="https://monishver11.github.io/blog/2025/gpu-notes/" rel="alternate" type="text/html" title="GPU Notes"/><published>2025-12-11T19:58:00+00:00</published><updated>2025-12-11T19:58:00+00:00</updated><id>https://monishver11.github.io/blog/2025/gpu-notes</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/gpu-notes/"><![CDATA[<h4 id="concepts"><strong>Concepts:</strong></h4> <ul> <li>Shared memory is divided into banks (usually 32 banks), so warp-level accesses can be parallelized if threads don’t conflict. If warp threads access different banks → all 32 loads happen in parallel. If multiple threads hit the same bank → bank conflict, serialized access, slower. That’s why memory access patterns matter (e.g. row-major vs col-major).</li> <li>There is an upper bound on the # threads that can within a block. Similarly, there is an upper bound on the # blocks per SM. This is because of the hardware as it needs to schedule the warps based on them and the hardware is fixed. SOTA is like 2048 threads per block for now. But, this upper bound is good enough and even higher for most applications.</li> <li>Compute capability: a property of the hardware</li> <li>Pay for branch divergence, instead of resolving idle SPs or inactive threads in a warp, as the former is less expensive.</li> <li>If you need a global/kernel level sync, then it’s a good point to cut to form a new kernel starting from that point.</li> <li>Don’t use __syncthreads in an if-else block, it leads to undefined behavior.</li> <li>CUDA only assigns a block to a SM, if all of the block resources are available beforehand. This is needed for zero context switch overhead for GPUs.</li> <li>Thread scheduling is totally outside our control. So, don’t make assumption on which warps finishes before other</li> <li>Maximize threads per SM to the upper bound, instead of # blocks per SM -&gt; latency hiding (how?)</li> <li>Warps are the unit of scheduling inside a SM and execution of a warp is the definition of SIMD.</li> <li>Branch prediction - applicable in every multi-core systems. In GPU, we try to avoid this if-else conditions, as it causes warp divergence.</li> <li>Latency hiding/ Keeping the gpu busy by switching between warps while other warps wait for data. This is also called latency tolerance.</li> <li>No. of warps and problems size are two key factors that play a important role in deciding the threads/block.</li> <li>Rule of thumb - use cuda get device properties and maximize threads/SM for max parallelism and best performance. Because this causes highest latency tolerance and as there will be some warps to execute.</li> <li>Maxing block/SM is not in your control, that scheduling happens at the chip level.</li> <li>Never make your code dependable on a hardware feature, like warp size.</li> <li>CGMA ratio - Compute to global memory access. Also called arithmetic intensity.</li> <li>Global memory persists for an application and not just the kernel execution</li> <li>Shared memory of a SM is virtualized, meaning if two blocks are assigned to a SM, then each block sees and accesses its own shared memory block only.</li> <li>Constant memory is written only by the host and its read only for the device. It’s much faster accessible than global memory. It’s accessed by everyone and in our direct control. The size of this memory is comparable to that of shared memory.</li> <li>Registers, shared memory, constant memory and global memory are in our direct control. Cache is not in our direct control/in-direct control.</li> <li>The life-time of a register is related to the thread that holds it.</li> <li>Each access to registers involves fewer machine instructions than global memory.</li> <li>Register files means group of registers inside a SM.</li> <li>Address space is the set of addresses your program can access.</li> <li>If in a kernel, I’ve int x, then every thread has that variable in its set of registers.</li> <li>If its <strong>shared</strong> int y, then every block in a SM, has their own y variable.</li> <li><strong>device</strong> int z, is in global memory, so everyone in the grid can access it.</li> <li>Automatic array variables local to a thread reside in local memory. This local memory doesn’t physically exist. It’s an abstractions to the local scope of a thread and put in global memory by the compiler. Again, since this is in global memory, its takes 100x more instructions to access this even though it appear local. So, try to make the variable put in registers as much as possible. Reduce this lmem(local memory) for performance improvements.</li> <li>Global memory access is a performance bottleneck. So, we try to reduce this. A common strategy is tiling, in which we partition the data we want to use into subset called tiles, such that each tile fits into the shared memory.</li> <li>In GPU, we want the performance/watt to be high and they’re quite good given the right problem.</li> <li> <p>Global synchronization across blocks of a grid isn’t possible and not expected. This is due to the possibility of deadlock and the fact that there can be many blocks waiting to be scheduled on the SM. In this case, blocks executing in a SM, will finish and reach the barrier, they wait for the others that are waiting to be scheduled in the SM, but can’t because the finished ones are waiting too and this is a deadlock.</p> </li> <li>Registers are faster, but those values are fetched from global memory. So, if I have a variable x in a thread execution, then each of x value for each thread will need to fetched by global memory. So, putting x in shared memory will only fetch once from the global memory. So, in this case putting in shared memory is best.</li> <li>A kernel that is memory bound is an indication that it must be optimized for performance. Always make sure the kernel is compute bound and most of its work is in this part. Also, to determine whether some kernel is compute/memory bound, we need both SW and HW details.</li> <li>In thread divergence, you lose 50%, even if 31 falls in if case and 1 falls in else case and 16 in if case and 16 in else case. The power consumption is double, as both cases are executed.</li> <li>System to device memory bandwidth link is a bottleneck. The GPU(SM &amp; shared memory) to device memory is better as its handled by NVIDIA an optimized for GPU usage.</li> <li>Sending large amount of data in one transfer is better than sending several small transfers. The reason is we avoid the overheads of the each individual transfer.</li> <li>Coalescing of the memory access is done at a per warp level, as the threads in the warp execute the same instructions at once.</li> <li>Accessing col wise in a 2D matrix of its memory locations is better than accessing the memory locations row wise. Draw a diagram to understand. So, in a matmul, accessing both col wise of its input is better. And there is a way to do, check about it. This is at a warp level and not at each individual thread level.</li> <li>Less global memory instructions -&gt; more opportunities to coalesce. Meaning, if there are less trips to the memory, we can get a coalesced block of data that we require in less trips.</li> <li>Making the memory accessed to be contiguous as much as possible. There is an alignment that is done by the OS, but as a programmer, you also need to make sure it’s aligned to get better performance. It reduces the no. of memory trips to the memory, there by increasing coalescing. Also, the cache picks a block of data from memory, so if the data you want is already contiguoos, you avoid a lot of cache misses.</li> <li>The # registers used by the thread/warp is directly based on the code we write, the variables and the structure determine it.</li> <li>Latency hiding - something must be executing while you’re retrieving the memory.</li> <li>If you’ve many instructions between the (instruction to memory access) and the actual usage, then for those many instructions you can schedule the computations, thus making it compute bound a little more.</li> <li> <p>Loop unrolling reduces the # instructions that are executed by the GPU assembly. Do this, once your code is already optimized for memory, resources etc. Follow the order of optimizations as we learned in class slides.</p> </li> <li>Reduce global memory trips using tiling, shared memory etc.</li> <li>Data prefetching becomes less beneficial as thread granularity increases. The reason is the shared memory is already used by the threads in this case to the max.</li> <li>What if we don’t know the shared memory size beforehand? Dynamic shared memory. Even here you need to know the size before kernel launch, but after the program launch. And you can’t have more than one dynamic allocation like this explicitly. But, we can play with pointers, but that’s not used much as the shared memory is small for this sophistication.</li> <li>If you’ve two files (c/c++) with int x; Then the linker, picks one at random, if we want to run both files. This can be a source of bug. To avoid this, we use extern keyword to tell the linker where to check for.</li> <li>The memory is cut into pieces to avoid it being big, power-hungry and slow. So, we’ve many pieces called banks. How about the address spaces for each?</li> <li>Shared memory accesses that span b distinct banks yield an effective bandwidth that is b times as high as the bandwidth of when accesses map to the same bank. Shared memory is now 32 banks with fixed width of four bytes. Consecutive four-byte data map to consecutive banks. So, if the threads is a warp access contiguous memory locations, it’ll take the memory from the 32 banks, which are independent and results b times higher than accessing the same bank. So, memory coalescing is important in this aspect too.</li> <li>Shared memory bank conflicts - since the width of the bank is minimal, only one at a time is possible for transfer, so it takes more time than if the accesses are from different banks which can be accessed in parallel.</li> <li>IEEE floating point standard (single-precision): (–1)^sign x (1+mantissa) x 2^(exponent – 127)</li> <li>With just this encoding, we can’t define 0 in this format. So, we’ve 3 encoding schemes based on exp.</li> <li>In tensor cores, the FMA is between 3 matrices, multiply two and add with third. The multiply is single precision and the additions is half precision.</li> <li>In multiplying two vectors, parallel compute approach results in more accurate than serial approach, but FMA is the most accurate when compared to the exact value. The GPU always uses FMA and the CPU is serial, and that’s the reason why GPU is more favorable in-terms of accuracy of floating point operations than CPU. Also, SP’s are optimized to do FMA in a single cycle.</li> <li>Asynchronous = returns to host right-away and doesn’t wait for device. (Non-blocking).</li> <li>Some CUDA API calls and all kernel launches are asynchronous with respect to the host code. This means error-reporting is also asynchronous.</li> <li>If the kernel launch is non-blocking and if I have a mem copy to host from device is next statement after the kernel launch statement, then it might lead to error right, because the kernel isn’t done yet, but we’re trying to load the result from device to host. So, what happens under the hood is that, there is a queue(streams) maintained in the device, and this queue holds the kernel launch first and then the mem copy, and the queue’s function will be executed sequentially, this is the reason, the prior works.</li> <li>You can have many queues, called streams and with CUDA, you’ve control to put functions in different streams. Until now, we’ve dealt with only 1 stream, so all operations are executed sequentially in that queue of the device.</li> <li>Operations in different streams can be interleaved and, when possible, they can even run concurrently. We can have several open streams to the same device at once. Need GPUs with concurrent transfer/execution capability.</li> <li>All operations to non-default streams are non-blocking wrt the host code.</li> <li>Some times for correctness reasons, you need to synchronize the host code with operations in a stream.</li> <li>Three options: cudaDeviceSynchronize() → blocks host for all streams, cudaStreamSynchronize(stream) → blocks host for this particular stream and cudaStreamQuery(stream) → does not block host for this stream.</li> <li>Streams are good way to overlap execution and transfer, hardware permits.</li> <li>Accessing host memory from device without explicit copy is called zero-copy mechanism. If it’s a small data access, then zero-copy is better, as it avoids the overhead of transferring the data to global memory and then accessing it within the device.</li> <li>As we program GPUs we need to pay attention to several performance bottlenecks: <ul> <li>Branch diversion</li> <li>Global memory latency</li> <li>Global memory bandwidth</li> <li>Shared memory bank conflicts</li> <li>Communication</li> <li>Limited resources</li> </ul> </li> <li>We have several techniques in our arsenal to enhance performance <ul> <li>Try to make threads in the same warp follow the same control flow</li> <li>Tiling</li> <li>Coalescing</li> <li>Loop unrolling</li> <li>Increase thread granularity</li> <li>Trade one resource for another</li> <li>Memory access pattern</li> <li>Streams</li> </ul> </li> <li>_syncthreads() - within one block - inside a kernel - waits for all threads in the same block to reach that point before continuing.</li> <li>cudaDeviceSynchronize() - across entire device - one host side - waits until all previously launched kernels on the GPU are complete.</li> <li> <p>cudaEvent_t - calculates the actual GPU execution time (when GPU was busy running your kernels). Its measures pure GPU time.</p> </li> <li>General rule of thumb: as the problem size increases, increasing the # threads must be done and that needs to increase performance, and not amount of work done by each thread. This applies to parallel computing in general.</li> <li>Order of cost: communication &amp; memory access &gt; computation</li> <li>In a CUDA program, if we suspect an error has occurred during a kernel launch, then we must explicitly check for it after the kernel has executed.</li> <li>If debugging, compile with: nvcc -DDEBUG code.cu -o code. This invokes the cuda error handling implicitly.</li> <li> <p>MPI, OPENMP and CUDA - Multicore + MultiGPU communication setup. MPI is for Multi-node GPU communication.</p> </li> <li>Zero-copy: nothing is copied, instead accessed directly. Between host to device or device to device. Transfer happens between the other memory to the current device’s L2 cache directly.</li> <li>Unified virtual address: puts all the host, device memory of all devices into a single address space.</li> <li>Unified memory: creates a pool of managed memory that is shared between the CPU and GPU. Under the hood, the data(pages) automatically migrates from CPU to GPU and among GPU’s for which ever needs that data.</li> <li>All this, is for ease of use and not for performance reasons. So, sometimes manual control is better/best as in the prior you might not exactly know the actual place where the data resides.</li> <li>UM is built on top of UVA, and UM has this extra capability of data(page) movements.</li> <li>All this, is only with respect to the GPU’s global memory. The shared memory, tiling, L1 cache are still within and unique for each devices.</li> <li>Zero-Copy: use it when you have a small piece of data for reading a few times. Manual: complex pattern between host and device. If it’s structured regular thing, use unified memory.</li> <li>Within the UVA, when we need data, we use the copy… option, but it actually transfers the pages. Without, this the copy can easily fill up the space and its redundant.</li> <li>UM is performant than UVA.</li> <li>Coherence: will not allow writes to the same page at the same time, even when it leads to low performance.</li> <li>Dynamic parallelism: something related to nested code/recursion. In GPUs, it means, the kernel can start new kernel. Streams can spawn new streams. It permits dynamic run time decisions.</li> <li>Until the child kernel finishes, the parent kernel isn’t done, subject to the availability of the resources. If there’s less resources, then there is a possibility of a deadlock.</li> <li>To speed-up, start a dummy kernel first. Because, the stage setting takes time with the CUDA runtime, so the first kernel launch takes more time than the subsequent ones.</li> <li>CudaThreadSynchronize() - the device waits for the work to be done on another device, for the work sent to the child kernel by the parent kernel. Even without this, the parent waits till the child kernel finishes. But does the parent kernel block, or it does some work itself? Check??</li> <li>Alignment with Cuda Pitch for 2D arrays/kernels. CUDA ensures that each row starts at an address aligned to 64 B or 128 B multiples. Aligned memory → fewer memory transactions → higher bandwidth utilization.</li> <li>Cuda Compilation: CUDA file(.cu)-&gt; PTX(Intermediate representation) -&gt; SASS(or other assemblies) -&gt; CuBit(Cuda binary) -&gt; execute</li> <li>-arch: for virtual compute architecture for generation of PTX code.</li> <li>-code: specifies the actual device that will be targeted by SASS ad the cuBin binary.</li> <li>Without -code, the final form is PTX, so every time for the GPU code generation, it uses a JIT compiler.</li> <li>Fat binary: an executable or object file that contains multiple versions of GPU code. One or more machine-specific binaries (SASS), compiled for concrete GPU architectures like sm_30, sm_35, etc. Optionally, one or more PTX versions, virtual assembly for JIT-compilation on future GPUs.</li> <li> <p>When a CUDA kernel is launched, the driver checks the GPU’s SM version and looks in the fatbinary for a matching compiled binary (cubin). If it finds one, it runs it directly for maximum speed; otherwise, it falls back to the embedded PTX, JIT-compiles it into a cubin, and caches it (in ~/.nv/ComputeCache) for future use. This allows the driver to automatically choose the optimal version at runtime with no code changes needed.</p> </li> <li>In the nvcc cmd, Arch gets you the PTX for that architecture and code gives you the assembly and binary for your specific device.</li> <li>(-arch=compute_Xi works with -code=sm_Xj, where i&lt;=j) - Check</li> <li>With just -arch and without -code, it’ll give you the ptx only. And at runtime, it uses the JIT compiler to compile this into an assembly and binary as executable. So, it takes a bit more time.</li> <li>Write code, that’s wrap friendly(that reduces thread divergence) and cache friendly for good memory access. This can be cultivated with experience and system design thinking.</li> <li><strong>shfl_sync</strong>: fastest way to share data between threads within a warp, instead of going into shared memory or cache. This goes into low-level hardware and used by Nvidia libraries.</li> <li>Thread block cluster - a group of thread blocks. We introduce this a layer in between the blocks and grids. The next layer is grid with cluster. For a thread block cluster, all blocks within must be assigned to each of the SMs for the single thread block to be executed. The shared memory of all the blocks within a thread block cluster are accessible and called distributed shared memory. This needs C++, Cuda Blackwell architecture. One advantage is that this now can give us block level synchronization. - Check last point.</li> <li>How to reduce the performance dip because of AtomicAdd, but still maintaining the atomicity? Solution: Privatization. This is always good when we’ve severe collision. But, it’s costly. There’s overhead for creating and initializing private copies for each thread block and the overhead for accumulating the contents of private copies into the final copy. The benefit is much less contention and serialization in accessing both the private copies and the final copy. The overall performance can often be improved more than 10x. These private copies are stored in shared memory. Even in AtomicAdd, now we’ve variations based on the level of atomicity we want. For block level, we can use AtomicAdd_Block with privatization for best performance.</li> <li>What if the copy is too large to privatize? Sometimes one can partially privatize an output copy and use range testing to go to either global memory or shared memory.</li> </ul> <h4 id="lecture-doubts"><strong>Lecture doubts:</strong></h4> <ul> <li>If the kernel call API is non-blocking, then the after steps like bring the results back to host will start immediately, how is it possible? Or why kernel call API is non-blocking? - A: CUDA streams.</li> <li>What is codaMalloc(void<strong>) -&gt; what’s this void</strong> means? cudaMalloc() allocates memory on the GPU and writes the GPU address into your device pointer variable. To let CUDA modify that pointer, you must pass its address (i.e., a pointer to your pointer). Since cudaMalloc() expects a void<strong>, we cast our variable’s address — e.g., (void</strong>)&amp;d_curr — to match its signature. This cast simply tells CUDA, “here’s the address of my pointer; fill it with the device memory location.”</li> <li>As a programmer, do we have access to coalesce the memory access before the actual accessing from the memory?</li> <li>What is data prefetch? technique used to hide memory access latency by loading data into faster memory (like shared memory or registers) before it is actually needed for computation.</li> <li>What is presorting overhead in floating point operations?</li> <li>For mem copy from host to device, will it be done via pinned pages or the usage of pinned pages in in programmers control? If so, what are the specific api’s that allows us to do this?</li> <li>Nvidia doesn’t support backward compatibility??</li> <li>Access vs Transfer in GPU peer-to-peer ??. Access is unto L2 cache and transfer goes till the GPU global memory. Check more??</li> <li>What is zero copy and how its different from/related to peer-to-peer copy??</li> <li>cudaHostAlloc() ??</li> <li>By default, grids launched within a thread block are executed sequentially. • This happens even if grids are launched by different threads within the block. • To deal with this drawback → streams • streams created on the host cannot be used on the device. • Streams created in a block can be used by all threads in that block. ??</li> <li><strong>shfl_sync</strong> and its variations. How its used and what happens due to this under the hood and what evens without this?</li> <li>Q: If threads in a warp should access contiguous memory for performance, but shared memory accesses must avoid bank conflicts to be parallel, isn’t this contradictory?</li> <li>A: No, because these rules apply to two different memory systems. Contiguous access refers to global memory, where consecutive addresses allow requests from a warp to be coalesced into fewer DRAM transactions, maximizing bandwidth. Bank conflicts apply to shared memory, which is divided into banks; here, threads must access different banks to avoid serialization. In practice, an optimal kernel loads data from global memory using coalesced (contiguous) accesses, then rearranges it in shared memory into a layout that avoids bank conflicts for computation. Both conditions are required: coalescing ensures efficient data movement into the SM, while conflict-free shared memory ensures fast, parallel use of that data once it is on-chip.</li> </ul> <h4 id="c--cuda"><strong>C++ &amp; CUDA:</strong></h4> <ul> <li>std::shared_ptr<T> → smart pointer, auto memory cleanup.</T></li> <li>Custom deleter = control how memory is freed (e.g. cudaFree).</li> <li>Constructor initializer list (: h(h_), w(w_)) → compact way to set members.</li> <li>Templates (template <typename T="">) → write generic code for any type.</typename></li> <li>Macros (#define Index(…)) → text substitution, quick shorthand.</li> <li>Exceptions (throw std::runtime_error(“msg”)) → safe error reporting.</li> <li>Header files: <ul> <li>#include = “paste the file here.”</li> <li>&lt;…&gt; = system/standard headers</li> <li>“…” = local project headers</li> <li>#pragma once -&gt; Ensures header file is included only once and prevents duplicate definitions.</li> </ul> </li> <li>Files: <ul> <li>.hh / .hpp → C++ headers (project-specific)</li> <li>.cuh → CUDA headers (contain CUDA-related declarations)</li> <li>.cpp / .cu → source files (where you usually put bigger function definitions, kernels, training loops)</li> </ul> </li> <li>operator() makes an object callable like a function</li> <li>functors (function objects)</li> <li>kernels can’t take references (&amp;) — arguments must be passed by value</li> </ul>]]></content><author><name></name></author><category term="GPU-NYU"/><category term="GPU"/><summary type="html"><![CDATA[GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes]]></summary></entry><entry><title type="html">Big Data Storage</title><link href="https://monishver11.github.io/blog/2025/big-data-2-storage/" rel="alternate" type="text/html" title="Big Data Storage"/><published>2025-10-22T15:25:00+00:00</published><updated>2025-10-22T15:25:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-2-storage</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-2-storage/"><![CDATA[<ul> <li>“Big Data” is not new. Oil companies, telecommunications companies, and other data-centric industries have had huge datasets for a long time.</li> <li>As storage capacity continues to expand, today’s “big” is tomorrow’s “small.”</li> <li>“Big Data” is when the size of the data itself becomes part of the problem.</li> <li>Why is Big Data a problem? <ul> <li>Where can I store my company’s ever-growing data?</li> <li>How much is that going to cost?</li> <li>How am I going to manage all the hardware and software?</li> <li>Users are asking bigger questions - how can I provide compute power?</li> <li>And, I/O speed is a problem too.</li> </ul> </li> <li>One 10 TB hard disk drive (HDD) vs 10 TB of storage with 100 HDDs, which is better?</li> <li>10 TB of storage with 100 HDDs has an advantage of having one read/write head per drive. So, the whole disk read/write time reduces overall when compared to the whole disk read/write time of the single 10 TB HDD.</li> <li>In practice, multiple disk drives are installed in one server, sometimes as many as 12 or more. Each drive is 2 TB ~ 6 TB in size.</li> <li>It is important to match the speed of the drives to the processing power of the server, because the CPU can become the bottleneck.</li> <li>New problems: <ul> <li>How can I read the parts of my file simultaneously from multiple drives on multiple servers?</li> <li>How do I even know where the pieces of my file are, if they’re stored on multiple servers?</li> </ul> </li> <li>Before answering these questions, we need to know some of the big data storage concepts that are used and involved.</li> <li>Big Data analytics uses highly scalable distributed technologies and frameworks to analyze large volumes of data from diﬀerent sources.</li> <li>To store Big Data datasets, often in multiple copies, innovative storage strategies and technologies have been created to achieve cost-eﬀective and highly scalable storage solutions.</li> <li>We’ll introduce the following concepts: Clusters, distributed file systems, relational database management systems, NoSQL, sharding, replication, CAP theorem, ACID, and BASE.</li> <li>A cluster is a tightly coupled collection of servers (“nodes”).</li> <li>These servers usually have the same hardware specifications and are connected together via a network to work as a single unit.</li> <li>Each node in the cluster has its own dedicated resources, such as memory, a processor, and a hard drive.</li> <li>A cluster can execute a job by splitting it into small pieces (“tasks”) and distributing their execution onto diﬀerent computers that belong to the cluster.</li> <li> <p>Clusters -&gt; Racks -&gt; Nodes(servers)</p> </li> <li>Next is Distributed file systems (DFS).</li> <li>A file is the most basic unit of storage to store data.</li> <li>A file system (FS) is the method of organizing files on a storage device.</li> <li>A DFS is a file system that can store large files spread across the nodes of a cluster. E.g., Google File System (GFS), Hadoop Distributed File System (HDFS).</li> <li>Next is Relational database management systems (RDBMS).</li> <li>A RDBMS is a product that presents a view of data as a collection of rows and columns.</li> <li>SQL (structured query language) is used for querying and maintaining the database.</li> <li>A transaction symbolizes a unit of work performed against a database, and treated in a coherent and reliable way independent of other transactions.</li> <li>Next is NoSQL.</li> <li>A Not-only SQL (NoSQL) database is a non-relational database that is highly scalable, fault-tolerant and specifically designed to house semi-structured and unstructured data.</li> <li> <p>E.g., Key-value store: Redis, Dynamo. Document store: MongoDB, CouchDB. Wide column store: Bigtable, HBase, Cassandra. Graph store: Pregel, Giraph.</p> </li> <li>Next is Sharding.</li> <li>Sharding is the process of horizontally partitioning a large dataset into a collection of smaller, more manageable datasets (“shards”).</li> <li>Each shard is stored on a separate node, and is responsible for only the data stored on it.</li> <li>All shards share the same schema. They collectively represent the complete dataset.</li> <li>How does sharding work in practice? <ul> <li>Each shard can independently service reads and writes for the specific subset of data that it is responsible for.</li> <li>Depending on the query, data may need to be fetched from both shards.</li> </ul> </li> <li>Benefits of sharding: <ul> <li>Sharding allows the distribution of processing loads across multiple nodes to achieve horizontal scalability.</li> <li>Sharding provides partial tolerance toward failures. In case of a node failure, only data stored on that node is aﬀected.</li> </ul> </li> <li>Concerns with sharding: <ul> <li>Queries requiring data from multiple shards will impose performance penalties.</li> <li>To mitigate such performance issues, data locality keeps commonly accessed data co-located on a single shard. This idea leads to the concept of replication.</li> </ul> </li> <li>Replication stores multiple copies of a dataset (“replicas”) on multiple nodes.</li> <li> <p>There are two methods of replication: Master-slave replication and Peer-to-peer replication.</p> </li> <li><strong>Master-slave replication</strong> <ul> <li>All data is written to a master node.</li> <li>Once saved, the data is replicated over to multiple slave nodes.</li> <li>Write requests, including insert, update and delete, occur on the master node.</li> <li>Read requests can be fulfilled by any slave node.</li> <li>This is ideal for read intensive loads. Growing read demands can be managed by horizontal scaling to add more slave nodes.</li> <li>The writes are consistent. All writes are coordinated by the master node. However, write performance will suffer as the amount of writes increases.</li> <li>If the master node fails, reads are still possible via any of the slave nodes. But, writes are not supported until a master node is reestablished.</li> <li>For recovery, we resurrect the master node from a backup or choose a new master node from the slave nodes.</li> <li>There is a concern of read inconsistency.</li> <li>Ex: User A updates data. The data is copied over to Slave A by the Master. Before the data is copied over to Slave B, User B tries to read the data from Slave B, which results in an inconsistent read. The data will eventually become consistent when Slave B is updated by the Master.</li> <li>There are other solutions as well, but we’ll see those as we go and later.</li> </ul> </li> <li><strong>Peer-to-peer replication</strong> <ul> <li>All nodes (“peers”) operate at the same level.</li> <li>Each peer is equally capable of handling reads and writes.</li> <li>Each write is copied to all peers.</li> <li>In this replication strategy, we might face both read and write inconsistency.</li> <li>Read inconsistency: User A updates data. The data is copied over to Peer A and Peer B. Before the data is copied over to Peer C, User B tries to read the data from Peer C, resulting in an inconsistent read. The data will eventually be updated on Peer C, and the database will once again become consistent.</li> <li>Write inconsistency: A simultaneous update of the same data may happen across multiple peers.</li> <li>Strategies to resolve these: <ul> <li>Pessimistic concurrency is a proactive strategy. It uses locking to ensure that only one update to a record can occur at a time. However, this is detrimental to availability since the database record being updated remains unavailable until all locks are released.</li> <li>Based on what is held by the lock and who holds the lock, there are many different ways of achieving this strategy. There can be write locks (exclusive locks) and read locks(shared locks), and the unit of locking depends on the system and its users, it could be fine-grained like at a record level, or coarse-grained at a table level. And since its peer-to-peer, the lock can be managed by centralized lock manager (as a service by zookeeper) or distributed lock manager (using a distributed consensus protocol like Paxos, Raft) or other application-level ownership.</li> <li>Optimistic concurrency is a reactive strategy that does not use locking. Instead, it allows inconsistency to occur with knowledge that eventually consistency will be achieved after all updates have propagated.</li> </ul> </li> </ul> </li> <li>Sharding vs replication <ul> <li>Actually, both sharding and replication can be used together.</li> <li>We can combine, sharding and master-slave replication, sharding and peer-to-peer replication or any other commendations.</li> </ul> </li> <li><strong>Sharding and master-slave replication</strong></li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharding_ms-480.webp 480w,/assets/img/sharding_ms-800.webp 800w,/assets/img/sharding_ms-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/sharding_ms.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="sharding_ms" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>Sharding and peer-to-peer replication</strong></li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sharding_pp-480.webp 480w,/assets/img/sharding_pp-800.webp 800w,/assets/img/sharding_pp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/sharding_pp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="sharding_pp" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>CAP theorem</strong> <ul> <li>A distributed database system may wish to provide three guarantees.</li> <li>Consistency: a read request from any node results in the same, most recently written data across multiple nodes.</li> <li>Availability: a read/write request will always be acknowledged in the form of a success or a failure.</li> <li>Partition tolerance: the database system can tolerate communication outages that split the cluster into multiple silos and can still service read/write requests.</li> <li>CAP theorem states that a distributed database system can only provide two of the three properties. C+A+P is not possible.</li> <li>Although communication outages are rare and temporary, partition tolerance (P) must be supported by a distributed database.</li> <li>Therefore, CAP is generally a choice between choosing C+P or A+P.</li> <li>Below explains how things can go wrong in each CAP combinations.</li> <li>C + A (no Partition Tolerance): When a network partition happens, nodes can’t communicate to stay consistent. Since the system isn’t partition-tolerant, it must shut down or reject requests, leading to unavailability.</li> <li>C + P (no Availability): During a partition, nodes stop serving requests until they can synchronize again to maintain consistency. This causes temporary unavailability, as the system pauses updates to avoid conflicts.</li> <li>A + P (no Consistency): When a partition occurs, all nodes continue serving requests independently. Because they can’t coordinate, updates may diverge, producing inconsistent or stale data until the partition heals and replicas reconcile.</li> <li>In summary: Choosing C + A means all nodes must stay in sync, but the system fails if a partition occurs. Choosing C + P ensures data consistency across partitions, but some nodes may become unavailable. Choosing A + P keeps the system running despite partitions, but nodes may serve inconsistent data until synchronization occurs.</li> </ul> </li> <li><strong>ACID</strong> <ul> <li>ACID is a traditional database design principle on transaction management.</li> <li>It stands for Atomicity, Consistency, Isolation and Durability.</li> <li>Traditional databases leverages pessimistic concurrency controls (i.e., locking) to provide ACID guarantees.</li> <li>Atomicity ensures that all transactions will always succeed or fail completely. In other words, there are no partial transactions.</li> <li>Ex: A user attempts to update three records as a part of a transaction. Two records are successfully updated before the occurrence of an error. As a result, the database rolls back any partial eﬀects of the transaction and puts the system back to its prior state.</li> <li>Consistency ensures that only data that conforms to the constraints of the database schema can be written to the database.</li> <li>Ex: A user attempts to update the “amount” column of the table that is of type “float” with a value of type “varchar.” The database rejects this update because the value violates the constraint checks for the “amount” column.</li> <li>Note: consistency here is different from the consistency in a distributed database system.</li> <li>Isolation ensures that the results of a transaction are not visible to other operations until it is complete.</li> <li>Ex: User A attempts to update two records as part of a transaction. The database successfully updates the first record. However, before it can update the second record, User B attempts to update the same record. The database does not permit User B’s update until User A’s update succeeds or fails completely. This occurs because the record with id = 3 is locked by the database until the transaction is complete.</li> <li>Durability ensures that the results of a transaction are permanent, regardless of any system failure.</li> <li>Ex: A user updates a record as part of a transaction. The database successfully updates the record. Right after this update, a power failure occurs. The database maintains its state while there is no power. The power is resumed. The database serves the record as per last update when requested by the user.</li> <li>This ACID property relates to the CAP theorem in a way that the database systems providing traditional ACID guarantees choose consistency over availability. So it ensures C+P.</li> <li>Ex: User A attempts to update a record as part of a transaction. The database validates the value and the update is successfully applied. After the successful completion of the transaction, when Users B and C request the same record, the database provides the updated value to both the users.</li> </ul> </li> <li><strong>BASE</strong> <ul> <li>BASE (pun intended) is a database design principle leveraged by many distributed database systems.</li> <li>It stands for Basically Available, Soft state and Eventual consistency.</li> <li>When a database supports BASE, it favors availability over consistency. So, it ensures A+P.</li> <li>BASE leverages optimistic concurrency by relaxing the strong consistency constraints mandated by the ACID properties.</li> <li>Basically available means that the database will always acknowledge a client’s request.</li> <li>This database is basically available, even though it has been partitioned as a result of a network failure. It can just return a failure response for the user request in this case.</li> <li>Soft state means that a database may be in an inconsistent state when data is read.</li> <li>The results may change if the same data is requested again.</li> <li>Ex: User A updates a record on Peer A. Before the other peers are updated, User B requests the same record from Peer C. The database is now in a soft state, and stale data is returned to User B.</li> <li>Eventual consistency means that the database only attains consistency once the changes have been propagated to all nodes.</li> <li>Ex: User A updates a record. The record only gets updated at Peer A, but before the other peers can be updated, User B requests the same record. The database is now in a soft state. Stale data is returned to User B from Peer C. However, the consistency is eventually attained, and User C gets the correct value.</li> </ul> </li> <li><strong>ACID vs BASE</strong> <ul> <li>ACID ensures immediate consistency at the expense of availability due to the record locking.</li> <li>BASE emphasizes availability over immediate consistency.</li> <li>This soft approach toward consistency allows BASE-compliant databases to serve multiple clients without any latency though serving inconsistent results.</li> <li>However, BASE-compliant databases are not useful for transactional systems where lack of consistency is a concern and needed.</li> <li>A distributed database system may choose to provide some ACID properties.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 2]]></summary></entry><entry><title type="html">Introduction to Realtime and Big Data Analytics</title><link href="https://monishver11.github.io/blog/2025/big-data-1-intro/" rel="alternate" type="text/html" title="Introduction to Realtime and Big Data Analytics"/><published>2025-10-22T15:23:00+00:00</published><updated>2025-10-22T15:23:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-1-intro</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-1-intro/"><![CDATA[<ul> <li>Big Data is a field dedicated to the analysis, processing, and storage of large collections of data that frequently originate from disparate sources.</li> <li>Big Data addresses distinct requirements: I) Combine multiple unrelated datasets. II) Process large amounts of unstructured data. III) Harvest hidden information in a time-sensitive manner.</li> <li>A dataset is a collection or group of related data.</li> <li>Each dataset member (“datum”) shares the same set of attributes or properties as others in the same dataset.</li> <li>Ex: An extract of rows from a database table stored in a CSV formatted file.</li> <li>Data analysis is the process of examining data to find facts, relationships, patterns, insights and/or trends.</li> <li>The overall goal of data analysis is to support better decision-making.</li> <li>Data analytics is a discipline that includes the management of the complete data lifecycle, which encompasses collecting, cleaning, organizing, storing, analyzing and governing data.</li> <li>Ex: In business-oriented environments, data analytics results can lower operational costs and facilitate strategic decision-making. In the scientific domain, data analytics can help identify the cause of a phenomenon to improve the accuracy of predictions. In service-based environments, data analytics can help strengthen the focus on delivering high-quality services by driving down costs.</li> <li>There are four general categories of analytics that are distinguished by the results they produce: descriptive, diagnostic, predictive and prescriptive analysis.</li> <li>Descriptive analytics aim to answer questions about events that have already occurred. Descriptive analytics contextualizes data to generate information.</li> <li>The operational systems (e.g., OLTP, CRM, ERP) are queried via descriptive analytics tools to generate static reports or dashboards.</li> <li>Diagnostic analysts aim to determine the cause of a phenomenon that occurred in the past using questions that focus on the reason behind the event.</li> <li>The goal is to determine what information is related to the phenomenon in order to answer questions that seek to determine why something has occurred.</li> <li>Diagnostic analytics usually collect data from multiple sources and store it in a structure (e.g., OLAP) so that users can perform interactive drill-down and roll-up analysis.</li> <li>Predictive analytics aim to determine the outcome of an event that might occur in the future.</li> <li>Information is associated to build models that are used to generate future predictions based upon past events.</li> <li>Predictive analytics use large datasets of internal and external data and various data analysis techniques to provide user-friendly front-end interfaces.</li> <li>Prescriptive analytics build upon the results of predictive analytics by prescribing actions that should be taken.</li> <li>The focus is not only on what prescribed option is best to follow, but why.</li> <li>Prescriptive analytics use business rules and large amounts of internal and external data to simulate outcomes and prescribe the best course of action.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bd-1-480.webp 480w,/assets/img/bd-1-800.webp 800w,/assets/img/bd-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/bd-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="bd-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Big data characteristics, the five V’s: Volume, Velocity, Variety, Veracity and Value.</li> <li>Veracity refers to the quality of data. Data with a high signal-to-noise ratio has more veracity.</li> <li>Value is defined as the usefulness of data for an enterprise.</li> <li>Value is also impacted by data lifecycle-related concerns, like how well has the data been stored? Were valuable attributes of data removed during data cleansing? Are the right types of questions being asked during data analysis? Are the results of the analysis being accurately communicated to the appropriate decision-makers?</li> <li>There are different types of data. Based on source, we’ve Human-generated data and Machine-generated data. Based on format, we’ve structured, unstructured, semi-structured data and metadata.</li> <li>Structured data conforms to a data model or schema. It is used to capture relationships between different entities and is therefore most often stored in relational database. Ex: banking transactions, invoices, customer records, etc.</li> <li>Unstructured data doesn’t conform to a data model or schema. It is either textual or binary and often conveyed via files that are self-contained and non-relational. The majority of data is unstructured. Ex: textual data, video, image files, audio, etc.</li> <li>Semi-structured data has a defined level of structure and consistency, but is not relational in nature. Instead, it is hierarchical or graph based. This kind of data is commonly stored in text-based files. Ex: XML data, JSON data, sensor data(CSV), etc.</li> <li>Metadata provides information about a dataset’s characteristics and structure. It is mostly machine-generated and can be appended to data. Ex: XML tags providing the author and creation date of a document, Attributes providing the file size and resolution of a digital photograph, etc.</li> <li>Big data solutions need to support multiple formats and types of data.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 1]]></summary></entry><entry><title type="html">GPU Essentials - A Concise Technical Guide</title><link href="https://monishver11.github.io/blog/2025/GPU-Intro/" rel="alternate" type="text/html" title="GPU Essentials - A Concise Technical Guide"/><published>2025-09-23T04:46:00+00:00</published><updated>2025-09-23T04:46:00+00:00</updated><id>https://monishver11.github.io/blog/2025/GPU-Intro</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/GPU-Intro/"><![CDATA[<p>This is a concise GPU introduction I found helpful. With it, you can start CUDA programming and understand the basic terms you’ll encounter. These notes are adapted from <a href="https://pages.cs.wisc.edu/~markhill/restricted/ieeemicro10_gpu.pdf">this article</a>, which was itself inspired by Jen-Hsun Huang’s keynote at Hot Chips 21 in 2009. Although the article was published in 2010—15 years ago—the GPU architecture concepts and terminology haven’t changed much. Modern GPUs include additional features for improved performance, but the fundamentals remain largely the same.</p> <hr/> <p>With the GPU’s rapid evolution from a configurable graphics processor to a programmable parallel processor, the ubiquitous GPU in every PC, laptop, desktop, and workstation is a many-core multi-threaded multiprocessor that excels at both graphics and computing applications.</p> <h4 id="gpu-computings-evolution"><strong>GPU computing’s evolution</strong></h4> <ul> <li>Rendering high-definition graphics scenes is a problem with tremendous inherent parallelism. A graphics programmer writes a single-thread program that draws one pixel, and the GPU runs multiple instances of this thread in parallel—drawing multiple pixels in parallel.</li> <li>Also, GPU computing programs—written in C or C++ with the CUDA parallel computing model, or using a parallel computing API inspired by CUDA such as Direct- Compute or OpenCL — scale transparently over a wide range of parallelism. Software scalability, too, has enabled GPUs to rapidly increase their parallelism and performance with increasing transistor density.</li> <li>Evolving to modern GPUs involved adding programmability incrementally—from fixed function pipelines to microcoded processors, configurable processors, programmable processors, and scalable parallel processors.</li> <li>GPUs first used floating-point arithmetic to calculate 3D geometry and vertices, then applied it to pixel lighting and color values to handle high-dynamic-range scenes and to simplify programming.</li> <li>The GeForce 6800 scalable processor core architecture facilitated multiple GPU implementations with different numbers of processor cores.</li> <li>Early GPGPU computing programs achieved high performance, but were difficult to write because programmers had to express non-graphics computations with a graphics API such as OpenGL.</li> <li>The GeForce 8800 introduced in 2006 featured the first unified graphics and computing GPU architecture7,8 programmable in C with the CUDA parallel computing model, in addition to using DX10 and OpenGL.</li> <li>Its unified streaming processor cores executed vertex, geometry, and pixel shader threads for DX10 graphics programs, and also executed computing threads for CUDA C programs.</li> <li>Hardware multithread- ing enabled the GeForce 8800 to efficiently execute up to 12,288 threads concurrently in 128 processor cores.</li> <li>NVIDIA deployed the scalable architecture in a family of GeForce GPUs with different numbers of processor cores for each market segment.</li> <li>The GeForce 8800 was the first GPU to use scalar thread processors rather than vector processors, matching standard scalar languages like C, and eliminating the need to manage vector registers and program vector operations.</li> <li>(# Note: Scalar processors execute one operation per thread on a single data element, while vector processors execute the same operation on multiple data elements at once, requiring explicit vector instructions #)</li> <li>It added instructions to support C and other general-purpose languages, including integer arithmetic, IEEE 754 floating-point arithmetic, and load/store memory access instructions with byte addressing.</li> <li>It provided hardware and instructions to support parallel computation, communication, and synchronization—including thread arrays, shared memory, and fast barrier synchronization.</li> <li>(# Note: Fast barrier synchronization is a mechanism that quickly pauses threads in a block until all have reached the same point, ensuring they proceed together without race conditions #)</li> <li>NVIDIA introduced the third-generation Fermi GPU computing architecture in 2009.</li> <li>Fermi implemented IEEE 754-2008 and significantly increased double-precision performance. It added error-correcting code (ECC) memory protection for large-scale GPU computing, 64-bit unified addressing, cached memory hierarchy, and instructions for C, C++, Fortran, OpenCL, and DirectCompute.</li> <li>The GPU computing ecosystem is expanding rapidly, enabled by the deployment of more than 180 million CUDA-capable GPUs.</li> <li>NVIDIA developed the parallel Nsight GPU development environment, debugger, and analyzer integrated with Microsoft Visual Studio.</li> </ul> <h4 id="cuda-scalable-parallel-architecture"><strong>CUDA scalable parallel architecture</strong></h4> <ul> <li>CUDA is a hardware and software coprocessing architecture for parallel computing that enables NVIDIA GPUs to execute programs written with C, C++, Fortran, OpenCL, DirectCompute, and other languages.</li> <li>Because most languages were designed for one sequential thread, CUDA preserves this model and extends it with a minimalist set of abstractions for expressing parallelism. This lets the programmer focus on the important issues of parallelism—how to design efficient parallel algorithms—using a familiar language.</li> <li>By design, CUDA enables the development of highly scalable parallel programs that can run across tens of thousands of concurrent threads and hundreds of processor cores.</li> <li>A compiled CUDA program executes on any size GPU, automatically using more parallelism on GPUs with more processor cores and threads.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-1-480.webp 480w,/assets/img/gpu-1-800.webp 800w,/assets/img/gpu-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>A CUDA program is organized into a host program, consisting of one or more sequential threads running on a host CPU, and one or more parallel kernels suitable for execution on a parallel computing GPU. A kernel executes a sequential program on a set of lightweight parallel threads. As Figure 1 shows, the programmer or compiler organizes these threads into a grid of thread blocks. The threads comprising a thread block can synchronize with each other via barriers and communicate via a high-speed, per-block shared memory.</li> <li>Threads from different blocks in the same grid can coordinate via atomic operations in global memory space shared by all threads. Sequentially dependent kernel grids can synchronize via global barriers and coordinate via global shared memory.</li> <li>CUDA requires that thread blocks be independent, which provides scalability to GPUs with different numbers of processor cores and threads.</li> <li><mark style="background: #FFF3A3A6;">Thread blocks implement coarse-grained scalable data parallelism, while the light-weight threads comprising each thread block provide fine-grained data parallelism. Thread blocks executing different kernels implement coarse-grained task parallelism. Threads executing different paths implement fine-grained thread-level parallelism.</mark></li> <li>(# Note: Imagine a restaurant kitchen — multiple kitchens (thread blocks) each cook the same dish in parallel = coarse-grained data parallelism; within one kitchen, many chefs (threads) chop ingredients simultaneously = fine-grained data parallelism; if different kitchens prepare entirely different dishes = coarse-grained task parallelism; if chefs in the same kitchen follow slightly different recipes = fine-grained thread-level parallelism #)</li> <li>(# Note: Think of a university — each class (thread block) works on the same assignment = coarse-grained data parallelism; within a class, each student (thread) solves a small part of the assignment = fine-grained data parallelism; if different classes work on different subjects = coarse-grained task parallelism; if students in the same class take different approaches to solving a problem = fine-grained thread-level parallelism #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-2-480.webp 480w,/assets/img/gpu-2-800.webp 800w,/assets/img/gpu-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Figure 2 shows some basic features of parallel programming with CUDA. It contains sequential and parallel implementations of the SAXPY routine defined by the basic linear algebra subroutines (BLAS) library.</li> <li>The serial implementation is a simple loop that computes one element of y per iteration. The parallel kernel executes each of these independent iterations in parallel, assigning a separate thread to compute each element of y.</li> <li>The __ global __ modifier indicates that the procedure is a kernel entry point, and the extended function-call syntax saxpy«&lt;B, T»&gt;(. . .) launches the kernel saxpy() in parallel across B blocks of T threads each.</li> <li>Each thread determines which element it should process from its integer thread block index blockIdx.x, its thread index within its block threadIdx.x, and the total number of threads per block blockDim.x.</li> <li>This example demonstrates a common parallelization pattern, where we can transform a serial loop with independent iterations to execute in parallel across many threads.</li> <li>In the CUDA paradigm, the programmer writes a scalar program—the parallel saxpy() kernel—that specifies the behavior of a single thread of the kernel. This lets CUDA leverage standard C language with only a few small additions, such as built-in thread and block index variables.</li> </ul> <h4 id="gpu-computing-architecture"><strong>GPU computing architecture</strong></h4> <ul> <li>To address different market segments, GPU architectures scale the number of processor cores and memories to implement different products for each segment while using the same scalable architecture and software.</li> <li>NVIDIA’s scalable GPU computing architecture varies the number of streaming multi-processors to scale computing performance, and varies the number of DRAM memories to scale memory bandwidth and capacity.</li> <li><mark style="background: #FFF3A3A6;">Each multithreaded streaming multiprocessor provides sufficient threads, processor cores, and shared memory to execute one or more CUDA thread blocks. The parallel processor cores within a streaming multi-processor execute instructions for parallel threads.</mark></li> <li>(# Note: Picture a library — the building is a streaming multiprocessor (SM), the reading tables inside are processor cores, and the shared bookshelf is shared memory. A group of students (a thread block) comes in; they can sit across tables, use the shared books, and study in parallel. Multiple groups can use the same library if resources allow #)</li> <li><mark style="background: #FFF3A3A6;">Multiple streaming multiprocessors provide coarse-grained scalable data and task parallelism to execute multiple coarse-grained thread blocks (possibly running different kernels) in parallel.</mark></li> <li>(# Note: Imagine a city with many libraries (multiple SMs). Each library can host different study groups (thread blocks). Some groups may study the same subject = data parallelism, while others study different subjects = task parallelism. Because there are many libraries, multiple groups can work in parallel at a larger scale #)</li> <li><mark style="background: #FFF3A3A6;">Multithreading and parallel-pipelined processor cores within each streaming multiprocessor implement fine-grained data and thread-level parallelism to execute hundreds of fine-grained threads in parallel.</mark></li> <li>(# Note: Think of an assembly line in a factory — each worker (core) handles a specific step, and many items (threads) move through simultaneously. Because there are many workers and multiple lines, hundreds of small tasks get done in parallel with no idle time = fine-grained parallelism #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-3-480.webp 480w,/assets/img/gpu-3-800.webp 800w,/assets/img/gpu-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>To illustrate GPU computing architecture, Figure 3 shows the third-generation Fermi computing architecture configured with 16 streaming multiprocessors, each with 32 CUDA processor cores, for a total of 512 cores.</li> <li><mark style="background: #FFF3A3A6;">The GigaThread work scheduler distributes CUDA thread blocks to streaming multiprocessors with available capacity, dynamically balancing the computing workload across the GPU, and running multiple kernel tasks in parallel when appropriate.</mark></li> <li>The multi-threaded streaming multiprocessors schedule and execute CUDA thread blocks and individual threads.</li> <li>Each streaming multiprocessor executes up to 1,536 concurrent threads to help cover long latency loads from DRAM memory. As each thread block completes executing its kernel program and releases its streaming multiprocessor resources, the work scheduler assigns a new thread block to that streaming multiprocessor.</li> <li>The PCIe host interface connects the GPU and its DRAM memory with the host CPU and system memory.</li> <li>The streaming multiprocessor threads access system memory via the PCIe interface, and CPU threads access GPU DRAM memory via PCIe.</li> <li><mark style="background: #FFF3A3A6;">The GPU architecture balances its parallel computing power with parallel DRAM memory controllers designed for high memory bandwidth.</mark></li> <li>Fermi introduces a parallel cached memory hierarchy for load, store, and atomic memory accesses by general applications.</li> <li>Each streaming multiprocessor has a first-level (L1) data cache, and the streaming multi- processors share a common 768-Kbyte unified second-level (L2) cache.</li> <li>The L2 cache connects with six 64-bit DRAM interfaces and the PCIe interface, which connects with the host CPU, system memory, and PCIe devices.</li> <li>It caches DRAM memory locations and system memory pages accessed via the PCIe interface.</li> <li>The unified L2 cache services load, store, atomic, and texture instruction requests from the streaming multiprocessors and requests from their L1 caches, and fills the streaming multiprocessor instruction caches and uniform data caches.</li> <li>(# Note: In GPU graphics, a texture is an image or data map applied to a 3D object’s surface. Texture instructions fetch and manipulate this data efficiently; in computing, “texture memory” can also be used as a read-only cached memory space optimized for certain access patterns #)</li> <li>Fermi implements a 40-bit physical address space that accesses GPU DRAM, CPU system memory, and PCIe device addresses. It provides a 40-bit virtual address space to each application context and maps it to the physical address space with translation lookaside buffers and page tables.</li> <li>Fermi ECC corrects single-bit errors and detects double-bit errors in the DRAM memory, GPU L2 cache, L1 caches, and streaming multiprocessor registers.</li> <li>The ECC lets us integrate thousands of GPUs in a system while maintaining a high mean time between failures (MTBF) for high-performance computing and super-computing systems.</li> <li>(# Note: Mean Time Between Failures (MTBF) is the average time a system or component operates before a failure occurs. Higher MTBF indicates more reliable hardware, crucial when thousands of GPUs work together in HPC systems #)</li> </ul> <h4 id="streaming-multiprocessor"><strong>Streaming multiprocessor</strong></h4> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-4-480.webp 480w,/assets/img/gpu-4-800.webp 800w,/assets/img/gpu-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The streaming multiprocessor implements zero-overhead multithreading and thread scheduling for up to 1,536 concurrent threads.</li> <li>(# Note: The SM supports zero-overhead multithreading by maintaining hardware context for each thread—registers, program counter, and state—so switching between 1,536 threads happens instantly without saving/restoring state. For example, if an SM has 32 cores and each warp has 32 threads, it can manage 48 warps concurrently: 32 × 48 = 1,536 threads #)</li> <li><mark style="background: #FFF3A3A6;">To efficiently manage and execute this many individual threads, the multiprocessor employs the single-instruction multiple-thread (SIMT) architecture introduced in the first unified computing GPU.</mark></li> <li>The SIMT instruction logic creates, manages, schedules, and executes concurrent threads in groups of 32 parallel threads called warps.</li> <li><mark style="background: #FFF3A3A6;">A CUDA thread block comprises one or more warps. Each Fermi streaming multiprocessor has two warp schedulers and two dispatch units that each select a warp and issue an instruction from the warp to 16 CUDA cores, 16 load/store units, or four SFUs.</mark></li> <li>(# Note: Imagine a classroom divided into smaller study groups (warps). The teacher (warp scheduler) chooses one group at a time and gives them an instruction. The students in that group then split into roles: some write (cores), some fetch books (load/store units), and some handle special tricky problems (SFUs). This shows how warps are managed and distributed to different execution resources #)</li> <li><mark style="background: #FFF3A3A6;">Because warps execute independently, the streaming multiprocessor can issue two warp instructions to appropriate sets of CUDA cores, load/store units, and SFUs.</mark></li> <li>To support C, C++, and standard single-thread programming languages, each streaming multiprocessor thread is independent, having its own private registers, condition codes and predicates, private per-thread memory and stack frame, instruction address, and thread execution state.</li> <li>The SIMT instructions control the execution of an individual thread, including arithmetic, memory access, and branching and control flow instructions.</li> <li><mark style="background: #FFF3A3A6;">For efficiency, the SIMT multiprocessor issues an instruction to a warp of 32 independent parallel threads.</mark></li> <li><mark style="background: #FFF3A3A6;">The streaming multiprocessor realizes full efficiency and performance when all threads of a warp take the same execution path.</mark></li> <li>If threads of a warp diverge at a data-dependent conditional branch, execution serializes for each branch path taken, and when all paths complete, the threads converge to the same execution path.</li> <li>Parallel thread execution (PTX) instructions describe the execution of a single thread in a parallel CUDA program.</li> <li>The PTX instructions focus on scalar (rather than vector) operations to match standard scalar programming languages.</li> <li><mark style="background: #FFF3A3A6;">Each pipelined CUDA core executes a scalar floating point or integer instruction per clock for a thread. With 32 cores, the streaming multiprocessor can execute up to 32 arithmetic thread instructions per clock.</mark></li> <li>The integer unit implements 32-bit precision for scalar integer operations, including 32-bit multiply and multiply-add operations, and efficiently supports 64-bit integer operations.</li> <li>The Fermi CUDA core floating-point unit implements the IEEE 754-2008 floating-point arithmetic standard for 32-bit single-including fused multiply-add (FMA) instructions.</li> <li>FMA computes D = A * B + C with no loss of precision by retaining full precision in the intermediate product and addition, then rounding the final sum to form the result.</li> <li>Using FMA enables fast division and square-root operations with exactly rounded results.</li> <li>Fermi raises the throughput of 64-bit double-precision operations to half that of single precision operations, a dramatic improvement over the T10 GPU.</li> <li>The SFUs execute 32-bit floating-point instructions for fast approximations of reciprocal, reciprocal square root, sin, cos, exp, and log functions.</li> <li>The streaming multiprocessor load/store units execute load, store, and atomic memory access instructions.</li> <li><mark style="background: #FFF3A3A6;">A warp of 32 active threads presents 32 individual byte addresses, and the instruction accesses each memory address. The load/store units coalesce 32 individual thread accesses into a minimal number of memory block accesses.</mark></li> <li>(# Note: Picture 32 people each ordering one item from a store. Instead of processing 32 separate trips, the store groups the orders into as few bulk deliveries as possible. This is how memory coalescing works — combining many small requests into fewer large, efficient ones #)</li> <li>(# Note: Think of 32 friends each mailing a letter to the same neighborhood. Instead of sending 32 separate mail trucks, the post office bundles the letters and sends them together in one truck. That’s memory coalescing — merging many nearby requests into one efficient transfer #)</li> <li>Fermi implements a unified thread address space that accesses the three separate parallel memory spaces of Figure 1: per-thread local, per-block shared, and global memory spaces.</li> <li><mark style="background: #FFF3A3A6;">A unified load/store instruction can access any of the three memory spaces, steering the access to the correct memory, which enables general C and C++ pointer access anywhere.</mark></li> <li>Fermi provides a terabyte 40-bit unified byte address space, and the load/store ISA supports 64-bit byte addressing for future growth. The ISA also provides 32-bit addressing instructions when the program can limit its accesses to the lower 4 Gbytes of address space.</li> <li>On-chip shared memory provides low-latency, high-bandwidth access to data shared by cooperating threads in the same CUDA thread block.</li> <li>Fast shared memory significantly boosts the performance of many applications having predictable regular addressing patterns, while reducing DRAM memory traffic.</li> <li>Fermi introduces a configurable-capacity L1 cache to aid unpredictable or irregular memory accesses, along with a configurable-capacity shared memory.</li> <li>Each streaming multiprocessor has 64 Kbytes of on-chip memory, configurable as 48 Kbytes of shared memory and 16 Kbytes of L1 cache, or as 16 Kbytes of shared memory and 48 Kbytes of L1 cache.</li> </ul> <h4 id="cpugpu-co-processing"><strong>CPU+GPU co-processing</strong></h4> <ul> <li>Heterogeneous CPU+GPU co-processing systems evolved because the CPU and GPU have complementary attributes that allow applications to perform best using both types of processors.</li> <li>CUDA programs are coprocessing programs—serial portions execute on the CPU, while parallel portions execute on the GPU. Coprocessing optimizes total application performance.</li> <li><mark style="background: #FFF3A3A6;">With coprocessing, we use the right core for the right job. We use a CPU core (optimized for low latency on a single thread) for a code’s serial portions, and we use GPU cores (optimized for aggregate throughput on a code’s parallel portions) for parallel portions of code.</mark></li> <li>This approach gives more performance per unit area or power than either CPU or GPU cores alone.</li> <li>The comparison in Table 2 illustrates the advantage of CPU+GPU coprocessing using Amdahl’s law.</li> <li>(# Note: Amdahl’s Law predicts the maximum speedup of a program using multiple processors, based on the fraction of the code that must run sequentially. Even if 90% of a program is parallelizable, the remaining 10% limits total speedup, showing why a CPU+GPU combination can outperform a pure GPU for mixed workloads #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-5-480.webp 480w,/assets/img/gpu-5-800.webp 800w,/assets/img/gpu-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The table compares the performance of four configurations: <ul> <li>a system containing one latency-optimized (CPU) core,</li> <li>a system containing 500 throughput-optimized (GPU) cores,</li> <li>a system containing 10 CPU cores, and</li> <li>a coprocessing system that contains a single CPU core and 450 GPU cores.</li> </ul> </li> <li>Table 2 assumes that a CPU core is 5 faster and 50 the area of a GPU core—numbers consistent with contemporary CPUs and GPUs. The coprocessing system devotes 10 percent of its area to the single CPU core and 90 percent of its area to the 450 GPU cores.</li> <li>The coprocessing architecture is the fastest on both programs.</li> <li><mark style="background: #FFF3A3A6;">On the parallel-intensive program, the coprocessing architecture is slightly slower on the parallel portion than the pure GPU configuration (0.44 seconds versus 0.40 seconds) but more than makes up for this by running the tiny serial portion 5 faster (1 second versus5 seconds). The heterogeneous architecture has an advantage over the pure throughput- optimized configuration here because serial performance is important even for mostly parallel codes.</mark></li> <li>Even on mostly sequential codes, it’s more efficient to run the code’s parallel portion on a throughput-optimized architecture.</li> <li>The coprocessing architecture provides the best performance across a wide range of the serial fraction because it uses the right core for each task.</li> <li>By using a latency- optimized CPU to run the code’s serial fraction, it gives the best possible performance on the serial fraction—which is important even for mostly parallel codes.</li> <li>By using throughput-optimized cores to run the code’s parallel portion, it gives near- optimal performance on the parallel fraction as well—which becomes increasingly important as codes become more parallel.</li> <li>It’s wasteful to use large, inefficient latency-optimized cores to run parallel code segments.</li> </ul> <h4 id="application-performance"><strong>Application performance</strong></h4> <ul> <li>Many applications consist of a mixture of fundamentally serial control logic and inherently parallel computations.</li> <li><mark style="background: #FFF3A3A6;">Furthermore, these parallel computations are frequently data-parallel in nature. This directly matches the CUDA coprocessing programming model, namely a sequential control thread capable of launching a series of parallel kernels.</mark></li> <li><mark style="background: #FFF3A3A6;">The use of parallel kernels launched from a sequential program also makes it relatively easy to parallelize an application’s individual components rather than rewrite the entire application.</mark></li> <li>(# Note: Imagine renovating a house — instead of rebuilding the whole house from scratch, you can work on individual rooms in parallel (kitchen, bathroom, bedroom) while the overall house structure stays the same. Similarly, parallel kernels let you speed up parts of a program without rewriting the entire application #)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gpu-6-480.webp 480w,/assets/img/gpu-6-800.webp 800w,/assets/img/gpu-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gpu-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="gpu-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Table 3 lists some representative applications along with the runtime speedups obtained for the whole application using CPU+GPU coprocessing over CPU alone, as measured by application developers.</li> <li>The speedups using GeForce 8800, Tesla T8, GeForce GTX 280, Tesla T10, and GeForce GTX 285 range from 9 to more than 130 , with the higher speedups reflecting applications where more of the work ran in parallel on the GPU.</li> <li>The lower speedups—while still quite attractive—represent applications that are limited by the code’s CPU portion, coprocessing overhead, or by divergence in the code’s GPU fraction.</li> <li>The speedups achieved on this diverse set of applications validate the programmability of GPUs—in addition to their performance.</li> <li>Applications with dense matrices, sparse matrices, and arbitrary pointer structures have all been successfully implemented in CUDA with impressive speedups. Similarly, applications with diverse control structures and significant data-dependent control, such as ray tracing, have achieved good performance in CUDA.</li> <li>Many real-world applications (such as interactive ray tracing) are composed of many different algorithms, each with varying degrees of parallelism.</li> <li>(# Note: Ray tracing is a graphics technique that simulates the path of light rays to produce realistic images with reflections, shadows, and refractions. Each ray’s calculation is independent, making it highly parallelizable on GPUs #)</li> <li>OptiX, our interactive ray-tracing software developer’s kit built in the CUDA architecture, provides a mechanism to control and schedule a wide variety of tasks on both the CPU and GPU.</li> <li>Some tasks are primarily serial and execute on the CPU, such as compilation, data structure management, and coordination with the operating system and user interaction.</li> <li>Other tasks, such as building an acceleration structure or updating animations, may run either on the CPU or the GPU depending on the choice of algorithms and the performance required.</li> <li>The net result is that CPUþGPU coprocessing enables fast, interactive ray tracing of complex scenes while you watch, which is an application that researchers previously considered too irregular for a GPU.</li> <li>GPU computing is at the tipping point. <mark style="background: #FFF3A3A6;">Single-threaded processor performance is no longer scaling at historic rates.</mark></li> <li>Thus, we must use parallelism for the increased performance required to deliver more value to users.</li> <li><mark style="background: #FFF3A3A6;">A GPU that’s optimized for throughput delivers parallel performance much more efficiently than a CPU that’s optimized for latency.</mark></li> <li>A heterogeneous coprocessing architecture that combines a single latency-optimized core (a CPU) with many throughput-optimized cores (a GPU) performs better than either alternative alone.</li> <li>This is because it uses the right processor for the right job—the CPU for serial sections and critical paths and the GPU for the parallel sections.</li> <li>In high-performance computing, technical computing, and consumer media processing, CPU+GPU coprocessing has become the architecture of choice.</li> <li>GPU architecture will evolve to further increase the span of applications that it can efficiently address.</li> <li>GPU cores will not become CPUs—they will continue to be optimized for throughput, rather than latency.</li> <li><mark style="background: #FFF3A3A6;">However, they will evolve to become more agile and better able to handle arbitrary control and data access patterns.</mark></li> </ul> <hr/> <h4 id="minor-details-that-are-frequently-misunderstood"><strong>Minor details that are frequently misunderstood:</strong></h4> <ul> <li>A thread block always executes on one SM. Multiple smaller thread blocks may be present on one SM. There are more threads than execution units (“cuda cores”) on an SM which means not every thread gets to schedule a new instruction each clock cycle. That’s okay because threads often wait for memory or floating point operations that take multiple clock cycles to finish – <a href="https://stackoverflow.com/users/17167312/homer512" title="14,961 reputation">Homer512</a></li> <li><a href="https://stackoverflow.com/questions/64624793/warp-and-block-scheduling-in-cuda-what-exactly-happens-and-questions-about-el">Warp and block scheduling in CUDA - what exactly happens, and questions about eligible warps</a></li> <li><a href="https://stackoverflow.com/questions/62147624/how-many-cuda-cores-is-used-to-process-a-cuda-warp">How many CUDA cores is used to process a CUDA warp?</a></li> <li><a href="https://stackoverflow.com/questions/76678083/confusion-around-no-of-cuda-cores-and-the-number-of-parallel-threads">Confusion around no of CUDA Cores and the number of parallel threads</a></li> </ul> <h4 id="references"><strong>References:</strong></h4> <ul> <li><a href="https://modal.com/gpu-glossary/readme">Modal - GPU Glossary</a> - Read</li> <li><a href="https://www.modular.com/blog/democratizing-compute-part-2-what-exactly-is-cuda">What exactly is “CUDA”? (Democratizing AI Compute, Part 2)</a> - Read</li> </ul>]]></content><author><name></name></author><category term="GPU-NYU"/><category term="GPU"/><summary type="html"><![CDATA[A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.]]></summary></entry></feed>