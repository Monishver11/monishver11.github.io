<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-22T12:50:43+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Multivariate Calculus - Prerequisites for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/multivariate-calculus/" rel="alternate" type="text/html" title="Multivariate Calculus - Prerequisites for Machine Learning"/><published>2024-12-20T00:10:00+00:00</published><updated>2024-12-20T00:10:00+00:00</updated><id>https://monishver11.github.io/blog/2024/multivariate-calculus</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/multivariate-calculus/"><![CDATA[<p>Before we dive into the math-heavy world of machine learning, it’s crucial to build a solid foundation in multivariate calculus. This blog post covers the essentials you’ll need to understand and work with the mathematical concepts underpinning ML.</p> <p>If you’re already familiar with undergraduate-level topics like functions and vectors, this should feel like a natural progression. If not, I recommend revisiting those fundamentals first—they’ll make this much easier to grasp. While this list isn’t exhaustive, it’s sufficient to get started and serves as a reference as we build on these ideas.</p> <p>Let’s dive in!</p> <hr/> <h3 id="what-is-calculus"><strong>What is Calculus?</strong></h3> <p>Calculus is the study of continuous change, centered on two main concepts:</p> <ol> <li><strong>Differentiation</strong>: The process of finding rates of change or slopes of curves. It helps analyze how quantities change over time or in response to other variables.</li> <li><strong>Integration</strong>: The reverse of differentiation, used to calculate areas under curves and accumulate quantities.</li> </ol> <p>These tools are applied in fields like physics, engineering, economics, and computer science to solve real-world problems involving continuous change.</p> <hr/> <h3 id="what-is-multivariate-calculus"><strong>What is Multivariate Calculus?</strong></h3> <p>Multivariate calculus, or multivariable calculus, extends single-variable calculus to functions with multiple variables. Key components include:</p> <ul> <li><strong>Partial Derivatives</strong>: Measure how a function changes with respect to one variable while keeping others constant.</li> <li><strong>Multiple Integration</strong>: Used to calculate volumes and higher-dimensional quantities for functions of multiple variables.</li> <li><strong>Vector Calculus</strong>: Explores vector fields, gradients, and theorems like Green’s and Stokes’ theorems.</li> </ul> <h4 id="why-multivariate-calculus-matters"><strong>Why Multivariate Calculus Matters</strong></h4> <p>Multivariate calculus is indispensable for:</p> <ul> <li>Analyzing systems with multiple inputs.</li> <li>Solving optimization problems in higher dimensions.</li> <li>Modeling physical phenomena in 3D or more dimensions.</li> <li>Understanding advanced concepts in physics, engineering, and data science.</li> </ul> <p>By extending the tools of calculus to multidimensional problems, multivariate calculus equips us to tackle complex systems and optimize machine learning models.</p> <hr/> <h3 id="derivatives-a-fundamental-concept"><strong>Derivatives: A Fundamental Concept</strong></h3> <p>Derivatives are the backbone of calculus, measuring how a function’s output changes in response to its input. They play a critical role in understanding rates of change and optimizing functions.</p> <p>Here are some analogies to grasp the concept of derivatives:</p> <ol> <li><strong>The Speedometer Analogy</strong>:<br/> Imagine driving a car—the speedometer shows your instantaneous speed at any given moment. Similarly, a derivative tells you how fast a function is changing at a specific point.</li> <li><strong>Slope of a Tangent Line</strong>:<br/> Visually, the derivative represents the slope of the tangent line at a point on a graph. Think of it as placing a ruler on a curve to measure its tilt.</li> <li><strong>Sensitivity Measure</strong>:<br/> A derivative reveals how sensitive a function’s output is to small changes in its input. For example, it’s like determining how much the water level in a bathtub rises when you add a cup of water.</li> </ol> <h3 id="partial-derivatives-and-their-role-in-ml"><strong>Partial Derivatives and Their Role in ML</strong></h3> <p>Expanding on the concept of derivatives, <strong>partial derivatives</strong> allow us to analyze how functions of multiple variables change with respect to a single variable while keeping others constant. This is a cornerstone of multivariate calculus and a fundamental tool in machine learning.</p> <h4 id="what-are-partial-derivatives"><strong>What are Partial Derivatives?</strong></h4> <p>A partial derivative extends the idea of a derivative to multivariable functions. It isolates the effect of one variable while treating the others as fixed.</p> <p>Here are a couple of analogies to help visualize partial derivatives:</p> <ol> <li><strong>Mountain Climbing</strong>:<br/> Imagine standing on a mountain. A partial derivative measures the steepness in one specific direction (e.g., north-south or east-west), while you stay fixed in the same spot.</li> <li><strong>Baking a Cake</strong>:<br/> Consider a recipe where the outcome (taste) depends on multiple ingredients (variables). A partial derivative tells you how the taste changes when you adjust one ingredient (e.g., sugar) while keeping all others constant.</li> </ol> <h4 id="applications-in-machine-learning"><strong>Applications in Machine Learning</strong></h4> <p>Partial derivatives are indispensable in machine learning, especially when working with models that involve multiple parameters.</p> <ol> <li><strong>Gradient Descent</strong>: <ul> <li>The <strong>gradient vector</strong>, composed of partial derivatives, points in the direction of the steepest increase of a function.</li> <li>Gradient descent uses this information to move in the opposite direction, minimizing the loss function and optimizing model parameters.</li> </ul> </li> <li><strong>Back-propagation in Neural Networks</strong>: <ul> <li>Partial derivatives calculate how each weight in a neural network contributes to the overall error, enabling efficient training through backpropagation.</li> </ul> </li> <li><strong>Optimization of Complex Loss Functions</strong>: <ul> <li>In high-dimensional parameter spaces, partial derivatives help navigate the loss landscape to find optimal solutions.</li> </ul> </li> <li><strong>Sensitivity Analysis</strong>: <ul> <li>They provide insights into how sensitive a model’s output is to changes in individual input variables, aiding interpretability and robustness.</li> </ul> </li> <li><strong>Second-Order Optimization</strong>: <ul> <li>Techniques like Newton’s method use the <strong>Hessian matrix</strong> (second-order partial derivatives) to achieve faster convergence.</li> </ul> </li> </ol> <p>By employing partial derivatives, machine learning algorithms can effectively optimize performance across multiple parameters and gain insights into intricate relationships within the data.</p> <hr/> <h3 id="gradient-vectors-a-multivariable-power-tool"><strong>Gradient Vectors: A Multivariable Power Tool</strong></h3> <p>The <strong>gradient vector</strong> combines partial derivatives to form a powerful tool for analyzing multivariable functions. It indicates both the direction and magnitude of the steepest ascent at any given point.</p> <h4 id="what-is-a-gradient-vector"><strong>What is a Gradient Vector?</strong></h4> <p>The gradient vector generalizes the derivative to functions with multiple variables, providing a way to “sense” the terrain of the function.</p> <p>Here are some analogies to conceptualize gradient vectors:</p> <ol> <li><strong>Mountain Climber’s Compass</strong>:<br/> Imagine you’re on a mountain, and you have a compass that always points uphill in the steepest direction. That’s the gradient vector—it guides you to the quickest ascent.</li> <li><strong>Water Flow on a Surface</strong>:<br/> Think of water droplets on a curved surface. The gradient vector at any point shows the direction water would flow—the steepest descent.</li> <li><strong>Heat-Seeking Missile</strong>:<br/> The gradient works like a heat-seeking missile’s guidance system, constantly recalibrating to move toward the function’s maximum.</li> </ol> <h4 id="applications-in-machine-learning-1"><strong>Applications in Machine Learning</strong></h4> <p>Gradient vectors are pivotal in optimization and training algorithms:</p> <ol> <li><strong>Gradient Descent</strong>: <ul> <li>The gradient vector points toward the steepest ascent, so moving in the opposite direction minimizes the loss function.</li> </ul> </li> <li><strong>Adaptive Learning Rate Methods</strong>: <ul> <li>Advanced algorithms like AdaGrad and Adam utilize the gradient vector to adjust learning rates dynamically for each parameter.</li> </ul> </li> <li><strong>Local Linear Approximation</strong>: <ul> <li>The gradient provides a local linear estimate of the function, helping algorithms make informed adjustments to parameters.</li> </ul> </li> </ol> <p>By leveraging the gradient vector, machine learning algorithms efficiently navigate complex parameter spaces, optimize models, and adapt to data characteristics. The gradient not only informs about the loss landscape but also helps shape strategies to improve performance.</p> <p>You might be wondering why the gradient points in the direction of steepest ascent—see the references below for more details.</p> <hr/> <h3 id="hessian-and-jacobian-higher-order-tools"><strong>Hessian and Jacobian: Higher-Order Tools</strong></h3> <p>Building on partial derivatives and gradient vectors, <strong>Hessian</strong> and <strong>Jacobian matrices</strong> offer even deeper insights into multivariable functions. These higher-order constructs are essential for advanced optimization techniques and will be explored in detail next down.</p> <h4 id="jacobian-matrix"><strong>Jacobian Matrix</strong></h4> <p>The Jacobian matrix generalizes the gradient vector for vector-valued functions, capturing how changes in multiple inputs affect multiple outputs.</p> <h5 id="definition"><strong>Definition</strong></h5> \[\text{For a function } \mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m, \text{ the Jacobian matrix } \mathbf{J} \text{ is an } m \times n \text{ matrix defined as:}\] \[\mathbf{J} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n} \end{bmatrix}\] \[\text{The entries of the Jacobian matrix are partial derivatives of the component functions } f_i\] \[\text{ with respect to the input variables } x_j.\] <h4 id="hessian-matrix"><strong>Hessian Matrix</strong></h4> <p>The Hessian matrix contains all second-order partial derivatives of a scalar-valued function. It provides information about the curvature of the function, making it essential for understanding the landscape of optimization problems.</p> <h5 id="definition-1"><strong>Definition</strong></h5> \[\text{For a function } f: \mathbb{R}^n \to \mathbb{R}, \text{ the Hessian matrix } \mathbf{H} \text{ is an } n \times n \text{ symmetric matrix defined as:}\] \[\mathbf{H} = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}\] \[\text{The Hessian provides information about the curvature of the function in all directions.}\] <p>Both the Jacobian and Hessian matrices extend the concept of partial derivatives to offer deeper insights into multivariable functions. In machine learning, these tools enhance the ability to analyze and optimize models by providing detailed information about variable interdependencies and the curvature of the function’s landscape. The Jacobian is particularly useful for understanding transformations and sensitivities in vector-valued functions, while the Hessian aids in second-order optimization by characterizing curvature and guiding efficient convergence. Together, they enable sophisticated analysis and optimization techniques, making it possible to tackle the complexities of high-dimensional spaces typical in machine learning tasks.</p> <hr/> <h3 id="taylor-series"><strong>Taylor Series</strong></h3> <p>The Taylor series is a powerful tool that approximates complex functions using simpler polynomial expressions. This approximation is widely used in optimization and machine learning.</p> <h4 id="definition-2"><strong>Definition</strong></h4> \[\text{For a function } f(x) \text{ that is infinitely differentiable at a point } a,\] \[\text{ the Taylor series is given by:}\] \[f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \cdots\] \[\text{And more generally the Taylor series is given by:}\] \[f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x - a)^n\] <p>Quick analogies to idealize it:</p> <ol> <li> <p><strong>Function Microscope</strong>:<br/> The Taylor series “zooms in” on a specific point of a function, describing its local behavior with increasing precision.</p> </li> <li> <p><strong>Prediction Machine</strong>:<br/> Think of it as a step-by-step refinement of a guess about the function’s behavior. It starts with a constant, then linear, quadratic, cubic, and so on, improving accuracy at each step.</p> </li> </ol> <h4 id="applications-in-machine-learning-2"><strong>Applications in Machine Learning</strong></h4> <ol> <li><strong>Function Approximation</strong>:<br/> Simplifies complex functions into polynomials that are computationally easier to work with.</li> <li><strong>Optimization Algorithms</strong>:<br/> Techniques like Newton’s method use Taylor approximations to estimate minima or maxima.</li> <li><strong>Gradient Estimation</strong>:<br/> When direct computation of gradients is challenging, Taylor series can approximate them.</li> </ol> <p>The Taylor series provides a detailed local understanding of functions, facilitating optimization, gradient estimation, and model interpretation. This makes it a valuable tool for bridging the gap between continuous mathematical phenomena and practical computational algorithms, playing a crucial role in solving complex problems and enhancing the efficiency of machine learning workflows.</p> <p>This foundational knowledge from multivariate calculus sets the stage for deeper exploration. If you’ve made it this far, congratulations! You’ve taken an important step in understanding and internalizing key concepts in machine learning. Take a moment to reflect, and when you’re ready, let’s move forward to the next topic. See you there!</p> <h3 id="references"><strong>References</strong></h3> <ul> <li><a href="https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent" target="_blank">Why is gradient the direction of steepest ascent?</a></li> <li><a href="https://betterexplained.com/articles/calculus-building-intuition-for-the-derivative/" target="_blank">Calculus: Building Intuition for the Derivative – BetterExplained</a></li> <li><a href="https://photomath.com/articles/what-is-calculus-definition-applications-and-concepts/" target="_blank">What is Calculus? Definition, Applications, and Concepts – Photomath</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.]]></summary></entry><entry><title type="html">Linear Algebra - Prerequisites for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/linear-algebra/" rel="alternate" type="text/html" title="Linear Algebra - Prerequisites for Machine Learning"/><published>2024-12-20T00:10:00+00:00</published><updated>2024-12-20T00:10:00+00:00</updated><id>https://monishver11.github.io/blog/2024/linear-algebra</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/linear-algebra/"><![CDATA[<p>Linear algebra forms the backbone of modern machine learning. As a branch of mathematics, it deals with vector spaces and the linear transformations between them. This area of study allows for the manipulation and efficient computation of datasets, making it fundamental to various machine learning algorithms. Whether you’re working with deep learning, regression models, or optimization techniques, a solid understanding of linear algebra will be crucial to mastering machine learning.</p> <hr/> <h2 id="core-components-of-linear-algebra"><strong>Core Components of Linear Algebra</strong></h2> <h3 id="vectors-and-matrices"><strong>Vectors and Matrices</strong></h3> <h4 id="1-vectors"><strong>1. Vectors</strong></h4> <p>A <strong>vector</strong> is a fundamental concept in linear algebra and is essentially a one-dimensional array of numbers. In machine learning, vectors can represent different elements, including features, weights, or data points.</p> <ul> <li><strong>Definition:</strong> A vector is a set of numbers arranged in a specific order, and it can be represented either as a <strong>row vector</strong> or a <strong>column vector</strong>. <ul> <li>Row vector: \(\mathbf{v} = [v_1, v_2, \dots, v_n]\)</li> <li>Column vector: \(\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n\end{bmatrix}\)</li> </ul> </li> <li> <p><strong>Properties:</strong></p> <ul> <li><strong>Magnitude (Norm):</strong> The magnitude of a vector, often referred to as its norm, measures the vector’s length. The most common norms used are the L2 norm (Euclidean norm) and L1 norm.</li> </ul> \[\|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2} \ \ ; \quad \|\mathbf{v}\|_1 = \sum_{i=1}^{n} |v_i|\] <ul> <li><strong>Dot Product:</strong> The dot product of two vectors measures their similarity. The dot product between two vectors \(\mathbf{v}_1​\) and \(\mathbf{v}_2\)​ is computed as:</li> </ul> \[\mathbf{v}_1 \cdot \mathbf{v}_2 = \sum_{i=1}^{n} v_{1i} v_{2i}\] <ul> <li><strong>Distance:</strong> The Euclidean distance is a common way to measure the difference between two vectors:</li> </ul> \[d(\mathbf{v}_1, \mathbf{v}_2) = \sqrt{\sum_{i=1}^{n} (v_{1i} - v_{2i})^2}\] </li> <li><strong>Operations on Vectors:</strong> <ul> <li><strong>Addition:</strong> Vectors of the same size can be added element-wise.</li> <li><strong>Scalar Multiplication:</strong> Multiplying each element of a vector by a scalar.</li> <li><strong>Dot Product:</strong> A fundamental operation for determining the similarity between two vectors.</li> </ul> </li> </ul> <h4 id="2-matrices"><strong>2. Matrices</strong></h4> <p>A <strong>matrix</strong> is a two-dimensional array of numbers, and it is widely used in machine learning for data storage, transformations, and solving systems of equations.</p> <ul> <li> <p><strong>Definition:</strong> A matrix consists of rows and columns and is denoted as \(A\), where \(A_{ij}\)​ represents the element in the i-th row and j-th column.</p> \[A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn} \end{bmatrix}\] </li> <li><strong>Properties of Matrices:</strong> <ul> <li><strong>Rank:</strong> The rank of a matrix is the maximum number of linearly independent rows or columns, indicating the number of independent dimensions in the matrix.</li> <li><strong>Trace:</strong> The trace is the sum of the diagonal elements of a square matrix. It is often involved in optimization problems.</li> <li><strong>Determinant:</strong> The determinant helps in determining whether a matrix is invertible. A non-zero determinant implies that the matrix is invertible.</li> <li><strong>Invertibility:</strong> A matrix \(A\) is invertible if it has full rank and a non-zero determinant. The inverse of a matrix \(A\) is denoted by \(A^{-1}\), and it satisfies the equation: \(A A^{-1} = I\) where \(I\) is the identity matrix.</li> </ul> </li> <li><strong>Operations on Matrices:</strong> <ul> <li><strong>Matrix Addition/Subtraction:</strong> Matrices of the same dimension can be added or subtracted element-wise.</li> <li><strong>Matrix Multiplication:</strong> Matrix multiplication is the dot product of rows and columns between two matrices. This operation is central to machine learning algorithms.</li> <li><strong>Transpose:</strong> The transpose of a matrix \(A\) is denoted as \(A^T\) and involves flipping its rows and columns.</li> <li><strong>Inverse:</strong> If a matrix is invertible, its inverse can be used to solve systems of linear equations.</li> </ul> </li> </ul> <h3 id="vectors-and-matrices-in-ml"><strong>Vectors and Matrices in ML</strong></h3> <p>Vectors and matrices play a pivotal role in representing both data and models in machine learning.</p> <p><strong>1. Data Representation</strong></p> <ul> <li>In supervised learning, each data point is typically represented as a feature vector. For example, if a dataset has \(m\) samples and \(n\) features, it can be represented as an \(m \times n\) matrix, where each row corresponds to a feature vector for a data point. The corresponding labels or target values are often stored in a vector.</li> </ul> <p><strong>2. Model Representation</strong></p> <ul> <li>In models like linear regression and neural networks, the weights that transform input data are stored in vectors or matrices. For example, in linear regression, the model is defined as: \(\hat{y} = Xw + b\) where \(X\) is the data matrix, \(w\) is the weight vector, and \(b\) is the bias term.</li> </ul> <p><strong>3. Operations in Machine Learning Algorithms</strong></p> <ul> <li><strong>Linear Regression:</strong> In linear regression, matrix operations are used to solve for the optimal weights. The normal equation for linear regression is:</li> </ul> \[w=(X^TX)^{−1}X^Ty\] <p>where \(X\) is the matrix of input features and \(y\) is the vector of target values.</p> <ul> <li><strong>Neural Networks:</strong> Each layer of a neural network applies a linear transformation to its input, which is represented by matrix multiplication:</li> </ul> \[y=XW+b\] <p>where \(X\) is the input matrix, \(W\) is the weight matrix, and \(b\) is the bias vector.</p> <ul> <li><strong>Gradient Descent:</strong> The gradient descent optimization algorithm frequently uses vector and matrix operations to update model parameters iteratively. In deep learning, the gradient of the loss function with respect to the weights and biases is calculated using matrix operations during back-propagation.</li> </ul> <p><strong>4. Dimensionality Reduction</strong></p> <ul> <li>Principal Component Analysis (PCA) is a popular technique for dimensionality reduction. It involves finding the eigenvectors and eigenvalues of the covariance matrix of the data. Eigenvalues and Eigenvectors are explained down below.</li> </ul> <hr/> <h3 id="eigenvalues-and-eigenvectors"><strong>Eigenvalues and Eigenvectors</strong></h3> <h5 id="definition"><strong>Definition:</strong></h5> <ul> <li> <p><strong>Eigenvector:</strong><br/> An eigenvector of a square matrix \(\mathbf{A}\) is a non-zero vector \(\mathbf{v}\) that, when the matrix \(\mathbf{A}\) is applied to it, only scales the vector without changing its direction:</p> \[\mathbf{A} \mathbf{v} = \lambda \mathbf{v}\] <p>where:</p> <ul> <li>\(\mathbf{v}\) is the eigenvector,</li> <li>\(\lambda\) is the eigenvalue, the scalar that represents how much the eigenvector is scaled by the transformation.</li> </ul> </li> <li> <p><strong>Eigenvalue:</strong><br/> The eigenvalue \(\lambda\) is the factor by which the eigenvector is scaled when the matrix \(\mathbf{A}\) acts on it.</p> </li> </ul> <p><strong>To build intuition, consider this analogy:</strong></p> <p>Imagine a squishy sheet of rubber (the matrix) and a point in space (the vector). If you apply a transformation (like stretching, rotating, or shearing) to the point using the rubber sheet, most points move to new locations. However, some special points, called <strong>eigenvectors</strong>, only get <strong>stretched</strong> or <strong>compressed</strong> but <strong>stay in the same direction</strong>. The amount of stretching or compression is determined by the <strong>eigenvalue</strong>.</p> <p><strong>Mathematical Properties of Eigenvalues and Eigenvectors</strong></p> <ul> <li>Eigenvectors must be <strong>non-zero vectors</strong>.</li> <li>Eigenvalues can be <strong>real</strong> or <strong>complex</strong> (but are often real in machine learning applications).</li> <li>A matrix can have multiple eigenvectors corresponding to the <strong>same eigenvalue</strong> (if it is <strong>degenerate</strong>) or distinct eigenvalues corresponding to distinct eigenvectors.</li> </ul> <h4 id="why-are-eigenvalues-and-eigenvectors-important-in-machine-learning"><strong>Why Are Eigenvalues and Eigenvectors Important in Machine Learning?</strong></h4> <p><strong>PCA</strong> is a widely used technique for <strong>dimensionality reduction</strong> in machine learning. It reduces the number of features while retaining the most important information in the dataset.</p> <ul> <li><strong>Covariance Matrix:</strong> PCA begins by computing the covariance matrix to capture relationships between features.</li> <li><strong>Eigenvectors of Covariance Matrix:</strong> The eigenvectors represent the directions of maximum variance in the data—these are the <strong>principal components</strong>.</li> <li><strong>Eigenvalues:</strong> The corresponding eigenvalues indicate the magnitude of variance in each direction.</li> </ul> <p>Key steps in PCA:</p> <ul> <li>Sort eigenvectors in decreasing order of their eigenvalues.</li> <li>Select the top <strong>k</strong> eigenvectors to reduce dimensionality while preserving most of the variance.</li> </ul> <h4 id="key-takeaways-of-eigenvalues-and-eigenvectors-in-ml"><strong>Key takeaways of Eigenvalues and Eigenvectors in ML</strong></h4> <ul> <li><strong>Diagonalizability:</strong><br/> A matrix is diagonalizable if it has enough eigenvectors to form a full basis. This property is essential in PCA and <strong>Singular Value Decomposition (SVD)</strong>, enabling efficient computation and interpretation.</li> <li><strong>Magnitude of Eigenvalues:</strong><br/> The magnitude of eigenvalues corresponds to the <strong>variance captured</strong> by the associated eigenvectors (principal components). Larger eigenvalues imply more variance explained.</li> <li><strong>Orthogonality of Eigenvectors (Symmetric Matrices):</strong><br/> For <strong>symmetric matrices</strong>, eigenvectors are <strong>orthogonal</strong>. This is critical in PCA, where the principal components are orthogonal, ensuring that reduced dimensions remain <strong>uncorrelated</strong>.</li> </ul> <hr/> <h3 id="a-few-more-key-matrices-types-relevant-to-ml"><strong>A Few More Key Matrices Types Relevant to ML</strong></h3> <h4 id="symmetric-matrix"><strong>Symmetric Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix \(A\) is <strong>symmetric</strong> if \(A = A^T\), meaning it is equal to its transpose.</li> <li><strong>Properties:</strong> <ul> <li>A symmetric matrix always has real eigenvalues and orthogonal eigenvectors.</li> <li>If \(A\) is symmetric, it is always diagonalizable.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Covariance Matrices:</strong> Covariance matrices are always symmetric because the covariance between two features is the same regardless of the order.</li> <li><strong>Optimization Problems:</strong> Many optimization problems in machine learning involve symmetric matrices (e.g., in second-order optimization methods like Newton’s method or in regularization).</li> </ul> </li> </ul> <h4 id="orthogonal-matrix"><strong>Orthogonal Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix \(A\) is <strong>orthogonal</strong> if \(A^T A = I\), where \(I\) is the identity matrix.</li> <li><strong>Properties:</strong> <ul> <li>The rows and columns of an orthogonal matrix are orthonormal (i.e., they are both orthogonal and of unit length).</li> <li>The inverse of an orthogonal matrix is equal to its transpose \((A^{-1} = A^T)\).</li> <li>The determinant of an orthogonal matrix is either +1 or -1.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Rotation and Transformation:</strong> Orthogonal matrices are used in certain machine learning algorithms for transformations that preserve distances and angles. For example, in PCA, orthogonal transformation is used to create new orthogonal basis vectors.</li> </ul> </li> </ul> <h4 id="positive-definite-matrix-pd"><strong>Positive Definite Matrix (PD):</strong></h4> <ul> <li><strong>Definition:</strong> A square matrix \(A\) is <strong>positive definite</strong> if for any non-zero vector \(\mathbf{v}\), \(\mathbf{v}^T A \mathbf{v} &gt; 0\). In simpler terms, it means that the matrix has strictly positive eigenvalues.</li> <li><strong>Properties:</strong> <ul> <li>All eigenvalues are positive.</li> <li>The matrix is invertible (non-singular).</li> <li>It implies that the quadratic form is always positive.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Optimization Problems:</strong> In convex optimization, the Hessian matrix of a convex function is often positive definite. This ensures that a function has a unique local minimum, making optimization well-posed.</li> <li><strong>Covariance Matrices:</strong> The covariance matrix of any dataset with multiple features is positive semi-definite. In special cases (e.g., full rank), it can be positive definite.</li> </ul> </li> </ul> <h4 id="positive-semi-definite-matrix-psd"><strong>Positive Semi-Definite Matrix (PSD):</strong></h4> <ul> <li><strong>Definition:</strong> A matrix \(A\) is <strong>positive semi-definite</strong> if for any vector \(\mathbf{v}\), \(\mathbf{v}^T A \mathbf{v} \geq 0\). In other words, all eigenvalues are non-negative (i.e., zero or positive).</li> <li><strong>Properties:</strong> <ul> <li>Eigenvalues are non-negative \((\lambda_i \geq 0)\).</li> <li>The matrix may not be invertible if it has zero eigenvalues.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Covariance Matrices:</strong> As mentioned, the covariance matrix of a dataset is positive semi-definite. This is essential because covariance cannot be negative and the matrix represents the relationship between features.</li> <li><strong>Kernel Matrices (in SVMs, Gaussian Processes, etc.):</strong> The kernel matrix in algorithms like SVM and kernel PCA is always positive semi-definite. It measures similarity between data points in a transformed feature space.</li> </ul> </li> </ul> <h4 id="covariance-matrix"><strong>Covariance Matrix:</strong></h4> <ul> <li> <p><strong>Definition:</strong> A <strong>covariance matrix</strong> is a square matrix that contains the covariances between pairs of features in a dataset. If a dataset has \(n\) features, the covariance matrix will be an \(n \times n\) matrix, where each entry represents the covariance between two features.</p> </li> <li> <p><strong>Covariance of two variables X and Y:</strong></p> \[\text{Cov}(X, Y) = \frac{1}{m} \sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})\] <p>where \(m\) is the number of data points, and \(\bar{x}\) and \(\bar{y}\)​ are the means of the features \(X\) and \(Y\), respectively.</p> </li> <li> <p><strong>Covariance Matrix Definition:</strong> For a dataset with \(n\) features, the covariance matrix \(\Sigma\) is an \(n \times n\) matrix where each entry is:</p> \[\Sigma_{ij} = \text{Cov}(X_i, X_j)\] <p>where \(X_i\)​ and \(X_j\)​ are the \(i\)-th and \(j\)-th features, respectively.</p> </li> <li><strong>Properties:</strong> <ul> <li><strong>Symmetry:</strong> The covariance matrix is always symmetric, i.e., \(\Sigma_{ij} = \Sigma_{ji}\)</li> <li><strong>Positive Semi-Definiteness (PSD):</strong> The covariance matrix is always positive semi-definite, meaning for any vector \(\mathbf{v}\), \(\mathbf{v}^T \Sigma \mathbf{v} \geq 0\).</li> <li><strong>Diagonal Entries (Variance):</strong> The diagonal entries represent the variance of individual features.</li> <li><strong>Off-Diagonal Entries (Covariance):</strong> The off-diagonal entries represent the covariance between different features. Positive covariance indicates that the features increase or decrease together, while negative covariance suggests they move inversely.</li> <li><strong>Eigenvalues and Eigenvectors:</strong> The eigenvectors of the covariance matrix represent the directions of maximum variance, while the eigenvalues represent the magnitude of variance along these directions.</li> <li><strong>Rank:</strong> The rank of the covariance matrix corresponds to the number of <strong>independent</strong> features. If the matrix is rank-deficient, it indicates linearly dependent features.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>PCA:</strong> In PCA, the covariance matrix is used to identify the directions (eigenvectors) of maximum variance in the dataset. Eigenvalues indicate how much variance is explained by each principal component. This helps in dimensionality reduction by selecting the most important components.</li> <li><strong>Multivariate Gaussian Distribution:</strong> In probabilistic models like <strong>Gaussian Mixture Models (GMM)</strong>, the covariance matrix defines the shape of the data distribution. It is used to model the distribution of features in a multi-dimensional space.</li> <li><strong>Feature Selection:</strong> Covariance matrices help identify correlated features. Features that show high covariance (i.e., strong correlation) can be dropped or combined to improve model performance and reduce dimensionality.</li> </ul> </li> </ul> <h4 id="full-rank-matrix"><strong>Full Rank Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix is <strong>full rank</strong> if its rank is equal to the smallest of its number of rows or columns. In other words, all rows (or columns) are linearly independent.</li> <li><strong>Properties:</strong> <ul> <li>A matrix \(A\) with full rank has no redundant or dependent rows or columns.</li> <li>If \(A\) is an \(m \times n\) matrix, and \(\text{rank}(A) = \min(m, n)\), the matrix is full rank.</li> <li>A full rank matrix is <strong>invertible</strong> if it is square (i.e., if \(m = n\)).</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Linear Regression:</strong> In linear regression, the design matrix \(X\) must be full rank to ensure a unique solution. If \(X\) is not full rank, the matrix \(X^T X\) is singular and cannot be inverted.</li> </ul> </li> </ul> <h4 id="singular-matrix"><strong>Singular Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix is <strong>singular</strong> if it is not invertible, meaning its determinant is zero. A singular matrix has linearly dependent rows or columns.</li> <li><strong>Properties:</strong> <ul> <li>The determinant of a singular matrix is 0.</li> <li>The matrix has at least one eigenvalue equal to 0.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Linear Dependence:</strong> If the feature matrix \(X\) in a linear model is singular, some features are perfectly correlated, and this leads to instability in training and difficulties in solving for the model parameters.</li> </ul> </li> </ul> <hr/> <p>That wraps up the key linear algebra concepts for machine learning. This post is designed as a quick reference rather than an exhaustive guide. Don’t stress about memorizing everything—focus instead on understanding the concepts and knowing when to revisit them if needed.</p> <p>Math is a language, and like any language, it’s more about learning to use it than memorizing rules. Treat this as a foundation to build on, and come back to refresh your knowledge whenever necessary.</p> <p>Up next, we’ll explore the prerequisites of <strong>Probability Theory</strong> for machine learning. Since probability can often feel trickier, we’ll focus more on “what,” “why,” and “how” questions to make the concepts intuitive and approachable.</p> <p>See you in the next one!</p> <h3 id="references"><strong>References:</strong></h3> ]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog post covers the key linear algebra concepts and their applications in machine learning.]]></summary></entry><entry><title type="html">Introduction to Machine Learning(ML)</title><link href="https://monishver11.github.io/blog/2024/intro-to-ml/" rel="alternate" type="text/html" title="Introduction to Machine Learning(ML)"/><published>2024-12-19T19:44:00+00:00</published><updated>2024-12-19T19:44:00+00:00</updated><id>https://monishver11.github.io/blog/2024/intro-to-ml</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/intro-to-ml/"><![CDATA[<p>What does it mean to learn? The Merriam-Webster dictionary defines learning as “the activity or process of gaining knowledge or skill by studying, practicing, being taught, or experiencing something.” Tom Mitchell, a pioneer in machine learning, extends this concept to machines:</p> <blockquote> <p>“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”</p> </blockquote> <p>Machine learning (ML) is about teaching machines to learn from data and solve problems. Let’s dive into what this means, how it works, and why it’s transforming the way we use technology.</p> <h4 id="what-is-machine-learning"><strong>What is Machine Learning?</strong></h4> <p>At its core, machine learning is like <strong>meta-programming</strong>—programming a system to program itself. For tasks like recognizing faces or understanding speech, coding specific rules is impractical. Instead, ML lets machines learn directly from data to solve problems such as:</p> <ul> <li>Predicting whether an email is spam.</li> <li>Diagnosing diseases from symptoms.</li> <li>Forecasting stock prices.</li> </ul> <p>The goal? Take an input x and predict an output y—it can be a label, a number, or a decision.</p> <hr/> <h4 id="why-use-machine-learning"><strong>Why Use Machine Learning?</strong></h4> <p>Machine learning is particularly valuable when:</p> <ul> <li><strong>Rules are too complex to define manually</strong>: Recognizing a face or interpreting speech involves subtleties that are hard to encode in rules.</li> <li><strong>The system needs to adapt</strong>: For instance, spam filters must evolve as spammers devise new tricks.</li> <li><strong>It outperforms human-built solutions</strong>: Algorithms can detect patterns or nuances humans might miss.</li> <li><strong>Fairness and privacy are critical</strong>: For example, ranking search results or filtering harmful content.</li> </ul> <h3 id="canonical-examples-of-machine-learning"><strong>Canonical Examples of Machine Learning</strong></h3> <ol> <li><strong>Spam Detection</strong> <ul> <li><strong>Input</strong>: Incoming email</li> <li><strong>Output</strong>: “SPAM” or “NOT SPAM”</li> <li>Problem Type: Binary Classification</li> </ul> </li> <li><strong>Medical Diagnosis</strong> <ul> <li><strong>Input</strong>: Patient symptoms (e.g., fever, cough)</li> <li><strong>Output</strong>: Diagnosis (e.g., flu, pneumonia)</li> <li>Problem Type: Multiclass Classification</li> <li>Probabilistic Classification: Uncertainty is expressed as probabilities, e.g., P(pneumonia)=0.7</li> </ul> </li> <li><strong>Stock Price Prediction</strong> <ul> <li><strong>Input</strong>: Historical stock prices</li> <li><strong>Output</strong>: Price at the close of the next day</li> <li>Problem Type: Regression (continuous outputs)</li> </ul> </li> </ol> <h4 id="ml-vs-rule-based-systems"><strong>ML vs. Rule-Based Systems</strong></h4> <p>Before ML, many problems were solved with <strong>rule-based systems</strong> (or expert systems). For instance, medical diagnosis might involve encoding expert knowledge into rules that map symptoms to diseases.</p> <p><strong>Strengths of Rule-Based Systems</strong></p> <ul> <li>Leverage domain expertise.</li> <li>Interpretable and explainable.</li> <li>Reliable for known scenarios.</li> </ul> <p><strong>Weaknesses of Rule-Based Systems</strong></p> <ul> <li>Labor-intensive to build and maintain.</li> <li>Poor generalization to new or unseen scenarios.</li> <li>Struggle with uncertainty and probabilistic reasoning.</li> </ul> <h4 id="how-ml-overcomes-these-weaknesses"><strong>How ML Overcomes These Weaknesses</strong></h4> <p>Instead of encoding rules, ML systems learn directly from <strong>training data</strong>—examples of input-output pairs. For instance:</p> <ul> <li>Input: Emails</li> <li>Output: SPAM or NOT SPAM<br/> This approach, called <strong>supervised learning</strong>, involves learning from labeled examples of input-output pairs.</li> </ul> <h4 id="key-concepts-in-ml"><strong>Key Concepts in ML</strong></h4> <ul> <li><strong>Common Problem Types</strong>: <ul> <li>Classification (binary, multi-class)</li> <li>Regression (continuous prediction)</li> </ul> </li> <li><strong>Core Elements</strong>: <ul> <li>Prediction function: Maps x (input) to y (output).</li> <li>Training data: A collection of input-output pairs for the model to learn from.</li> <li>Algorithms: Methods to produce the best prediction function from data.</li> </ul> </li> <li><strong>Beyond Supervised Learning</strong>: <ul> <li><strong>Unsupervised Learning</strong>: Discovering patterns, like clustering similar users.</li> <li><strong>Reinforcement Learning</strong>: Optimizing long-term objectives or learning through rewards, e.g., chess and other gameplays.</li> <li><strong>Representation Learning</strong>: Automatically discovering useful features, e.g., learning word embeddings.</li> </ul> </li> </ul> <h4 id="core-questions-in-machine-learning"><strong>Core Questions in Machine Learning</strong></h4> <ol> <li><strong>Modeling</strong>: What kinds of prediction functions should we consider?</li> <li><strong>Learning</strong>: How do we find the best prediction function from training data?</li> <li><strong>Inference</strong>: How do we compute predictions for new inputs?</li> </ol> <h2 id="well-tackle-each-of-these-questions-as-we-move-forward-so-stick-around">We’ll tackle each of these questions as we move forward, so stick around!</h2> <h4 id="ml-vs-statistics-key-differences"><strong>ML vs. Statistics: Key Differences</strong></h4> <p>While both fields use mathematical tools like calculus, probability, and linear algebra, they differ in focus:</p> <ul> <li><strong>Statistics</strong>: Emphasizes interpretability and aiding human decision-making.</li> <li><strong>ML</strong>: Prioritizes scalability, automation, and predictive performance.</li> </ul> <h4 id="ml-in-ai-and-human-learning"><strong>ML in AI and Human Learning</strong></h4> <p><strong>Relation to AI:</strong> Machine learning is a crucial subset of artificial intelligence (AI). While AI encompasses a broad range of approaches to simulate human-like intelligence, machine learning focuses specifically on learning patterns from data to make predictions or decisions.</p> <p><strong>Relation to Human Learning:</strong> Though inspired by human cognition, ML differs significantly:</p> <ul> <li>We humans are highly efficient with limited data.</li> <li>Machines require large datasets but excel at specific tasks.</li> </ul> <p>ML systems, like neural networks, borrow ideas from biology but don’t aim to replicate human learning entirely. Instead, their focus remains on solving specialized problems effectively.</p> <h4 id="conclusion"><strong>Conclusion</strong></h4> <p>Machine learning bridges the gap between raw data and intelligent decision-making. By enabling systems to learn and adapt, ML transforms how we approach problems in healthcare, finance, and beyond. With its foundations rooted in mathematics and its applications spanning diverse domains, ML continues to redefine the boundaries of what technology can achieve.</p> <p>That’s it for this, see you in the next post! 👋</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[An easy guide to machine learning, its applications, and how it connects to AI and human learning.]]></summary></entry><entry><title type="html">Preface &amp;amp; Introduction</title><link href="https://monishver11.github.io/blog/2024/preface-ml/" rel="alternate" type="text/html" title="Preface &amp;amp; Introduction"/><published>2024-12-18T17:43:00+00:00</published><updated>2024-12-18T17:43:00+00:00</updated><id>https://monishver11.github.io/blog/2024/preface-ml</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/preface-ml/"><![CDATA[<h4 id="welcome-to-my-machine-learning-blog-series"><strong>Welcome to My Machine Learning Blog Series!</strong></h4> <p>This blog series draws inspiration from my machine learning course at NYU—a graduate-level adventure designed to explore concepts deeply and build a strong foundation from scratch. A solid grasp of the fundamentals is essential for mastering advanced topics and making meaningful progress. The course was initially designed by <a href="https://scholar.google.com/citations?user=YsHFgSAAAAAJ&amp;hl=en" target="_blank">Prof. David Rosenburg</a> and later adapted by <strong>Prof. He He</strong>, <strong>Tal Linzen</strong>, and others. I had the privilege of learning under <a href="https://mengyeren.com/" target="_blank">Prof. Mengye Ren</a>, whose teaching and structured content have greatly influenced this blog series.</p> <p>A heartfelt thank you to everyone who has supported me, continues to support me, or will support me in the future. I’m incredibly grateful for all the experiences that have brought me to this point in my journey.</p> <h4 id="what-to-expect-in-this-blog-series"><strong>What to Expect in This Blog Series</strong></h4> <p>The focus here is on the <strong>theoretical aspects</strong> of machine learning rather than programming. Why? Because understanding theory forms the critical intuition that separates a beginner randomly trying things from an expert who knows exactly where to focus for impactful results. While I consider myself a beginner, I’m determined to keep learning, growing, and sharing insights along the way.</p> <p>Since <strong>mathematics forms the backbone</strong> of machine learning and much of computer science, I’ll ensure no critical details are overlooked. For challenging or abstract concepts, I’ll include <strong>analogies</strong> to help you remember them more easily. For key topics, expect to see discussions about their real-world applications, <strong>industry relevance</strong>, and <strong>tips and tricks</strong> that can significantly boost performance, along with the reasoning behind why they work.</p> <p>While the focus is theoretical, I won’t leave you hanging! I’ll include <strong>code snippets</strong> and <strong>programming references</strong> for topics where information isn’t easily accessible online or through tools like ChatGPT</p> <p>If you’re someone with undergraduate-level knowledge in mathematics and programming, you’ll find this series accessible. While topics like Convex optimization or Lagrangians might not be covered in standard undergraduate curricula, don’t worry—I’ll explain them in detail as we go along.</p> <h4 id="attention-to-detail-and-collaboration"><strong>Attention to Detail and Collaboration</strong></h4> <p>Every blog post will include a list of <strong>references</strong> I’ve read or used while preparing the content. I plan to thoroughly review each post multiple times before publishing, incorporating feedback from classmates and my professor to ensure accuracy. If you spot any mistakes, please don’t hesitate to reach out—I’d be thrilled to correct them and keep this content as reliable as possible.</p> <h4 id="a-lifelong-learning-project"><strong>A Lifelong Learning Project</strong></h4> <p>This blog series is more than just a project—it’s a lifelong commitment. My goal is to gradually evolve this resource as I publish posts one by one. I’ll strive to remain consistent and ensure this series not only serves as a learning resource but also inspires more people to dive into machine learning and contribute to the community.</p> <p>It’s time to get to work—fingers crossed!</p> <p>Thank you for being part of this journey. See you in the next post! 👋</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[First blog post—more of a preface, setting the stage for the journey ahead.]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://monishver11.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://monishver11.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://monishver11.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://monishver11.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://monishver11.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://monishver11.github.io/blog/2024/tabs</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="ceafd2a9-2b4a-4825-ae30-f8d0cf4a4065" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="ceafd2a9-2b4a-4825-ae30-f8d0cf4a4065" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="cf00836a-5452-40f9-9031-31920e1e47d9" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="cf00836a-5452-40f9-9031-31920e1e47d9" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="28daa02b-50ea-43f7-9e56-c0ddd8cb3bcc" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="28daa02b-50ea-43f7-9e56-c0ddd8cb3bcc" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://monishver11.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://monishver11.github.io/blog/2024/typograms</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://monishver11.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://monishver11.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://monishver11.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://monishver11.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry></feed>