<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-30T05:58:54+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A clear, theory-focused approach to machine learning, designed to take you beyond the basics. </subtitle><entry><title type="html">Randomized Weighted Majority Algorithm</title><link href="https://monishver11.github.io/blog/2025/RWM/" rel="alternate" type="text/html" title="Randomized Weighted Majority Algorithm"/><published>2025-01-29T16:59:00+00:00</published><updated>2025-01-29T16:59:00+00:00</updated><id>https://monishver11.github.io/blog/2025/RWM</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/RWM/"><![CDATA[<p>The <strong>Randomized Weighted Majority (RWM) algorithm</strong> is an extension of the <strong>deterministic Weighted Majority (WM) algorithm</strong>, designed to overcome its limitations in adversarial settings, particularly in the <strong>zero-one loss</strong> scenario. This post explores why the deterministic approach struggles, how randomization helps, and what makes the RWM algorithm effective.</p> <h4 id="problem-with-the-deterministic-wm-algorithm"><strong>Problem with the Deterministic WM Algorithm</strong></h4> <p>The <strong>deterministic Weighted Majority (WM) algorithm</strong> operates by maintaining a set of experts, assigning them weights, and updating these weights based on their correctness. However, this approach suffers from <strong>high regret</strong> in adversarial settings.</p> <ul> <li> <p><strong>Regret in adversarial settings</strong><br/> No deterministic algorithm can achieve a <strong>sublinear regret</strong> of \(R_T = o(T)\) for all possible sequences under zero-one loss.</p> </li> <li> <p><strong>Worst-case scenario leading to linear regret</strong><br/> If the adversary knows the algorithm’s strategy, it can force it to make mistakes at every step.</p> <ul> <li>Suppose we have two experts: one always predicts <strong>0</strong>, the other always predicts <strong>1</strong>.</li> <li>If the best expert is correct <strong>only half the time</strong>, it makes at most <strong>\(T/2\)</strong> mistakes.</li> <li>The regret is defined as: \(R_T = m_T - m_T^*\)<br/> where: <ul> <li>\(m_T\) is the number of mistakes made by the algorithm.</li> <li>\(m_T^*\) is the number of mistakes made by the best expert.</li> </ul> </li> </ul> <p>Since \(m_T^* \leq T/2\), the regret in the worst case is at least: \(R_T \geq T/2\) which grows <strong>linearly</strong> with \(T\).</p> </li> </ul> <h4 id="the-randomized-weighted-majority-algorithm"><strong>The Randomized Weighted Majority Algorithm</strong></h4> <p>To address this issue, the <strong>Randomized Weighted Majority (RWM)</strong> algorithm introduces <strong>randomness</strong> into the decision-making process. Instead of deterministically following the highest-weighted expert, it assigns a <strong>probabilistic prediction</strong> based on expert weights.</p> <h5 id="key-idea-behind-rwm"><strong>Key Idea Behind RWM</strong></h5> <ul> <li>Instead of picking the expert with the highest weight <strong>deterministically</strong>, the algorithm selects predictions <strong>probabilistically</strong>, based on expert weights.</li> <li>Experts that have made fewer mistakes are given <strong>higher weights</strong>, making them more likely to be followed.</li> <li>This <strong>randomization prevents the adversary</strong> from forcing the algorithm to always make the same mistakes.</li> </ul> <h5 id="benefits-of-randomization"><strong>Benefits of Randomization</strong></h5> <ul> <li> <p><strong>Sublinear regret in adversarial settings</strong><br/> Unlike the deterministic approach, RWM can achieve: \(R_T = O(\sqrt{T})\) making it significantly better in the long run.</p> </li> <li> <p><strong>More balanced decision-making</strong><br/> By updating expert weights probabilistically, the algorithm avoids overly trusting any one expert too soon.</p> </li> </ul> <h4 id="the-randomized-weighted-majority-algorithm-step-by-step"><strong>The Randomized Weighted Majority Algorithm: Step-by-Step</strong></h4> <p>The algorithm follows these steps:</p> <ol> <li><strong>Initialize Weights:</strong> Each expert starts with an equal weight of <strong>1</strong>.</li> <li><strong>Compute Probabilities:</strong> The probability of selecting an expert is proportional to its weight.</li> <li><strong>Make a Prediction:</strong> Instead of following a single expert, the algorithm chooses its prediction probabilistically.</li> <li><strong>Update Weights:</strong> Experts that make mistakes have their weights <strong>decreased</strong> by a factor \(\beta\), where \(0 &lt; \beta &lt; 1\).</li> </ol> <p><strong><mark>Pseudocode:</mark></strong></p> \[\begin{array}{l} \textbf{Randomized-Weighted-Majority} \ (N) \\[5pt] \quad 1. \quad \textbf{for } i \gets 1 \text{ to } N \textbf{ do} \\ \quad 2. \quad \quad w_{1,i} \gets 1 \\ \quad 3. \quad \quad p_{1,i} \gets \frac{1}{N} \\[5pt] \quad 4. \quad \textbf{for } t \gets 1 \text{ to } T \textbf{ do} \\ \quad 5. \quad \quad \textbf{Receive } l_t \\ \quad 6. \quad \quad \textbf{for } i \gets 1 \text{ to } N \textbf{ do} \\ \quad 7. \quad \quad \quad \textbf{if } (l_{t,i} = 1) \textbf{ then} \\ \quad 8. \quad \quad \quad \quad w_{t+1,i} \gets \beta w_{t,i} \\ \quad 9. \quad \quad \quad \textbf{else} \\ \quad10. \quad \quad \quad \quad w_{t+1,i} \gets w_{t,i} \\[5pt] \quad11. \quad \quad W_{t+1} \gets \sum_{i=1}^{N} w_{t+1,i} \\[5pt] \quad12. \quad \quad \textbf{for } i \gets 1 \text{ to } N \textbf{ do} \\ \quad13. \quad \quad \quad p_{t+1,i} \gets w_{t+1,i} / W_{t+1} \\[5pt] \quad14. \quad \textbf{return } \mathbf{w}_{T+1} \end{array}\] <p>At this point, we’ve introduced the RWM algorithm, but a key question remains:</p> <blockquote> <p>How does randomization <strong>actually prevent</strong> the algorithm from making repeated mistakes, and how is the probabilistic selection <strong>used effectively</strong>?</p> </blockquote> <p>We’ll dive into this in the next section.</p> <hr/> <h4 id="how-randomization-prevents-repeated-mistakes"><strong>How Randomization Prevents Repeated Mistakes</strong></h4> <p>The <strong>Randomized Weighted Majority (RWM)</strong> algorithm prevents repeated mistakes in adversarial settings by making predictions <strong>probabilistically based on expert weights</strong>. Here’s how this works step by step:</p> <p><strong>1. Maintaining Expert Weights</strong></p> <ul> <li>We assign an initial weight to each expert, typically \(w_i^{(1)} = 1\) for all experts \(i\).</li> <li>Over time, we <strong>update the weights</strong> of experts based on their performance, penalizing those who make mistakes.</li> </ul> <p><strong>2. Making Probabilistic Predictions</strong></p> <ul> <li>Instead of deterministically following the best expert (which an adversary could exploit), RWM <strong>randomly selects a prediction</strong> based on the current expert weights.</li> <li>The probability of choosing a particular expert’s prediction is proportional to their weight: \(P(y_t = y_i) = \frac{w_i^{(t)}}{\sum_{j=1}^{N} w_j^{(t)}}\)<br/> where \(w_i^{(t)}\) is the weight of expert \(i\) at time \(t\).</li> <li>This means that if an expert has a high weight (i.e., has made fewer mistakes), their prediction is <strong>more likely</strong> to be chosen, but not always.</li> <li>If an adversary tries to force mistakes by targeting a specific deterministic strategy, the randomization ensures that the algorithm <strong>does not always follow a single pattern</strong>, making it harder for the adversary to exploit.</li> </ul> <p><strong>3. Weight Update Rule</strong></p> <ul> <li>After making a prediction, the algorithm observes the true outcome \(y_t\).</li> <li>The weights of experts who made mistakes are <strong>exponentially decreased</strong> using a multiplicative update rule: \(w_i^{(t+1)} = w_i^{(t)} \cdot \beta^{\ell_i^{(t)}}\)<br/> where: <ul> <li>\(\ell_i^{(t)}\) is the loss (1 if the expert made a mistake, 0 otherwise),</li> <li>\(\beta \in (0,1)\) is a parameter that determines how aggressively the weights are updated.</li> </ul> </li> <li>This ensures that over time, experts who consistently make mistakes lose influence, while those with good predictions gain more say in future predictions.</li> </ul> <p><strong>4. Why This Prevents Repeated Mistakes</strong></p> <ul> <li>Since the algorithm chooses predictions probabilistically, it does not <strong>consistently</strong> make the same mistakes like a deterministic algorithm would.</li> <li>Even if an adversary tries to construct a sequence that forces a mistake, RWM’s randomization means that <strong>the same incorrect choice won’t always be made</strong>.</li> <li>Moreover, since weights adjust dynamically, experts who perform better in the long run <strong>gradually dominate</strong> the prediction process.</li> </ul> <h5 id="takeaways"><strong><mark>Takeaways:</mark></strong></h5> <ul> <li><strong>Randomization prevents predictable failures</strong>: The algorithm does not follow a fixed pattern, making it harder for an adversary to force mistakes.</li> <li><strong>Probabilities favor better experts</strong>: Instead of blindly following one expert, the algorithm balances between exploration (randomization) and exploitation (favoring high-weight experts).</li> <li><strong>Weights adjust over time</strong>: Poor-performing experts lose influence, ensuring the algorithm improves as more data is observed.</li> </ul> <p>By incorporating randomness, the <strong>Randomized Weighted Majority Algorithm</strong> provides a <strong>powerful and adaptive approach</strong> to online learning, making it a fundamental tool in adversarial learning settings.</p> <hr/> <p>Here’s an analogy to make the <strong>Randomized Weighted Majority (RWM) algorithm</strong> more intuitive:</p> <p>Imagine you are in a <strong>new city</strong> for an extended stay, and you have to decide <strong>where to eat dinner every night</strong>. There are multiple restaurants (experts), and each night, you choose one based on your past experiences.</p> <p><strong>1. Initial Equal Preference (Assigning Weights)</strong></p> <p>At the start, you <strong>have no idea</strong> which restaurant is the best. So, you assign them equal preference:</p> <ul> <li>Restaurant A, B, and C all seem equally good, so you <strong>randomly pick one</strong>.</li> </ul> <p><strong>2. Evaluating Performance (Tracking Mistakes)</strong></p> <p>Each time you eat at a restaurant, you observe whether the meal was <strong>good</strong> or <strong>bad</strong>.</p> <ul> <li>If the meal was great, you <strong>trust the restaurant more</strong>.</li> <li>If it was terrible, you <strong>trust it less</strong>.</li> </ul> <p><strong>3. Adjusting Your Choices Over Time (Weight Updates)</strong></p> <p>Instead of always sticking to a single restaurant (which might backfire if it suddenly declines in quality), you <strong>adjust your preferences probabilistically</strong>:</p> <ul> <li>If <strong>Restaurant A</strong> has served consistently good food, you start <strong>choosing it more often</strong>, but you <strong>don’t completely ignore</strong> B and C.</li> <li>If <strong>Restaurant B</strong> has had a few bad meals, you reduce your visits there <strong>but still give it a chance occasionally</strong>.</li> </ul> <p><strong>4. Why Randomization Helps</strong></p> <p>Imagine there’s a <strong>food critic (the adversary)</strong> trying to ruin your dining experience.</p> <ul> <li>If you <strong>always follow a deterministic rule</strong> (e.g., always picking the currently best restaurant), the critic can <strong>sabotage your choices</strong>—perhaps by tipping off the restaurant to serve bad food only when you visit.</li> <li>However, by <strong>randomizing your choices</strong> (with a bias toward better restaurants), the critic <strong>can’t predict where you’ll go</strong>, making it much harder to force repeated bad experiences.</li> </ul> <p><strong>5. Long-Term Adaptation (Minimizing Regret)</strong></p> <p>Over time, bad restaurants get <strong>fewer chances</strong>, and good ones <strong>dominate your choices</strong>. But, because you <strong>never completely eliminate</strong> any option, you still have room to adjust if a once-bad restaurant improves.</p> <h5 id="mapping-back-to-rwm"><strong>Mapping Back to RWM</strong></h5> <ul> <li><strong>Restaurants = Experts</strong></li> <li><strong>Your decision = Algorithm’s prediction</strong></li> <li><strong>Good meal = Correct prediction (no loss)</strong></li> <li><strong>Bad meal = Mistake (loss)</strong></li> <li><strong>Reducing visits to bad restaurants = Lowering expert weights</strong></li> <li><strong>Randomly choosing where to eat = Making probabilistic predictions</strong></li> </ul> <p>By <strong>not always following the same pattern</strong>, you prevent predictable failures and <strong>gradually learn the best strategy</strong> while adapting to changes.</p> <hr/> <h4 id="randomized-weighted-majority-algorithm-regret-bound-and-proof"><strong>Randomized Weighted Majority Algorithm: Regret Bound and Proof</strong></h4> <p>The main objective of the RWM algorithm is to minimize the <strong>regret</strong>, which is the difference between the cumulative loss of the algorithm and that of the best possible decision (in hindsight) over time.</p> <p>Now, we’ll dive into the <strong>regret bound</strong> for the RWM algorithm. Specifically, we’ll present a theorem that gives a strong guarantee on the regret \(R_T\) of the algorithm, and follow up with a proof that demonstrates the result.</p> <h5 id="setting--notations"><strong>Setting &amp; Notations</strong></h5> <p>At each round \(t \in [T]\), an online algorithm \(A\) selects a distribution \(p_t\) over the set of actions, receives a loss vector \(\mathbf{l}_t\), whose \(i\)-th component \(l_{t,i} \in [0, 1]\) is the loss associated with action \(i\), and incurs the expected loss:</p> \[L_t = \sum_{i=1}^{N} p_{t,i} l_{t,i}\] <p>The total loss incurred by the algorithm over \(T\) rounds is:</p> \[\mathcal{L}_T = \sum_{t=1}^{T} L_t\] <p>The total loss associated with action \(i\) is:</p> \[\mathcal{L}_{T,i} = \sum_{t=1}^{T} l_{t,i}\] <p>The minimal loss of a single action is denoted by:</p> \[\mathcal{L}_{\text{min}}^T = \min_{i \in A} \mathcal{L}_{T,i}\] <p>The regret \(R_T\) of the algorithm after \(T\) rounds is typically defined as the difference between the loss of the algorithm and that of the best single action:</p> \[R_T = \mathcal{L}_T - \mathcal{L}_{\text{min}}^T\] <p><strong>Note:</strong> Whenever you’re confused by the notations of \(L\) and \(\mathcal{L}\), refer to this.</p> <hr/> <h5 id="rwm-regret-bound"><strong>RWM Regret Bound</strong></h5> <p>The following <strong>theorem</strong> provides a regret bound for the RWM algorithm, showing that the regret \(R_T\) is in \(O(\sqrt{T \log N})\), where \(T\) is the number of rounds, and \(N\) is the number of experts.</p> <p><strong>Theorem</strong> : Fix \(\beta \in [\frac{1}{2}, 1)\). Then, for any \(T \geq 1\), the loss of the algorithm \(\text{RWM}\) on any sequence of decisions can be bounded as follows:</p> \[\mathcal{L}_T \leq \frac{\log N}{1 - \beta} + (2 - \beta) \mathcal{L}^{\min}_T \tag{1}\] <p>In particular, for \(\beta = \max\left(\frac{1}{2}, 1 - \sqrt{\frac{\log N}{T}}\right)\), the loss can be further bounded as:</p> \[\mathcal{L}_T \leq \mathcal{L}^{\min}_T + 2 \sqrt{T \log N} \tag{2}\] <p>Here, \(\mathcal{L}_T\) is the total loss incurred by the algorithm till \(T\) rounds, and \(\mathcal{L}^{\min}_T\) is the minimal possible loss achievable by any expert till \(T\) rounds.</p> <hr/> <h5 id="proof-outline-deriving-the-regret-bound"><strong>Proof Outline: Deriving the Regret Bound</strong></h5> <p>The proof of this result relies on analyzing the <strong>potential function</strong> \(W_t\), which represents the total weight assigned to the experts at each round \(t\). We derive upper and lower bounds for \(W_t\) and combine them to establish the regret bound.</p> <p>Let’s walk through the key steps of the proof.</p> <hr/> <p><strong>Step 1: The Weight Update Rule</strong> The weight of expert \(i\) at round \(t+1\) is updated based on their incurred loss \(l_{t,i}\):</p> \[w_{t+1, i} = \begin{cases} w_{t, i} \cdot \beta, &amp; \text{if } l_{t, i} = 1 \\ w_{t, i}, &amp; \text{if } l_{t, i} = 0 \end{cases}\] <p>where \(\beta \in (0,1)\) is a fixed discount factor.</p> <p>The total weight at round \(t+1\) is then:</p> \[W_{t+1} = \sum_{i=1}^{N} w_{t+1, i}\] <hr/> <p><strong>Step 2: Evolution of Total Weight</strong> Using the update rule, we can express \(W_{t+1}\) in terms of \(W_t\):</p> \[W_{t+1} = \sum_{i: l_{t,i} = 0} w_{t,i} + \beta \sum_{i: l_{t,i} = 1} w_{t,i}\] \[= W_t + (\beta - 1) \sum_{i: l_{t,i} = 1} w_{t,i}\] \[= W_t + (\beta - 1) W_t \sum_{i: l_{t,i} = 1} p_{t,i}\] \[= W_t + (\beta - 1) W_t L_t\] \[= W_t (1 - (1 - \beta) L_t)\] <hr/> <p><strong>Note:</strong> If you’re unsure, refer to the items listed below, which should be used appropriately to achieve the desired result.</p> <p>Using the probability interpretation of the weights:</p> \[\sum_{i: l_{t,i}=1} w_{t,i} = W_t L_t,\] <p>where \(L_t\) is the expected loss at time \(t\):</p> \[L_t = \sum_{i=1}^{N} p_{t,i} l_{t,i}\] <p>Thus, we obtain:</p> \[W_{t+1} = W_t(1 - (1 - \beta) L_t)\] <hr/> <p>By recursion, since \(W_1 = N\), we get:</p> \[W_{T+1} = N \prod_{t=1}^{T} (1 - (1 - \beta) L_t)\] <hr/> <p><strong>Step 3: Lower Bound on \(W_{T+1}\)</strong> The minimum weight of any expert at round \(T+1\) satisfies:</p> \[W_{T+1} \geq \max_{i \in [N]} w_{T+1, i} = \beta^{\mathcal{L}_T^{\min}}\] <p>where \(\mathcal{L}_T^{\min}\) is the loss of the best expert.</p> <p>How did we arrive at this version?</p> <p>Each expert’s weight evolves according to the <strong>multiplicative update rule</strong>. If expert \(i\) incurs a loss \(l_{t,i}\) at round \(t\), its weight is updated as:</p> \[w_{t+1, i} = w_{t,i} \cdot \beta^{l_{t,i}}\] <p>where \(\beta \in (0,1]\) is the update factor.</p> <p>Define the <strong>best expert</strong> as the one with the <strong>minimum cumulative loss</strong> over \(T\) rounds. Let \(\mathcal{L}_T^{\min}\) denote this minimum loss:</p> \[\mathcal{L}_T^{\min} = \min_{i \in [N]} \sum_{t=1}^{T} l_{t,i}\] <p>For this best expert (say expert \(i^*\)), its weight after \(T\) rounds evolves as:</p> \[w_{T+1, i^*} = w_{1, i^*} \cdot \prod_{t=1}^{T} \beta^{l_{t,i^*}}\] <p>Since all experts start with an equal initial weight \(w_{1, i} = 1\) (assuming uniform initialization), we have:</p> \[w_{T+1, i^*} = \beta^{\mathcal{L}_T^{\min}}\] <p>Since the <strong>total weight</strong> at round \(T+1\) is at least the weight of the best expert, we get:</p> \[W_{T+1} = \sum_{i=1}^{N} w_{T+1, i} \geq w_{T+1, i^*} = \beta^{\mathcal{L}_T^{\min}}\] <p>Thus, the lower bound holds:</p> \[W_{T+1} \geq \beta^{\mathcal{L}_T^{\min}}\] <p>This ensures that the total weight does not shrink too fast, preserving a lower bound based on the best expert’s performance.</p> <hr/> <p><strong>Step 4: Taking Logarithms</strong> Taking the logarithm of both bounds:</p> \[\log W_{T+1} = \log N + \sum_{t=1}^{T} \log (1 - (1 - \beta) L_t)\] <p>For the second term, using the inequality \(\log(1 - x) \leq -x\) for \(x &lt; 1\), we get:</p> \[\sum_{t=1}^{T} \log (1 - (1 - \beta) L_t) \leq \sum_{t=1}^{T} - (1 - \beta) L_T = - (1 - \beta) \mathcal{L}_T\] <p>Thus,</p> \[\log W_{T+1} \leq \log N - (1 - \beta) \mathcal{L}_T.\] <p>Similarly, for the lower bound:</p> \[\log W_{T+1} \geq \mathcal{L}_T^{\min} \log \beta\] <p>Combining these,</p> \[\mathcal{L}_T^{\min} \log \beta \leq \log N - (1 - \beta) \mathcal{L}_T.\] <p>Rearranging,</p> \[\mathcal{L}_T \leq \frac{\log N}{1 - \beta} - \frac{\log \beta}{1 - \beta} \mathcal{L}_T^{\min}\] \[\mathcal{L}_T \leq \log N - \frac{\log (1 - (1 - \beta))}{1 - \beta} \mathcal{L}_T^{\min}\] <p>Again, for this second term, using the inequality \(-\log(1 - x) \leq x+x^2\) for \(x \in [0, \frac{1}{2}]\), we get:</p> \[\mathcal{L}_T \leq \frac{\log N}{1 - \beta} + (2 - \beta) \mathcal{L}_T^{\min} \tag{1}\] <p>This is the main result, and it provides a clear bound on the cumulative loss \(\mathcal{L}_T\).</p> <hr/> <p><strong>Step 5: Choosing Optimal \(\beta\)</strong></p> <p>We differentiate with respect to \(\beta\) and setting it to zero gives:</p> \[\frac{\log N}{(1 - \beta)^2} - T = 0\] <p>Solving for \(\beta\):</p> \[\beta = 1 - \sqrt{\frac{\log N}{T}} &lt; 1\] <p>If \(1 - \sqrt{\frac{\log N}{T}} \geq \frac{1}{2}\), then:</p> \[\beta_0 = 1 - \sqrt{\frac{\log N}{T}}\] <p>Otherwise, we use the boundary value \(\beta_0 = \frac{1}{2}\) is the optimal value.</p> <p>Substituting this choice in \((1)\), we get:</p> \[\mathcal{L}_T \leq \mathcal{L}_T^{\min} + 2\sqrt{T \log N} \tag{2}\] <p>Thus, the <strong>regret bound</strong> is:</p> \[R_T = \mathcal{L}_T - \mathcal{L}_T^{\min} \leq 2 \sqrt{T \log N}\] <p><strong>Key Essence:</strong></p> <ul> <li>The <strong>regret</strong> of the RWM algorithm is \(O(\sqrt{T \log N})\).</li> <li>The <strong>average regret per round \(R_T/T\)</strong> decreases as \(O(1/\sqrt{T})\).</li> </ul> <p>This result shows that RWM achieves <strong>sublinear regret</strong>, meaning that as the number of rounds \(T\) grows, the algorithm performs almost as well as the best expert.</p> <hr/> <p>Do we really grasp what this formula is conveying? It highlights a remarkable bound in online learning. Alright, let’s dig into that further.</p> <p><strong>What does sublinear regret mean?</strong></p> <p>When we say that an algorithm has <strong>sublinear regret</strong>, we mean that the total regret <strong>grows slower than the number of rounds</strong>. As the number of rounds \(T\) increases, the gap between the algorithm’s performance and the best expert’s performance doesn’t increase linearly. Instead, it grows at a slower rate (e.g., \(\sqrt{T}\)).</p> <p><strong>The meaning of the formula:</strong></p> <ul> <li> <p><strong>Regret \(O(\sqrt{T \log N})\)</strong>: This tells you that after \(T\) rounds, the total regret will grow roughly as \(\sqrt{T}\), with an additional logarithmic factor based on \(N\) (the number of possible actions). The logarithmic term grows slowly and doesn’t significantly affect the overall growth for large \(T\).</p> </li> <li> <p><strong>Average regret per round \(O(1/\sqrt{T})\)</strong>: This shows that, on average, the regret per round decreases as the number of rounds increases. As \(T\) gets larger, the average regret (the loss per round) decreases.</p> </li> </ul> <p><strong>Sublinear regret in action:</strong></p> <ol> <li><strong>At the start</strong>, when the algorithm has few rounds to learn, it might perform poorly (larger regret).</li> <li><strong>Over time</strong>, as \(T\) grows, the algorithm’s performance improves. It makes fewer mistakes as it “learns” from past rounds, and the regret per round decreases.</li> <li><strong>After many rounds</strong>, the algorithm performs almost as well as the best possible action, and the regret becomes quite small.</li> </ol> <p><strong>Key takeaway:</strong></p> <ul> <li><strong>Sublinear regret</strong> means that the algorithm’s performance gets closer to the best possible action as the number of rounds increases, but it does so at a slower pace than linear growth. The algorithm doesn’t just keep getting worse with more rounds; instead, it converges toward optimal performance.</li> </ul> <p><strong>Note:</strong> The bound \((2)\) assumes that the algorithm additionally receives as a parameter the number of rounds \(T\). However, as we learned from the <a href="https://monishver11.github.io/blog/2025/doubling-trick/">Doubling trick</a> in the previous blog, this requirement can be relaxed at the cost of a small constant factor increase.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>The <strong>Randomized Weighted Majority (RWM) Algorithm</strong> provides a powerful and efficient method for decision-making and prediction in online learning. The regret bound we’ve derived shows that, under the right conditions, the RWM algorithm can perform nearly as well as the best possible expert in hindsight, with a regret that grows at most as \(O(\sqrt{T \log N})\).</p> <p>This result is optimal, as demonstrated by further lower bound theorems, and provides a strong theoretical guarantee for the RWM algorithm’s performance in practice.</p> <h5 id="references"><strong>References</strong></h5>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.]]></summary></entry><entry><title type="html">Bayesian Decision Theory - Concepts and Recap</title><link href="https://monishver11.github.io/blog/2025/bayes-decision-theory/" rel="alternate" type="text/html" title="Bayesian Decision Theory - Concepts and Recap"/><published>2025-01-28T16:18:00+00:00</published><updated>2025-01-28T16:18:00+00:00</updated><id>https://monishver11.github.io/blog/2025/bayes-decision-theory</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/bayes-decision-theory/"><![CDATA[<p>Bayesian decision theory is a powerful framework for making decisions under uncertainty. It provides a principled way to combine prior knowledge with observed data to make optimal choices. In this post, we’ll take a closer look at its key components, revisit Bayesian point estimation, and connect these ideas to classical probability modeling. Let’s dive in!</p> <hr/> <h4 id="ingredients-of-bayesian-decision-theory"><strong>Ingredients of Bayesian Decision Theory</strong></h4> <p>At the heart of Bayesian decision theory lie several key components. First, we have the <strong>parameter space</strong>, denoted as \(\Theta\), which represents all possible values of the unknown parameter we aim to estimate or make decisions about. Next, we have the <strong>prior distribution</strong>, \(p(\theta)\), which encodes our beliefs about \(\theta\) before observing any data. This prior serves as a starting point in the Bayesian framework.</p> <p>Equally important is the <strong>action space</strong>, \(A\), which includes all possible actions we might take. To evaluate these actions, we rely on a <strong>loss function</strong>, \(\ell : A \times \Theta \to \mathbb{R}\), which quantifies the cost of taking a specific action \(a \in A\) when the true parameter value is \(\theta \in \Theta\).</p> <p>With these components, we can define the <strong>posterior risk</strong> of an action \(a \in A\), which represents the expected loss under the posterior distribution:</p> \[r(a) = \mathbb{E}[\ell(\theta, a) \mid D] = \int \ell(\theta, a) p(\theta \mid D) d\theta\] <p>The goal is to minimize this risk. The action that achieves this minimization is called the <strong>Bayes action</strong>, \(a^*\), which satisfies:</p> \[r(a^*) = \min_{a \in A} r(a)\] <h5 id="bayesian-point-estimation"><strong>Bayesian Point Estimation</strong></h5> <p>Bayesian point estimation builds upon this foundation. Imagine we have data \(D\) generated from some distribution \(p(y \mid \theta)\), where \(\theta \in \Theta\) is unknown. Our task is to find a single point estimate, \(\hat{\theta}\), that best represents \(\theta\).</p> <p>To do this, we first specify a prior distribution \(p(\theta)\) over \(\Theta\), which reflects our beliefs about \(\theta\) before observing the data. We then define a loss function, \(\ell(\hat{\theta}, \theta)\), to measure the cost of estimating \(\theta\) with \(\hat{\theta}\). Finally, we seek the point estimate \(\hat{\theta} \in \Theta\) that minimizes the posterior risk:</p> \[r(\hat{\theta}) = \mathbb{E}[\ell(\hat{\theta}, \theta) \mid D] = \int \ell(\hat{\theta}, \theta) p(\theta \mid D) d\theta.\] <h5 id="important-loss-functions-and-their-role"><strong>Important Loss Functions and Their Role</strong></h5> <p>The choice of loss function significantly influences the optimal estimate. Here are three commonly used loss functions and the corresponding Bayes actions:</p> <ul> <li> <p><strong>Squared Loss</strong>: \(\ell(\hat{\theta}, \theta) = (\theta - \hat{\theta})^2\).<br/> For squared loss, the Bayes action is the <strong>posterior mean</strong>, \(\mathbb{E}[\theta \mid D]\).</p> </li> <li> <p><strong>Zero-One Loss</strong>: \(\ell(\hat{\theta}, \theta) = 1[\theta \neq \hat{\theta}]\).<br/> For zero-one loss, the Bayes action is the <strong>posterior mode</strong>, the most probable value of \(\theta\) under the posterior.</p> </li> <li> <p><strong>Absolute Loss</strong>: \(\ell(\hat{\theta}, \theta) = |\theta - \hat{\theta}|\).<br/> For absolute loss, the Bayes action is the <strong>posterior median</strong>, the value that splits the posterior distribution into two equal halves.</p> </li> </ul> <h5 id="example-card-drawing"><strong>Example: Card Drawing</strong></h5> <p>To see this in action, consider drawing a card from a deck consisting of the values \(\{2, 3, 3, 4, 4, 5, 5, 5\}\). Suppose you are asked to guess the value of the card. Based on the posterior distribution:</p> <ul> <li>The <strong>mean</strong> of the distribution is \(3.875\).</li> <li>The <strong>mode</strong> (most frequent value) is \(5\).</li> <li>The <strong>median</strong> (middle value) is \(4\).</li> </ul> <p>This simple example highlights how different loss functions lead to different optimal estimates.</p> <h5 id="bayesian-point-estimation-with-squared-loss"><strong>Bayesian Point Estimation with Squared Loss</strong></h5> <p>We seek an action \(\hat{\theta}\) that minimizes the <strong>posterior risk</strong>, given by:</p> \[r(\hat{\theta}) = \int (\theta - \hat{\theta})^2 p(\theta | \mathcal{D}) \, d\theta\] <p>To find the optimal \(\hat{\theta}\), we differentiate:</p> \[\frac{d r(\hat{\theta})}{d\hat{\theta}} = - \int 2 (\theta - \hat{\theta}) p(\theta | \mathcal{D}) \, d\theta\] <p>Rearranging,</p> \[= -2 \int \theta p(\theta | \mathcal{D}) \, d\theta + 2\hat{\theta} \int p(\theta | \mathcal{D}) \, d\theta\] <p>Since the total probability integrates to 1,</p> \[\int p(\theta | \mathcal{D}) \, d\theta = 1,\] <p>this simplifies to:</p> \[\frac{d r(\hat{\theta})}{d\hat{\theta}} = -2 \int \theta p(\theta | \mathcal{D}) \, d\theta + 2\hat{\theta}\] <p>Setting the derivative to zero,</p> \[-2 \int \theta p(\theta | \mathcal{D}) \, d\theta + 2\hat{\theta} = 0\] <p>Solving for \(\hat{\theta}\),</p> \[\hat{\theta} = \int \theta p(\theta \mid D) d\theta = \mathbb{E}[\theta \mid D]\] <p>Thus, under squared loss, the Bayes action is the <strong>posterior mean</strong>.</p> <hr/> <h4 id="recap-and-interpretation"><strong>Recap and Interpretation</strong></h4> <p>Bayesian Decision Theory is built on a few core ideas that tie together probability, decision-making, and inference. Let’s revisit these concepts and unpack their meaning(again) all at once to gain the full picture.</p> <p><strong>Note:</strong> If you feel this isn’t necessary, feel free to skip it. However, I believe it’s helpful to reinforce these concepts periodically to build strong intuition and apply them effectively when needed.</p> <h5 id="the-prior-ptheta"><strong>The Prior (\(p(\theta)\))</strong></h5> <p>The prior represents our initial beliefs about the unknown parameter \(\theta\) before observing any data. It encapsulates what we know (or assume) about \(\theta\) based on prior knowledge, expert opinion, or historical data.</p> <p>For example, if \(\theta\) represents the probability of success in a coin toss, a reasonable prior might be a Beta distribution centered around 0.5, reflecting our belief that the coin is fair.</p> <h5 id="the-posterior-ptheta-mid-d"><strong>The Posterior (\(p(\theta \mid D)\))</strong></h5> <p>The posterior is the updated belief about \(\theta\) after observing the data \(D\). It combines the prior \(p(\theta)\) with the likelihood of the data \(p(D \mid \theta)\) using Bayes’ theorem:</p> \[p(\theta \mid D) = \frac{p(D \mid \theta) p(\theta)}{p(D)}\] <p>The posterior is the foundation of all Bayesian inference. It reflects how the data has rationally updated our initial beliefs.</p> <h5 id="inferences-and-actions"><strong>Inferences and Actions</strong></h5> <p>In the Bayesian framework, all inferences (e.g., estimating \(\theta\)) and actions (e.g., making decisions) are based on the posterior distribution. This is because the posterior contains all the information we have about \(\theta\), combining both prior knowledge and observed data.</p> <p>For example, if we want to estimate \(\theta\), we might compute the posterior mean, median, or mode, depending on the loss function we choose.</p> <h5 id="no-need-to-justify-an-estimator"><strong>No Need to Justify an Estimator</strong></h5> <p>In classical statistics, we often need to justify why a particular estimator (e.g., the sample mean) is a good choice. In Bayesian statistics, this issue doesn’t arise because the estimator is derived directly from the posterior distribution, which is fully determined by the prior and the data.</p> <p>The only choices we need to make are:</p> <ol> <li>The family of distributions (e.g., Gaussian, Beta) that model the data.</li> <li>The prior distribution on the parameter space \(\Theta\).</li> </ol> <p><strong>Role of the Loss Function</strong></p> <p>The loss function \(\ell(a, \theta)\) quantifies the cost of taking action \(a\) when the true parameter is \(\theta\). It bridges the gap between inference and decision-making.</p> <p>The optimal action \(a^*\) is the one that minimizes the posterior risk, which is the expected loss under the posterior distribution:</p> \[r(a) = \mathbb{E}[\ell(a, \theta) \mid D] = \int \ell(a, \theta) p(\theta \mid D) \, d\theta\] <p>Different loss functions lead to different optimal actions. For example:</p> <ul> <li><strong>Squared loss</strong> leads to the posterior mean.</li> <li><strong>Absolute loss</strong> leads to the posterior median.</li> <li><strong>Zero-one loss</strong> leads to the posterior mode.</li> </ul> <h5 id="philosophical-interpretation"><strong>Philosophical Interpretation</strong></h5> <p>Bayesian Decision Theory is fundamentally about rational decision-making under uncertainty. It provides a coherent framework for updating beliefs and making decisions that minimize expected loss.</p> <p>Unlike frequentist methods, which focus on long-run properties of estimators, Bayesian methods focus on the current state of knowledge, as represented by the posterior distribution.</p> <h5 id="why-does-this-matter"><strong>Why Does This Matter?</strong></h5> <p>Understanding these concepts is crucial because they form the backbone of Bayesian thinking. Here’s why:</p> <ol> <li><strong>Flexibility</strong>: The Bayesian approach allows us to incorporate prior knowledge into our analysis, which can be especially useful when data is limited.</li> <li><strong>Transparency</strong>: All assumptions (e.g., the choice of prior) are explicitly stated, making the analysis transparent and interpretable.</li> <li><strong>Decision-Oriented</strong>: By focusing on minimizing expected loss, Bayesian Decision Theory directly addresses the practical goal of making optimal decisions.</li> </ol> <h5 id="example-estimating-the-mean-of-a-normal-distribution"><strong>Example: Estimating the Mean of a Normal Distribution</strong></h5> <p>Suppose we want to estimate the mean \(\theta\) of a normal distribution based on observed data \(D\). Here’s how the Bayesian approach works:</p> <ol> <li><strong>Prior</strong>: We choose a normal prior \(p(\theta) = N(\mu_0, \sigma_0^2)\), where \(\mu_0\) and \(\sigma_0^2\) reflect our initial beliefs about \(\theta\).</li> <li><strong>Likelihood</strong>: The data \(D\) is modeled as \(p(D \mid \theta) = N(\theta, \sigma^2)\).</li> <li><strong>Posterior</strong>: Using Bayes’ theorem, the posterior \(p(\theta \mid D)\) is also a normal distribution, with updated mean and variance that balance the prior and the data.</li> <li><strong>Decision</strong>: If we use squared loss, the optimal estimate \(\hat{\theta}\) is the posterior mean.</li> </ol> <p>This example illustrates how the Bayesian approach seamlessly integrates prior knowledge with observed data to produce a rational and optimal estimate.</p> <h5 id="example-estimating-the-mean-of-a-normal-distribution-frequentist-approach"><strong>Example: Estimating the Mean of a Normal Distribution (Frequentist Approach)</strong></h5> <ol> <li> <p><strong>Model Assumption</strong>:<br/> Assume the data \(D\) comes from a normal distribution: \(p(D \mid \theta) = N(\theta, \sigma^2),\)<br/> where \(\theta\) is the unknown mean and \(\sigma^2\) is the known variance.</p> </li> <li> <p><strong>Estimator</strong>:<br/> Use the sample mean: \(\bar{D} = \frac{1}{n} \sum_{i=1}^n D_i\)<br/> as the estimator for \(\theta\). This is derived as the <strong>maximum likelihood estimator (MLE)</strong> because it maximizes the likelihood function:</p> \[L(\theta) = \prod_{i=1}^n p(D_i \mid \theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(D_i - \theta)^2}{2\sigma^2}\right).\] </li> <li><strong>Properties of the Estimator</strong>: <ul> <li>The sample mean \(\bar{D}\) is an <strong>unbiased estimator</strong> of \(\theta\), meaning: \(E[\bar{D}] = \theta.\)</li> <li>Its variance is: \(\text{Var}(\bar{D}) = \frac{\sigma^2}{n},\)<br/> which decreases as the sample size \(n\) increases.</li> </ul> </li> <li><strong>Decision</strong>:<br/> The sample mean \(\bar{D}\) is reported as the estimate of \(\theta\). There is no explicit loss function in the frequentist framework; instead, the sample mean is justified by its desirable properties (e.g., unbiasedness, efficiency, and consistency).</li> </ol> <p><strong>Note:</strong> If you’re unsure about this derivation from MLE, check out this <a href="https://monishver11.github.io/blog/2025/NB-continuous-features/">link</a> for clarification.</p> <p><strong><mark>Key Takeaways</mark></strong></p> <ol> <li>The prior represents initial beliefs, the posterior represents updated beliefs, and the loss function guides decision-making.</li> <li>Bayesian methods are inherently decision-oriented, focusing on minimizing expected loss.</li> <li>The only choices we need to make are the family of distributions and the prior—everything else follows logically from these choices.</li> </ol> <hr/> <p>A few follow-up questions you might have:</p> <h5 id="explanation-of-actionsdecision-making"><strong>Explanation of “Actions/Decision-Making”</strong></h5> <p>In the Bayesian framework, <strong>actions</strong> or <strong>decision-making</strong> refer to the choices or decisions we make based on the information encoded in the posterior distribution. These decisions could range from estimating a parameter to choosing between different courses of action based on the expected outcomes.</p> <p>For example:</p> <ul> <li>If you’re estimating a parameter \(\theta\), the <strong>action</strong> could be selecting the posterior mean, median, or mode as your estimate.</li> <li>If you’re deciding whether to launch a product, the <strong>action</strong> could involve calculating the probability of success using the posterior and deciding based on a predefined threshold.</li> <li>In medical diagnostics, the <strong>action</strong> could be choosing a treatment plan based on the likelihood of a disease inferred from the posterior.</li> </ul> <p>In essence, <strong>actions</strong> are the outcomes of the decision-making process, guided by the posterior distribution and a loss function that quantifies the cost of making an incorrect decision.</p> <h5 id="what-is-an-estimator-in-the-frequentist-approach"><strong>What is an “Estimator” in the frequentist approach?</strong></h5> <p>An <strong>estimator</strong> is a statistical function or rule used to estimate an unknown parameter \(\theta\) based on observed data. In frequentist statistics, estimators are often chosen based on their theoretical properties, such as:</p> <ul> <li><strong>Unbiasedness</strong>: The estimator’s expected value equals the true parameter value.</li> <li><strong>Efficiency</strong>: The estimator has the smallest possible variance among all unbiased estimators.</li> <li><strong>Consistency</strong>: The estimator converges to the true parameter value as the sample size increases.</li> </ul> <p>For example:</p> <ul> <li>The <strong>sample mean</strong> is a common estimator for the population mean.</li> <li>The <strong>sample variance</strong> is an estimator for the population variance.</li> </ul> <p>In Bayesian statistics, however, the <strong>estimator</strong> is derived directly from the posterior distribution. For instance:</p> <ul> <li>The <strong>posterior mean</strong> minimizes squared error loss.</li> <li>The <strong>posterior median</strong> minimizes absolute error loss.</li> <li>The <strong>posterior mode</strong> corresponds to the most likely value of \(\theta\).</li> </ul> <p>The key difference is that Bayesian methods do not require separate justification for an estimator because the posterior distribution naturally incorporates both the prior beliefs and observed data, making the choice of estimator a consequence of the decision-making process.</p> <hr/> <p>We’ve covered most of it, right? Now, let’s revisit the foundational concepts that underpin everything we’ve discussed so far, including conditional probability, the likelihood function, and MLE in a general sense.</p> <h5 id="conditional-probability-modeling"><strong>Conditional Probability Modeling</strong></h5> <p>In this context, we have:</p> <ul> <li>An <strong>input space</strong>, \(X\), which represents the features or predictors.</li> <li>An <strong>outcome space</strong>, \(Y\), which represents the possible outputs.</li> <li>An <strong>action space</strong>, \(A\), consisting of probability distributions on \(Y\).</li> </ul> <p>A prediction function \(f : X \to A\) maps each input \(x \in X\) to a distribution on \(Y\). This setup allows us to model the relationship between inputs and outputs probabilistically.</p> <p>In a parametric framework, we define a family of conditional densities:</p> \[\{p(y \mid x, \theta) : \theta \in \Theta\},\] <p>where \(p(y \mid x, \theta)\) is a density on \(Y\) for each \(x \in X\), and \(\theta\) is a parameter in the finite-dimensional space \(\Theta\). This is the common starting point for either classical or Bayesian regression.</p> <h5 id="classical-treatment-likelihood-function"><strong>Classical Treatment: Likelihood Function</strong></h5> <p>In the classical approach, we begin with data \(D = (y_1, \dots, y_n)\) and assume it is generated by the conditional density \(p(y \mid x, \theta)\). The probability of the data is:</p> \[p(D \mid x_1, \dots, x_n, \theta) = \prod_{i=1}^n p(y_i \mid x_i, \theta)\] <p>For fixed \(D\), the likelihood function is defined as:</p> \[L_D(\theta) = p(D \mid x, \theta),\] <p>where \(x = (x_1, \dots, x_n)\).</p> <h5 id="maximum-likelihood-estimator-mle"><strong>Maximum Likelihood Estimator (MLE)</strong></h5> <p>The <strong>Maximum Likelihood Estimator (MLE)</strong> for \(\theta\) is the value that maximizes the likelihood function:</p> \[\hat{\theta}_{\text{MLE}} = \arg\max_{\theta \in \Theta} L_D(\theta)\] <p>Interestingly, MLE corresponds to <strong>Empirical Risk Minimization (ERM)</strong> if we set the loss function to the negative log-likelihood. The resulting prediction function is:</p> \[\hat{f}(x) = p(y \mid x, \hat{\theta}_{\text{MLE}})\] <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>So, we’ve reached the end of this blog. I know it can be confusing at times—I’ve been confused too—but take your time to get a solid grasp on it. This is the foundation of Bayesian ML. In the past few blogs, we’ve discussed these concepts, but do you recall if we’ve applied them to any prediction tasks yet? The answer is no. In the next one, we’ll put them into practice through Bayesian conditional models.</p> <h5 id="references"><strong>References</strong></h5>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.]]></summary></entry><entry><title type="html">Reinforcement Learning - An Introductory Guide</title><link href="https://monishver11.github.io/blog/2025/rl-intro/" rel="alternate" type="text/html" title="Reinforcement Learning - An Introductory Guide"/><published>2025-01-28T03:45:00+00:00</published><updated>2025-01-28T03:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/rl-intro</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/rl-intro/"><![CDATA[<p>Reinforcement Learning (RL) is a fascinating field that focuses on teaching agents how to make decisions based on their environment. The agent’s goal is to learn a strategy that maximizes a cumulative reward over time by interacting with its surroundings. But before we dive into the details of RL, let’s explore the fundamental concepts of decision-making and intelligence.</p> <h5 id="what-is-decision-making"><strong>What is Decision Making?</strong></h5> <p>At its core, decision-making is the process of choosing the best possible action from a set of alternatives. It lies at the heart of intelligence and is fundamental to building intelligent systems.</p> <h5 id="what-is-intelligence"><strong>What is Intelligence?</strong></h5> <p>Intelligence can be broadly defined as the ability to:</p> <ul> <li>Understand and process what’s happening around you.</li> <li>Make informed decisions.</li> <li>Reason and analyze complex situations.</li> </ul> <p>In simpler terms, intelligence enables us to learn, adapt, and tackle new or challenging scenarios effectively.</p> <h5 id="why-intelligence-and-how-does-it-work"><strong>Why Intelligence and How Does It Work?</strong></h5> <p>A strong driving force for the evolution of intelligence is survival. For instance, early single-celled organisms relied on simple hunting strategies. Over time, the emergence of multicellular organisms, like <em>C. elegans</em>, marked a leap in complexity. They developed neurons, enabling coordination and more sophisticated strategies.</p> <p>While the exact mechanisms of intelligence remain elusive, certain principles stand out:</p> <ul> <li>Neurons form the foundation of biological intelligence.</li> <li>Artificial neural networks via backpropagation, despite their inspiration from biology, cannot replicate human intelligence <strong>yet</strong>.</li> <li>Understanding the principles of intelligence is arguably more important than mimicking its mechanisms.</li> </ul> <hr/> <h4 id="six-lessons-from-babies"><strong>Six Lessons from Babies</strong></h4> <p>Developmental psychology offers valuable insights into intelligence. Here are six key lessons we can learn from how babies develop:</p> <ol> <li><strong>Be Multimodal</strong>: Babies combine sensory inputs (sight, sound, touch, etc.) to create a cohesive understanding of their environment.</li> <li><strong>Be Incremental</strong>: Learning is gradual. Babies adapt as they encounter new information. <ul> <li>Unlike i.i.d. data in supervised learning, real-world learning is sequential and non-i.i.d., posing unique challenges.</li> <li>RL algorithms often simulate i.i.d.-like scenarios to work effectively.</li> </ul> </li> <li><strong>Be Physical</strong>: Interaction with the environment is crucial. Babies learn by manipulating objects and observing the outcomes.</li> <li><strong>Explore</strong>: Exploration is central to learning. Babies experiment with their surroundings to gather information and refine their actions.</li> <li><strong>Be Social</strong>: Social interactions play a significant role. Babies learn by observing and imitating others.</li> <li><strong>Learn a Language</strong>: Language serves as a symbolic framework to organize thoughts and retrieve information efficiently.</li> </ol> <p>These principles are directly relevant to reinforcement learning (RL), where agents learn by interacting with and exploring their environment.</p> <h5 id="takeaways-thus-far"><strong>Takeaways Thus Far;</strong></h5> <ul> <li>Intelligence is fundamentally rooted in decision-making.</li> <li>Decision-making occurs at various levels, from low-level motor control to high-level reasoning and coordination.</li> <li>Algorithms for decision-making depend heavily on the specific task.</li> <li>Neuroscience, motor control, and cognitive psychology provide valuable insights for designing intelligent systems.</li> <li>Translating biological insights into computational systems remains challenging due to a lack of foundational understanding.</li> </ul> <hr/> <h4 id="a-computational-lens-on-decision-making"><strong>A Computational Lens on Decision Making</strong></h4> <p>Decision-making can be viewed through the following computational frameworks:</p> <h5 id="from-an-agents-perspective"><strong>From an Agent’s Perspective:</strong></h5> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Agent_Persepective-480.webp 480w,/assets/img/Agent_Persepective-800.webp 800w,/assets/img/Agent_Persepective-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Agent_Persepective.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Agent_Persepective" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>Sense</li> <li>Think</li> <li>Act</li> <li>Repeat</li> </ol> <h5 id="from-a-global-perspective"><strong>From a Global Perspective:</strong></h5> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Global_Persepective-480.webp 480w,/assets/img/Global_Persepective-800.webp 800w,/assets/img/Global_Persepective-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Global_Persepective.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Global_Persepective" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li>Observations</li> <li>Sense → Think → Act</li> <li>Effects of Actions</li> <li>Repeat</li> </ol> <p>Alternate terminologies for decision-making systems include policy, strategy, and controller.</p> <h5 id="examples-of-computational-decision-making"><strong>Examples of Computational Decision Making</strong></h5> <ul> <li><strong>Atari Games (Deep RL)</strong>: Agents maximize scores by analyzing game frames and selecting actions accordingly.</li> <li><strong>Google Robots</strong>: Robots perform complex tasks using intricate joint control mechanisms.</li> <li><strong>Go</strong>: RL has enabled agents to outperform humans in this game with a vast decision space.</li> <li><strong>Dota 2</strong>: RL systems have been trained to defeat top human players by optimizing strategies in a dynamic environment. Though this success is currently limited to controlled and restricted settings.</li> </ul> <p>Reinforcement learning provides a robust framework to model decision-making by teaching agents to optimize actions based on rewards.</p> <h5 id="the-rewards-mechanism-and-sequential-decision-making"><strong>The Rewards Mechanism and Sequential Decision Making</strong></h5> <p>In RL, decision-making unfolds over time through a sequence of observations, actions, and rewards. This sequence can be represented as:</p> \[(o_1, a_1, r_1) \to (o_2, a_2, r_2) \to \dots \to (o_n, a_n, r_n)\] <p>Where:</p> <ul> <li>\(o_t\): Observation at time \(t\),</li> <li>\(a_t\): Action taken at time \(t\),</li> <li>\(r_t\): Reward received after taking action \(a_t\).</li> </ul> <p>The objective is to maximize the cumulative reward:</p> \[\max_{a_1, a_2, \dots, a_{T-1}} \sum_{t=1}^{T} r_t\] <p>The key challenge is determining the optimal actions \(a_t\) at each time step to achieve the maximum long-term reward. This leads to the concept of <em>policy optimization</em>.</p> <h5 id="planning-and-world-models"><strong>Planning and World Models</strong></h5> <p>Another approach to decision-making is through planning, where the agent uses a model of the world to simulate the effects of its actions. This allows the agent to reason about potential future states and make decisions accordingly.</p> <p>However, planning is limited in its applicability:</p> <ul> <li>It works well in discrete, well-defined environments like games.</li> <li>It struggles with complex, dynamic tasks like conversational AI or real-world robotics.</li> </ul> <h5 id="the-limits-of-current-approaches"><strong>The Limits of Current Approaches</strong></h5> <p>Despite their potential, RL and planning-based models face significant challenges:</p> <ul> <li>Real-world scenarios often lack the i.i.d. assumption that RL sometime relies on.</li> <li>Bridging the gap between controlled simulations and dynamic real-world environments remains a key hurdle.</li> </ul> <h5 id="a-note-of-caution-amidst-progress"><strong>A Note of Caution Amidst Progress</strong></h5> <p>While reinforcement learning and computational decision-making have seen remarkable progress, it’s important to recognize the challenges that remain. This brings us to <strong>Moravec’s Paradox</strong>, a fascinating insight into the nature of artificial intelligence:</p> <blockquote> <p>“It is comparatively easy to make computers exhibit adult-level performance on intelligence tests or play games like chess, yet it is extremely difficult to give them the skills of a one-year-old when it comes to perception and mobility.”<br/> — <strong>Hans Moravec, 1988</strong></p> </blockquote> <p>Steven Pinker elaborated further:</p> <blockquote> <p>“The main lesson of thirty-five years of AI research is that the hard problems are easy, and the easy problems are hard.”<br/> — <strong>Steven Pinker, 1994</strong></p> </blockquote> <p>What this paradox highlights is that tasks humans find effortless—such as walking, recognizing faces, or interacting physically with the environment—require immense computational power and intricate modeling to replicate in machines. Conversely, tasks like playing chess, solving mathematical problems, or optimizing game strategies are relatively easier for computers.</p> <p>This paradox underscores the fact that intelligence, especially in its perceptual and physical forms, is deeply rooted in evolutionary processes. The interplay of sensory data, motor control, and real-world adaptation—elements essential for robust intelligence—remains a significant challenge for machines.</p> <p>In reinforcement learning, we see this reflected in the difficulty of training agents to generalize to unstructured, real-world environments. RL agents perform admirably in games like <strong>Atari</strong> or <strong>Go</strong>, but replicating even the basic capabilities of a human child—like balancing on uneven surfaces or adapting to novel stimuli—remains an open frontier.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>Reinforcement learning provides a powerful framework for decision-making, allowing agents to learn from their interactions with the environment. By leveraging concepts such as reward mechanisms, policy optimization, and planning, we have achieved significant milestones in fields like gaming, robotics, and autonomous systems.</p> <p>However, the journey toward creating truly intelligent systems is far from over. While computational models continue to evolve, there is a need for a deeper understanding of the principles of intelligence, both biological and artificial.</p> <p>As we wrap up this introduction to reinforcement learning and decision-making, it’s clear that intelligence is fundamentally about making informed decisions. Whether through supervised learning, reinforcement learning, or planning, the goal remains the same: enabling machines to reason, adapt, and thrive in dynamic environments.</p> <p>In the next post, we’ll dive deeper into decision-making in supervised learning and explore how it serves as the foundation for many modern AI systems. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li></li> </ul>]]></content><author><name></name></author><category term="RL-NYU"/><category term="ML"/><summary type="html"><![CDATA[Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.]]></summary></entry><entry><title type="html">Conjugate Priors and Bayes Point Estimates</title><link href="https://monishver11.github.io/blog/2025/bayes-point-estimate/" rel="alternate" type="text/html" title="Conjugate Priors and Bayes Point Estimates"/><published>2025-01-27T16:38:00+00:00</published><updated>2025-01-27T16:38:00+00:00</updated><id>https://monishver11.github.io/blog/2025/bayes-point-estimate</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/bayes-point-estimate/"><![CDATA[<p>Bayesian statistics provides us with a framework to incorporate prior knowledge, update our beliefs with new evidence, and make principled decisions. Two key concepts in this framework are <strong>Bayes Point Estimates</strong> and <strong>Conjugate Priors</strong>, which we will explore in this blog post. These ideas not only simplify computations but also help us build intuition for Bayesian inference.</p> <hr/> <h4 id="what-are-conjugate-priors"><strong>What are Conjugate Priors?</strong></h4> <p>Imagine having a prior belief about the world, represented by a probability distribution. When new data arrives, Bayesian inference updates this belief, resulting in a posterior distribution. Now, if the updated posterior distribution belongs to the same family as the prior, we call the prior <strong>conjugate</strong> for the chosen likelihood model.</p> <p>Formally, let \(\pi\) denote a family of prior distributions over the parameter space \(\Theta\), and let \(P\) represent a parametric likelihood model. We say that the family \(\pi\) is <strong>conjugate</strong> to \(P\) if, for any prior in \(\pi\), the posterior distribution also belongs to \(\pi\).</p> <p>A classic example of conjugate priors is the <strong>Beta distribution</strong> for the <strong>Bernoulli model</strong> (used to model coin flips). If the prior belief about a coin’s bias follows a Beta distribution, the posterior distribution after observing the coin flips will also follow a Beta distribution. This conjugacy greatly simplifies the mathematical computations in Bayesian updating.</p> <p>A simple way to form an intuition about this is to think of conjugate priors as allowing the mathematical form of your beliefs (prior) and your updated beliefs (posterior) to stay consistent. It’s like using a “<strong>familiar language</strong>” for both your starting beliefs and your updated beliefs—making things easier to compute and interpret.</p> <p>So, with this, you can see why we chose the beta distribution in the last blog, right? Great! Now, let’s continue with the same example to explore further.</p> <hr/> <h4 id="a-concrete-example-coin-flipping"><strong>A Concrete Example: Coin Flipping</strong></h4> <p>Let’s make this more concrete by revisiting the example from the last blog. Suppose we have a coin that may be biased, and we want to estimate its bias \(\theta\) (the probability of landing heads).</p> <p><strong>Note:</strong> If the notation or its meaning is unclear, check out the previous <a href="http://localhost:8080/blog/2025/Bayesian-ML/">blog</a> for clarification.</p> <p>The probability model for a single coin flip is given by:</p> \[P(\text{Heads} \mid \theta) = \theta, \quad \theta \in [0,1]\] <p>Before flipping the coin, we encode our prior belief about \(\theta\) using a <strong>Beta distribution</strong>: \(\theta \sim \text{Beta}(2, 2).\)</p> <p>This Beta distribution reflects a prior belief that the coin is likely fair (centered around \(\theta = 0.5\)), but with some uncertainty.</p> <p>Now, we flip the coin multiple times and observe the following data: \(D = \{\text{H, H, T, T, T, T, T, H, ..., T}\},\)<br/> where the coin lands on heads 75 times and tails 60 times.</p> <h5 id="updating-beliefs-the-posterior-distribution"><strong>Updating Beliefs: The Posterior Distribution</strong></h5> <p>Using Bayes’ theorem, we combine the prior and the likelihood of the observed data to compute the posterior distribution:</p> \[p(\theta \mid D) \propto p(D \mid \theta) p(\theta)\] <p>For the Beta-Bernoulli model, the posterior distribution is also Beta-distributed, with updated parameters:</p> \[\theta \mid D \sim \text{Beta}(\alpha + \text{Heads}, \beta + \text{Tails})\] <p>In our example, the prior parameters are \(\alpha = 2\) and \(\beta = 2\). After observing 75 heads and 60 tails, the posterior becomes:</p> \[\theta \mid D \sim \text{Beta}(2 + 75, 2 + 60) = \text{Beta}(77, 62)\] <p>This posterior distribution captures our updated belief about the coin’s bias after incorporating the observed data.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Bayes-Prior-480.webp 480w,/assets/img/Bayes-Prior-800.webp 800w,/assets/img/Bayes-Prior-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Bayes-Prior.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Bayes-Prior" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Bayes-Posterior-480.webp 480w,/assets/img/Bayes-Posterior-800.webp 800w,/assets/img/Bayes-Posterior-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Bayes-Posterior.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Bayes-Posterior" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong><mark>Takeaway:</mark></strong> The Beta distribution’s conjugacy with the Bernoulli likelihood model makes updating beliefs straightforward. The posterior parameters are simply the sum of the prior parameters and the counts of heads and tails, which means Bayesian updating is computationally efficient and intuitive in this case.</p> <h4 id="bayesian-point-estimates"><strong>Bayesian Point Estimates</strong></h4> <p>Once we have the posterior distribution, the next question is: how do we summarize it into a single value? This is where Bayes Point Estimates come into play. These estimates provide a representative value for \(\theta\) based on the posterior.</p> <p>One common approach is the <strong>posterior mean</strong>, which represents the expected value of \(\theta\) under the posterior distribution:</p> \[\hat{\theta}_{\text{mean}} = \mathbb{E}[\theta \mid D] = \frac{\alpha + \text{Heads}}{\alpha + \beta + \text{Heads} + \text{Tails}}\] <p>Substituting the parameters from our example:</p> \[\hat{\theta}_{\text{mean}} = \frac{77}{77 + 62} \approx 0.554\] <p>Another popular estimate is the <strong>Maximum a Posteriori (MAP)</strong> estimate, which is the mode of the posterior distribution. For the Beta distribution, the MAP estimate is given by:</p> \[\hat{\theta}_{\text{MAP}} = \frac{\alpha - 1}{\alpha + \beta - 2}, \quad \text{if } \alpha &gt; 1 \text{ and } \beta &gt; 1\] <p>Alternatively, the MAP estimate can be expressed as:</p> \[\hat{\theta}_{\text{MAP}} = \arg\max_\theta p(\theta \mid D)\] <p>Using our parameters:</p> \[\hat{\theta}_{\text{MAP}} = \frac{77 - 1}{77 + 62 - 2} = \frac{76}{137} \approx 0.555\] <p>Both the posterior mean and the MAP estimate provide valuable insights into the coin’s bias, with slight differences depending on how they summarize the posterior.</p> <h5 id="posterior-mean-vs-map-estimate"><strong>Posterior Mean vs. MAP Estimate</strong></h5> <ol> <li><strong>Posterior Mean (Expected Value)</strong>: <ul> <li><strong>Analogy</strong>: Think of the posterior mean like a <strong>“balanced average”</strong>. It provides a representative value for \(\theta\) by averaging all possible values, weighted by their likelihood under the posterior distribution. This is especially useful when you want an overall estimate that incorporates all the data.</li> <li><strong>When to use</strong>: The posterior mean is often a good choice when the data is <strong>symmetric</strong> or you have a <strong>large sample size</strong>, and you need a balanced estimate.</li> <li><strong>Example</strong>: The posterior mean gives us a fair estimate of the coin’s bias, accounting for both the prior belief and the observed data.</li> </ul> </li> <li><strong>Maximum a Posteriori (MAP) Estimate (Mode)</strong>: <ul> <li><strong>Analogy</strong>: The MAP estimate is like finding the <strong>“peak”</strong> of a hill. It’s the value of \(\theta\) where the posterior distribution is most concentrated. This is useful when you care about the <strong>most probable</strong> value of \(\theta\) based on both the data and the prior.</li> <li><strong>When to use</strong>: The MAP estimate is helpful when you have a <strong>strong prior</strong> or <strong>sparse data</strong>. It gives you the point estimate where the posterior distribution is maximized.</li> <li><strong>Example</strong>: The MAP estimate gives us the most likely value of the coin’s bias after considering both the data and prior belief.</li> </ul> </li> </ol> <p>So, The <strong>posterior mean</strong> gives a more <strong>general</strong>, balanced estimate, while the <strong>MAP</strong> estimate is more focused on the <strong>most likely</strong> value based on your prior and data. The choice between the two depends on your data, the role of your prior, and what you want to emphasize in your estimate.</p> <hr/> <h4 id="beyond-point-estimates-what-else-can-we-do-with-the-posterior"><strong>Beyond Point Estimates: What Else Can We Do with the Posterior?</strong></h4> <p>The posterior distribution is far more than just a tool for calculating point estimates. It encapsulates the full range of uncertainty about \(\theta\) and opens up a variety of analytical possibilities. Here are a few ways we can make use of the posterior:</p> <h5 id="visualize-uncertainty"><strong>Visualize Uncertainty</strong></h5> <ul> <li><strong>Means</strong>: One of the most powerful features of the posterior distribution is its ability to represent uncertainty. By plotting the posterior, we can visually assess how concentrated or spread out our beliefs about \(\theta\) are.</li> <li><strong>Mental Image</strong>: Imagine you’re standing at the top of a mountain, looking at a wide valley. The <strong>posterior distribution</strong> is like the shape of the valley, and the <strong>height of the valley</strong> at each point shows how likely that value of \(\theta\) is. A tall peak indicates a strong belief in that value, while a flatter, broader area indicates more uncertainty.</li> <li><strong>When to Use</strong>: Visualizing uncertainty helps you understand not just where the most likely value lies, but also how uncertain we are about that estimate. A wide, flat posterior suggests we’re less confident about \(\theta\), while a narrow, tall one suggests we’re very confident.</li> </ul> <h5 id="construct-credible-intervals"><strong>Construct Credible Intervals</strong></h5> <ul> <li><strong>Means</strong>: A <strong>credible interval</strong> is the Bayesian equivalent of a confidence interval.</li> <li><strong>Mental Image</strong>: Think of a credible interval as a <strong>fence</strong> that you build around the values of \(\theta\) that you believe are most likely. If you set a 95% credible interval, the fence will surround the areas where there’s a 95% chance that \(\theta\) falls within, based on the data.</li> <li><strong>When to Use</strong>: Credible intervals are great when you want a <strong>range of plausible values</strong>. It’s like saying, “We’re 95% confident that \(\theta\) lies somewhere in this range.” The fence keeps out the outliers and shows you where the most likely values lie.</li> </ul> <h5 id="make-decisions-using-loss-functions"><strong>Make Decisions Using Loss Functions</strong></h5> <ul> <li><strong>Means</strong>: Bayesian decision theory provides a framework for making decisions based on the posterior distribution, incorporating a specific loss function.</li> <li><strong>Mental Image</strong>: Imagine you’re a chess player deciding on the best move based on the board’s current state. The <strong>loss function</strong> is like your strategy guide, telling you which move will minimize your risk or maximize your reward. In the case of Bayesian decision theory, the <strong>posterior distribution</strong> is your chessboard, and the <strong>loss function</strong> guides you to the best move. <ul> <li>For <strong>mean-squared error</strong>, it’s like choosing the move that <strong>optimizes</strong> your position across the entire game (this corresponds to the <strong>posterior mean</strong>).</li> <li>For <strong>0-1 loss</strong>, you’re making a more <strong>focused decision</strong>, like trying to choose the most <strong>immediate</strong> or <strong>most probable</strong> move (this corresponds to the <strong>MAP estimate</strong>).</li> </ul> </li> <li><strong>When to Use</strong>: Loss functions help you make decisions based on the costs or benefits of various choices. If you’re optimizing for accuracy and minimizing error, go with the posterior mean. If you’re trying to maximize the most likely outcome, go with the MAP.</li> </ul> <p>These tools allow for flexible, data-driven decision-making, enabling us to quantify uncertainty and select the best possible estimate of \(\theta\) based on our specific objectives.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>In this post, we explored the concepts of <strong>conjugate priors</strong> and <strong>Bayesian point estimates</strong> through the lens of a coin-flipping example. Conjugate priors simplify Bayesian updating by keeping the posterior distribution within the same family as the prior. Meanwhile, point estimates and posterior analysis allow us to extract actionable insights and communicate uncertainty effectively.</p> <p>By mastering these foundational concepts, we’re now equipped to apply Bayesian inference to a variety of real-world problems. But, if you feel like you’re still struggling to link all the pieces together and form a high-level view of what’s happening, you’re not alone—I felt the same way too. So, next, we’ll dive into Bayesian decision theory and see how all these concepts connect, helping us make predictions for our ML problems at hand. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how conjugate priors streamline Bayesian inference and discover ways to summarize posterior distributions using Bayes point estimates.]]></summary></entry><entry><title type="html">Doubling Trick - A Clever Strategy to Handle Unknown Horizons</title><link href="https://monishver11.github.io/blog/2025/doubling-trick/" rel="alternate" type="text/html" title="Doubling Trick - A Clever Strategy to Handle Unknown Horizons"/><published>2025-01-25T17:38:00+00:00</published><updated>2025-01-25T17:38:00+00:00</updated><id>https://monishver11.github.io/blog/2025/doubling-trick</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/doubling-trick/"><![CDATA[<p>Online learning algorithms often rely on carefully chosen parameters, such as the learning rate (\(\eta\)), to achieve good performance. However, many theoretical analyses assume that the total time horizon (\(T\)) is known in advance. In practical scenarios, this is rarely the case.</p> <p>The <strong>Doubling Trick</strong> is a clever method to overcome this limitation. By dividing time into intervals of exponentially increasing lengths and resetting parameters at the start of each interval, this method ensures that the algorithm performs almost as well as if \(T\) were known from the beginning.</p> <hr/> <h5 id="why-do-we-need-the-doubling-trick"><strong>Why Do We Need the Doubling Trick?</strong></h5> <p>Imagine you’re tasked with predicting the weather for the next \(T\) days using an algorithm. If you knew \(T\), you could optimize your algorithm’s parameters to minimize prediction errors. But what if you don’t know how long you’ll be predicting for? Setting parameters without knowing \(T\) can lead to suboptimal performance.</p> <p>The Doubling Trick resolves this problem by <strong>resetting the algorithm at specific intervals</strong>. These intervals grow exponentially, ensuring that the algorithm adapts dynamically without sacrificing much performance.</p> <h5 id="the-idea-behind-the-doubling-trick"><strong>The Idea Behind the Doubling Trick</strong></h5> <p>The Doubling Trick divides time into periods of exponentially increasing lengths:</p> \[I_k = [2^k, 2^{k+1} - 1],\] <p>where \(k = 0, 1, 2, \dots\) represents the interval index and \(T \geq 2^n-1\) [Why?]. Each interval is twice as long as the previous one. For example:</p> <ul> <li>\(I_0 = [1, 1]\) (length = 1),</li> <li>\(I_1 = [2, 3]\) (length = 2),</li> <li>\(I_2 = [4, 7]\) (length = 4),</li> <li>\(I_3 = [8, 15]\) (length = 8),</li> <li>and so on.</li> </ul> <h5 id="steps-in-the-doubling-trick"><strong>Steps in the Doubling Trick</strong></h5> <ol> <li> <p><strong>Divide Time into Intervals</strong>: Time is divided into intervals of lengths \(1, 2, 4, 8, \dots\). These intervals grow exponentially, covering the entire time horizon \(T\).</p> </li> <li> <p><strong>Choose Parameters for Each Interval</strong>: At the start of each interval, parameters (e.g., the learning rate \(\eta_k\)) are chosen based on the length of the interval. For instance, the learning rate could be set as:</p> \[\eta_k = \sqrt{\frac{8 \log N}{2^k}},\] <p>where \(N\) is the number of options (or experts) the algorithm is learning from.</p> </li> <li> <p><strong>Reset the Algorithm</strong>: The algorithm is reset at the beginning of each interval, treating it as a fresh start.</p> </li> <li> <p><strong>Run the Algorithm Independently in Each Interval</strong>: The algorithm operates independently in each interval, accumulating loss or regret for that specific period.</p> </li> <li> <p><strong>Sum Up Regret Across Intervals</strong>: After \(T\) rounds, the total regret is the sum of regrets from all intervals.</p> </li> </ol> <hr/> <h4 id="regret-bound-with-the-doubling-trick"><strong>Regret Bound with the Doubling Trick</strong></h4> <h5 id="theorem"><strong><mark>Theorem</mark></strong></h5> <p>Assume the same conditions as those in the Exponential Weighted Average Algorithm. For any total time horizon \(T\), the regret achieved by the Doubling Trick satisfies:</p> \[\text{Regret}(T) \leq \frac{\sqrt{2}}{\sqrt{2}-1} \cdot \sqrt{\frac{T}{2} \log N} + \sqrt{\log N / 2}\] <p>This result demonstrates that the Doubling Trick achieves regret bounds that are only slightly worse (by a constant factor) than if the total time horizon \(T\) were known in advance.</p> <h5 id="proof-sketch"><strong><mark>Proof Sketch</mark></strong></h5> <p><strong>Setup</strong></p> <ol> <li> <p><strong>Time Intervals</strong>:<br/> Divide the time horizon \(T\) into intervals \(I_k = [2^k, 2^{k+1} - 1]\), where \(k \in \{0, 1, \dots, n\}\) and \(n = \lfloor \log_2(T + 1) \rfloor\).<br/> Each interval \(I_k\) has length \(2^k\), and the total time horizon \(T\) satisfies \(T \geq 2^n - 1\).</p> </li> <li> <p><strong>Learning Rate</strong>:<br/> Within each interval \(I_k\), the learning rate is chosen as:</p> \[\eta_k = \sqrt{\frac{8 \log N}{2^k}}\] </li> </ol> <p>For any interval \(I_k\), the regret is bounded using:</p> \[L_{I_k} - \min_{i \in [N]} L_{I_k, i} \leq \sqrt{\frac{2^k}{2} \log N}\] <p>Here:</p> <ul> <li>\(L_{I_k}\) is the total loss incurred by the algorithm in interval \(I_k\).</li> <li>\(L_{I_k, i}\) is the total loss incurred by expert \(i\) in interval \(I_k\).</li> </ul> <p>The total loss after \(T\) rounds is the sum of losses over all intervals:</p> \[L_T = \sum_{k=0}^n L_{I_k}\] <p>The total regret can then be expressed as:</p> \[R_T = \sum_{k=0}^n \left( L_{I_k} - \min_{i \in [N]} L_{I_k, i} \right)\] <p>Substituting the bound for \(L_{I_k} - \min_{i \in [N]} L_{I_k, i}\):</p> \[R_T \leq \sum_{k=0}^n \sqrt{\frac{2^k}{2} \log N}\] <p><strong>Simplifying the Geometric Sum</strong></p> <p>The term \(\sum_{k=0}^n \sqrt{\frac{2^k}{2} \log N}\) can be simplified as:</p> \[\sum_{k=0}^n \sqrt{\frac{2^k}{2} \log N} = \sqrt{\frac{\log N}{2}} \cdot \sum_{k=0}^n \sqrt{2^k}\] <p>The sum \(\sum_{k=0}^n \sqrt{2^k}\) forms a geometric series:</p> \[\sum_{k=0}^n 2^{k/2} = \frac{2^{(n+1)/2} - 1}{\sqrt{2} - 1} \tag{How?}\] \[\leq \frac{\sqrt{2} \sqrt{T + 1} - 1}{\sqrt{2} - 1}\] \[\leq \frac{\sqrt{2} (\sqrt{T} + 1) - 1}{\sqrt{2} - 1}\] \[= \frac{\sqrt{2} \sqrt{T}}{\sqrt{2} - 1} + 1\] <p>Plugging this result back into the regret expression, we get:</p> \[\text{Regret}(T) \leq \frac{\sqrt{2}}{\sqrt{2}-1} \cdot \sqrt{\frac{T}{2} \log N} + \sqrt{\log N / 2}\] <p>This establishes the regret bound for the EWA algorithm with the doubling trick.</p> <hr/> <p>There is one question to address before we can confidently say we fully understand this proof, specifically the equation tagged with <strong>‘How?’</strong>. However, the answer is actually quite simple.</p> <p>The sum you are dealing with is of the form:</p> \[S_n = \sum_{k=0}^n 2^{k/2}\] <p>To solve this, we notice that the series is geometric in nature, where each term is of the form \(2^{k/2}\). This suggests a geometric series with the first term \(a = 2^0 = 1\) and the common ratio \(r = 2^{1/2} = \sqrt{2}\).</p> <p>A standard formula for the sum of the first \(n\) terms of a geometric series is:</p> \[S_n = \frac{a(r^{n+1} - 1)}{r - 1}\] <p>Substituting \(a = 1\) and \(r = \sqrt{2}\), we get:</p> \[S_n = \frac{(\sqrt{2})^{n+1} - 1}{\sqrt{2} - 1}\] <p>Now, simplify \((\sqrt{2})^{n+1}\) to \(2^{(n+1)/2}\):</p> \[S_n = \frac{2^{(n+1)/2} - 1}{\sqrt{2} - 1}\] <p>Thus, we arrive at the desired result:</p> \[\sum_{k=0}^n 2^{k/2} = \frac{2^{(n+1)/2} - 1}{\sqrt{2} - 1}\] <p>Now we’re all set. Yes. Next, we’ll tackle a few more questions and wrap up this blog.</p> <p><strong>Note:</strong> The \(O(\sqrt{T})\) dependency on \(T\) presented in this bound cannot be improved for general loss functions. This is because the case we discussed here is the Exponentiated Weighted Average (EWA), where we assumed a convex loss with respect to its first parameter. The Doubling Trick is simply an additional technique applied on top of this, and that’s all.</p> <hr/> <h5 id="why-does-the-doubling-trick-work"><strong>Why Does the Doubling Trick Work?</strong></h5> <ol> <li> <p><strong>Exponential Growth Covers Any Horizon</strong>: By doubling the length of intervals, the Doubling Trick ensures that any unknown time horizon \(T\) will fall within the union of the intervals.</p> </li> <li> <p><strong>Dynamic Parameter Adjustment</strong>: Since parameters are reset at the start of each interval, the algorithm adapts to the growing time horizon. This avoids overcommitting to a parameter setting based on an incorrect estimate of \(T\).</p> </li> <li> <p><strong>Minimal Regret Accumulation</strong>: The regret within each interval is bounded, and the total regret is the sum of these bounds. Using properties of geometric sums, the total regret remains manageable.</p> </li> </ol> <h5 id="advantages-of-the-doubling-trick"><strong>Advantages of the Doubling Trick</strong></h5> <ol> <li> <p><strong>No Prior Knowledge of \(T\)</strong>: The Doubling Trick eliminates the need to know \(T\) in advance, making it practical for real-world applications.</p> </li> <li> <p><strong>Near-Optimal Performance</strong>: The regret bound is close to the best possible bound achievable with full knowledge of \(T\).</p> </li> <li> <p><strong>Simplicity</strong>: The method is straightforward to implement and applies to a wide range of online learning algorithms.</p> </li> </ol> <h5 id="applications-of-the-doubling-trick"><strong>Applications of the Doubling Trick</strong></h5> <p>The Doubling Trick, while commonly used in regret minimization, also finds applications in scenarios where the time horizon is uncertain or unknown. Here are a few key areas where this trick proves useful;</p> <p>[I have this question but haven’t come up with a convincing answer or a way to phrase it clearly yet. I’ll update once I find it.]</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>The Doubling Trick is a powerful and versatile technique that addresses the challenge of unknown time horizons in online learning. By dividing time into exponentially growing intervals and resetting parameters dynamically, it achieves near-optimal regret bounds with minimal computational overhead. This method underscores the elegance of combining mathematical rigor with practical adaptability.</p> <p>Next, we’ll define a general setting for online learning and briefly touch on the different setups within it. Stay tuned for more!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li></li> </ul>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.]]></summary></entry><entry><title type="html">Exponential Weighted Average Algorithm</title><link href="https://monishver11.github.io/blog/2025/EWA/" rel="alternate" type="text/html" title="Exponential Weighted Average Algorithm"/><published>2025-01-25T05:28:00+00:00</published><updated>2025-01-25T05:28:00+00:00</updated><id>https://monishver11.github.io/blog/2025/EWA</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/EWA/"><![CDATA[<p>The <strong>Exponential Weighted Average Algorithm (EWAA)</strong> is an online learning algorithm that provides elegant guarantees for minimizing regret in adversarial settings. It extends the principles of the Weighted Majority Algorithm by incorporating exponential weight updates, making it particularly effective for handling convex loss functions.</p> <h4 id="how-the-exponential-weighted-average-algorithm-works"><strong>How the Exponential Weighted Average Algorithm Works</strong></h4> <p>At its core, the EWAA maintains and updates weights for a set of experts, similar to the Weighted Majority Algorithm. However, it uses an <strong>exponential weighting scheme</strong> to achieve better bounds on regret, especially for convex losses.</p> <h5 id="steps-of-the-algorithm"><strong>Steps of the Algorithm</strong></h5> <p><strong>Initialization</strong> Set initial weights for all \(N\) experts:</p> \[w_{1,i} = 1, \quad \forall i \in \{1, \dots, N\}\] <p><strong>Prediction at Round \(t\)</strong></p> <ul> <li>Observe the predictions \(\hat{y}_{t,i}\) from all experts.</li> <li>Compute the aggregate prediction as a weighted average:</li> </ul> \[\hat{y}_t = \frac{\sum_{i=1}^N w_{t,i} \cdot \hat{y}_{t,i}}{\sum_{i=1}^N w_{t,i}}\] <p><strong>Update Weights</strong></p> <ul> <li>After receiving the true outcome \(y_t\), update the weights for the next round:</li> </ul> \[w_{t+1,i} = w_{t,i} \cdot e^{-\eta L(\hat{y}_{t,i}, y_t)}\] <ul> <li>The weight update can also be expressed directly using the <strong>cumulative loss</strong> \(L_t(i)\) of expert \(i\) after \(t\) rounds:</li> </ul> \[w_{t+1,i} = e^{-\eta L_t(i)}\] <p><strong><mark>Key Points to Highlight:</mark></strong></p> <p><strong>Simplification of Weight Updates</strong> While the equation appears to involve \(w_{t,i}\) in the update, the final weight \(w_{t+1,i}\) depends only on the cumulative loss \(L_t(i)\) and not on previous weights.<br/> This is why it shows \(w_{t+1,i} = e^{-\eta L_t(i)}\), as the weights can be normalized afterward.</p> <p><strong>Interpretation of \(e^{-x}\)</strong> The term \(e^{-\eta x}\) decreases exponentially as \(x\) (loss) increases.<br/> This ensures poorly performing experts are rapidly down-weighted. A plot of \(e^{-x}\) can visually illustrate this decay.</p> <p>If you’re still unclear about the final weight update rule, keep reading — the explanation below should clarify things.</p> <hr/> <p>We start from the original weight update equation and simplify it step by step to express it in terms of the <strong>cumulative loss</strong>.</p> <p><strong>1. Original Weight Update</strong> The weight update at time \(t+1\) is defined as:</p> \[w_{t+1,i} = w_{t,i} \cdot e^{-\eta L(\hat{y}_{t,i}, y_t)}\] <p><strong>2. Recursive Application</strong> Expanding the recursion over all previous rounds \(1, \dots, t\):</p> \[w_{t+1,i} = w_{1,i} \cdot e^{-\eta L(\hat{y}_{1,i}, y_1)} \cdot e^{-\eta L(\hat{y}_{2,i}, y_2)} \cdots e^{-\eta L(\hat{y}_{t,i}, y_t)}\] <p><strong>3. Simplify the Product</strong> Using the property of exponents \(a^x \cdot a^y = a^{x+y}\):</p> \[w_{t+1,i} = w_{1,i} \cdot e^{-\eta \sum_{s=1}^t L(\hat{y}_{s,i}, y_s)}\] <p><strong>4. Initial Weights</strong> Since the initial weights are set to \(w_{1,i} = 1\) for all experts, this simplifies to:</p> \[w_{t+1,i} = e^{-\eta \sum_{s=1}^t L(\hat{y}_{s,i}, y_s)}\] <p><strong>5. Cumulative Loss Definition</strong> Define the <strong>cumulative loss</strong> of expert \(i\) after \(t\) rounds as:</p> \[L_t(i) = \sum_{s=1}^t L(\hat{y}_{s,i}, y_s)\] <p><strong>6. Final Simplified Form</strong> Substituting \(L_t(i)\) into the equation gives the simplified weight update:</p> \[w_{t+1,i} = e^{-\eta L_t(i)}\] <p><strong><mark>Key Insight:</mark></strong></p> <ul> <li>The weight at time \(t+1\) depends only on the <strong>cumulative loss</strong> \(L_t(i)\), not on the individual losses at previous rounds or the intermediate weights.</li> <li>This simplification is possible because the update rule is <strong>multiplicative</strong>, and the cumulative loss naturally aggregates all penalties from previous rounds.</li> </ul> <hr/> <h5 id="theorem-regret-bound-for-ewaa"><strong>Theorem: Regret Bound for EWAA</strong></h5> <p>Let \(L(y, y')\) be a convex loss function in its first argument, taking values in \([0, 1]\). For any \(\eta &gt; 0\) and any sequence of labels \(y_1, \dots, y_T \in \mathcal{Y}\), the regret of the Exponential Weighted Average Algorithm satisfies:</p> \[R_T \leq \frac{\log N}{\eta} + \frac{\eta T}{8}\] <p>Here, the regret is defined as the difference between the total loss of the algorithm and the loss of the best expert:</p> \[R_T = \sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i \in [N]} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p><strong>Optimized Learning Rate</strong></p> <p>By choosing \(\eta = \sqrt{\frac{8 \log N}{T}}\), we minimize the regret bound, resulting in: \(R_T \leq \sqrt{\frac{T}{2} \log N}\)</p> <p>This demonstrates that the regret grows logarithmically with the number of experts \(N\) and sublinearly with the number of time steps \(T\), indicating the efficiency of EWAA.</p> <p><strong>Convex Loss Function in Its First Argument</strong></p> <p>Before we dive deeper, let’s clarify what we mean by a <strong>convex loss function in its first argument</strong>. In this context, the phrase refers to the loss function \(L(y, y')\) being convex with respect to its first argument, \(y\) (which could be the true label or the model output).</p> <p>To break it down:</p> <ul> <li>The loss function \(L(y, y')\) measures the difference between the true label \(y\) and the predicted label \(y'\).</li> <li><strong>Convexity in the first argument</strong> means that for any fixed value of \(y'\), the function \(L(y, y')\) is convex in \(y\). This implies that as you vary the predicted label \(y'\), the loss function increases in a “bowl-shaped” manner when considering its first argument \(y\). This property is important for optimization because convex functions are easier to minimize, ensuring that algorithms like EWAA can efficiently adjust to minimize cumulative loss over time.</li> </ul> <p>In mathematical terms, for any fixed \(y'\), the function \(L(y, y')\) satisfies the condition of convexity:</p> \[L(\lambda y_1 + (1-\lambda) y_2, y') \leq \lambda L(y_1, y') + (1-\lambda) L(y_2, y')\] <p>for any \(y_1, y_2 \in \mathcal{Y}\) and \(\lambda \in [0, 1]\).</p> <hr/> <h4 id="proof-of-the-regret-bound"><strong>Proof of the Regret Bound</strong></h4> <p>Define the <strong>potential function</strong>:</p> \[\Phi_t = \log \left( \sum_{i=1}^N w_{t,i} \right)\] <p>The goal is to derive an upper bound and a lower bound for \(\Phi_t\), and then combine them to establish the regret bound.</p> <h5 id="upper-bound"><strong>Upper Bound</strong></h5> <p><strong>Step 1: Change in Potential Function</strong></p> <p>From the weight update rule:</p> \[w_{t+1,i} = w_{t,i} e^{-\eta L(\hat{y}_{t,i}, y_t)},\] <p>we can write the change in the potential function as:</p> \[\Phi_{t+1} - \Phi_t = \log \left( \frac{\sum_{i=1}^N w_{t,i} e^{-\eta L(\hat{y}_{t,i}, y_t)}}{\sum_{i=1}^N w_{t,i}} \right) = \log \left( \mathbb{E}_{p_t}[e^{-\eta X}] \right),\] <p>where \(X = -L(\hat{y}_{t,i}, y_t) \in [-1, 0]\) and \(p_t(i) = \frac{w_{t,i}}{\sum_{j=1}^N w_{t,j}}\) is the probability distribution over experts.</p> <p><strong>Step 2: Centering the Random Variable</strong></p> <p>Define \(X = -L(\hat{y}_{t,i}, y_t)\), where \(X \in [-1, 0]\). The expectation can be centered around its mean:</p> \[\mathbb{E}_{p_t} \left[ e^{-\eta L(\hat{y}_{t,i}, y_t)} \right] = \mathbb{E}_{p_t} \left[ e^{\eta (X - \mathbb{E}_{p_t}[X])} \right] e^{\eta \mathbb{E}_{p_t}[X]}\] <p>Substituting this back, we have:</p> \[\Phi_{t+1} - \Phi_t = \log \mathbb{E}_{p_t} \left[ e^{\eta (X - \mathbb{E}_{p_t}[X])} \right] + \eta \mathbb{E}_{p_t}[X]\] <p><strong>Step 3: Applying Hoeffding’s Lemma</strong></p> <p>By Hoeffding’s Lemma, for any centered random variable \(X - \mathbb{E}_{p_t}[X]\) bounded in \([-1, 0]\):</p> \[\log \mathbb{E}_{p_t} \left[ e^{\eta (X - \mathbb{E}_{p_t}[X])} \right] \leq \frac{\eta^2}{8}\] <p>Substituting this bound:</p> \[\Phi_{t+1} - \Phi_t \leq \eta \mathbb{E}_{p_t}[X] + \frac{\eta^2}{8}\] <p><strong>Step 4: Substituting \(X = -L(\hat{y}_{t,i}, y_t)\)</strong></p> <p>Recall that \(X = -L(\hat{y}_{t,i}, y_t)\), so \(\mathbb{E}_{p_t}[X] = -\mathbb{E}_{p_t}[L(\hat{y}_{t,i}, y_t)]\). Substituting:</p> \[\Phi_{t+1} - \Phi_t \leq -\eta \mathbb{E}_{p_t}[L(\hat{y}_{t,i}, y_t)] + \frac{\eta^2}{8}\] <p><strong>Step 5: Applying Convexity of \(L\)</strong></p> <p>By the convexity of \(L\) in its first argument:</p> \[\mathbb{E}_{p_t}[L(\hat{y}_{t,i}, y_t)] \geq L(\mathbb{E}_{p_t}[\hat{y}_{t,i}], y_t) = L(\hat{y}_t, y_t),\] <p>where \(\hat{y}_t = \mathbb{E}_{p_t}[\hat{y}_{t,i}]\) is the prediction of the algorithm. Using this:</p> \[\Phi_{t+1} - \Phi_t \leq -\eta L(\hat{y}_t, y_t) + \frac{\eta^2}{8}\] <p>[How? - Unclear]</p> <p><strong>Step 6: Summing Over \(t = 1, \dots, T\)</strong></p> <p>Summing this inequality over all time steps:</p> \[\sum_{t=1}^T (\Phi_{t+1} - \Phi_t) \leq -\eta \sum_{t=1}^T L(\hat{y}_t, y_t) + \frac{\eta^2 T}{8}\] <p>The left-hand side telescopes:</p> \[\Phi_{T+1} - \Phi_1 \leq -\eta \sum_{t=1}^T L(\hat{y}_t, y_t) + \frac{\eta^2 T}{8}\] <p><strong>Final Upper Bound</strong></p> <p>This establishes the <strong>upper bound</strong> for the change in potential:</p> \[\Phi_{T+1} - \Phi_1 \leq -\eta \sum_{t=1}^T L(\hat{y}_t, y_t) + \frac{\eta^2 T}{8}\] <hr/> <h5 id="lower-bound"><strong>Lower Bound</strong></h5> <p><strong>Step 1: Potential Function at \(T+1\)</strong></p> <p>From the definition of the potential function:</p> \[\Phi_{T+1} = \log \left( \sum_{i=1}^N w_{T+1,i} \right),\] <p>where \(w_{T+1,i}\) is the weight of expert \(i\) at time \(T+1\).</p> <p><strong>Step 2: Weight Update Rule</strong></p> <p>Using the weight update rule:</p> \[w_{T+1,i} = e^{-\eta L_{T,i}},\] <p>where \(L_{T,i} = \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\) is the <strong>cumulative loss</strong> of expert \(i\) up to time \(T\).</p> <p>Substituting into the potential function:</p> \[\Phi_{T+1} = \log \left( \sum_{i=1}^N e^{-\eta L_{T,i}} \right).\] <p><strong>Step 3: Lower Bound for Log-Sum-Exp</strong></p> <p>Applying the <strong>lower bound for the log-sum-exp function</strong>:</p> \[\log \left( \sum_{i=1}^N e^{-\eta L_{T,i}} \right) \geq \max_{i \in [N]} \left( -\eta L_{T,i} \right) + \log N.\] <p>Rewriting:</p> \[\Phi_{T+1} \geq -\eta \min_{i \in [N]} L_{T,i} + \log N,\] <p>where \(\min_{i \in [N]} L_{T,i}\) is the smallest cumulative loss among all experts.</p> <p><strong>Note:</strong> If this isn’t clear, refer to the <a href="https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/">log-sum-exp</a> trick—it’s essentially the same approach we’ve used here.</p> <p><strong>Step 4: Initial Potential Function</strong></p> <p>From the initial condition, the potential function at \(t = 1\) is:</p> \[\Phi_1 = \log N\] <p><strong>Step 5: Combining Results</strong></p> <p>Combining the expressions for \(\Phi_{T+1}\) and \(\Phi_1\), we obtain:</p> \[\Phi_{T+1} - \Phi_1 \geq -\eta \min_{i \in [N]} L_{T,i}\] <p><strong>Final Lower Bound</strong></p> <p>Thus, the lower bound for the change in potential is:</p> \[\Phi_{T+1} - \Phi_1 \geq -\eta \min_{i \in [N]} L_{T,i}\] <hr/> <h5 id="combining-bounds"><strong>Combining Bounds</strong></h5> <p>From the upper and lower bounds:</p> \[-\eta \min_{i \in [N]} L_{T,i} \leq -\eta \sum_{t=1}^T L(\hat{y}_t, y_t) + \frac{\eta^2 T}{8}\] <p>Rearranging terms:</p> \[\sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i \in [N]} L_{T,i} \leq \frac{\log N}{\eta} + \frac{\eta T}{8}\] <p>Thus, the regret satisfies:</p> \[R_T \leq \frac{\log N}{\eta} + \frac{\eta T}{8}\] <p><strong>Note:</strong> <strong>\(\min_{i \in [N]} L_{T,i}\) and Its Meaning</strong></p> <p>The term \(\min_{i \in [N]} L_{T,i}\) refers to the <strong>minimum cumulative loss</strong> among all the experts (or models) after \(T\) rounds. Specifically:</p> <ul> <li>\(L_{T,i}\) is the cumulative loss of expert (or model) \(i\) after \(T\) rounds.</li> <li>\(\min_{i \in [N]} L_{T,i}\) represents the smallest cumulative loss incurred by any expert over the \(T\) rounds.</li> </ul> <p>This is the term we need in our calculation to compute the regret, right!</p> <p><strong><mark>Key Takeaways:</mark></strong></p> <ul> <li>The <strong>regret bound</strong> of the EWAA is a function of both the learning rate \(\eta\) and the time horizon \(T\).</li> <li>By choosing \(\eta = \sqrt{\frac{8 \log N}{T}}\), the regret grows sublinearly with \(T\) and logarithmically with \(N\), ensuring the algorithm’s efficiency.</li> </ul> <hr/> <h5 id="advantages-and-disadvantages-of-ewaa"><strong>Advantages and Disadvantages of EWAA</strong></h5> <p><strong>Advantages</strong></p> <ol> <li><strong>Strong Theoretical Guarantees</strong>: <ul> <li>The regret bound for the <strong>(EWAA)</strong> is logarithmic in the number of experts \(N\) and sublinear in the number of time steps \(T\). This means that even as the number of experts or rounds increases, the regret grows slowly, offering a strong theoretical guarantee on performance. [Why? - Think]</li> </ul> </li> <li><strong>Applicability to Convex Losses</strong>: <ul> <li>Unlike algorithms specifically tailored for binary losses, EWAA can handle <strong>convex loss functions</strong>. This makes it a more versatile algorithm since convex losses are more general and can cover a wider range of applications beyond binary classification.</li> </ul> </li> <li><strong>Weight Adaptivity</strong>: <ul> <li>The <strong>exponential weight updates</strong> in EWAA ensure that poor-performing experts are penalized efficiently over time. This adaptive mechanism allows the algorithm to focus more on better-performing experts, while discouraging the influence of worse-performing ones, improving its overall performance.</li> </ul> </li> </ol> <p><strong>Disadvantages</strong></p> <ul> <li><strong>Requires Knowledge of Horizon \(T\)</strong>: <ul> <li>A disadvantage of the EWAA is that it requires knowledge of the <strong>horizon</strong> \(T\), which refers to the total number of rounds or time steps the algorithm will run. Specifically, the learning rate \(\eta\) in the regret bound often depends on \(T\) (for example, \(\eta\) might be chosen as \(\frac{1}{\sqrt{T}}\)).</li> <li>This means that to optimize the regret bound, you need to have some insight or knowledge about the total number of rounds \(T\) in advance. This can be a significant limitation in practical applications, where \(T\) is not always known or fixed in advance. In real-world scenarios, you might need to adapt to changing environments without prior knowledge of how long the process will last, making it challenging to choose the best parameters like \(\eta\).</li> </ul> </li> </ul> <hr/> <p>Before we wrap up, let’s take a step back and get a clearer picture of the whole thing:</p> <h5 id="why-convexity-helps-in-ewaa"><strong><mark>Why Convexity Helps in EWAA</mark></strong></h5> <ol> <li><strong>Optimization and Regret Minimization</strong>: <ul> <li>The convexity of the loss function with respect to the predicted labels \(y\) (the first argument) ensures that the algorithm can effectively minimize cumulative loss. Since convex functions have a single global minimum, optimization is straightforward and guarantees convergence toward a solution with low regret.</li> </ul> </li> <li><strong>Exponential Weight Updates</strong>: <ul> <li>In EWAA, the weight updates are based on the <strong>exponential</strong> of the loss, and convexity allows these updates to be well-behaved. Specifically, since the loss function increases in a convex manner as the difference between \(y\) and \(y'\) increases, the exponential weight updates ensure that poorly performing experts are penalized more heavily. This ensures that the algorithm focuses on the most promising experts while reducing the influence of poor ones.</li> </ul> </li> <li><strong>Efficient Learning</strong>: <ul> <li>Convexity ensures that the loss function grows in a predictable manner, which helps in adjusting the weights efficiently across time steps. This is important for the overall performance of the algorithm, as it leads to effective adaptation and faster convergence to a good solution.</li> </ul> </li> <li><strong>Theoretical Guarantees</strong>: <ul> <li>The convexity property allows the <strong>theoretical regret bounds</strong> for EWAA to be derived more easily. Since convex functions have well-defined gradients and curvature properties, we can make rigorous claims about the regret bound, such as the logarithmic growth in the number of experts \(N\) and sublinear growth in the number of time steps \(T\). Without convexity, such guarantees would not be as strong or as easily established.</li> </ul> </li> </ol> <p>And, If you’re unsure about the difference between linear and sublinear growth, here’s a quick clarification:</p> <ul> <li><strong>Linear growth</strong> means the value grows at a constant rate (proportional to the parameter). Mathematically: \(f(x) = O(x)\).</li> <li><strong>Sublinear growth</strong> means the value grows at a slower rate than the parameter, such that the output doesn’t keep up at the same pace. Mathematically: \(f(x) = O(x^a)\) where \(0 &lt; a &lt; 1\), or \(f(x) = O(\log(x))\).</li> </ul> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>The Exponential Weighted Average Algorithm provides strong guarantees for regret minimization with convex loss functions. Its use of exponential weight updates makes it both adaptable and theoretically elegant, though its dependence on the time horizon \(T\) can present practical challenges.</p> <p>In the next post, we’ll dive into the doubling trick for selecting \(\eta\) and how it improves regret bounds. Stay tuned—see you in the next one!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/21.pdf">Online Learning: Halving Algorithm and Exponential Weights(Notes)</a></li> </ul>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.]]></summary></entry><entry><title type="html">Bayesian Machine Learning - Mathematical Foundations</title><link href="https://monishver11.github.io/blog/2025/Bayesian-ML/" rel="alternate" type="text/html" title="Bayesian Machine Learning - Mathematical Foundations"/><published>2025-01-24T14:56:00+00:00</published><updated>2025-01-24T14:56:00+00:00</updated><id>https://monishver11.github.io/blog/2025/Bayesian-ML</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/Bayesian-ML/"><![CDATA[<p>When working with machine learning models, it’s crucial to understand the underlying statistical principles that drive our methods. Whether you’re a frequentist or a Bayesian, the starting point often involves a <strong>parametric family of densities</strong>. This concept forms the foundation for inference and is used to model the data we observe.</p> <h4 id="parametric-family-of-densities"><strong>Parametric Family of Densities</strong></h4> <p>A <strong>parametric family of densities</strong> is defined as a set</p> \[\{p(y \mid \theta) : \theta \in \Theta\},\] <p>where \(p(y \mid \theta)\) is a density function over some sample space \(Y\), and \(\theta\) represents a parameter in a finite-dimensional parameter space \(\Theta\).</p> <p>In simpler terms, this is a collection of probability distributions, each associated with a specific value of the parameter \(\theta\). When we refer to “density,” it’s worth noting that this can be replaced with “mass function” if we’re dealing with discrete random variables. Similarly, integrals can be replaced with summations in such cases.</p> <p>This framework is the common starting point for both <strong>classical statistics</strong> and <strong>Bayesian statistics</strong>, as it provides a structured way to think about modeling the data.</p> <h5 id="frequentist-or-classical-statistics"><strong>Frequentist or “Classical” Statistics</strong></h5> <p>In frequentist statistics, we also work with the parametric family of densities \(\{p(y \mid \theta) : \theta \in \Theta\}\), assuming that the true distribution \(p(y \mid \theta)\) governs the world we observe. This means there exists some unknown parameter \(\theta \in \Theta\) that determines the true nature of the data.</p> <p>If we had direct access to this true parameter \(\theta\), we wouldn’t need statistics at all! However, in practice, we only have a dataset, denoted as</p> \[D = \{y_1, y_2, \dots, y_n\},\] <p>where each \(y_i\) is sampled independently from the true distribution \(p(y \mid \theta)\).</p> <p>This brings us to the heart of statistics: <strong>how do we make inferences about the unknown parameter \(\theta\) using only the observed data \(D\)?</strong></p> <h5 id="point-estimation"><strong>Point Estimation</strong></h5> <p>One fundamental problem in statistics is <strong>point estimation</strong>, where the goal is to estimate the true value of the parameter \(\theta\) as accurately as possible.</p> <p>To do this, we use a <strong>statistic</strong>, denoted as \(s = s(D)\), which is simply a function of the observed data. When this statistic is designed to estimate \(\theta\), we call it a <strong>point estimator</strong>, represented as \(\hat{\theta} = \hat{\theta}(D)\).</p> <p>A <strong>good point estimator</strong> is one that is both:</p> <ul> <li><strong>Consistent</strong>: As the sample size \(n\) grows larger, the estimator \(\hat{\theta}_n\) converges to the true parameter \(\theta\).</li> <li><strong>Efficient</strong>: The estimator \(\hat{\theta}_n\) extracts the maximum amount of information about \(\theta\) from the data, achieving the best possible accuracy for a given sample size.</li> </ul> <p>One of the most popular methods for point estimation is the <strong>maximum likelihood estimator (MLE)</strong>. While we’ve already covered it, let’s revisit it through a concrete example to reinforce our understanding.</p> <h5 id="example-coin-flipping-and-maximum-likelihood-estimation"><strong>Example: Coin Flipping and Maximum Likelihood Estimation</strong></h5> <p>Let’s consider the simple yet illustrative problem of estimating the probability of a coin landing on heads.</p> <p>Here, the parametric family of mass functions is given by:</p> \[p(\text{Heads} \mid \theta) = \theta, \quad \text{where } \theta \in \Theta = (0, 1).\] <p>The parameter \(\theta\) represents the probability of the coin landing on heads. Our goal is to estimate this parameter based on observed data.</p> <p>If this seems a bit confusing, seeing \(\theta\) in two places, let’s clarify it first.</p> <p>Imagine you have a coin, and you’re curious about how “fair” it is. A perfectly fair coin has a 50% chance of landing heads or tails, but your coin might be biased. To capture this bias mathematically, you introduce a parameter, \(\theta\), which represents the probability of the coin landing on heads.</p> <p>We write this as:</p> \[p(\text{Heads} \mid \theta) = \theta\] <p>Let’s break this down with intuition:</p> <ol> <li><strong>What does \(\theta\) mean?</strong><br/> \(\theta\) is the coin’s “personality.” For example: <ul> <li>If \(\theta = 0.8\), it means the coin “loves” heads, and there’s an 80% chance it will land heads on any given flip.</li> <li>If \(\theta = 0.3\), the coin is biased toward tails, and there’s only a 30% chance of heads.</li> </ul> </li> <li> <p><strong>What does \(p(\text{Heads} \mid \theta) = \theta\) mean?</strong><br/> This equation ties the probability of getting heads to the parameter \(\theta\). It’s like saying: “The parameter \(\theta\) <em>is</em> the probability of heads.” For every coin flip, \(\theta\) directly determines the likelihood of heads.</p> </li> <li><strong>Why is this useful?</strong><br/> It simplifies modeling. Instead of treating each flip as random and unconnected, we assume there’s a fixed bias, \(\theta\), that governs the coin’s behavior. Once we observe enough flips (data), we can estimate \(\theta\) and predict future outcomes.</li> </ol> <p><strong>A relatable example might be…</strong></p> <p>Imagine a factory making coins with varying biases. Each coin is labeled with its bias, \(\theta\), ranging between 0 (always tails) and 1 (always heads). If you’re handed a coin without a label, your job is to figure out its bias by flipping it multiple times and observing the outcomes.</p> <p>This is the setup for the equation \(p(\text{Heads} \mid \theta) = \theta\). It tells us the coin’s behavior is entirely controlled by its bias, \(\theta\), and allows us to estimate it from observed data. <strong>Data and Likelihood Function</strong></p> <p>I hope that clears things up, and we’re good to proceed!</p> <hr/> <p>Suppose we observe the outcomes of \(n\) independent coin flips, represented as:</p> \[D = (\text{H, H, T, T, T, T, T, H, ... , T}),\] <p>where \(n_h\) is the number of heads, and \(n_t\) is the number of tails. Since each flip is independent, the likelihood function for the observed data is:</p> \[L_D(\theta) = p(D \mid \theta) = \theta^{n_h} (1 - \theta)^{n_t}.\] <p><strong>Log-Likelihood and Optimization</strong></p> <p>Rather than working directly with the likelihood function, which involves products and can become cumbersome, we typically maximize the <strong>log-likelihood function</strong> for computational simplicity. The log-likelihood is:</p> \[\log L_D(\theta) = n_h \log \theta + n_t \log (1 - \theta).\] <p>The <strong>maximum likelihood estimate (MLE)</strong> of \(\theta\) is the value that maximizes this log-likelihood:</p> \[\hat{\theta}_{\text{MLE}} = \underset{\theta \in \Theta}{\text{argmax}} \, \log L_D(\theta).\] <p><strong>Derivation of the MLE</strong></p> <p>To find the MLE, we compute the derivative of the log-likelihood with respect to \(\theta\), set it to zero, and solve for \(\theta\):</p> \[\frac{\partial}{\partial \theta} \big[ n_h \log \theta + n_t \log (1 - \theta) \big] = \frac{n_h}{\theta} - \frac{n_t}{1 - \theta}.\] <p>Setting this derivative to zero:</p> \[\frac{n_h}{\theta} = \frac{n_t}{1 - \theta}.\] <p>Simplifying this equation gives:</p> \[\theta = \frac{n_h}{n_h + n_t}.\] <p>Thus, the MLE for \(\theta\) is:</p> \[\hat{\theta}_{\text{MLE}} = \frac{n_h}{n_h + n_t}.\] <p><strong>Intuition Behind the MLE</strong></p> <p>The result makes intuitive sense: the MLE simply calculates the proportion of heads observed in the data. It uses the empirical frequency as the best estimate of the true probability of heads, given the observed outcomes.</p> <hr/> <p>While frequentist approaches like MLE provide a single “best” estimate for \(\theta\), Bayesian methods take a different perspective. Instead of finding a point estimate, Bayesian inference quantifies uncertainty about \(\theta\) using probability distributions. This leads to the concepts of <strong>prior distributions</strong> and <strong>posterior inference</strong>, which is what we’re going to explore next.</p> <h4 id="bayesian-statistics-an-introduction"><strong>Bayesian Statistics: An Introduction</strong></h4> <p>In the frequentist framework, the goal is to estimate the true parameter \(\theta\) using the observed data. However, <strong>Bayesian statistics</strong> takes a fundamentally different approach by introducing an important concept: the <strong>prior distribution</strong>. This addition allows us to explicitly incorporate prior beliefs about the parameter into our analysis and update them rationally as we observe new data.</p> <h5 id="the-prior-distribution-reflecting-prior-beliefs"><strong>The Prior Distribution: Reflecting Prior Beliefs</strong></h5> <p>A <strong>prior distribution</strong>, denoted as \(p(\theta)\), is a probability distribution over the parameter space \(\Theta\). It represents our belief about the value of \(\theta\) <strong>before</strong> observing any data. For instance, if we believe that \(\theta\) is more likely to lie in a specific range, we can encode this belief directly into the prior.</p> <h5 id="a-bayesian-model-combining-prior-and-data"><strong>A Bayesian Model: Combining Prior and Data</strong></h5> <p>A <strong>[parametric] Bayesian model</strong> is constructed from two key components:</p> <ol> <li>A <strong>parametric family of densities</strong> \(\{p(D \mid \theta) : \theta \in \Theta\}\) that models the likelihood of the observed data \(D\) given \(\theta\).</li> <li>A <strong>prior distribution</strong> \(p(\theta)\) on the parameter space \(\Theta\).</li> </ol> <p>These two components combine to form a <strong>joint density</strong> over \(\theta\) and \(D\):</p> \[p(D, \theta) = p(D \mid \theta) p(\theta).\] <p>This joint density encapsulates both the likelihood of the data and our prior beliefs about the parameter.</p> <h5 id="posterior-distribution-updating-beliefs"><strong>Posterior Distribution: Updating Beliefs</strong></h5> <p>The real power of Bayesian statistics lies in the ability to <strong>update prior beliefs</strong> after observing data. This is achieved through the <strong>posterior distribution</strong>, denoted as \(p(\theta \mid D)\).</p> <ul> <li>The <strong>prior distribution</strong> \(p(\theta)\) captures our initial beliefs about \(\theta\).</li> <li>The <strong>posterior distribution</strong> \(p(\theta \mid D)\) reflects our updated beliefs after observing the data \(D\).</li> </ul> <p>By applying <strong>Bayes’ rule</strong>, we can express the posterior distribution as:</p> \[p(\theta \mid D) = \frac{p(D \mid \theta) p(\theta)}{p(D)},\] <p>where:</p> <ul> <li>\(p(D \mid \theta)\) is the <strong>likelihood</strong>, capturing how well \(\theta\) explains the observed data.</li> <li>\(p(\theta)\) is the <strong>prior</strong>, encoding our initial beliefs about \(\theta\).</li> <li>\(p(D)\) is a normalizing constant, ensuring the posterior integrates to 1.</li> </ul> <h5 id="simplifying-the-posterior"><strong>Simplifying the Posterior</strong></h5> <p>When analyzing the posterior distribution, we often focus on terms that depend on \(\theta\). Dropping constant factors that are independent of \(\theta\), we write:</p> \[p(\theta \mid D) \propto p(D \mid \theta) \cdot p(\theta),\] <p>where \(\propto\) denotes proportionality.</p> <p>In practice, this allows us to analyze and work with the posterior distribution more efficiently. For instance, the <strong>maximum a posteriori (MAP) estimate</strong> of \(\theta\) is given by:</p> \[\hat{\theta}_{\text{MAP}} = \underset{\theta \in \Theta}{\text{argmax}} \, p(\theta \mid D).\] <p><strong>A Way to Think About It:</strong></p> <p>A helpful way to think of Bayesian methods is to imagine you’re trying to predict the outcome of an event, but you have some prior knowledge (or beliefs) about it. For example, let’s say you’re predicting whether a student will pass an exam, and you have prior knowledge that most students in the class have been doing well. This prior belief can be represented as a probability distribution, which reflects how confident you are about the parameter (like the likelihood of passing).</p> <p>As you collect more data (say, the student’s past performance or study hours), Bayesian methods update your belief (the prior) to form a new, updated belief, called the <strong>posterior distribution</strong>. The more data you have, the more confident the posterior becomes about the true outcome.</p> <p>So, in essence:</p> <ul> <li><strong>Prior distribution</strong> = What you believe before observing data (your initial guess).</li> <li><strong>Likelihood</strong> = How the observed data might be related to your belief.</li> <li><strong>Posterior distribution</strong> = Your updated belief after observing the data.</li> </ul> <p>In Bayesian inference, the goal is to calculate the posterior, which balances the prior belief with the observed data.</p> <hr/> <h4 id="example-bayesian-coin-flipping"><strong>Example: Bayesian Coin Flipping</strong></h4> <p>Let’s revisit the coin-flipping example, but this time from a Bayesian perspective. We start with the parametric family of mass functions:</p> \[p(\text{Heads} \mid \theta) = \theta, \quad \text{where } \theta \in \Theta = (0, 1).\] <p>To complete our Bayesian model, we also need to specify a <strong>prior distribution</strong> over \(\theta\). One common choice is the <strong>Beta distribution</strong>, which is particularly convenient for this problem.</p> <h5 id="beta-prior-distribution"><strong>Beta Prior Distribution</strong></h5> <p>The Beta distribution, denoted as \(\text{Beta}(\alpha, \beta)\), is a flexible family of distributions defined on the interval \((0, 1)\). Its density function is:</p> \[p(\theta) \propto \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}.\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Beta_Distribution-480.webp 480w,/assets/img/Beta_Distribution-800.webp 800w,/assets/img/Beta_Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Beta_Distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Beta_Distribution" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For our coin-flipping example, we can use:</p> \[p(\theta) \propto \theta^{h - 1} (1 - \theta)^{t - 1},\] <p>where \(h\) and \(t\) represent our prior “counts” of heads and tails, respectively.</p> <p>The <strong>mean</strong> of the Beta distribution is:</p> \[\mathbb{E}[\theta] = \frac{h}{h + t},\] <p>and its <strong>mode</strong> (for \(h, t &gt; 1\)) is:</p> \[\text{Mode} = \frac{h - 1}{h + t - 2}.\] <p><strong>A Way to Think of This Distribution:</strong></p> <p>Imagine you’re trying to estimate the probability of rain on a given day in a city you’ve never visited. You don’t have any direct weather data yet, but you do have some general knowledge about the region. Based on this, you form an initial belief about how likely it is to rain—maybe you’re unsure, so you assume it’s equally likely to rain or not, or maybe you’ve heard that it’s usually dry there.</p> <ul> <li><strong>The Beta distribution</strong> helps you represent this uncertainty. It’s like a flexible tool that encodes your prior beliefs about the probability of rain, and you can adjust these beliefs based on what you know or expect. <ul> <li>If you’re totally uncertain, you might use a <strong>uniform prior</strong> (where \(\alpha = \beta = 1\)), meaning you’re equally unsure whether rain is more likely or not.</li> <li>If you’ve already heard that it tends to rain more often, say 70% of the time, you could choose \(\alpha = 7\) and \(\beta = 3\) to reflect this prior information.</li> </ul> </li> </ul> <p>As you gather more data—say, after several days of weather observations—you can update your beliefs about the likelihood of rain. Each new observation (rain or no rain) “shapes” your distribution.</p> <ul> <li> <p><strong>The mean</strong> \(\mathbb{E}[\theta] = \frac{h}{h + t}\) represents the average likelihood of rain after considering all your prior knowledge and the observed days. This is your updated best guess about how likely it is to rain on any given day.</p> </li> <li> <p><strong>The mode</strong> \(\text{Mode} = \frac{h - 1}{h + t - 2}\), which reflects the most probable value of \(\theta\) after observing data, might give you a better estimate if the weather has shown a clear tendency over time (e.g., if it’s rained most days).</p> </li> </ul> <p>In essence, the Beta distribution allows you to start with an initial belief (or no belief) about the probability of rain, and as you observe more data, you continuously refine that belief. This is what makes Bayesian inference powerful—it enables you to <strong>update</strong> your beliefs rationally based on new evidence.</p> <p><strong>Why Use the Beta Prior Distribution in this Coin Flipping Problem?</strong></p> <p>The <strong>Beta distribution</strong> is particularly well-suited for modeling probabilities in Bayesian statistics, especially in problems like coin flipping. Here are a few reasons why it’s a good choice:</p> <ol> <li> <p><strong>Support on (0, 1):</strong> The Beta distribution is defined over the interval \(\theta \in (0, 1)\), which matches the range of possible values for \(\theta\) in the coin-flipping example. Since \(\theta\) represents the probability of getting heads, it must lie between 0 and 1.</p> </li> <li><strong>Flexibility:</strong> The Beta distribution is very flexible in shaping its probability density. By adjusting the parameters \(\alpha\) and \(\beta\), we can model a wide variety of prior beliefs about \(\theta\): <ul> <li>When \(\alpha = \beta = 1\), the Beta distribution is uniform, indicating that we have no strong prior belief about whether heads or tails is more likely.</li> <li>When \(\alpha &gt; \beta\), the distribution is biased towards heads, and when \(\alpha &lt; \beta\), it is biased towards tails.</li> <li>The parameters can also reflect <strong>observed data</strong>: if you’ve already seen \(h\) heads and \(t\) tails, the Beta distribution can be chosen with \(\alpha = h + 1\) and \(\beta = t + 1\), which matches the idea of “updating” your beliefs based on the data you observe.</li> </ul> </li> <li><strong>Intuitive Interpretation:</strong> The Beta distribution is easy to interpret in terms of prior knowledge. The parameters \(\alpha\) and \(\beta\) can be seen as counts of prior observations of heads and tails, respectively. This makes it a natural choice when we have prior information or beliefs about the likelihood of different outcomes, and want to update them as new data comes in.</li> </ol> <p><strong>Note:</strong> I highly suggest taking a look at the Beta distribution graph. As \(\alpha\) increases, the distribution tends to skew towards higher values of \(\theta\) (closer to 1), reflecting a higher likelihood of success. On the other hand, as \(\beta\) increases, the distribution skews towards lower values of \(\theta\) (closer to 0), indicating a higher likelihood of failure. If \(\alpha\) and \(\beta\) are equal, the distribution is symmetric and uniform, reflecting no prior preference between the two outcomes.</p> <hr/> <p>After observing data \(D = (\text{H, H, T, T, T, H, ...})\), where \(n_h\) is the number of heads and \(n_t\) is the number of tails, we combine the <strong>prior</strong> and <strong>likelihood</strong> to obtain the <strong>posterior distribution</strong>.</p> <p>The likelihood function, based on the observed data, is:</p> \[L(\theta) = p(D \mid \theta) = \theta^{n_h} (1 - \theta)^{n_t}.\] <p>Combining the prior and likelihood, the posterior density is:</p> \[p(\theta \mid D) \propto p(\theta) \cdot L(\theta),\] <p>which simplifies to:</p> \[p(\theta \mid D) \propto \theta^{h - 1} (1 - \theta)^{t - 1} \cdot \theta^{n_h} (1 - \theta)^{n_t}.\] <p>Simplifying further, we get:</p> \[p(\theta \mid D) \propto \theta^{h - 1 + n_h} (1 - \theta)^{t - 1 + n_t}.\] <p>This posterior distribution is also a Beta distribution:</p> \[\theta \mid D \sim \text{Beta}(h + n_h, t + n_t).\] <h5 id="interpreting-the-posterior"><strong>Interpreting the Posterior</strong></h5> <p>The posterior distribution shows how our prior beliefs are updated by the observed data:</p> <ul> <li>The prior \(\text{Beta}(h, t)\) initializes our counts with \(h\) heads and \(t\) tails.</li> <li>The posterior \(\text{Beta}(h + n_h, t + n_t)\) updates these counts by adding the observed \(n_h\) heads and \(n_t\) tails.</li> </ul> <p>For example, if our prior belief was \(\text{Beta}(2, 2)\) (a uniform prior), and we observed \(n_h = 3\) heads and \(n_t = 1\) tails, the posterior would be:</p> \[\text{Beta}(2 + 3, 2 + 1) = \text{Beta}(5, 3).\] <p>This reflects our updated belief about the probability of heads after observing the data.</p> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>In this blog, we explored the essence of Bayesian statistics, focusing on how priors, likelihoods, and posteriors interact to update our beliefs. Using the coin-flipping example, we demonstrated key Bayesian tools like the Beta distribution and how to compute posterior updates. Also, as we mentioned, there’s one more important reason for choosing the Beta distribution—its technical term is <strong>conjugate priors</strong>. In the next blog, we’ll dive deeper into this concept and explore Bayesian point estimates, comparing them to the frequentist MLE estimate. Stay tuned as we continue to build intuition and delve further into Bayesian inference! 👋</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Bayesian Statistics</li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.]]></summary></entry><entry><title type="html">Understanding the Weighted Majority Algorithm in Online Learning</title><link href="https://monishver11.github.io/blog/2025/WMA/" rel="alternate" type="text/html" title="Understanding the Weighted Majority Algorithm in Online Learning"/><published>2025-01-23T16:33:00+00:00</published><updated>2025-01-23T16:33:00+00:00</updated><id>https://monishver11.github.io/blog/2025/WMA</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/WMA/"><![CDATA[<p><strong>The Weighted Majority Algorithm: A Powerful Online Learning Technique</strong></p> <p>In the continuation of our exploration into online learning, we turn to the <strong>Weighted Majority Algorithm (WMA)</strong>, an influential approach introduced by Littlestone and Warmuth in 1988. This algorithm builds upon the foundational principles of online learning and offers remarkable theoretical guarantees for handling adversarial scenarios.</p> <p>Let’s dive into the workings of the Weighted Majority Algorithm, analyze its performance, and understand its strengths and limitations.</p> <h4 id="the-weighted-majority-algorithm"><strong>The Weighted Majority Algorithm</strong></h4> <p>The Weighted Majority Algorithm operates in a framework where predictions are made by combining the advice of multiple experts. Unlike the Halving Algorithm, which outright eliminates incorrect hypotheses, WMA assigns and updates weights to experts based on their performance, ensuring a more adaptive approach.</p> <h5 id="the-algorithm-steps"><strong>The Algorithm Steps</strong></h5> <ol> <li><strong>Initialization</strong>: <ul> <li>Start with \(N\) experts, each assigned an initial weight of 1:<br/> \(w_{1,i} = 1 \quad \text{for } i = 1, 2, \dots, N\)</li> </ul> </li> <li><strong>Prediction</strong>: <ul> <li>At each time step \(t\): <ul> <li>Receive the instance \(x_t\).</li> <li>Predict the label \(\hat{y}_t\) using a <strong>weighted majority vote</strong>: \(\hat{y}_t = \begin{cases} 1 &amp; \text{if } \sum_{i: y_{t,i}=1}^{N} w_{t,i} &gt; \sum_{i: y_{t,i}=0}^{N} w_{t,i}, \\ 0 &amp; \text{otherwise.} \end{cases}\)</li> </ul> </li> </ul> </li> <li><strong>Update Weights</strong>: <ul> <li>After receiving the true label \(y_t\), update the weights of the experts: <ul> <li>For each expert \(i\): \(w_{t+1,i} = \begin{cases} \beta w_{t,i} &amp; \text{if } y_{t,i} \neq y_t, \\ w_{t,i} &amp; \text{otherwise,} \end{cases}\) where \(\beta \in [0,1)\) is a parameter that reduces the weight of experts who make incorrect predictions.</li> </ul> </li> </ul> </li> </ol> <p>[The condition check matters here right??, 2 cases: 1. if the majority vote is correct, then no update for the experts that did mistake. 2. If the majoriy vote is wrong, then we update correctly for experts that are wrong. So, its better to update without checking if the majority vote gives correct result right? ]</p> <ol> <li><strong>Termination</strong>: <ul> <li>After \(T\) iterations, return the final weights of all experts.</li> </ul> </li> </ol> <h4 id="theoretical-performance-of-the-weighted-majority-algorithm"><strong>Theoretical Performance of the Weighted Majority Algorithm</strong></h4> <h5 id="mistake-bound"><strong>Mistake Bound</strong></h5> <p><strong>Theorem:</strong> The Weighted Majority (WM) algorithm guarantees a bound on the number of mistakes it makes compared to the best expert. Let:</p> <ul> <li>\(m_t\): Total number of mistakes made by the WM algorithm up to time \(t\),</li> <li>\(m_t^*\): Number of mistakes made by the best expert up to time \(t\),</li> <li>\(N\): Total number of experts,</li> <li>\(\beta\): Parameter controlling the weight decay for incorrect experts.</li> </ul> <p>The mistake bound is given by:</p> \[m_t \leq \frac{\log N + m_t^* \log \frac{1}{\beta}}{\log \frac{2}{1+\beta}}\] <p><strong>Interpretation of the Bound:</strong></p> <ol> <li><strong>First Term (\(\log N\))</strong>: <ul> <li>This term accounts for the initial uncertainty due to having \(N\) experts. The algorithm needs to explore to identify the best expert, and the logarithmic dependence on \(N\) ensures scalability.</li> </ul> </li> <li><strong>Second Term (\(m_t^* \log \frac{1}{\beta}\))</strong>: <ul> <li>This term reflects the cost of following the best expert, scaled by \(\log \frac{1}{\beta}\). A smaller \(\beta\) increases the penalty for mistakes, leading to slower adaptation but potentially fewer overall mistakes. [<strong>Think it through, Why?</strong>]</li> </ul> </li> <li><strong>Denominator (\(\log \frac{2}{1+\beta}\))</strong>: <ul> <li>This represents the efficiency of the weight adjustment. When \(\beta\) is close to 0 (as in the Halving Algorithm), the denominator becomes larger, leading to tighter bounds.</li> </ul> </li> </ol> <p><strong>Special Cases:</strong></p> <ol> <li><strong>Realizable Case (\(m_t^* = 0\))</strong>: <ul> <li>If there exists an expert with zero mistakes, the bound simplifies to: \(m_t \leq \frac{\log N}{\log \frac{2}{1+\beta}}\) <ul> <li>For the Halving Algorithm (\(\beta = 0\)), this further simplifies to: \(m_t \leq \log N\)</li> </ul> </li> </ul> </li> <li><strong>General Case</strong>: <ul> <li>When no expert is perfect (\(m_t^* &gt; 0\)), the algorithm incurs an additional cost proportional to \(m_t^* \log \frac{1}{\beta}\). This reflects the cost of distributing weights among experts.</li> </ul> </li> </ol> <p><strong>Intuition:</strong></p> <ul> <li>The Weighted Majority Algorithm balances exploration (trying all experts) and exploitation (focusing on the best expert). The logarithmic terms indicate that the algorithm adapts efficiently, even with a large number of experts.</li> <li>By tuning \(\beta\), the algorithm can trade off between faster adaptation and resilience to noise. A smaller \(\beta\) (e.g., \(\beta = 0\)) emphasizes rapid adaptation, while a larger \(\beta\) smoothens weight updates.</li> </ul> <p><strong>Note:</strong> If this isn’t clear, I can simplify it further, but this time, I encourage you to try and understand it on your own. There’s a reason behind this—it helps you build a mental model of how the algorithm works and strengthens your intuition. This kind of reinforcement is essential as we dive deeper into advanced ML concepts. I hope this makes sense. If you’re still struggling, consider using tools like ChatGPT or Perplexity to gain additional clarity.</p> <hr/> <h4 id="proof-of-the-mistake-bound"><strong>Proof of the Mistake Bound</strong></h4> <p><mark>A general method for deducing bounds and guarantees involves defining a potential function, establishing both upper and lower bounds for it, and deriving results from the resulting inequality.</mark> This powerful approach is central to deducing several proofs. Specifically, the proof of the mistake bound relies on defining a potential function that tracks the total weight of all experts over time.</p> <h5 id="potential-function"><strong>Potential Function</strong></h5> <p>The potential function at time \(t\) is defined as:</p> \[\Phi_t = \sum_{i=1}^N w_{t,i},\] <p>where \(w_{t,i}\) is the weight of expert \(i\) at time \(t\).</p> <h5 id="upper-bound-on-potential"><strong>Upper Bound on Potential</strong></h5> <p>Initially, the weights of all \(N\) experts sum up to \(\Phi_0 = N\) since each expert starts with a weight of 1.</p> <p><strong>Step 1: Effect of a Mistake</strong></p> <p>When the Weighted Majority Algorithm makes a mistake, the weights of the experts who predicted incorrectly are reduced by a factor of \(\beta\), where \(0 &lt; \beta &lt; 1\). This means the total weight at the next time step, \(\Phi_{t+1}\), will be less than or equal to the weighted average of the weights of the correct and incorrect experts.</p> <p><strong>Step 2: Fraction of Correct and Incorrect Experts</strong></p> <p>Let’s say a fraction \(p\) of the total weight belongs to the correct experts, and a fraction \(1 - p\) belongs to the incorrect experts. After the mistake, the incorrect experts’ weights are scaled by \(\beta\). Thus, the new potential is:</p> \[\Phi_{t+1} = p \Phi_t + \beta (1 - p) \Phi_t\] <p><strong>Step 3: Simplifying the Expression</strong></p> <p>Factor out \(\Phi_t\) from the equation:</p> \[\Phi_{t+1} = \Phi_t \left[ p + \beta (1 - p) \right]\] <p>Rewriting \(p + \beta (1 - p)\):</p> \[\Phi_{t+1} = \Phi_t \left[ 1 - (1 - \beta)(1 - p) \right]\] <p>Since \(p + (1 - p) = 1\), this simplifies to:</p> \[\Phi_{t+1} \leq \Phi_t \left[ 1 + \frac{\beta}{2} \right]\] <p>This inequality holds because the worst-case scenario assumes \(p = \frac{1}{2}\), where half the weight comes from correct predictions and half from incorrect predictions.</p> <p><strong>Step 4: Over Multiple Mistakes</strong></p> <p>If the algorithm makes \(m_t\) mistakes, the inequality applies iteratively. After \(m_t\) mistakes, the potential becomes:</p> \[\Phi_t \leq \Phi_0 \left[ 1 + \frac{\beta}{2} \right]^{m_t}\] <p>Substituting \(\Phi_0 = N\) (the initial total weight):</p> \[\Phi_t \leq N \left[ 1 + \frac{\beta}{2} \right]^{m_t}\] <p><strong>Intuition:</strong> The factor \(1 + \frac{\beta}{2}\) reflects the reduction in potential after each mistake. As \(\beta\) decreases, the penalty for incorrect experts increases, leading to a faster reduction in \(\Phi_t\). This bound shows how the potential decreases exponentially with the number of mistakes \(m_t\).</p> <hr/> <h5 id="lower-bound-on-potential"><strong>Lower Bound on Potential</strong></h5> <p>The <strong>lower bound</strong> on the potential is based on the performance of the best expert in the algorithm.</p> <p>Let’s define \(w_{t,i}\) as the weight of expert \(i\) at time \(t\). The total potential \(\Phi_t\) is the sum of the weights of all experts. Since the best expert is the one with the highest weight at any time, we can state that:</p> \[\Phi_t \geq w_{t,i^*}\] <p>where \(i^*\) is the index of the best expert.</p> <p>Now, the key point is that the weight of the best expert, \(w_{t,i^*}\), decays over time as it makes mistakes. Let \(m_t^*\) be the number of mistakes made by the best expert up to time \(t\). Since each mistake reduces the weight of the best expert by a factor of \(\beta\), the weight of the best expert at time \(t\) is given by:</p> \[w_{t,i^*} = \beta^{m_t^*}\] <p>Therefore, the potential at time \(t\) is at least the weight of the best expert:</p> \[\Phi_t \geq \beta^{m_t^*}\] <p>This provides a <strong>lower bound</strong> on the potential.</p> <h5 id="combining-the-upper-and-lower-bounds"><strong>Combining the Upper and Lower Bounds</strong></h5> <p>Now that we have both an upper bound and a lower bound on the potential, we can combine them to get a more useful inequality.</p> <p>From the upper bound, we know:</p> \[\Phi_t \leq \left[\frac{1 + \beta}{2}\right]^{m_t} N\] <p>From the lower bound, we know:</p> \[\Phi_t \geq \beta^{m_t^*}\] <p>By combining these two inequalities, we get:</p> \[\beta^{m_t^*} \leq \left[\frac{1 + \beta}{2}\right]^{m_t} N\] <p>This inequality tells us that the weight of the best expert at time \(t\), \(\beta^{m_t^*}\), is less than or equal to the total potential \(\Phi_t\) after \(m_t\) mistakes.</p> <p>To solve for \(m_t\), we take the logarithm of both sides of the inequality:</p> \[\log \left( \beta^{m_t^*} \right) \leq \log \left( \left[\frac{1 + \beta}{2}\right]^{m_t} N \right)\] <p>Using the logarithmic property \(\log(a^b) = b \log(a)\), we simplify both sides:</p> \[m_t^* \log \beta \leq m_t \log \left[\frac{1 + \beta}{2}\right] + \log N\] <p>Now, we want to isolate \(m_t\) on one side of the inequality. First, subtract \(\log N\) from both sides:</p> \[m_t^* \log \beta - \log N \leq m_t \log \left[\frac{1 + \beta}{2}\right].\] <p>Now, divide both sides by \(\log \left[\frac{1 + \beta}{2}\right]\). Note that \(\log \left[\frac{1 + \beta}{2}\right]\) is negative because \(\frac{1 + \beta}{2} &lt; 1\), so dividing by it reverses the inequality:</p> \[m_t \geq \frac{m_t^* \log \beta - \log N}{\log \left[\frac{1 + \beta}{2}\right]}.\] <p>We can simplify the expression further by recognizing that \(\log \frac{1}{\beta} = -\log \beta\). This gives us:</p> \[m_t \leq \frac{\log N + m_t^* \log \frac{1}{\beta}}{\log \frac{2}{1 + \beta}}.\] <p>This is the final inequality, which gives a bound on the number of mistakes \(m_t\) made by the algorithm in terms of the number of mistakes \(m_t^*\) made by the best expert, the total number of experts \(N\), and the factor \(\beta\).</p> <p><strong>Note:</strong> The <strong>less than or equal to (≤)</strong> sign appears because of the <strong>inequality reversal</strong> when dividing by a negative quantity. This is why the final inequality has the \(\leq\) sign instead of \(\geq\).</p> <p>This completes the proof of the mistake bound. The inequality shows that the number of mistakes \(m_t\) made by the algorithm is related to the mistakes \(m_t^*\) made by the best expert, and that the algorithm’s performance improves as \(\beta\) decreases.</p> <p>[Self - Understand it better, think it through]</p> <hr/> <h5 id="strengths-and-weaknesses-of-the-weighted-majority-algorithm"><strong>Strengths and Weaknesses of the Weighted Majority Algorithm</strong></h5> <p><strong>Advantages</strong></p> <ul> <li><strong>Strong Theoretical Bound</strong>: <ul> <li>The Weighted Majority Algorithm (WMA) achieves a remarkable theoretical bound on regret without requiring any assumptions about the data distribution or the performance of individual experts.</li> <li>This makes it robust, particularly in adversarial environments.</li> </ul> </li> </ul> <p><strong>Disadvantages</strong></p> <ul> <li><strong>Limitation with Binary Loss</strong>: <ul> <li>For binary loss, no deterministic algorithm (including WMA) can achieve a regret \(R_T = o(T)\).</li> <li>This means deterministic WMA cannot guarantee sublinear regret in such settings.</li> </ul> </li> </ul> <p>In the context of binary loss, where predictions are either correct or incorrect (0 or 1), deterministic algorithms like the Weighted Majority Algorithm (WMA) face a fundamental limitation. Regret, which measures how much worse the algorithm performs compared to the best expert in hindsight, ideally should grow sublinearly with the number of rounds (\(T\)). However, deterministic WMA cannot achieve this in adversarial environments. The fixed and predictable nature of deterministic strategies allows adversaries to exploit the algorithm, forcing it into repeated mistakes. As a result, the regret \(R_T\) grows at least linearly with \(T\), meaning the algorithm’s performance does not improve relative to the best expert over time. This inability to achieve sublinear regret (\(R_T = o(T)\)) under binary loss is a significant disadvantage of deterministic WMA, necessitating alternative approaches like randomization to overcome this limitation.</p> <p>[Still, Not clear of this limitation, How and Why?]</p> <p>In the following cases, the Weighted Majority Algorithm offers improved guarantees:</p> <ol> <li><strong>Randomization Improves Regret Bounds</strong>: <ul> <li><strong>Randomized versions</strong> of WMA (e.g., RWMA) can improve the regret bound by introducing randomness, which helps the algorithm avoid getting stuck with poor-performing experts.</li> </ul> </li> <li><strong>Extensions for Convex Losses</strong>: <ul> <li>WMA can be adapted to handle <strong>convex loss functions</strong>, such as in regression tasks. In these cases, the algorithm provides improved theoretical guarantees, ensuring more reliable and efficient performance.</li> </ul> </li> </ol> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>The Weighted Majority Algorithm offers an elegant and efficient approach to handling adversarial settings. By adaptively updating the weights of experts, it ensures robust performance with minimal assumptions.</p> <p>In the next post, we’ll dive into alternative versions of WMA and explore powerful algorithms designed for online learning. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li></li> </ul>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.]]></summary></entry><entry><title type="html">Online Learning in ML - A Beginner’s Guide to Adaptive Learning</title><link href="https://monishver11.github.io/blog/2025/Online-Learning/" rel="alternate" type="text/html" title="Online Learning in ML - A Beginner’s Guide to Adaptive Learning"/><published>2025-01-23T14:45:00+00:00</published><updated>2025-01-23T14:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/Online-Learning</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/Online-Learning/"><![CDATA[<p>In this post, we’ll dive into one of the foundational topics that was discussed in the first class of my advanced machine learning course: online learning. This course is centered on theoretical insights and encourages students to think critically, experiment fearlessly, and embrace confusion as a stepping stone toward innovation. Let’s explore how online learning fits into the broader landscape of machine learning and why it’s such a powerful concept.</p> <h5 id="the-advanced-ml-course-a-brief-overview"><strong>The Advanced ML Course: A Brief Overview</strong></h5> <p>The course takes a deeply theoretical approach, focusing on critical analysis and research to build innovative machine learning algorithms. It’s not just about solving problems but about challenging established ideas, learning from mistakes, and having the courage to be wrong. Confusion, in this context, is not a roadblock—it’s a catalyst for deeper thinking.</p> <p>Two primary domains form the core of present ML/AI space:</p> <ol> <li><strong>Neural Networks</strong>: The cornerstone of modern machine learning.</li> <li><strong>Online Learning</strong>: A versatile and influential approach with deep connections to game theory and optimization.</li> </ol> <h4 id="what-is-online-learning"><strong>What is Online Learning?</strong></h4> <p>Online learning stands out as an area of machine learning with rich literature and numerous practical applications. It bridges the gap between supervised learning and game-theoretic optimization while offering efficient solutions for large-scale problems.</p> <p>Unlike traditional batch learning, where algorithms process the entire dataset at once, online learning operates iteratively, processing one sample at a time. This makes it computationally efficient and ideal for large datasets. Moreover, online learning does not rely on the common assumption that data points are independent and identically distributed (i.i.d.). Instead, it is designed to handle adversarial scenarios, making it incredibly flexible and applicable to situations where data distributions are unknown or variable.</p> <p>Even though these algorithms are inherently designed for adversarial settings, they can, under specific conditions, yield accurate predictions in scenarios where data does follow a distribution.</p> <p><strong>What do we mean by adversarial in the context of online learning?</strong></p> <p>In online learning, “adversarial” refers to settings where data is not assumed to follow a fixed probabilistic distribution. Instead, the data sequence might be unpredictable, dependent, or deliberately chosen to challenge the algorithm. This flexibility makes online learning particularly robust in real-world applications.</p> <ul> <li><strong>Real-world Examples</strong>: <ul> <li><strong>Financial Models</strong>: Adapting to volatile or externally influenced stock price movements.</li> <li><strong>Recommendation Systems</strong>: Managing biased or strategically influenced user feedback.</li> <li><strong>Security Systems</strong>: Responding to data manipulations or malicious attacks.</li> </ul> </li> </ul> <p>By focusing on resilience and adaptability, online learning algorithms excel in handling evolving data and challenging environments, making them indispensable for a wide range of applications.</p> <h5 id="why-online-learning"><strong>Why Online Learning?</strong></h5> <p>Traditional machine learning approaches often rely on the PAC (Probably Approximately Correct) framework, where:</p> <ul> <li>The data distribution remains fixed over time.</li> <li>Both training and testing data are assumed to follow the same i.i.d. distribution.</li> </ul> <p><strong>What is the PAC Learning Framework?</strong></p> <p>The PAC learning framework provides a theoretical foundation for understanding the feasibility of learning in a probabilistic setting. Under this framework:</p> <ul> <li>The algorithm’s goal is to find a hypothesis that is <em>probably approximately correct</em>, meaning it performs well on the training data and generalizes to unseen data with high probability.</li> <li>It assumes that data points are drawn independently and identically distributed (i.i.d.) from a fixed, unknown distribution.</li> <li>Key metrics include the error rate of the hypothesis on future samples and its convergence to the true distribution as more data is provided.</li> </ul> <p>While this framework is powerful for traditional batch learning, it relies on strong assumptions about the stability and predictability of the data distribution, making it less suitable for dynamic or adversarial scenarios.</p> <p>In contrast, online learning assumes no such distributional stability. It operates under the following key principles:</p> <ul> <li><strong>No Assumptions on Data Distribution</strong>: The data can follow any sequence, including adversarially generated ones. This flexibility allows online learning to adapt to real-world scenarios where data patterns may shift unpredictably.</li> <li><strong>Mixed Training and Testing</strong>: Training and testing are not separate phases but occur simultaneously, enabling the algorithm to continuously learn and improve from new data.</li> <li><strong>Worst-Case Analysis</strong>: Algorithms are designed to perform well even under the most challenging conditions, ensuring robustness in unpredictable environments.</li> <li><strong>Performance Metrics</strong>: Instead of accuracy or loss functions commonly used in batch learning, online learning evaluates performance using measures like: <ul> <li><strong><em>Mistake Model</em></strong>: The total number of incorrect predictions made during the learning process.</li> <li><strong><em>Regret</em></strong>: The difference between the cumulative loss of the algorithm and the loss of the best possible strategy in hindsight.</li> </ul> </li> </ul> <p><strong>What are some practical applications of online learning?</strong></p> <p>Online learning has proven invaluable in a variety of real-world domains where data is dynamic, unpredictable, or arrives sequentially:</p> <ol> <li><strong>Stock Market Predictions</strong>: Continuously adapting to ever-changing financial data, helping traders and financial systems make real-time decisions.</li> <li><strong>Online Advertising</strong>: Personalizing ads based on user behavior that evolves with every click or interaction.</li> <li><strong>Recommendation Systems</strong>: Adapting suggestions in real time as users interact with platforms like Netflix, Amazon, or YouTube.</li> <li><strong>Autonomous Systems</strong>: Enabling self-driving cars or robots to learn and adapt to new scenarios as they encounter them.</li> <li><strong>Spam Filtering</strong>: Continuously updating filters to catch new spam types as they emerge.</li> <li><strong>Security Systems</strong>: Responding to cyberattacks or new threats by learning and adapting on the fly.</li> </ol> <p>This shift in perspective allows online learning to address a broader range of real-world problems. For now, if all of this feels a bit abstract, don’t worry—hang tight! We’ll dive deeper and make sure to explore it thoroughly, leaving no stone unturned.</p> <h4 id="the-general-online-learning-framework"><strong>The General Online Learning Framework</strong></h4> <p>The online learning process follows a simple yet powerful framework. At each step:</p> <ol> <li>The algorithm receives an instance, denoted as \(x_t\).</li> <li>It makes a prediction, \(\hat{y}_t\).</li> <li>The true label, \(y_t\), is revealed.</li> <li>A loss is incurred, calculated as \(L(\hat{y}_t, y_t)\), which quantifies the prediction error.</li> </ol> <p>The overarching goal of online learning is to minimize the total loss over a sequence of predictions: \(\sum_{t=1}^T L(\hat{y}_t, y_t)\)</p> <p>For classification tasks, a common choice of loss is the 0-1 loss: \(L(\hat{y}_t, y_t) = \mathbb{1}(\hat{y}_t \neq y_t) \; or \; \vert \hat{y}_t - y_t \vert\) For regression tasks, the squared loss is often used: \(L(\hat{y}_t, y_t) = (\hat{y}_t - y_t)^2\)</p> <hr/> <h4 id="prediction-with-expert-advice"><strong>Prediction with Expert Advice</strong></h4> <p>One particularly compelling framework in online learning is <strong>Prediction with Expert Advice</strong>. Imagine you have multiple “experts,” each providing advice on how to predict the label for a given instance. The challenge lies in aggregating their advice to make accurate predictions while minimizing the regret associated with poor decisions.</p> <p>The process unfolds as follows:</p> <ol> <li>At each time step, the algorithm receives an instance, \(x_t\), and predictions from \(N\) experts, \(\{y_{t,1}, y_{t,2}, \dots, y_{t,N}\}\).</li> <li>Based on this advice, the algorithm predicts \(\hat{y}_t\).</li> <li>The true label, \(y_t\), is revealed, and the loss, \(L(\hat{y}_t, y_t)\), is incurred.</li> </ol> <p>The performance of the algorithm is measured by its <strong><em>regret</em></strong>, which is the difference between the total loss incurred by the algorithm and the total loss of the best-performing expert:</p> \[\text{Regret}(T) = \sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i=1, \dots, N} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p>Minimizing regret ensures that the algorithm’s predictions improve over time and closely approximate the performance of the best expert.</p> <p><strong>What does the regret equation convey and how do we interpret it?</strong></p> <p>The regret equation provides a way to evaluate the algorithm’s performance in hindsight by comparing it to the best expert. Here’s what each term means:</p> <ol> <li> <p><strong>Algorithm’s Loss</strong> (\(\sum_{t=1}^T L(\hat{y}_t, y_t)\)):<br/> This is the cumulative loss incurred by the algorithm over \(T\) time steps. It reflects how well the algorithm performs when making predictions based on the aggregated advice of all experts.</p> </li> <li> <p><strong>Best Expert’s Loss</strong> (\(\min_{i=1, \dots, N} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\)):<br/> This represents the cumulative loss of the single best-performing expert in hindsight. Note that the best expert is identified after observing all \(T\) instances, which gives it an advantage over the algorithm that has to predict in real time.</p> </li> <li> <p><strong>Regret</strong>:<br/> The difference between these two terms quantifies how much worse the algorithm performs compared to the best expert.</p> <ul> <li><strong>Low regret</strong> indicates that the algorithm’s predictions are close to those of the best expert, demonstrating effective learning.</li> <li><strong>High regret</strong> suggests that the algorithm is failing to learn effectively from the experts’ advice.</li> </ul> </li> </ol> <p><strong>Why is regret important?</strong></p> <p>Regret is a crucial metric in online learning because:</p> <ul> <li>It provides a measure of how well the algorithm adapts to the expert advice over time.</li> <li>It ensures that, as the number of time steps \(T\) increases, the algorithm’s performance converges to that of the best expert (ideally achieving sublinear regret, such as \(O(\sqrt{T})\) or better).</li> <li>It accounts for the dynamic nature of predictions, focusing on learning improvement rather than static accuracy.</li> </ul> <p>A few more questions to make our understadning better.</p> <p><strong>How to Calculate the Best Expert’s Loss?</strong></p> <p>The <strong>Best Expert’s Loss</strong> is the cumulative loss of the single expert that performs best over the entire sequence of predictions, \(T\). Here’s how to calculate it:</p> <ol> <li> <p><strong>Track each expert’s cumulative loss</strong>:<br/> For each expert \(i\), maintain a running sum of their losses over the rounds:</p> \[L_{\text{expert } i} = \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p>Here, \(\hat{y}_{t,i}\) is the prediction made by expert \(i\) at time \(t\), and \(y_t\) is the true label. \(L\) could represent any loss function, such as zero-one loss or squared loss.</p> </li> <li> <p><strong>Find the expert with the minimum cumulative loss</strong>:<br/> After summing the losses for all \(N\) experts over \(T\) rounds, identify the expert whose cumulative loss is the smallest:</p> \[\min_{i=1, \dots, N} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p>This value represents the <strong>Best Expert’s Loss</strong>, which serves as the benchmark for evaluating the algorithm’s regret.</p> </li> </ol> <p><strong>Do We Pick the Best Expert After Each Round?</strong></p> <p>No, the <strong>Best Expert’s Loss</strong> is determined in hindsight, <strong><em>after</em></strong> observing the entire sequence of \(T\) rounds. The algorithm does not know in advance which expert is the best. Instead, it aggregates predictions from all experts during the process (e.g., using techniques like weighted averaging).</p> <ul> <li>The <strong>best expert</strong> is identified retrospectively after all rounds.</li> <li>The cumulative loss of this best expert is used to compute regret.</li> </ul> <p><strong>Example:</strong></p> <p>Suppose we have 3 experts, and their losses over 5 rounds are:</p> <table> <thead> <tr> <th>Round (t)</th> <th>Expert 1 Loss</th> <th>Expert 2 Loss</th> <th>Expert 3 Loss</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>0.2</td> <td>0.3</td> <td>0.1</td> </tr> <tr> <td>2</td> <td>0.1</td> <td>0.4</td> <td>0.2</td> </tr> <tr> <td>3</td> <td>0.3</td> <td>0.2</td> <td>0.3</td> </tr> <tr> <td>4</td> <td>0.4</td> <td>0.1</td> <td>0.3</td> </tr> <tr> <td>5</td> <td>0.2</td> <td>0.5</td> <td>0.1</td> </tr> </tbody> </table> <ol> <li><strong>Calculate the cumulative loss for each expert</strong>: <ul> <li><strong>Expert 1</strong>: \(0.2 + 0.1 + 0.3 + 0.4 + 0.2 = 1.2\)</li> <li><strong>Expert 2</strong>: \(0.3 + 0.4 + 0.2 + 0.1 + 0.5 = 1.5\)</li> <li><strong>Expert 3</strong>: \(0.1 + 0.2 + 0.3 + 0.3 + 0.1 = 1.0\)</li> </ul> </li> <li> <p><strong>Find the minimum cumulative loss</strong>: \(\min(1.2, 1.5, 1.0) = 1.0\)</p> <p>Hence, the <strong>Best Expert’s Loss</strong> is <strong>1.0</strong>, achieved by Expert 3.</p> </li> </ol> <p>Prediction with Expert Advice is a powerful framework for dynamic environments where multiple sources of information or strategies need to be combined effectively. It ensures robustness and adaptability by iteratively improving predictions while minimizing regret.</p> <hr/> <h4 id="the-halving-algorithm-simple-and-powerful"><strong>The Halving Algorithm: Simple and Powerful</strong></h4> <p>The <strong>Halving Algorithm</strong> is a simple yet effective online learning algorithm designed to minimize mistakes. It works by maintaining a set of hypotheses (or experts) and systematically eliminating those that make incorrect predictions.</p> <p>Here’s how it works:</p> <ol> <li><strong>Initialization</strong>: Start with a set of hypotheses, \(H_1 = H\).</li> <li><strong>Iteration</strong>: At each time step, \(t\): <ul> <li>Receive an instance, \(x_t\).</li> <li>Predict the label, \(\hat{y}_t\), using majority voting among the hypotheses in \(H_t\).</li> <li>Receive the true label, \(y_t\).</li> <li>If \(\hat{y}_t \neq y_t\), update the hypothesis set: \(H_{t+1} = \{h \in H_t : h(x_t) = y_t\}\)</li> </ul> </li> <li><strong>Termination</strong>: After all iterations, return the final hypothesis set, \(H_{T+1}\).</li> </ol> <h5 id="mistake-bound-for-the-halving-algorithm"><strong>Mistake Bound for the Halving Algorithm</strong></h5> <p><strong>Theorem</strong>: If the initial hypothesis set \(H\) is finite, the number of mistakes made by the Halving Algorithm is bounded by:</p> \[M_{Halving(H)} \leq \log_2 |H|\] <p><strong>Proof Outline</strong>:</p> <ul> <li>Each mistake reduces the size of the hypothesis set by at least half: \(|H_{t+1}| \leq \frac{|H_t|}{2}\)</li> <li>Initially, \(|H_1| = |H|\). After \(M\) mistakes: \(|H_{M+1}| \leq \frac{|H|}{2^M}\)</li> <li>To ensure \(|H_{M+1}| \geq 1\) (at least one hypothesis remains), we require: \(M \leq \log_2 |H|\)</li> </ul> <p>This logarithmic bound demonstrates the efficiency of the Halving Algorithm, even in adversarial settings.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>Online learning offers a powerful framework for making predictions in dynamic and adversarial environments. Its ability to adapt, operate under minimal assumptions, and deliver robust performance makes it a cornerstone of modern machine learning research. The Halving Algorithm provides a concrete example of how online learning methods can be both intuitive and theoretically grounded.</p> <p>In upcoming posts, we’ll delve deeper into other online learning algorithms and explore their theoretical guarantees, practical applications, and connections to broader machine learning principles. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>PAC</li> <li>Online Learning Intro</li> </ul>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.]]></summary></entry><entry><title type="html">Multivariate Gaussian Distribution and Naive Bayes</title><link href="https://monishver11.github.io/blog/2025/Multivariate-GNB/" rel="alternate" type="text/html" title="Multivariate Gaussian Distribution and Naive Bayes"/><published>2025-01-23T03:01:00+00:00</published><updated>2025-01-23T03:01:00+00:00</updated><id>https://monishver11.github.io/blog/2025/Multivariate-GNB</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/Multivariate-GNB/"><![CDATA[<p>When analyzing data in higher dimensions, we often encounter scenarios where input features are not independent. In such cases, the <strong>Multivariate Gaussian Distribution</strong> provides a robust probabilistic framework to model these relationships. It extends the familiar univariate Gaussian distribution to multiple dimensions, enabling us to capture dependencies and correlations between variables effectively.</p> <h4 id="understanding-the-multivariate-gaussian-distribution"><strong>Understanding the Multivariate Gaussian Distribution</strong></h4> <p>A multivariate Gaussian distribution is defined as:</p> \[x \sim \mathcal{N}(\mu, \Sigma),\] <p>where \(\mu\) is the mean vector, and \(\Sigma\) is the covariance matrix. Its probability density function is given by:</p> \[p(x) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu)^\top \Sigma^{-1} (x - \mu)\right),\] <p>Here, \(d\) represents the dimensionality of the input \(x\), \(\vert \Sigma \vert\) denotes the determinant of the covariance matrix, and \(\Sigma^{-1}\) is its inverse.</p> <p>The term \((x - \mu)^\top \Sigma^{-1} (x - \mu)\) is referred to as the <strong>Mahalanobis distance</strong>, which measures the distance of a point \(x\) from the mean \(\mu\). Unlike the Euclidean distance, the Mahalanobis distance normalizes for differences in variances and accounts for correlations between the dimensions. This normalization makes it particularly useful in multivariate data analysis.</p> <h5 id="intuition-and-analogy-for-multivariate-gaussian"><strong>Intuition and Analogy for Multivariate Gaussian</strong></h5> <p>Think of the multivariate Gaussian distribution as a <strong>3D bell-shaped curve</strong> (or higher-dimensional equivalent) where:</p> <ul> <li>The <strong>peak</strong> of the bell is at \(\mu\), the mean vector.</li> <li>The <strong>spread</strong> of the bell in different directions is determined by \(\Sigma\), the covariance matrix. It stretches or compresses the curve along certain axes depending on the variances and correlations.</li> </ul> <h6 id="analogy-a-weighted-balloon"><strong>Analogy: A Weighted Balloon</strong></h6> <p>Imagine a balloon filled with air. If the balloon is perfectly spherical, it represents a distribution where all dimensions are independent and have the same variance (this corresponds to \(\Sigma\) being a diagonal matrix with equal entries).</p> <p>Now, if you squeeze the balloon in one direction:</p> <ul> <li>It elongates in one direction and compresses in another. This reflects <strong>correlations</strong> between dimensions in the data, encoded by the off-diagonal elements of \(\Sigma\).</li> <li>The shape of the balloon changes, and distances (like Mahalanobis distance) now account for these correlations, unlike Euclidean distance.</li> </ul> <h5 id="how-to-think-about-mahalanobis-distance"><strong>How to Think About Mahalanobis Distance</strong></h5> <p>The Mahalanobis distance:</p> \[d_M(x) = \sqrt{(x - \mu)^\top \Sigma^{-1} (x - \mu)}\] <p>can be understood as the distance from a point \(x\) to the center \(\mu\), scaled by the shape and orientation of the distribution:</p> <ol> <li> <p><strong>Scaling by Variance</strong>: In directions where the variance is large (the distribution is “spread out”), the Mahalanobis distance will consider points farther from the mean as less unusual. Conversely, in directions where variance is small, even small deviations from the mean are considered significant.</p> </li> <li> <p><strong>Accounting for Correlations</strong>: If two dimensions are correlated, the Mahalanobis distance adjusts for this by using the covariance matrix \(\Sigma\). The covariance matrix captures both the variances of individual dimensions and the relationships (correlations) between them.</p> </li> </ol> <p><strong>Role of the Inverse Covariance Matrix:</strong></p> <p>The term \(\Sigma^{-1}\) (the inverse of the covariance matrix) in the Mahalanobis distance ensures that the contribution of each dimension is scaled appropriately. For example:</p> <ul> <li>If two dimensions are strongly correlated, deviations along one dimension are partially “explained” by deviations along the other. The Mahalanobis distance reduces the weight of such deviations, treating them as less unusual.</li> <li>Conversely, if two dimensions are uncorrelated, the deviations are treated independently.</li> </ul> <p><strong>Example:</strong></p> <p>In a dataset of height and weight, a taller-than-average person is likely to weigh more than average. The covariance matrix captures this relationship, and \(\Sigma^{-1}\) adjusts the distance calculation to reflect that such deviations are expected. Without this adjustment (as in Euclidean distance), the relationship would be ignored, leading to an overestimation of the “unusualness” of the point.</p> <p><strong>Returning to the Balloon Analogy,</strong></p> <p>The Mahalanobis distance incorporates the “shape” of the balloon (determined by \(\Sigma\)) to measure distances:</p> <p><strong>Shape and Scaling:</strong></p> <ul> <li>A spherical balloon corresponds to a covariance matrix where all dimensions are independent and have equal variance. In this case, the Mahalanobis distance reduces to the Euclidean distance.</li> <li>A stretched or compressed balloon reflects correlations or differences in variance. The Mahalanobis distance scales the contribution of each dimension based on the covariance structure, ensuring that distances are measured relative to the shape of the distribution.</li> </ul> <p><strong>How It Works:</strong></p> <ul> <li>Points on the surface of the balloon correspond to a Mahalanobis distance of 1, regardless of the balloon’s shape. This is because the Mahalanobis distance normalizes for the stretching or compressing of the balloon in different directions.</li> <li>Mathematically, this is achieved by transforming the space using \(\Sigma^{-1}\), effectively “flattening” the correlations and variances. In this transformed space, the balloon becomes a perfect sphere, and distances are measured uniformly.</li> </ul> <p>These adjustments make the Mahalanobis distance a powerful metric for detecting outliers and understanding the distribution of data in a multivariate context.</p> <hr/> <p>If you’re still unsure about the concept, let’s walk through an example and explore it together.</p> <h6 id="1-covariance-matrix"><strong>1. Covariance Matrix</strong>:</h6> <p>For a dataset with two variables, say <strong>height</strong> (\(x_1\)) and <strong>weight</strong> (\(x_2\)), the covariance matrix \(\Sigma\) looks like this:</p> \[\Sigma = \begin{pmatrix} \sigma_{11} &amp; \sigma_{12} \\ \sigma_{21} &amp; \sigma_{22} \end{pmatrix}\] <p>Where:</p> <ul> <li>\(\sigma_{11}\) is the variance of height (\(x_1\)).</li> <li>\(\sigma_{22}\) is the variance of weight (\(x_2\)).</li> <li>\(\sigma_{12} = \sigma_{21}\) is the covariance between height and weight.</li> </ul> <h6 id="2-inverse-covariance-matrix"><strong>2. Inverse Covariance Matrix</strong>:</h6> <p>The inverse of the covariance matrix \(\Sigma^{-1}\) is used to “normalize” the data and account for correlations. The inverse of a 2x2 matrix is given by:</p> \[\Sigma^{-1} = \frac{1}{\text{det}(\Sigma)} \begin{pmatrix} \sigma_{22} &amp; -\sigma_{12} \\ -\sigma_{21} &amp; \sigma_{11} \end{pmatrix}\] <p>Where the determinant of the covariance matrix is:</p> \[\text{det}(\Sigma) = \sigma_{11} \sigma_{22} - \sigma_{12}^2\] <h6 id="3-example-correlated-data-height-and-weight"><strong>3. Example: Correlated Data (Height and Weight)</strong></h6> <p>Suppose we have a dataset of heights and weights, and the covariance matrix looks like this:</p> \[\Sigma = \begin{pmatrix} 100 &amp; 80 \\ 80 &amp; 200 \end{pmatrix}\] <p>This means:</p> <ul> <li>The variance of height (\(\sigma_{11}\)) is 100.</li> <li>The variance of weight (\(\sigma_{22}\)) is 200.</li> <li>The covariance between height and weight (\(\sigma_{12} = \sigma_{21}\)) is 80, indicating a strong positive correlation between height and weight.</li> </ul> <p>Now, let’s say we have a data point:</p> \[x = \begin{pmatrix} 180 \\ 75 \end{pmatrix}\] <p>This means the person is 180 cm tall and weighs 75 kg. The mean of the dataset is:</p> \[\mu = \begin{pmatrix} 170 \\ 70 \end{pmatrix}\] <h6 id="31-euclidean-distance-without-accounting-for-correlation"><strong>3.1. Euclidean Distance</strong> (Without Accounting for Correlation)</h6> <p>The Euclidean distance between the data point \(x\) and the mean \(\mu\) is:</p> \[D_E(x) = \sqrt{(x_1 - \mu_1)^2 + (x_2 - \mu_2)^2}\] <p>Substituting the values:</p> \[D_E(x) = \sqrt{(180 - 170)^2 + (75 - 70)^2} = \sqrt{10^2 + 5^2} = \sqrt{100 + 25} = \sqrt{125} \approx 11.18\] <p>This distance doesn’t account for the correlation between height and weight. It treats the two dimensions as if they are independent, and gives a straightforward measure of how far the point is from the mean in Euclidean space.</p> <h6 id="32-mahalanobis-distance-with-covariance-adjustment"><strong>3.2. Mahalanobis Distance</strong> (With Covariance Adjustment)</h6> <p>Now, let’s compute the Mahalanobis distance. First, we need to compute the inverse of the covariance matrix \(\Sigma^{-1}\).</p> <p>The determinant of \(\Sigma\) is:</p> \[\text{det}(\Sigma) = 100 \times 200 - 80^2 = 20000 - 6400 = 13600\] <p>So, the inverse covariance matrix is:</p> \[\Sigma^{-1} = \frac{1}{13600} \begin{pmatrix} 200 &amp; -80 \\ -80 &amp; 100 \end{pmatrix} = \begin{pmatrix} 0.0147 &amp; -0.0059 \\ -0.0059 &amp; 0.0074 \end{pmatrix}\] <p>Now, we compute the Mahalanobis distance:</p> \[D_M(x) = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)}\] <p>Substituting the values:</p> \[x - \mu = \begin{pmatrix} 180 - 170 \\ 75 - 70 \end{pmatrix} = \begin{pmatrix} 10 \\ 5 \end{pmatrix}\] <p>Now, calculate the Mahalanobis distance:</p> \[D_M(x) = \sqrt{\begin{pmatrix} 10 &amp; 5 \end{pmatrix} \begin{pmatrix} 0.0147 &amp; -0.0059 \\ -0.0059 &amp; 0.0074 \end{pmatrix} \begin{pmatrix} 10 \\ 5 \end{pmatrix}}\] <p>First, multiply the vectors:</p> \[\begin{pmatrix} 10 &amp; 5 \end{pmatrix} \begin{pmatrix} 0.0147 &amp; -0.0059 \\ -0.0059 &amp; 0.0074 \end{pmatrix} = \begin{pmatrix} 10 \times 0.0147 + 5 \times (-0.0059) \\ 10 \times (-0.0059) + 5 \times 0.0074 \end{pmatrix} = \begin{pmatrix} 0.147 - 0.0295 \\ -0.059 + 0.037 \end{pmatrix} = \begin{pmatrix} 0.1175 \\ -0.022 \end{pmatrix}\] <p>Now, multiply this result by the vector \(\begin{pmatrix} 10 \\ 5 \end{pmatrix}\):</p> \[\begin{pmatrix} 0.1175 &amp; -0.022 \end{pmatrix} \begin{pmatrix} 10 \\ 5 \end{pmatrix} = 0.1175 \times 10 + (-0.022) \times 5 = 1.175 - 0.11 = 1.065\] <p>Thus, the Mahalanobis distance is:</p> \[D_M(x) = \sqrt{1.065} \approx 1.03\] <h6 id="4-interpretation-of-the-results"><strong>4. Interpretation of the Results</strong>:</h6> <ul> <li>The <strong>Euclidean distance</strong> between the point and the mean was approximately <strong>11.18</strong>. This suggests that the point is far from the mean, without considering the correlation between height and weight.</li> <li>The <strong>Mahalanobis distance</strong> is <strong>1.03</strong>, which is much smaller. This is because the Mahalanobis distance accounts for the fact that height and weight are correlated. The deviation in weight is expected given the deviation in height, so the Mahalanobis distance treats this as less “unusual.”</li> </ul> <h6 id="takeaways"><strong>Takeaways:</strong></h6> <ul> <li><strong>Euclidean distance</strong> treats each dimension as independent, ignoring correlations, which can lead to an overestimation of how unusual a point is.</li> <li><strong>Mahalanobis distance</strong>, by using the inverse covariance matrix \(\Sigma^{-1}\), adjusts for correlations and scales the deviations accordingly. This results in a more accurate measure of how far a point is from the mean, considering the underlying structure of the data (e.g., the correlation between height and weight in this example).</li> </ul> <hr/> <h5 id="grasping-better-with-bivariate-normal-distributions"><strong>Grasping Better with Bivariate Normal Distributions</strong></h5> <p>To build a deeper understanding, let’s focus on a specific case: the two-dimensional Gaussian, commonly referred to as the <strong>bivariate normal distribution</strong>.</p> <h6 id="case-1-identity-covariance-matrix"><strong>Case 1: Identity Covariance Matrix</strong></h6> <p>Suppose the covariance matrix is given as:</p> \[\Sigma = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}.\] <p>In this scenario, the contours of the distribution are circular. This indicates that there is no correlation between the two variables, and both have equal variances. The shape of the contours reflects the isotropic nature of the distribution.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-1-480.webp 480w,/assets/img/Multivariate-GNB-1-800.webp 800w,/assets/img/Multivariate-GNB-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h6 id="case-2-scaled-identity-covariance"><strong>Case 2: Scaled Identity Covariance</strong></h6> <p>If we scale the covariance matrix, say:</p> \[\Sigma = 0.5 \cdot \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix},\] <p>the variances of both variables decrease, resulting in smaller circular contours. Conversely, if we scale it up:</p> \[\Sigma = 2 \cdot \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix},\] <p>the variances increase, leading to larger circular contours. This demonstrates how scaling the covariance matrix affects the spread of the distribution.</p> <h6 id="case-3-anisotropic-variance"><strong>Case 3: Anisotropic Variance</strong></h6> <p>When the variances of the variables are different, such as when \(\text{var}(x_1) \neq \text{var}(x_2)\), the contours take on an elliptical shape. The orientation and eccentricity of the ellipse are determined by the relative variances along each axis.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-2-480.webp 480w,/assets/img/Multivariate-GNB-2-800.webp 800w,/assets/img/Multivariate-GNB-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h6 id="case-4-correlated-variables"><strong>Case 4: Correlated Variables</strong></h6> <p>Correlation between variables introduces an additional layer of complexity.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-3-480.webp 480w,/assets/img/Multivariate-GNB-3-800.webp 800w,/assets/img/Multivariate-GNB-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For instance, if:</p> \[\Sigma = \begin{bmatrix} 1 &amp; \rho \\ \rho &amp; 1 \end{bmatrix},\] <p>where \(\rho\) is the correlation coefficient:</p> <ul> <li>When \(\rho &gt; 0\), the variables are positively correlated, and the ellipse tilts along the diagonal.</li> <li>When \(\rho &lt; 0\), the variables are negatively correlated, and the ellipse tilts in the opposite direction.</li> <li>When \(\rho = 0\), the variables remain uncorrelated, resulting in circular or axis-aligned ellipses.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-4-480.webp 480w,/assets/img/Multivariate-GNB-4-800.webp 800w,/assets/img/Multivariate-GNB-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h4 id="gaussian-bayes-classifier"><strong>Gaussian Bayes Classifier</strong></h4> <p>The <strong>Gaussian Bayes Classifier (GBC)</strong> extends the Gaussian framework to classification tasks. It assumes that the conditional distribution \(p(x \vert y)\) follows a multivariate Gaussian distribution. Mathematically, for a class \(k\):</p> \[p(x|t = k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_k)^\top \Sigma_k^{-1} (x - \mu_k)\right),\] <p>where each class \(k\) has its own mean vector \(\mu_k\) and covariance matrix \(\Sigma_k\). The determinant \(\vert \Sigma_k \vert\) and the inverse \(\Sigma_k^{-1}\) are crucial components for computing probabilities.</p> <p>Estimating the parameters for each class becomes computationally challenging in high dimensions, as the covariance matrix has \(O(d^2)\) parameters. This complexity often necessitates simplifying assumptions to make the model tractable.</p> <p><strong>How do we arrive at this complexity, and why is it considered computationally challenging?</strong></p> <p>In the Gaussian Bayes Classifier, each class \(k\) has its own covariance matrix \(\Sigma_k\), which is a \(d \times d\) matrix where \(d\) is the dimensionality of the feature space. For a single class, this covariance matrix has \(\frac{d(d+1)}{2}\) unique parameters. This is because a covariance matrix is symmetric, meaning that the upper and lower triangular portions are mirrors of each other. Specifically, the diagonal elements represent the variances, while the off-diagonal elements represent the covariances between different features.</p> <p>For \(K\) classes, the total number of parameters required to estimate all the covariance matrices would be: \(K \times \frac{d(d+1)}{2}\)</p> <p>This can become computationally expensive, especially when \(d\) (the number of features) is large.</p> <p>This large number of parameters is the reason why the Gaussian Bayes Classifier faces challenges in high-dimensional settings, as the model needs to estimate these parameters from data, and estimating a large number of parameters requires a substantial amount of data. Moreover, when the dimensionality \(d\) is large relative to the number of training samples, the covariance matrix can become ill-conditioned or singular, which might lead to poor performance.</p> <p>To handle this, simplifications such as assuming diagonal covariance matrices (where off-diagonal covariances are set to zero) or sharing a common covariance matrix across all classes are often made, which reduces the number of parameters that need to be estimated.</p> <hr/> <h5 id="special-cases-of-gaussian-bayes-classifier"><strong>Special Cases of Gaussian Bayes Classifier</strong></h5> <p>To address the computational challenges, we consider the following special cases of the Gaussian Bayes Classifier:</p> <ol> <li> <p><strong>Full Covariance Matrix</strong><br/> Each class has its own covariance matrix \(\Sigma_k\). This allows for flexible modeling of the class distributions, as it can capture correlations between different features. The decision boundary, however, is quadratic, as the posterior probability depends on the quadratic term involving \((x - \mu_k)^\top \Sigma_k^{-1} (x - \mu_k)\).</p> <p><strong>Decision Boundary Derivation</strong>:<br/> The decision rule is based on the ratio of the posterior probabilities:</p> \[\frac{p(x \vert t = k)}{p(x \vert t = l)} &gt; 1\] <p>which leads to a quadratic expression involving the covariance matrices \(\Sigma_k\) and \(\Sigma_l\). This quadratic form creates a curved decision boundary.</p> <p><strong>Insight</strong>:<br/> Since each class can have a different covariance matrix, the decision boundary can bend and adapt to the data’s true distribution, allowing for accurate classification even in complex scenarios. However, the computational cost is high because each class requires estimating a full covariance matrix with \(O(d^2)\) parameters.</p> </li> <li> <p><strong>Shared Covariance Matrix</strong><br/> If all classes share a common covariance matrix \(\Sigma\), the decision boundary becomes linear. This assumption simplifies the model by treating all classes as having the same spread, reducing the number of parameters to estimate.</p> <p><strong>Decision Boundary Derivation</strong>:<br/> In this case, the likelihood for each class is given by:</p> \[p(x \vert t = k) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_k)^\top \Sigma^{-1} (x - \mu_k)\right)\] <p>The decision rule between two classes \(k\) and \(l\) simplifies to:</p> \[(x - \mu_k)^\top \Sigma^{-1} (x - \mu_k) - (x - \mu_l)^\top \Sigma^{-1} (x - \mu_l) = \text{constant}\] <p>which is linear in \(x\). This results in a linear decision boundary between the classes.</p> <p><strong>Insight</strong>:<br/> By assuming a common covariance matrix, we treat the class distributions as having the same shape and orientation. This simplifies the model and makes the decision boundary linear, leading to reduced computational cost and faster training. However, this may be less flexible if the true distributions of the classes are significantly different.</p> </li> <li> <p><strong>Naive Bayes Assumption</strong><br/> The Naive Bayes classifier assumes that the features are conditionally independent given the class, meaning that the covariance matrix is diagonal. This leads to a model where each feature is treated independently when making class predictions.</p> <p><strong>Decision Boundary Derivation</strong>:<br/> Under the Naive Bayes assumption, the covariance matrix is diagonal, so the likelihood for each class becomes:</p> \[p(x \vert t = k) = \prod_{i=1}^{d} \frac{1}{\sqrt{2\pi \sigma_{k,i}^2}} \exp\left(-\frac{(x_i - \mu_{k,i})^2}{2\sigma_{k,i}^2}\right)\] <p>The decision rule between two classes \(k\) and \(l\) leads to a quadratic expression for each feature, but since the features are independent, the decision boundary remains quadratic overall, as the product of exponentials leads to terms that depend on the squares of the feature values.</p> <p><strong>Insight</strong>:<br/> Even though the features are assumed to be independent, the decision boundary remains quadratic because of the feature-wise variances. The strong independence assumption makes the model computationally efficient, as it reduces the number of parameters to estimate (each class only requires \(d\) variances), but it limits the model’s flexibility to capture interactions between features.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-5-480.webp 480w,/assets/img/Multivariate-GNB-5-800.webp 800w,/assets/img/Multivariate-GNB-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="gaussian-bayes-classifier-vs-logistic-regression"><strong>Gaussian Bayes Classifier vs. Logistic Regression</strong></h5> <p>One interesting connection between GBC and logistic regression arises when the data is truly Gaussian. If we assume shared covariance matrices, the decision boundaries produced by GBC become identical to those of logistic regression. However, logistic regression is more versatile since it does not rely on Gaussian assumptions and can learn other types of decision boundaries.</p> <p><strong>Note:</strong> Even though both methods produce the same linear decision boundary under the Gaussian assumption with shared covariance, the actual parameter values (weights and means) will be different because they are derived from different models and assumptions.</p> <hr/> <h5 id="final-thoughts"><strong>Final Thoughts</strong></h5> <p>The multivariate Gaussian distribution provides a probabilistic framework for understanding data with correlated features. By extending this to classification tasks, the Gaussian Bayes Classifier offers an elegant and interpretable approach to modeling. However, its reliance on assumptions like Gaussianity and the complexity of covariance estimation in high dimensions present practical challenges.</p> <p>Generative models, like GBC, aim to model the joint distribution \(p(x, y)\), which contrasts with discriminative models, such as logistic regression, that focus directly on \(p(y \vert x)\). While generative models offer a principled way to derive loss functions via maximum likelihood, they can struggle with small datasets, where estimating the joint distribution becomes difficult. <strong>Why?</strong></p> <p>Generative models typically use the product of the likelihood and the prior. Specifically, the likelihood \(p(x \mid y)\), can become challenging with small datasets because:</p> <ul> <li><strong>Insufficient data for accurate parameter estimation</strong>: With limited data, the model may not have enough examples to accurately estimate the parameters of the distribution (such as the means and variances in the case of Gaussian distributions).</li> <li><strong>Overfitting risk</strong>: With small datasets, the model may overfit to noise or specific patterns that don’t generalize well, leading to poor estimates of the joint distribution.</li> </ul> <p>In contrast, discriminative models like logistic regression focus directly on \(p(y \mid x)\) and are less affected by small sample sizes because they only need to model the decision boundary between classes, making them more robust in such situations.</p> <p>As you delve deeper into probabilistic frameworks, a question worth pondering is: Do generative models have an equivalent form of regularization to mitigate overfitting? This opens up avenues for exploring how these models can be made more robust in practice.</p> <p>To address this question, we’ll next explore <strong>Bayesian Inference</strong>, where generative models can incorporate priors over their parameters. For instance, in a Gaussian Bayes Classifier, instead of relying on point estimates for means and variances, Bayesian methods treat these parameters as distributions. This approach naturally regularizes the model by spreading probability mass over plausible parameter values, reducing the risk of overfitting. Stay tuned—we’ll dive into this in the next post!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.]]></summary></entry></feed>