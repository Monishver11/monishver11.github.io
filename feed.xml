<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-22T05:39:12+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A clear, theory-focused approach to machine learning, designed to take you beyond the basics. </subtitle><entry><title type="html">Gaussian Naive Bayes - A Natural Extension</title><link href="https://monishver11.github.io/blog/2025/NB-continuous-features/" rel="alternate" type="text/html" title="Gaussian Naive Bayes - A Natural Extension"/><published>2025-01-20T20:39:00+00:00</published><updated>2025-01-20T20:39:00+00:00</updated><id>https://monishver11.github.io/blog/2025/NB-continuous-features</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/NB-continuous-features/"><![CDATA[<p>In the previous blog, we explored the Naive Bayes (NB) model for binary features and how it works under the assumption of conditional independence. However, real-world datasets often include continuous features. How can we extend the NB framework to handle such cases? Let’s dive into Gaussian Naive Bayes (GNB), a variant of NB that uses Gaussian distributions to model continuous inputs.</p> <p><strong>Before we start:</strong> I know this might be challenging to follow just by reading through, especially for this part. So, grab a pen and paper and work through it yourself. You’ll notice that within the summations, all terms except the one you’re differentiating with respect to are constants and will drop out (i.e., become zero). As you write it out, you’ll also understand why certain steps involve a change in sign. Working through it once will make everything much clearer and easier to grasp.</p> <hr/> <p>Consider a multiclass classification problem where each input feature \(x_i\) is continuous. To model \(p(x_i \mid y)\), we assume that the feature values follow a Gaussian (normal) distribution:</p> \[p(x_i \mid y = k) \sim \mathcal{N}(\mu_{i,k}, \sigma^2_{i,k}),\] <p>where \(\mu_{i,k}\) and \(\sigma^2_{i,k}\) are the mean and variance of \(x_i\) for class \(y = k\), respectively. Additionally, we model the class prior probabilities as:</p> \[p(y = k) = \theta_k.\] <p>With these assumptions, the likelihood of the dataset becomes:</p> \[p(D) = \prod_{n=1}^N p_\theta(x^{(n)}, y^{(n)})\] \[p(D) = \prod_{n=1}^N p(y^{(n)}) \prod_{i=1}^d p(x_i^{(n)} \mid y^{(n)}).\] <p>Substituting the Gaussian distribution for \(p(x_i \mid y)\), we get:</p> \[p(D) = \prod_{n=1}^N \theta_{y^{(n)}} \prod_{i=1}^d \frac{1}{\sqrt{2\pi\sigma_{i,y^{(n)}}^2}} \exp\left(-\frac{\left(x_i^{(n)} - \mu_{i,y^{(n)}}\right)^2}{2\sigma_{i,y^{(n)}}^2}\right).\] <p>It may seem complex at first, but if you look closely, you’ll see that we’re applying the same principle. The only difference is in the distribution. To visualize this, we’ve essentially applied the distribution to a familiar form \((1)\) once again to obtain the result. Take a moment to reflect on this.</p> \[\hat{y} = \arg\max_{y \in \mathcal{Y}} p(x, y; \theta) = \arg\max_{y} p(y \mid x; \theta) = \arg\max_{y} p(x \mid y; \theta) p(y; \theta) \tag{1}\] <hr/> <h4 id="learning-parameters-with-maximum-likelihood-estimation-mle"><strong>Learning Parameters with Maximum Likelihood Estimation (MLE)</strong></h4> <p>To train the Gaussian Naive Bayes model, we estimate the parameters \(\mu_{i,k}\), \(\sigma^2_{i,k}\), and \(\theta_k\) using MLE.</p> <h5 id="mean-mu_ik"><strong>Mean (\(\mu_{i,k}\)):</strong></h5> <p>The log-likelihood of the data is:</p> \[\ell = \sum_{n=1}^N \log \theta_{y^{(n)}} + \sum_{n=1}^N \sum_{i=1}^d \left[-\frac{1}{2} \log (2\pi \sigma_{i,y^{(n)}}^2) - \frac{\left(x_i^{(n)} - \mu_{i,y^{(n)}}\right)^2}{2\sigma_{i,y^{(n)}}^2}\right]\] <p>Taking the derivative with respect to \(\mu_{j,k}\) and setting it to zero gives:</p> \[\mu_{j,k} = \frac{\sum_{n:y^{(n)}=k} x_j^{(n)}}{\sum_{n:y^{(n)}=k} 1}\] <p>This is simply the sample mean of \(x_j\) for class \(k\).</p> <h5 id="derivation-of-mu_jk-for-gaussian-naive-bayes"><strong>Derivation of \(\mu_{j,k}\) for Gaussian Naive Bayes</strong></h5> <p>To estimate the parameter \(\mu_{j,k}\), the mean of feature \(x_j\) for class \(k\), we maximize the log-likelihood with respect to \(\mu_{j,k}\).</p> <p><strong>Step 1: Compute the Derivative of the Log-Likelihood</strong></p> <p>The log-likelihood is differentiated with respect to \(\mu_{j,k}\):</p> \[\frac{\partial}{\partial \mu_{j,k}} \ell = \frac{\partial}{\partial \mu_{j,k}} \sum_{n: y^{(n)} = k} \left( -\frac{1}{2 \sigma_{j,k}^2} \left( x_j^{(n)} - \mu_{j,k} \right)^2 \right)\] <p>Ignoring irrelevant terms (constants that do not depend on \(\mu_{j,k}\)), this simplifies to:</p> \[\frac{\partial}{\partial \mu_{j,k}} \ell = \sum_{n: y^{(n)} = k} \frac{1}{\sigma_{j,k}^2} \left( x_j^{(n)} - \mu_{j,k} \right)\] <p><strong>Step 2: Set the Derivative to Zero</strong></p> <p>To find the maximum likelihood estimate, set the derivative to zero:</p> \[\sum_{n: y^{(n)} = k} \frac{1}{\sigma_{j,k}^2} \left( x_j^{(n)} - \mu_{j,k} \right) = 0\] <p><strong>Step 3: Solve for \(\mu_{j,k}\)</strong></p> <p>Rearranging terms:</p> \[\sum_{n: y^{(n)} = k} x_j^{(n)} = \mu_{j,k} \sum_{n: y^{(n)} = k} 1\] <p>Divide both sides by \(\sum_{n: y^{(n)} = k} 1\):</p> \[\mu_{j,k} = \frac{\sum_{n: y^{(n)} = k} x_j^{(n)}}{\sum_{n: y^{(n)} = k} 1}\] <p><strong>Final Expression</strong></p> <p>The maximum likelihood estimate of \(\mu_{j,k}\) is:</p> \[\mu_{j,k} = \frac{\sum_{n: y^{(n)} = k} x_j^{(n)}}{\sum_{n: y^{(n)} = k} 1}\] <p><strong>Interpretation:</strong></p> <ul> <li>\(\mu_{j,k}\) is the sample mean of \(x_j\) for all data points in class \(k\).</li> <li>This parameter is essential for defining the Gaussian distribution for feature \(x_j\) given class \(k\) in Gaussian Naive Bayes.</li> </ul> <h5 id="variance-sigma2_ik"><strong>Variance (\(\sigma^2_{i,k}\)):</strong></h5> <p>Similarly, the variance for feature \(x_j\) in class \(k\) is:</p> \[\sigma^2_{j,k} = \frac{\sum_{n:y^{(n)}=k} \left(x_j^{(n)} - \mu_{j,k}\right)^2}{\sum_{n:y^{(n)}=k} 1}\] <h5 id="class-prior-theta_k"><strong>Class Prior (\(\theta_k\)):</strong></h5> <p>The class prior \(\theta_k\) is estimated as the proportion of data points belonging to class \(k\):</p> \[\theta_k = \frac{\sum_{n:y^{(n)}=k} 1}{N}\] <h5 id="derivation-of-sigma_jk2-sample-variance-and-theta_k-class-prior"><strong>Derivation of \(\sigma_{j,k}^2\) (Sample Variance) and \(\theta_k\) (Class Prior)</strong></h5> <p><strong>1. Derivation of \(\sigma_{j,k}^2\) (Sample Variance)</strong></p> <p>To derive the sample variance \(\sigma_{j,k}^2\), we start from the log-likelihood of the Gaussian distribution for feature \(x_j\) within class \(k\):</p> \[\ell = \sum_{n: y^{(n)} = k} \left[ -\frac{1}{2} \log(2\pi \sigma_{j,k}^2) - \frac{\left( x_j^{(n)} - \mu_{j,k} \right)^2}{2\sigma_{j,k}^2} \right]\] <p>We take the derivative of \(\ell\) with respect to \(\sigma_{j,k}^2\) and set it to zero:</p> \[\frac{\partial \ell}{\partial \sigma_{j,k}^2} = \sum_{n: y^{(n)} = k} \left[ -\frac{1}{2\sigma_{j,k}^2} + \frac{\left( x_j^{(n)} - \mu_{j,k} \right)^2}{2\sigma_{j,k}^4} \right] = 0\] <p>Simplify the equation:</p> \[\sum_{n: y^{(n)} = k} \left[ -\sigma_{j,k}^2 + \left( x_j^{(n)} - \mu_{j,k} \right)^2 \right] = 0\] <p>Divide by \(\sigma_{j,k}^2\) and rearrange:</p> \[\sigma_{j,k}^2 = \frac{\sum_{n: y^{(n)} = k} \left( x_j^{(n)} - \mu_{j,k} \right)^2}{\sum_{n: y^{(n)} = k} 1}\] <p>Thus, the MLE for \(\sigma_{j,k}^2\) is:</p> \[\sigma_{j,k}^2 = \frac{\sum_{n: y^{(n)} = k} \left( x_j^{(n)} - \mu_{j,k} \right)^2}{\sum_{n: y^{(n)} = k} 1}\] <p><strong>2. Derivation of \(\theta_k\) (Class Prior)</strong></p> <p>The class prior \(\theta_k\) represents the proportion of data points belonging to class \(k\) in the dataset. It is given by:</p> \[\theta_k = \frac{\sum_{n: y^{(n)} = k} 1}{N}\] <p><strong>Steps:</strong></p> <ol> <li><strong>Numerator</strong>: \(\sum_{n: y^{(n)} = k} 1\) counts the total number of data points that belong to class \(k\).</li> <li><strong>Denominator</strong>: \(N\) is the total number of data points in the entire dataset.</li> </ol> <p><strong>Finally,</strong></p> <ol> <li> <p><strong>Sample Variance</strong>:</p> \[\sigma_{j,k}^2 = \frac{\sum_{n: y^{(n)} = k} \left( x_j^{(n)} - \mu_{j,k} \right)^2}{\sum_{n: y^{(n)} = k} 1}\] </li> <li> <p><strong>Class Prior</strong>:</p> \[\theta_k = \frac{\sum_{n: y^{(n)} = k} 1}{N}\] </li> </ol> <ul> <li>The sample variance \(\sigma_{j,k}^2\) measures the spread of feature \(x_j\) for class \(k\), derived using MLE.</li> <li>The class prior \(\theta_k\) represents the proportion of data points in class \(k\), computed directly from the dataset.</li> </ul> <hr/> <h4 id="decision-boundary-of-the-gaussian-naive-bayes-gnb-model"><strong>Decision Boundary of the Gaussian Naive Bayes (GNB) Model</strong></h4> <p><strong>General Formulation of the Decision Boundary:</strong></p> <p>For binary classification (\(y \in \{0, 1\}\)), the <strong>log odds ratio</strong> is expressed as:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \log \frac{p(x \mid y=1)p(y=1)}{p(x \mid y=0)p(y=0)}.\] <p>If you’re unclear about what the log odds ratio is, it represents the logarithm of the ratio of the probabilities of the two classes. By setting the log odds ratio to zero, we identify the points where the model is equally likely to classify a sample as belonging to either class.</p> <p>In Gaussian Naive Bayes, this involves substituting the Gaussian distributions for \(p(x \mid y)\), simplifying the expression, and determining whether the resulting decision boundary is quadratic or linear based on the assumptions about the variances.</p> <p>Thus, the log odds ratio serves as a straightforward mathematical tool to derive the decision boundary by locating the regions where the probabilities of the two classes are equal.</p> <p>So, the conditional distributions \(p(x_i \mid y)\) are Gaussian:</p> \[p(x_i \mid y) = \frac{1}{\sqrt{2\pi \sigma_{i,y}^2}} \exp\left(-\frac{(x_i - \mu_{i,y})^2}{2\sigma_{i,y}^2}\right).\] <p>Substituting this into the log odds equation, we get:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \log \frac{\theta_1}{\theta_0} + \sum_{i=1}^d \left[\log \sqrt{\frac{\sigma_{i,0}^2}{\sigma_{i,1}^2}} + \frac{(x_i - \mu_{i,0})^2}{2\sigma_{i,0}^2} - \frac{(x_i - \mu_{i,1})^2}{2\sigma_{i,1}^2}\right].\] <p>This equation represents the <strong>general case</strong> of the GNB decision boundary.</p> <h5 id="linear-vs-quadratic-decision-boundaries"><strong>Linear vs. Quadratic Decision Boundaries</strong></h5> <h6 id="quadratic-decision-boundary"><strong>Quadratic Decision Boundary:</strong></h6> <p>In the general case, where the variances \(\sigma_{i,0}^2\) and \(\sigma_{i,1}^2\) differ between classes, the decision boundary is <strong>quadratic</strong>. This is due to the presence of quadratic terms in the numerator:</p> \[\frac{(x_i - \mu_{i,0})^2}{2\sigma_{i,0}^2} - \frac{(x_i - \mu_{i,1})^2}{2\sigma_{i,1}^2}.\] <h6 id="linear-decision-boundary"><strong>Linear Decision Boundary:</strong></h6> <p>When we assume the variances are equal for both classes \((\sigma_{i,0}^2 = \sigma_{i,1}^2 = \sigma_i^2)\), the quadratic terms cancel out. Simplifying the log odds equation yields:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \sum_{i=1}^d \frac{\mu_{i,1} - \mu_{i,0}}{\sigma_i^2} x_i + \sum_{i=1}^d \frac{\mu_{i,0}^2 - \mu_{i,1}^2}{2\sigma_i^2}.\] <p>In matrix form, this becomes:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \theta^\top x + \theta_0,\] <p>where:</p> <ul> <li> \[\theta_i = \frac{\mu_{i,1} - \mu_{i,0}}{\sigma_i^2}, \quad i \in [1, d]\] </li> <li> \[\theta_0 = \sum_{i=1}^d \frac{\mu_{i,0}^2 - \mu_{i,1}^2}{2\sigma_i^2}.\] </li> </ul> <p>Thus, under the shared variance assumption, the decision boundary is <strong>linear</strong>.</p> <p><strong>Takeaways:</strong></p> <ul> <li><strong>Quadratic Boundary</strong>: The difference in variances between the two classes introduces curvature, resulting in a nonlinear boundary.</li> <li><strong>Linear Boundary</strong>: Equal variances lead to a linear boundary, making the model behave similarly to logistic regression.</li> </ul> <p>This derivation connects Gaussian Naive Bayes to logistic regression and helps to understand its behavior under different assumptions.</p> <hr/> <h4 id="naive-bayes-vs-logistic-regression"><strong>Naive Bayes vs. Logistic Regression</strong></h4> <p>Both Naive Bayes and logistic regression are popular classifiers, but they differ fundamentally in their approach:</p> <hr/> <table> <thead> <tr> <th> </th> <th><strong>Logistic Regression</strong></th> <th><strong>Gaussian Naive Bayes</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Model Type</strong></td> <td>Conditional/Discriminative</td> <td>Generative</td> </tr> <tr> <td><strong>Parametrization</strong></td> <td>\(p(y \mid x)\)</td> <td>\(p(x \mid y), p(y)\)</td> </tr> <tr> <td><strong>Assumptions on Y</strong></td> <td>Bernoulli</td> <td>Bernoulli</td> </tr> <tr> <td><strong>Assumptions on X</strong></td> <td>—</td> <td>Gaussian</td> </tr> <tr> <td><strong>Decision Boundary</strong></td> <td>\(\theta_{LR}^\top x\)</td> <td>\(\theta_{GNB}^\top x\)</td> </tr> </tbody> </table> <hr/> <ul> <li> <p><strong>Logistic Regression (LR)</strong> is a discriminative model that directly models the conditional probability \(p(y \mid x)\). It does not make assumptions about the distribution of features \(X\) but instead focuses on finding a decision boundary that separates the classes based on the observed data.</p> </li> <li> <p><strong>Gaussian Naive Bayes (GNB)</strong>, on the other hand, is a generative model that explicitly models the joint distribution \(p(x, y)\) by assuming that the features \(X\) are conditionally Gaussian given the class \(y\).</p> </li> </ul> <p>A few questions to address before we wrap up.</p> <p><strong>Question 1:</strong> Given the same training data, is \(\theta_{LR} = \theta_{GNB}\)?</p> <ul> <li>This is a critical question to explore the relationship between discriminative and generative models. While the forms of the decision boundary (e.g., linear) may look similar under certain assumptions (e.g., shared variance in GNB), the parameters \(\theta_{LR}\) and \(\theta_{GNB}\) are generally not the same due to differences in how the two models approach the learning process.</li> </ul> <p><strong>Question 2:</strong> Relationship Between LR and GNB</p> <ul> <li>Logistic regression and Gaussian naive Bayes <strong>converge to the same classifier asymptotically</strong>, assuming the GNB assumptions hold: <ol> <li>Data points are generated from Gaussian distributions for each class.</li> <li>Each dimension of the feature vector is generated independently.</li> <li>Both classes share the same variance for each feature (shared variance assumption).</li> </ol> </li> <li>Under these conditions, the decision boundary derived from GNB becomes identical to that of logistic regression as the amount of data increases.</li> </ul> <p><strong>Question 3:</strong> What Happens if the GNB Assumptions Are Not True?</p> <ul> <li>If the assumptions of GNB are violated (e.g., features are not Gaussian, dimensions are not independent, or variances are not shared), the decision boundary derived by GNB can deviate significantly from the optimal boundary. In such cases: <ul> <li><strong>Logistic Regression</strong> is likely to perform better, as it does not rely on specific assumptions about the feature distributions.</li> <li><strong>GNB</strong> may produce suboptimal results because its assumptions are hardcoded into the model and do not adapt to the true data distribution.</li> </ul> </li> </ul> <p>Thus, the choice between LR and GNB depends heavily on whether the data aligns with GNB’s assumptions.</p> <hr/> <h4 id="generative-vs-discriminative-models-trade-offs"><strong>Generative vs. Discriminative Models: Trade-offs</strong></h4> <p>The contrast between Naive Bayes and logistic regression highlights the differences between <strong>generative</strong> and <strong>discriminative</strong> models. Generative models like Naive Bayes model the joint distribution \(p(x, y)\), allowing them to generate data as well as make predictions. In contrast, discriminative models like logistic regression focus directly on \(p(y \mid x)\), optimizing for classification accuracy.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Generative_vs_Discriminative_models-480.webp 480w,/assets/img/Generative_vs_Discriminative_models-800.webp 800w,/assets/img/Generative_vs_Discriminative_models-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Generative_vs_Discriminative_models.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Generative_vs_Discriminative_models" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This tradeoff is explored in the classic paper by Ng, A. and Jordan, M. (2002), On discriminative versus generative classifiers: A comparison of logistic regression and naive Bayes., which shows that generative models converge faster but may have higher asymptotic error compared to their discriminative counterparts.</p> <hr/> <p>In the next section, we’ll explore the Multivariate Gaussian Distribution and the Gaussian Bayes Classifier in greater detail. Stay tuned👋!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.]]></summary></entry><entry><title type="html">An Introduction to Generative Models - Naive Bayes for Binary Features</title><link href="https://monishver11.github.io/blog/2025/generative-models/" rel="alternate" type="text/html" title="An Introduction to Generative Models - Naive Bayes for Binary Features"/><published>2025-01-20T20:29:00+00:00</published><updated>2025-01-20T20:29:00+00:00</updated><id>https://monishver11.github.io/blog/2025/generative-models</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/generative-models/"><![CDATA[<p>Generative models represent a powerful class of machine learning techniques. Unlike methods that directly map inputs \(x\) to outputs \(y\), such as generalized linear models or perceptrons, generative models take a broader approach. They aim to model the <strong>joint distribution</strong> \(p(x, y; \theta)\), which allows us to capture the underlying relationships between inputs and outputs in a holistic manner.</p> <h4 id="generalized-linear-models-vs-generative-models"><strong>Generalized Linear Models vs. Generative Models</strong></h4> <p>To recall, generalized linear models focus on the conditional distribution \(p(y \mid x; \theta)\). By contrast, generative models model \(p(x, y; \theta)\). Once we have the joint distribution, we can predict labels for new data by leveraging the following rule:</p> \[\hat{y} = \arg\max_{y \in \mathcal{Y}} p(x, y; \theta)\] <p>This prediction process connects naturally to conditional distributions. Using <strong>Bayes’ Rule</strong>, we can rewrite \(p(y \mid x)\) as:</p> \[p(y \mid x) = \frac{p(x \mid y) p(y)}{p(x)}\] <p>In practice, we often bypass computing \(p(x)\), as it is independent of \(y\). Instead, predictions simplify to:</p> \[\hat{y} = \arg\max_{y} p(y \mid x) = \arg\max_{y} p(x \mid y) p(y)\] <p>With this foundation, let us explore one of the most straightforward and widely used generative models: <strong>Naive Bayes (NB)</strong>.</p> <p>If you’re unable to follow the above formulation, here’s a quick refresher on Bayes’ Rule to help you out.</p> <p>Bayes’ Rule relates conditional probabilities to joint and marginal probabilities. It can be expressed as:</p> \[p(y \mid x) = \frac{p(x, y)}{p(x)} = \frac{p(x \mid y) p(y)}{p(x)},\] <p>where:</p> <ul> <li>\(p(y \mid x)\): Posterior probability of \(y\) given \(x\),</li> <li>\(p(x, y)\): Joint probability of \(x\) and \(y\),</li> <li>\(p(x \mid y)\): Likelihood of \(x\) given \(y\),</li> <li>\(p(y)\): Prior probability of \(y\),</li> <li>\(p(x)\): Marginal probability of \(x\), which ensures proper normalization.</li> </ul> <hr/> <h4 id="naive-bayes-a-simple-and-effective-generative-model"><strong>Naive Bayes: A Simple and Effective Generative Model</strong></h4> <p>To understand Naive Bayes, consider a simple yet practical problem: binary text classification. Imagine we want to classify a document as either a <strong>fake review</strong> or a <strong>genuine review</strong>. This setup offers a clear context to explore the mechanics of generative modeling.</p> <h5 id="representing-documents-as-features"><strong>Representing Documents as Features</strong></h5> <p>To make this task computationally feasible, we use a <strong>bag-of-words representation</strong>. A document is expressed as a binary vector \(x\), where:</p> \[x = [x_1, x_2, \dots, x_d].\] <p>Here, \(d\) represents the vocabulary size, and each \(x_i\) indicates whether the \(i\)-th word in the vocabulary exists in the document (\(x_i = 1\)) or not (\(x_i = 0\)).</p> <h5 id="modeling-the-joint-probability-of-documents-and-labels"><strong>Modeling the Joint Probability of Documents and Labels</strong></h5> <p>For a document \(x\) with label \(y\), the joint probability \(p(x, y)\) can be expressed using the <strong>chain rule of probability</strong>:</p> \[p(x \mid y) = p(x_1, x_2, \dots, x_d \mid y) = p(x_1 \mid y) p(x_2 \mid y, x_1) \cdots p(x_d \mid y, x_{d-1}, \dots, x_1).\] \[p(x \mid y) = \prod_{i=1}^d p(x_i \mid y, x_{&lt;i}),\] <p>However, modeling the dependencies between features (\(x_1, x_2, \dots, x_d\)) becomes intractable as the number of features grows and hard to estimate. This is where Naive Bayes introduces its defining assumption.</p> <h5 id="the-naive-bayes-assumption"><strong>The Naive Bayes Assumption</strong></h5> <p>Naive Bayes simplifies the problem by assuming that <strong>features are conditionally independent given the label \(y\)</strong>. Mathematically, this means:</p> \[p(x \mid y) = \prod_{i=1}^d p(x_i \mid y)\] <p>This assumption significantly reduces computational complexity while often delivering excellent results in practice. While the assumption of conditional independence may not hold in all cases, it is surprisingly effective in many real-world applications.</p> <hr/> <h4 id="parameterizing-the-naive-bayes-model"><strong>Parameterizing the Naive Bayes Model</strong></h4> <p>To make predictions, we need to parameterize the probabilities \(p(x_i \mid y)\) and \(p(y)\).</p> <p><strong><em>Why?</em></strong> Parameterizing these distributions allows us to learn the necessary values (e.g., \(\theta\)) from data in a structured way.</p> <h5 id="binary-features"><strong>Binary Features</strong></h5> <p>For simplicity, let us assume the features \(x_i\) are binary (\(x_i \in \{0, 1\}\)). We model \(p(x_i \mid y)\) as Bernoulli distributions:</p> \[p(x_i = 1 \mid y = 1) = \theta_{i,1}, \quad p(x_i = 1 \mid y = 0) = \theta_{i,0}\] <p>Similarly, the label distribution is modeled as:</p> \[p(y = 1) = \theta_0\] <p><strong><em>How do we arrive at these definitions?</em></strong> These definitions arise from the following assumptions and modeling principles:</p> <ol> <li><strong>Binary Nature of Features</strong>: Since the features \(x_i\) are binary (\(x_i \in \{0, 1\}\)), we need a probability distribution that models the likelihood of binary outcomes. The Bernoulli distribution is a natural choice for this.</li> <li><strong>Parameterization with Bernoulli Distributions</strong>: <ul> <li>For \(p(x_i \mid y)\), the Bernoulli distribution models the probability that \(x_i = 1\) for each possible value of \(y\).</li> <li>We introduce parameters \(\theta_{i,1}\) and \(\theta_{i,0}\), which represent the probability of \(x_i = 1\) given \(y = 1\) and \(y = 0\), respectively.</li> </ul> </li> <li><strong>Label Distribution \(p(y)\)</strong>: <ul> <li>The label \(y\) is also binary (\(y \in \{0, 1\}\)), so we model \(p(y)\) using a Bernoulli distribution with a single parameter \(\theta_0\), where \(\theta_0 = p(y = 1)\).</li> <li>This parameter reflects the prior probability of the positive class.</li> </ul> </li> <li><strong>Learning from Data</strong>: These parameters (\(\theta_{i,1}, \theta_{i,0}, \theta_0\)) are learned from data using methods like Maximum Likelihood Estimation (MLE), ensuring that the model reflects the observed distribution of features and labels in the dataset.</li> </ol> <p>Thus, the definitions provide a straightforward and interpretable way to model binary features and labels within the Naive Bayes framework.</p> <p>With these definitions, the joint probability \(p(x, y)\) can be written as (<strong>with NB assumption</strong>):</p> \[p(x, y) = p(y) \prod_{i=1}^d p(x_i \mid y)\] <p>Substituting the probabilities for binary features:</p> \[p(x, y) = p(y) \prod_{i=1}^d \theta_{i,y}{\mathbb{I}\{x_i = 1\}} + (1 - \theta_{i,y}){\mathbb{I}\{x_i = 0\}}\] <p>Here, \(\mathbb{I}\{\text{condition}\}\) is an indicator function that evaluates to 1 if the condition is true and 0 otherwise.</p> <p><strong><em>How to intuitively understand this equation?</em></strong> This equation represents the joint probability \(p(x, y)\) by combining the prior probability \(p(y)\) with the product of the individual probabilities \(p(x_i \mid y)\) for each feature \(x_i\). Here:</p> <ol> <li>For each feature \(x_i\), the term \(\mathbb{I}\{x_i = 1\}\) ensures that the corresponding parameter \(\theta_{i,y}\) is used if \(x_i = 1\), while \(\mathbb{I}\{x_i = 0\}\) ensures that \((1 - \theta_{i,y})\) is used if \(x_i = 0\).</li> <li>The product \(\prod_{i=1}^d\) combines the contributions of all features under the Naive Bayes assumption of conditional independence.</li> <li>Finally, multiplying by \(p(y)\) incorporates the prior belief about the label \(y\), providing the full joint distribution \(p(x, y)\).</li> </ol> <p>By this decomposition, we can efficiently compute \(p(x, y)\) for classification tasks.</p> <hr/> <h4 id="learning-parameters-with-maximum-likelihood-estimation-mle"><strong>Learning Parameters with Maximum Likelihood Estimation (MLE)</strong></h4> <p>The parameters \(\theta\) of the Naive Bayes model are learned by maximizing the likelihood of the observed data. Given a dataset of \(N\) labeled examples \(\{(x^{(n)}, y^{(n)})\}_{n=1}^N\), the likelihood of the data is:</p> \[\prod_{n=1}^N p_\theta(x^{(n)}, y^{(n)})\] <p>Taking the logarithm of the likelihood to simplify optimization, we obtain the log-likelihood:</p> \[\ell(\theta) = \sum_{n=1}^N \log p_\theta(x^{(n)}, y^{(n)})\] <p>For binary features, substituting the joint probability \(p_\theta(x, y)\) (as defined earlier) gives:</p> \[\ell(\theta) = \sum_{n=1}^N \left[ \sum_{i=1}^d \log ( \mathbb{I}\{x_i^{(n)} = 1\} \theta_{i,y^{(n)}} + \mathbb{I}\{x_i^{(n)} = 0\} (1 - \theta_{i,y^{(n)}}) ) \right] + \log p_\theta(y^{(n)})\] <p>Focusing on a specific feature \(x_j\) and label \(y = 1\), the relevant portion of the log-likelihood is:</p> \[\ell(\theta) = \sum_{n=1}^N \log \left[ \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\} \theta_{j,1} + \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\} (1 - \theta_{j,1}) \right] \tag{1}\] <p><strong>Step 1: Derivative of the Log-Likelihood</strong></p> <p>Taking the derivative of the log-likelihood with respect to \(\theta_{j,1}\):</p> \[\frac{\partial \ell}{\partial \theta_{j,1}} = \sum_{n=1}^N \left[ \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}}{\theta_{j,1}} - \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\}}{1 - \theta_{j,1}} \right] \tag{2}\] <p><strong>Did you follow the derivative?</strong> You might be wondering how derivative \(\log(a + b)\) can be written as \(\frac{1}{a} + \frac{1}{b}\), right? If so, that’s a completely valid question — I had the same thought myself. Here’s the explanation.</p> <p>The transition from equation (1) to equation (2) involves taking the derivative of the log-likelihood with respect to \(\theta_{j,1}\). Let’s break it down:</p> \[\frac{\partial}{\partial \theta_{j,1}} \ell = \frac{\partial}{\partial \theta_{j,1}} \sum_{n=1}^{N} \left[ \log \left( \theta_{j,1} I\{ x_j(n) = 1 \} + (1 - \theta_{j,1}) I\{ x_j(n) = 0 \} \right) \right]\] <p>Here, the derivative is applied to the logarithm term. Using the chain rule, we first compute the derivative of the logarithm, which is:</p> \[\frac{\partial}{\partial \theta_{j,1}} \log(f(\theta_{j,1})) = \frac{1}{f(\theta_{j,1})} \cdot \frac{\partial f(\theta_{j,1})}{\partial \theta_{j,1}},\] <p>where</p> \[f(\theta_{j,1}) = \theta_{j,1} I\{ x_j(n) = 1 \} + (1 - \theta_{j,1}) I\{ x_j(n) = 0 \}.\] <p>For a single \(n\), the term inside the logarithm is:</p> \[f(\theta_{j,1}) = \begin{cases} \theta_{j,1}, &amp; \text{if } x_j(n) = 1, \\ 1 - \theta_{j,1}, &amp; \text{if } x_j(n) = 0. \end{cases}\] <p>The derivative of \(f(\theta_{j,1})\) with respect to \(\theta_{j,1}\) is:</p> \[\frac{\partial f(\theta_{j,1})}{\partial \theta_{j,1}} = \begin{cases} 1, &amp; \text{if } x_j(n) = 1, \\ -1, &amp; \text{if } x_j(n) = 0. \end{cases}\] <p>Using the chain rule:</p> \[\frac{\partial}{\partial \theta_{j,1}} \log(f(\theta_{j,1})) = \begin{cases} \frac{1}{\theta_{j,1}}, &amp; \text{if } x_j(n) = 1, \\ \frac{-1}{1 - \theta_{j,1}}, &amp; \text{if } x_j(n) = 0. \end{cases}\] <p>Applying this to the summation over \(N\):</p> \[\frac{\partial}{\partial \theta_{j,1}} \ell = \sum_{n=1}^{N} \left[ I\{ y(n) = 1 \land x_j(n) = 1 \} \frac{1}{\theta_{j,1}} - I\{ y(n) = 1 \land x_j(n) = 0 \} \frac{1}{1 - \theta_{j,1}} \right].\] <p>This is exactly what equation (48) represents, showing the decomposition of the derivative into two terms for \(x_j(n) = 1\) and \(x_j(n) = 0\).</p> <p>The simplification uses the indicator functions \(I\) to select the appropriate cases, where:</p> \[I\{ x_j(n) = 1 \} \quad \text{contributes} \quad \frac{1}{\theta_{j,1}},\] <p>and</p> \[I\{ x_j(n) = 0 \} \quad \text{contributes} \quad \frac{1}{1 - \theta_{j,1}}.\] <p><strong>Key Insight:</strong></p> <p>At each step of the derivation, we are dealing with a <strong>single term</strong> inside the logarithm. As a result, when we take the derivative of the logarithm, the result is simply \(\frac{1}{\text{term}}\), where the term is either \(\theta_{j,1}\) or \(1 - \theta_{j,1}\), depending on the value of \(x_j(n)\).</p> <ul> <li>If \(x_j(n) = 1\), the term inside the log is \(\theta_{j,1}\), and its derivative is \(\frac{1}{\theta_{j,1}}\).</li> <li>If \(x_j(n) = 0\), the term inside the log is \(1 - \theta_{j,1}\), and its derivative is \(\frac{-1}{1 - \theta_{j,1}}\).</li> </ul> <p>Thus, at each \(n\), we compute the derivative as \(\frac{1}{\text{term}}\), with the specific term depending on the value of \(x_j(n)\). This makes the process more straightforward as we apply it term by term across all \(N\) data points.</p> <p>I hope that makes sense now. Let’s continue.</p> <p><strong>Step 2: Setting the Derivative to Zero</strong></p> <p>To find the maximum likelihood estimate, we set the derivative to zero:</p> \[\sum_{n=1}^N \left[ \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}}{\theta_{j,1}} \right] = \sum_{n=1}^N \left[ \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\}}{1 - \theta_{j,1}} \right]\] <p>Simplifying:</p> \[\theta_{j,1} \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \} = \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}\] <p>The above simplification is quite straightforward. I encourage you to write it out for yourself and work through the steps. Simply <strong>multiply both sides</strong> by \(\theta_{j,1}(1 - \theta_{j,1})\) to eliminate the denominators, then expand both sides and <strong>isolate</strong> \(\theta_{j,1}\).</p> <p><strong>Note</strong>:</p> \[\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\} + \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\} = \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \}\] <p><strong>Step 3: Solving for \(\theta_{j,1}\)</strong></p> <p>Rearranging to isolate \(\theta_{j,1}\):</p> \[\theta_{j,1} = \frac{\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}}{\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1\}}\] <p><strong>Interpretation:</strong> This estimate corresponds to the fraction of examples with \(y = 1\) in which the \(j\)-th feature \(x_j\) is active (i.e., \(x_j = 1\)). Intuitively, it represents the conditional probability \(p(x_j = 1 \mid y = 1)\) under the Naive Bayes assumption.</p> <hr/> <h5 id="next-steps"><strong>Next Steps:</strong></h5> <ol> <li> <p><strong>Compute the other \(\theta_{i,y}\) values</strong>: You should calculate the parameters for all other features in the model (for example, \(\theta_{i,0}\) and \(\theta_{i,1}\) for binary features). These values represent the probability of a feature given a class, so you’ll continue by maximizing the likelihood for each \(i\) and class \(y\) to estimate these parameters.</p> </li> <li> <p><strong>Estimate \(p(y)\)</strong>: You’ll also need to compute the class prior probability \(p(y)\), which is simply the proportion of each class in the training data. This can be done by counting how many times each class label appears and normalizing by the total number of examples.</p> </li> </ol> \[\theta_0 = \frac{\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1\}}{N}\] <ul> <li>\(\theta_0\) is the proportion of samples in the dataset that belong to the class \(y = 1\).</li> <li>It serves as the prior probability of \(y = 1\).</li> </ul> <p><strong>Substituting the Probabilities for Binary Features:</strong></p> <p>The likelihood of the joint probability \(p(x, y)\) can be expressed as:</p> \[p(x, y) = p(y) \prod_{i=1}^d \theta_{i,y} {\mathbb{I}\{x_i = 1\}} + (1 - \theta_{i,y}) {\mathbb{I}\{x_i = 0\}}\] <p>Where \(\mathbb{I}\{x_i = 1\}\) is an indicator function that equals 1 when \(x_i = 1\), and \(\mathbb{I}\{x_i = 0\}\) equals 1 when \(x_i = 0\).</p> <p><strong>Remember this equation; it’s the one we started with.</strong></p> <p>Once all parameters are estimated, you will have a fully parameterized Naive Bayes model. The model can then be used for prediction by computing the posterior probabilities for each class \(y\) given an input \(x\). For prediction, you would use the formula:</p> \[\hat{y} = \arg\max_{y \in Y} p(y) \prod_{i=1}^d p(x_i \mid y)\] <p>Where \(p(x_i \mid y)\) are the feature likelihoods, and \(p(y)\) is the class prior. The class with the highest posterior probability is chosen as the predicted label. This approach allows Naive Bayes to make efficient, probabilistic predictions based on the learned parameters.</p> <p><strong>So, the fundamental idea is:</strong></p> <p>You are estimating the parameters \(\theta\) for all possible features and classes. Once the parameters are learned, you apply Bayes’ rule to compute the posterior probability for each class \(y\). Finally, you take the class that maximizes the posterior probability using:</p> \[\hat{y} = \arg\max_y p(y \mid x)\] <p>This gives you the predicted class \(\hat{y}\), based on the learned parameters from the training data.</p> <hr/> <h5 id="recipe-for-learning-a-naive-bayes-model"><strong>Recipe for Learning a Naive Bayes Model:</strong></h5> <ol> <li><strong>Choose \(p(x_i \mid y)\)</strong>: Select an appropriate distribution for the features, e.g., Bernoulli distribution for binary features \(x_i\).</li> <li><strong>Choose \(p(y)\)</strong>: Typically, use a categorical distribution for the class labels.</li> <li><strong>Estimate Parameters by MLE</strong>: Use Maximum Likelihood Estimation (MLE) to estimate the parameters, following the same strategy used in conditional models.</li> </ol> <h5 id="where-do-we-go-from-here"><strong>Where Do We Go From Here?</strong></h5> <p>So far, we have focused on modeling binary features. However, many real-world datasets involve continuous features. How can Naive Bayes be extended to handle such cases? In the next blog, we’ll explore Naive Bayes for continuous features and see how this simple model adapts to more complex data types. See you there!</p> ]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.]]></summary></entry><entry><title type="html">Generalized Linear Models Explained - Leveraging MLE for Regression and Classification</title><link href="https://monishver11.github.io/blog/2025/MLE/" rel="alternate" type="text/html" title="Generalized Linear Models Explained - Leveraging MLE for Regression and Classification"/><published>2025-01-18T15:45:00+00:00</published><updated>2025-01-18T15:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/MLE</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/MLE/"><![CDATA[<p>When building machine learning models, one of the most important tasks is estimating the parameters of a model in a way that best explains the observed data. This is where the principle of <strong>Maximum Likelihood Estimation (MLE)</strong> comes into play. MLE provides a rigorous framework for parameter estimation, grounded in probability theory, and is widely used across regression, classification, and beyond.</p> <p>Suppose we have a probabilistic model and a dataset \(D\). The central question is: how do we estimate the parameters \(\theta\) of the model? According to MLE, we should choose \(\theta\) to maximize the likelihood of the observed data. Formally, the likelihood function is defined as:</p> \[L(\theta) \stackrel{\text{def}}{=} p(D; \theta),\] <p>which captures how likely the dataset \(D\) is, given the model parameters \(\theta\).</p> <p>If the dataset consists of \(N\) independent and identically distributed (iid) examples, the likelihood simplifies to a product of individual data likelihoods:</p> \[L(\theta) = \prod_{n=1}^{N} p\left(y^{(n)} \mid x^{(n)}; \theta\right)\] <p>While this expression is mathematically correct, the product of many probabilities can be unwieldy. To simplify the computation, we typically work with the <strong>log-likelihood</strong>, \(\ell(\theta)\), which is simply the natural logarithm of the likelihood:</p> \[\ell(\theta) \stackrel{\text{def}}{=} \log L(\theta) = \sum_{n=1}^{N} \log p\left(y^{(n)} \mid x^{(n)}; \theta\right)\] <p>Maximizing \(\ell(\theta)\) is equivalent to maximizing \(L(\theta)\), as the logarithm is a monotonic function. Alternatively, minimizing the <strong>negative log-likelihood (NLL)</strong> is a common approach, as it frames the problem as a minimization task.</p> <hr/> <h4 id="mle-for-linear-regression"><strong>MLE for Linear Regression</strong></h4> <p>To make these concepts more concrete, let’s see how MLE applies to a simple and widely known model: linear regression.</p> <p>In linear regression, we assume the output \(Y\) is conditionally Gaussian given the input \(X\). Specifically,</p> \[Y \mid X = x \sim \mathcal{N}(\theta^\top x, \sigma^2),\] <p>where \(\theta^\top x\) is the mean and \(\sigma^2\) is the variance of the Gaussian distribution.</p> <p>The log-likelihood for this model can be written as:</p> \[\ell(\theta) \stackrel{\text{def}}{=} \log L(\theta) = \log \prod_{n=1}^{N} p\left(y^{(n)} \mid x^{(n)}; \theta\right) = \sum_{n=1}^{N} \log p\left(y^{(n)} \mid x^{(n)};\theta\right)\] <p>Substituting the Gaussian probability density function into the equation, we get:</p> \[\ell(\theta) = \sum_{n=1}^{N} \log \left( \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\frac{\left(y^{(n)} - \theta^\top x^{(n)}\right)^2}{2\sigma^2} \right) \right)\] <p>Simplifying further, the log-likelihood becomes:</p> \[\ell(\theta) = N \log \frac{1}{\sqrt{2\pi \sigma^2}} - \frac{1}{2\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right)^2\] <p>Notice that the first term, \(N \log \frac{1}{\sqrt{2\pi \sigma^2}}\), is independent of \(\theta\). This means that to maximize \(\ell(\theta)\), we only need to minimize the second term, which is proportional to the sum of squared residuals.</p> <p>This brings us to an important insight: <strong>maximizing the log-likelihood in linear regression is equivalent to minimizing the squared error.</strong></p> <h5 id="deriving-the-gradient-of-the-log-likelihood"><strong>Deriving the Gradient of the Log-Likelihood</strong></h5> <p>To find the parameters that maximize the log-likelihood, we compute its gradient with respect to \(\theta\) and set it to zero. From our earlier expression for \(\ell(\theta)\):</p> \[\ell(\theta) = N \log \frac{1}{\sqrt{2\pi \sigma^2}} - \frac{1}{2\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right)^2,\] <p>the gradient with respect to \(\theta_i\), the \(i\)-th parameter, is:</p> \[\frac{\partial \ell}{\partial \theta_i} = -\frac{1}{\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right) x_i^{(n)}\] <p>Setting \(\frac{\partial \ell}{\partial \theta_i} = 0\) gives us the familiar <strong>normal equations</strong> for linear regression, which are typically solved to find the optimal \(\theta\).</p> <p>This yields:</p> \[-\frac{1}{\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right) x_i^{(n)} = 0\] <p>Multiplying both sides by \(\sigma^2\) gives:</p> \[\sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right) x_i^{(n)} = 0\] <p>We can write the equation in matrix form as:</p> \[\mathbf{X}^\top \left( \mathbf{y} - \mathbf{X} \theta \right) = 0\] <p>Rewriting the equation:</p> \[\mathbf{X}^\top \mathbf{y} - \mathbf{X}^\top \mathbf{X} \theta = 0\] <p>Rearranging gives the normal equation:</p> \[\mathbf{X}^\top \mathbf{X} \theta = \mathbf{X}^\top \mathbf{y}\] <p>Solving for \(\theta\):</p> \[\theta = \left( \mathbf{X}^\top \mathbf{X} \right)^{-1} \mathbf{X}^\top \mathbf{y}\] <p>Through this derivation, we’ve established a key connection between the probabilistic interpretation of linear regression and the classical squared error minimization. The principle of MLE not only provides a mathematically grounded way to estimate parameters but also reveals the assumptions underlying different models.</p> <p>What’s fascinating is that this approach generalizes beyond regression. For instance, in classification tasks, MLE leads to the cross-entropy loss. This will be the focus of the next section, where we’ll explore how MLE ties into classification problems and the role of log-loss in optimizing model parameters.</p> <hr/> <h4 id="from-linear-to-logistic-regression-expanding-the-scope-of-mle"><strong>From Linear to Logistic Regression: Expanding the Scope of MLE</strong></h4> <p>In the previous section, we explored how the Maximum Likelihood Estimation (MLE) principle naturally connects with linear regression. We saw that linear regression assumes the target \(Y \vert X = x\) follows a Gaussian distribution, and maximizing the likelihood aligns with minimizing the squared loss.</p> <p>But is the Gaussian assumption always valid? Not necessarily. For example, in classification tasks where \(Y\) takes on discrete values (e.g., 0 or 1), assuming a Gaussian distribution is inappropriate. This raises an important question: <strong>can we use the same MLE-based modeling approach for tasks beyond regression?</strong></p> <p>The answer is yes, and this brings us to <strong>logistic regression</strong>, which is tailored for classification tasks.</p> <h4 id="logistic-regression-assumptions-and-foundations"><strong>Logistic Regression: Assumptions and Foundations</strong></h4> <p>Consider a binary classification problem where the target \(Y \in \{0, 1\}\). What should the conditional distribution of \(Y\) given \(X = x\) look like? For logistic regression, we model \(p(y \mid x)\) using a <strong>Bernoulli distribution</strong>:</p> \[p(y \mid x) = h(x)^y (1 - h(x))^{1-y},\] <p>where \(h(x) \in (0, 1)\) represents the probability \(p(y = 1 \mid x)\).</p> <h5 id="parameterizing-hx"><strong>Parameterizing \(h(x)\):</strong></h5> <p>In linear regression, the mean \(\mathbb{E}[Y \mid X = x]\) was parameterized as \(\theta^\top x\). However, for classification, \(h(x)\) must map the linear predictor \(\theta^\top x\) (which lies in \(\mathbb{R}\)) to the interval \((0, 1)\). To achieve this, we use the <strong>logistic function</strong>:</p> \[f(\eta) = \frac{1}{1 + e^{-\eta}}, \quad \text{where } \eta = \theta^\top x\] <p>Thus, the probability \(p(y \mid x)\) becomes:</p> \[p(y \mid x) = \text{Bernoulli}(f(\theta^\top x)),\] <p>or equivalently:</p> \[p(y = 1 \mid x) = f(\theta^\top x), \quad p(y = 0 \mid x) = 1 - f(\theta^\top x)\] <p>[any reason why bernoulli?]</p> <p><strong>Why do we use the Bernoulli distribution in logistic regression?</strong></p> <p>In logistic regression, the target variable \(Y\) is binary, taking values in \(\{0, 1\}\), making the <strong>Bernoulli distribution</strong> a natural choice. The Bernoulli distribution models the probability of success (1) or failure (0) in a single trial.</p> <p>We model the conditional probability \(p(y \mid x)\) using the logistic function, which ensures that the predicted probabilities lie in the interval \((0, 1)\):</p> \[p(y = 1 \mid x) = f(\theta^\top x), \quad p(y = 0 \mid x) = 1 - f(\theta^\top x),\] <p>where \(f(\eta) = \frac{1}{1 + e^{-\eta}}\) is the logistic function. The Bernoulli distribution is then used to model the binary outcomes given these probabilities.</p> <h5 id="interpreting-the-logistic-function"><strong>Interpreting the Logistic Function</strong></h5> <p>The logistic function is smooth and monotonically increasing, mapping any real-valued input to a value in the range \((0, 1)\). It has the characteristic “S-shape” and is particularly useful for modeling probabilities.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Logistic_Fn-480.webp 480w,/assets/img/Logistic_Fn-800.webp 800w,/assets/img/Logistic_Fn-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Logistic_Fn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Logistic_Fn" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>One interesting property of the logistic function is its connection to the <strong>log-odds</strong>. For logistic regression:</p> \[\log \frac{p(y = 1 \mid x)}{p(y = 0 \mid x)} = \theta^\top x\] <p>This shows that the log-odds (or logit) of \(y = 1\) depend linearly on the input \(x\). Moreover, the decision boundary, where \(p(y = 1 \mid x) = p(y = 0 \mid x) = 0.5\), is defined by \(\theta^\top x = 0\), making it a <strong>linear decision boundary</strong>.</p> <h5 id="mle-for-logistic-regression"><strong>MLE for Logistic Regression</strong></h5> <p>As with linear regression, the parameters \(\theta\) in logistic regression are estimated by maximizing the conditional log-likelihood:</p> \[\ell(\theta) = \sum_{n=1}^{N} \log p\left(y^{(n)} \mid x^{(n)}; \theta\right)\] <p>For a Bernoulli-distributed \(y\), substituting \(p(y \mid x)\):</p> \[\ell(\theta) = \sum_{n=1}^{N} \left[ y^{(n)} \log f(\theta^\top x^{(n)}) + (1 - y^{(n)}) \log (1 - f(\theta^\top x^{(n)})) \right]\] <p>Unlike linear regression, this log-likelihood does not have a closed-form solution for \(\theta\). However, it is <strong>concave</strong>, meaning that optimization techniques like gradient ascent can efficiently find the unique optimal solution.</p> <h5 id="gradient-ascent-for-logistic-regression"><strong>Gradient Ascent for Logistic Regression</strong></h5> <p>The gradient of the log-likelihood \(\ell(\theta)\) with respect to \(\theta_i\) (the \(i\)-th parameter) is given by:</p> \[\frac{\partial \ell}{\partial \theta_i} = \sum_{n=1}^{N} \left( y^{(n)} - f(\theta^\top x^{(n)}) \right) x_i^{(n)}\] <p><strong>Derivation of the above form:</strong></p> <p>Math Review: Chain Rule</p> <p>If \(z\) depends on \(y\), which itself depends on \(x\), e.g., \(z = (y(x))^2\), then:</p> \[\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}\] <p>Likelihood for a Single Example:</p> \[\ell^n = y^{(n)} \log f(\theta^\top x^{(n)}) + (1 - y^{(n)}) \log(1 - f(\theta^\top x^{(n)}))\] <p>The gradient with respect to \(\theta_i\) is:</p> \[\frac{\partial \ell^n}{\partial \theta_i} = \frac{\partial \ell^n}{\partial f^n} \frac{\partial f^n}{\partial \theta_i}\] <p>Using the chain rule:</p> \[= \left(\frac{y^{(n)}}{f^n} - \frac{1 - y^{(n)}}{1 - f^n}\right) \frac{\partial f^n}{\partial \theta_i}\] <p>Simplify:</p> \[= \left(\frac{y^{(n)}}{f^n} - \frac{1 - y^{(n)}}{1 - f^n}\right) \left(f^n (1 - f^n) x_i^{(n)}\right)\] \[= \left(y^{(n)} - f^n\right) x_i^{(n)}.\] <p>The full gradient is thus:</p> \[\frac{\partial \ell}{\partial \theta_i} = \sum_{n=1}^{N} \left(y^{(n)} - f(\theta^\top x^{(n)})\right) x_i^{(n)}.\] <p>This gradient looks strikingly similar to that of linear regression, except for the presence of the logistic function \(f(\cdot)\).</p> <p>Using this gradient, we iteratively update the parameters using gradient ascent:</p> \[\theta \leftarrow \theta + \alpha \nabla_\theta \ell(\theta),\] <p>where \(\alpha\) is the learning rate.</p> <p><strong>Note the distinction:</strong> since the function is concave, we apply gradient ascent rather than descent.</p> <h5 id="a-comparison-linear-vs-logistic-regression"><strong>A Comparison: Linear vs Logistic Regression</strong></h5> <p>Here’s a side-by-side comparison to highlight the similarities and differences:</p> <hr/> <table> <thead> <tr> <th>Feature</th> <th>Linear Regression</th> <th>Logistic Regression</th> </tr> </thead> <tbody> <tr> <td>Input combination</td> <td>\(\theta^\top x\) (linear)</td> <td>\(\theta^\top x\) (linear)</td> </tr> <tr> <td>Output</td> <td>Real-valued</td> <td>Categorical (0 or 1)</td> </tr> <tr> <td>Conditional distribution</td> <td>Gaussian</td> <td>Bernoulli</td> </tr> <tr> <td>Transfer function \(f(\theta^\top x)\)</td> <td>Identity</td> <td>Logistic</td> </tr> <tr> <td>Mean \(\mathbb{E}[Y \mid X = x; \theta]\)</td> <td>\(f(\theta^\top x)\)</td> <td>\(f(\theta^\top x)\)</td> </tr> </tbody> </table> <hr/> <p>The main difference lies in the conditional distribution of \(Y\) and the transfer function \(f(\cdot)\), which maps \(\theta^\top x\) to the appropriate range for each model.</p> <h4 id="generalizing-logistic-regression"><strong>Generalizing Logistic Regression</strong></h4> <p>The principles behind logistic regression can be extended to handle other types of outputs, such as counts or probabilities for multiple classes. This generalization leads to the broader family of <strong>generalized linear models (GLMs)</strong>.</p> <h5 id="steps-for-generalized-regression-models"><strong>Steps for Generalized Regression Models</strong></h5> <ol> <li><strong>Task</strong>: Given \(x\), predict \(p(y \mid x)\).</li> <li><strong>Modeling</strong>: <ul> <li>Choose a parametric family of distributions \(p(y; \theta)\) with parameters \(\theta \in \Theta\).</li> <li>Choose a transfer function that maps a linear predictor in \(\mathbb{R}\) to \(\Theta\):</li> </ul> \[x \in \mathbb{R}^d \mapsto w^\top x \in \mathbb{R} \mapsto f(w^\top x) = \theta \in \Theta\] </li> <li> <p><strong>Learning</strong>: Use MLE to estimate the parameters:</p> \[\hat{\theta} = \arg\max_\theta \log p(D; \hat{\theta})\] </li> <li> <p><strong>Inference</strong>: For prediction, map \(x\) through the learned transfer function:</p> \[x \mapsto f(w^\top x)\] </li> </ol> <p>In the next section, we’ll dive deeper into these generalized models, exploring their flexibility and application to diverse prediction tasks.</p> <hr/> <h4 id="extending-generalized-linear-models-from-poisson-to-multinomial-logistic-regression"><strong>Extending Generalized Linear Models: From Poisson to Multinomial Logistic Regression</strong></h4> <p>In our journey through generalized linear models (GLMs), we’ve seen how logistic regression extends MLE principles to classification tasks. Now, let’s explore other use cases where GLMs shine, including <strong>Poisson regression</strong> for count-based predictions and <strong>multinomial logistic regression</strong> for multiclass classification.</p> <h4 id="example-poisson-regression"><strong>Example: Poisson Regression</strong></h4> <p>Imagine we want to predict the number of people entering a New York restaurant during lunchtime. What features could help? Time of day, day of the week, weather conditions, or nearby events might all be relevant. Importantly, the target variable \(Y\), representing the number of visitors, is a non-negative integer: \(Y \in \{0, 1, 2, \dots\}\).</p> <h5 id="why-use-the-poisson-distribution"><strong>Why Use the Poisson Distribution?</strong></h5> <p>The Poisson distribution is a natural choice for modeling count data. A random variable \(Y \sim \text{Poisson}(\lambda)\) has the probability mass function:</p> \[p(Y = k; \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k \in \{0, 1, 2, \dots\},\] <p>where \(\lambda &gt; 0\) is the rate parameter. The expected value \(\mathbb{E}[Y] = \lambda\), making \(\lambda\) both the mean and variance of \(Y\).</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Poisson_pmf.svg-480.webp 480w,/assets/img/Poisson_pmf.svg-800.webp 800w,/assets/img/Poisson_pmf.svg-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Poisson_pmf.svg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Poisson_pmf" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="what-does-the-poisson-distribution-mean-intuitively"><strong>What Does the Poisson Distribution Mean, Intuitively?</strong></h5> <p>The Poisson distribution can be understood through a simple analogy: imagine standing at a bus stop.</p> <ol> <li><strong>The Events</strong>: Each bus that arrives at the stop is an “event.”</li> <li><strong>Constant Rate</strong>: On average, buses arrive every 10 minutes, meaning we expect about 6 buses per hour. This average rate, \(\lambda = 6\), is constant.</li> <li><strong>Independence</strong>: The arrival of one bus doesn’t affect when the next one will come (events are independent).</li> </ol> <p>Now, if you wait at the bus stop for an hour, the Poisson distribution models the probability of seeing exactly 5, 6, or 7 buses in that time. While the average is 6 buses, randomness may cause the actual count to vary, with probabilities decreasing for more extreme deviations (e.g., 0 buses or 12 buses in an hour are unlikely).</p> <p>This shows how the Poisson distribution captures both the expected rate (\(\lambda\)) and the variability in the number of events.</p> <h5 id="constructing-the-poisson-regression-model"><strong>Constructing the Poisson Regression Model</strong></h5> <p>We assume \(Y \mid X = x \sim \text{Poisson}(\lambda)\). The challenge is to ensure \(\lambda\), the rate parameter, is positive. This is achieved using a transfer function \(f\):</p> \[x \mapsto w^\top x \quad \text{(linear predictor in } \mathbb{R} \text{)} \mapsto \lambda = f(w^\top x) \quad \text{(rate parameter in } (0, \infty) \text{)}.\] <p>The standard transfer function is the exponential function:</p> \[f(w^\top x) = e^{w^\top x}\] <h5 id="log-likelihood-for-poisson-regression"><strong>Log-Likelihood for Poisson Regression</strong></h5> <p>Given a dataset \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\), the log-likelihood is:</p> \[\log p(y_i; \lambda_i) = \left[y_i \log \lambda_i - \lambda_i - \log(y_i!)\right]\] \[\log p(D; w) = \sum_{i=1}^{n} \left[ y_i \log f(w^\top x_i) - f(w^\top x_i) - \log(y_i!) \right],\] <p>where \(f(w^\top x_i) = e^{w^\top x_i}\). Substituting \(\lambda_i\), we get:</p> \[\log p(D; w) = \sum_{i=1}^{n} \left[ y_i (w^\top x_i) - e^{w^\top x_i} - \log(y_i!) \right]\] <p>As with logistic regression, the likelihood is concave, so gradient-based methods can efficiently optimize \(w\).</p> <h5 id="gradient-for-poisson-regression"><strong>Gradient for Poisson Regression</strong></h5> <p>To optimize the log-likelihood, we compute its gradient with respect to the weight vector \(w\).</p> <p>The gradient is:</p> \[\frac{\partial \log p(D; w)}{\partial w} = \sum_{i=1}^{n} \left[ y_i x_i - e^{w^\top x_i} x_i \right]\] <p>Factoring out common terms, we get:</p> \[\frac{\partial \log p(D; w)}{\partial w} = \sum_{i=1}^{n} x_i \left[ y_i - e^{w^\top x_i} \right]\] <p>The gradient indicates the update direction for \(w\), with each term capturing the difference between the observed count \(y_i\) and the predicted count \(e^{w^\top x_i}\), weighted by the feature vector \(x_i\). Gradient ascent can then be used to maximize the log-likelihood, as the likelihood is concave for Poisson regression. Again, notice how similar the gradient is to the other problems we’ve explored so far—the transfer function differs in each case.</p> <hr/> <h4 id="example-multinomial-logistic-regression"><strong>Example: Multinomial Logistic Regression</strong></h4> <p>Next, let’s tackle multiclass classification, where the target \(Y \in \{1, 2, \dots, k\}\) spans multiple categories. Logistic regression’s Bernoulli distribution extends to the <strong>categorical distribution</strong>, which is parameterized by a probability vector \(\theta = (\theta_1, \dots, \theta_k)\). For valid probabilities:</p> \[\sum_{i=1}^{k} \theta_i = 1, \quad \theta_i \geq 0 \text{ for all } i.\] <p>For a given \(y \in \{1, \dots, k\}\):</p> \[p(y) = \theta_y.\] <h5 id="what-does-the-categorical-distribution-mean-intuitively"><strong>What Does the Categorical Distribution Mean, Intuitively?</strong></h5> <p>The categorical distribution assigns a probability to each class. For a given input \(x\), we want to predict the probability of each class \(y\) belonging to the target set. The probability of the class \(y\) is \(\theta_y\), where \(\theta_y\) is the component of the probability vector corresponding to the class \(y\). This allows us to perform multiclass classification by selecting the class with the highest probability.</p> \[p(y) = \theta_y.\] <h5 id="constructing-the-multinomial-logistic-regression-model"><strong>Constructing the Multinomial Logistic Regression Model</strong></h5> <p>The key idea in multinomial logistic regression is to compute a linear score for each class. For a given input vector \(x\), we compute a vector of scores \(s \in \mathbb{R}^k\) for all classes:</p> \[s = (w_1^\top x, \dots, w_k^\top x),\] <p>where \(w_i\) represents the weight vector associated with class \(i\). These scores are then transformed using the <strong>softmax function</strong> to produce valid probabilities. The softmax function is defined as:</p> \[\text{softmax}(s)_i = \frac{e^{s_i}}{\sum_{j=1}^{k} e^{s_j}} \quad \text{for } i = 1, \dots, k.\] <p>The softmax function ensures that the resulting probabilities form a valid probability distribution, satisfying:</p> \[\sum_{i=1}^{k} \theta_i = 1, \quad \theta_i \geq 0 \text{ for all } i.\] <h5 id="log-likelihood-for-multinomial-logistic-regression"><strong>Log-Likelihood for Multinomial Logistic Regression</strong></h5> <p>Given a dataset \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\), the log-likelihood of the model is the sum of the log probabilities for the true classes. The log-likelihood is given by:</p> \[\log p(D; W) = \sum_{i=1}^{n} \log \theta_{y_i},\] <p>where \(\theta_{y_i} = \text{softmax}(W^\top x_i)_{y_i}\). The parameters \(W\) are learned by maximizing the log-likelihood using gradient-based optimization methods.</p> <p><strong>Note:</strong> Don’t be misled by the use of theta and softmax notations. The way this is mentioned can be confusing. For now, let’s proceed with caution.</p> <h5 id="gradient-for-multinomial-logistic-regression"><strong>Gradient for Multinomial Logistic Regression</strong></h5> <p>To optimize the parameters \(W\), we compute the gradient of the log-likelihood with respect to \(W\).</p> <h6 id="step-by-step-derivation"><strong>Step-by-Step Derivation</strong></h6> <ol> <li> <p><strong>Log-Likelihood for Multinomial Logistic Regression:</strong></p> <p>The log-likelihood for the dataset \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\) is:</p> \[\log p(D; W) = \sum_{i=1}^{n} \log \theta_{y_i},\] <p>where \(\theta_{y_i} = \text{softmax}(W^\top x_i)_{y_i}\) is the predicted probability of the true class \(y_i\) for the \(i\)-th data point.</p> </li> <li> <p><strong>Softmax Function:</strong></p> <p>The softmax function for class \(j\) is given by:</p> \[\theta_j(x_i; W) = \frac{e^{w_j^\top x_i}}{\sum_{k=1}^{k} e^{w_k^\top x_i}}.\] </li> <li> <p><strong>Log-Likelihood Expansion:</strong></p> <p>Substituting the softmax expression into the log-likelihood:</p> \[\log p(D; W) = \sum_{i=1}^{n} \log \left( \frac{e^{w_{y_i}^\top x_i}}{\sum_{k=1}^{k} e^{w_k^\top x_i}} \right),\] <p>which simplifies to:</p> \[\log p(D; W) = \sum_{i=1}^{n} \left( w_{y_i}^\top x_i - \log \left( \sum_{k=1}^{k} e^{w_k^\top x_i} \right) \right).\] </li> <li> <p><strong>Gradient of the Log-Likelihood:</strong></p> <p>The gradient of the log-likelihood with respect to the weight vector \(w_j\) is computed by differentiating each term in the log-likelihood expression:</p> <ul> <li> <p>The derivative of the first term, \(w_{y_i}^\top x_i\), with respect to \(w_j\) is simply \(x_i\) when \(y_i = j\) and 0 otherwise.</p> </li> <li> <p>The second term involves the <strong>log-sum-exp</strong>, and its derivative with respect to \(w_j\) is:</p> \[\nabla_{w_j} \log \left( \sum_{k=1}^{k} e^{w_k^\top x_i} \right) = \theta_j(x_i; W) \cdot x_i.\] </li> </ul> </li> <li> <p><strong>Final Gradient Expression:</strong></p> <p>Combining the two terms, the gradient of the log-likelihood with respect to \(w_j\) is:</p> \[\nabla_{w_j} \log p(D; W) = \sum_{i=1}^{n} \left( \mathbf{1}_{\{y_i = j\}} - \theta_j(x_i; W) \right) x_i,\] <p>where \(\mathbf{1}_{\{y_i = j\}}\) is the indicator function that is 1 if the true class \(y_i\) equals \(j\), and 0 otherwise.</p> </li> </ol> <p>This gradient is used to adjust the weight vectors during training to improve the model’s predictions.</p> <hr/> <h4 id="review-recipe-for-conditional-models"><strong>Review: Recipe for Conditional Models</strong></h4> <p>GLMs provide a unified approach for constructing conditional models. Here’s a quick recipe:</p> <ol> <li><strong>Define the Input and Output Space</strong>: Start with the features \(x\) and target \(y\).</li> <li><strong>Choose an Output Distribution</strong>: Based on the nature of \(y\), select an appropriate probability distribution \(p(y \mid x; \theta)\).</li> <li><strong>Select a Transfer Function</strong>: Map the linear predictor \(w^\top x\) into the required range of the distribution parameters.</li> <li><strong>Optimize via MLE</strong>: Estimate \(\theta\) by maximizing the log-likelihood using gradient-based methods.</li> </ol> <p>This framework, called <strong>generalized linear models</strong>, can be adapted for a wide range of prediction tasks.</p> <hr/> <p>Before we wrap up, did we overlook something? Ah, yes—let’s revisit the log-sum-exp function and its significance.</p> <h4 id="log-sum-exp-function"><strong>Log-Sum-Exp Function</strong></h4> <p>The <strong>log-sum-exp</strong> (LSE) function is a mathematical expression used frequently in machine learning and statistics, particularly in contexts involving probabilities and normalization. It is defined as:</p> \[\text{LSE}(s) = \log \left( \sum_{k=1}^K e^{s_k} \right),\] <p>where \(s = (s_1, s_2, \dots, s_K)\) is a vector of real-valued scores.</p> <h5 id="key-properties-of-log-sum-exp"><strong>Key Properties of Log-Sum-Exp</strong></h5> <ol> <li> <p><strong>Smooth Maximum Approximation</strong><br/> The log-sum-exp function can be thought of as a “soft maximum” of the elements in \(s\) because, for large values of \(s_k\), the largest term dominates the sum:</p> \[\text{LSE}(s) \approx \max(s_k)\] </li> <li> <p><strong>Numerical Stability</strong><br/> To ensure numerical stability when computing \(e^{s_k}\) for large values of \(s_k\), the log-sum-exp function is often rewritten as:</p> \[\text{LSE}(s) = \log \left( \sum_{k=1}^K e^{s_k - \max(s)} \right) + \max(s),\] <p>where \(\max(s)\) is the maximum value in \(s\). This adjustment ensures that the exponentials remain within a manageable range. Check why it’s written this way in the referenced resource; I highly recommend it.</p> </li> </ol> <h5 id="log-sum-exp-in-multinomial-logistic-regression"><strong>Log-Sum-Exp in Multinomial Logistic Regression</strong></h5> <p>In multinomial logistic regression, the log-sum-exp term naturally arises when computing the log-likelihood. The predicted probabilities are computed using the softmax function:</p> \[\theta_j(x_i; W) = \frac{e^{w_j^\top x_i}}{\sum_{k=1}^{K} e^{w_k^\top x_i}}\] <p>The denominator of the softmax involves a sum of exponentials. Taking the logarithm of the denominator gives the log-sum-exp term:</p> \[\log \left( \sum_{k=1}^K e^{w_k^\top x_i} \right)\] <p>This term normalizes the probabilities so that they sum to 1 across all classes.</p> <h5 id="derivative-of-log-sum-exp"><strong>Derivative of Log-Sum-Exp</strong></h5> <p>The derivative of the log-sum-exp function with respect to a specific score \(s_j\) is:</p> \[\frac{\partial}{\partial s_j} \log \left( \sum_{k=1}^K e^{s_k} \right) = \frac{e^{s_j}}{\sum_{k=1}^K e^{s_k}}\] <p>This is equivalent to the probability assigned to class \(j\) by the softmax function:</p> \[\frac{\partial}{\partial s_j} \log \left( \sum_{k=1}^K e^{s_k} \right) = \text{softmax}(s)_j.\] <p>In the case of multinomial logistic regression, where \(s_j = w_j^\top x_i\), the derivative with respect to \(w_j\) is:</p> \[\nabla_{w_j} \log \left( \sum_{k=1}^K e^{w_k^\top x_i} \right) = \theta_j(x_i; W) \cdot x_i,\] <p>where \(\theta_j(x_i; W) = \text{softmax}(W^\top x_i)_j\) is the predicted probability for class \(j\).</p> <h5 id="still-why-the-use-of-log-sum-exp"><strong>Still, Why the Use of Log-Sum-Exp</strong></h5> <p>In multinomial logistic regression, we use the <strong>softmax function</strong> to convert the raw class scores (logits) into probabilities. The softmax function itself involves summing the exponentials of the scores, not the log-sum-exp.</p> <p>However, when calculating the <strong>log-likelihood</strong> of the model during optimization, we encounter the <strong>log-sum-exp</strong> function. The log-sum-exp is used in the log-likelihood to handle the sum of the exponentials in a numerically stable way. It’s primarily used to:</p> <ol> <li><strong>Avoid overflow and underflow</strong>: When working with large or small exponentiated values (as in the softmax function), exponentiation can cause numerical instability. The log-sum-exp helps stabilize these computations.</li> <li><strong>Ensure proper normalization</strong>: In the context of the softmax function, it ensures the sum of the probabilities is 1, making them valid probabilities for classification.</li> </ol> <p>In summary, while softmax uses the sum of exponentials, the <strong>log-sum-exp</strong> appears in the log-likelihood computation to stabilize the logarithmic transformation, enabling proper optimization.</p> <h5 id="analogy-for-the-log-sum-exp-function"><strong>Analogy for the Log-Sum-Exp Function</strong></h5> <p>Imagine you’re at a sports competition with several players, and their scores are exponentially amplified (think of \(e^{s_k}\) as the “hype” around each player’s performance). The log-sum-exp function acts like a judge summarizing all the scores into a single value that reflects the overall competition, but with a bias toward the top performers.</p> <ul> <li>The <strong>“log”</strong> compresses the scale, keeping the summary manageable.</li> <li>The <strong>“sum”</strong> captures the contributions of <em>all</em> players, not just the best one.</li> <li>The <strong>“exp”</strong> amplifies the impact of the highest scores, making it feel like a weighted average that leans toward the standout performers.</li> </ul> <p>In short, the <strong>log-sum-exp is like a “soft maximum”</strong>: it highlights the best, considers the rest, and ensures the result is stable and interpretable.</p> <p>If you’re having trouble with this analogy, go through the example in the reference. It’ll help clarify things.</p> <hr/> <p>Alright, it’s time to wrap this up. In the next section, we’ll dive into another type of probabilistic modeling: generative models. We’ll explore what they are and how they work. Stay tuned, and see you there!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://benhay.es/posts/exploring-distributions/">Exploring Probability Distributions</a></li> <li><a href="https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/">The Log-Sum-Exp Trick</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.]]></summary></entry><entry><title type="html">Unveiling Probabilistic Modeling</title><link href="https://monishver11.github.io/blog/2025/probabilistic-modeling/" rel="alternate" type="text/html" title="Unveiling Probabilistic Modeling"/><published>2025-01-17T04:51:00+00:00</published><updated>2025-01-17T04:51:00+00:00</updated><id>https://monishver11.github.io/blog/2025/probabilistic-modeling</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/probabilistic-modeling/"><![CDATA[<h4 id="why-probabilistic-modeling"><strong>Why Probabilistic Modeling?</strong></h4> <p>Probabilistic modeling offers a unified framework that underpins many machine learning methods, from linear regression to logistic regression and beyond. At its core, probabilistic modeling allows us to handle uncertainty and make informed decisions based on observed data. It provides a principled way to update our beliefs about the data-generating process as new information becomes available.</p> <p>In machine learning, we often think of learning as statistical inference, where the goal is to use data to draw conclusions about the underlying distribution or process that generated it, rather than simply fitting a model to the observed data. In this view, the goal is not just to fit a model to data, but to estimate the underlying distribution that best explains the observed data. Probabilistic methods give us a powerful tool to incorporate our beliefs about the world—often referred to as inductive biases—into the learning process. This allows us to make more informed predictions and gain deeper insights into the data.</p> <p>For example, in Bayesian inference, prior beliefs are combined with evidence from the data to update our understanding of the underlying process. This principled approach enables us to not only predict outcomes but also quantify our confidence in those predictions, making probabilistic modeling a powerful tool for developing robust, interpretable, and informed machine learning systems.</p> <h5 id="two-ways-of-generating-data"><strong>Two Ways of Generating Data</strong></h5> <p>When we think about how data is generated, there are two main perspectives to consider. The first is through <strong>conditional models</strong>, where we model the likelihood of the output \(y\) given the input \(x\). This is often denoted as \(p(y \vert x)\).</p> <p>The second perspective is through <strong>generative models</strong>, where we model the joint distribution of both the input \(x\) and the output \(y\), denoted as \(p(x, y)\).</p> <p>To understand the distinction between conditional models and generative models, let’s use the analogy of handwriting recognition:</p> <ul> <li> <p><strong>Conditional Models</strong>: Imagine you are given a handwritten letter and asked to identify the corresponding alphabet. Here, you focus only on the relationship between the handwriting (\(x\)) and the letter it represents (\(y\)). This corresponds to modeling \(p(y \mid x)\), where you predict the output \(y\) (the letter) conditioned on the input \(x\) (the handwriting). Essentially, you’re answering the question, <em>“What is the most likely letter given this handwriting?”</em></p> </li> <li> <p><strong>Generative Models</strong>: Now, imagine that instead of just recognizing handwriting, you also aim to generate realistic handwriting for any letter. To do this, you need to understand how the letters (\(y\)) and handwriting styles (\(x\)) are generated together. This involves modeling the joint distribution \(p(x, y)\), where you learn how inputs and outputs are related as part of a larger generative process. The question here becomes, <em>“How are handwriting (\(x\)) and letters (\(y\)) jointly produced?”</em></p> </li> </ul> <p>Each approach offers different advantages depending on the context. However, both share the common goal of estimating the parameters of the model, often using a technique called <strong>Maximum Likelihood Estimation (MLE)</strong>.</p> <hr/> <h4 id="conditional-models"><strong>Conditional Models</strong></h4> <p>Conditional models focus on predicting the output given the input. One of the most well-known and widely used conditional models is <strong>linear regression</strong>. Let’s take a closer look at linear regression and how it fits within this probabilistic framework.</p> <h4 id="linear-regression"><strong>Linear Regression</strong></h4> <p>Linear regression is a fundamental technique in both machine learning and statistics. Its primary goal is to predict a real-valued target \(y\) (also called the response variable) from a vector of features \(x\) (also known as covariates). Linear regression is often used in situations where we want to predict a continuous value, such as:</p> <ul> <li>Predicting house prices based on factors like location, condition, and age of the house.</li> <li>Estimating medical costs of a person based on their age, sex, region, and BMI.</li> <li>Predicting someone’s age from their photograph.</li> </ul> <h5 id="the-problem-setup"><strong>The Problem Setup</strong></h5> <p>In linear regression, we are given a set of training examples, \(D = \{(x^{(n)}, y^{(n)})\}_{n=1}^N\), where \(x^{(n)} \in \mathbb{R}^d\) represents the features and \(y^{(n)} \in \mathbb{R}\) represents the target. The task is to model the relationship between the features \(x\) and the target \(y\).</p> <p>To do this, we assume that there is a linear relationship between \(x\) and \(y\), which can be expressed as:</p> \[h(x) = \theta^T x = \sum_{i=0}^{d} \theta_i x_i\] <p>Here, \(\theta \in \mathbb{R}^d\) represents the parameters (also known as the weights) of the model, and \(x_0 = 1\) is the bias term. The goal is to find the values of \(\theta\) that best explain the observed data.</p> <blockquote> <p><em>“We use superscript to denote the example id and subscript to denote the dimension id”</em></p> </blockquote> <h5 id="unveiling-probabilistic-modeling"><strong>Unveiling Probabilistic Modeling</strong></h5> <p>To estimate the parameters \(\theta\), we use the <strong>least squares method</strong>, which involves minimizing the squared loss between the predicted and observed values. The loss function is defined as:</p> \[J(\theta) = \frac{1}{N} \sum_{n=1}^{N} \left( y^{(n)} - \theta^T x^{(n)} \right)^2\] <p>This function represents the <strong>empirical risk</strong>, which quantifies the difference between the predicted and actual values across all the training examples.</p> <h5 id="matrix-formulation"><strong>Matrix Formulation</strong></h5> <p>We can also express this problem in matrix form for efficiency. Let \(X \in \mathbb{R}^{N \times d}\) be the design matrix, whose rows represent the input features for each training example. Let \(y \in \mathbb{R}^N\) be the vector of all target values. The objective is to solve for the parameter vector \(\hat{\theta}\) that minimizes the loss:</p> \[\hat{\theta} = \arg\min_\theta \left( (X\theta - y)^T (X\theta - y) \right)\] <h5 id="closed-form-solution"><strong>Closed-Form Solution</strong></h5> <p>The closed-form solution to this optimization problem is:</p> \[\hat{\theta} = (X^T X)^{-1} X^T y\] <p>This gives us the values for \(\theta\) that minimize the squared loss, and hence, provide the best linear model for the data.</p> <hr/> <p>Before proceeding further, here are a few review questions. Ask yourself these and check the answers.</p> <h5 id="how-do-we-derive-the-solution-for-linear-regression"><strong>How do we derive the solution for linear regression?</strong></h5> <p>The squared loss function in matrix form is:</p> \[J(\theta) = \frac{1}{N} (X\theta - y)^T (X\theta - y)\] <p>To minimize \(J(\theta)\), we compute the gradient with respect to \(\theta\):</p> <ul> <li>Expand the quadratic term:</li> </ul> \[J(\theta) = \frac{1}{N} \left[ \theta^T X^T X \theta - 2y^T X \theta + y^T y \right]\] <ul> <li>Take the derivative with respect to \(\theta\): <ul> <li>Recall that for any vector \(a\), \(b\), and matrix \(A\), the following derivatives are useful: <ul> <li> \[\frac{\partial (a^T b)}{\partial a} = b\] </li> <li> \[\frac{\partial (a^T A a)}{\partial a} = 2A a\] </li> </ul> <p>(when \(A\) is symmetric).</p> </li> </ul> </li> </ul> <p>Applying these rules:</p> \[\nabla_\theta J(\theta) = \frac{1}{N} \left[ 2X^T X \theta - 2X^T y \right].\] <ul> <li>Set the gradient to zero to find the minimizer:</li> </ul> \[X^T X \theta = X^T y\] <ul> <li>Solve for \(\theta\):</li> </ul> \[\theta = (X^T X)^{-1} X^T y,\] <p>provided \(X^T X\) is invertible.</p> <p><strong>Note:</strong> The \(\frac{1}{N}\) normalization factor is constant and cancels out when setting the gradient to zero, so it does not affect the solution for \(\theta\).</p> <h5 id="why-do-transposes-appear-or-disappear"><strong>Why Do Transposes Appear or Disappear?</strong></h5> <ol> <li> <p><strong>Symmetry of Quadratic Terms</strong>:<br/> In the term \(\theta^T X^T X \theta\), note that \(X^T X\) is a symmetric matrix (because \(X^T X = (X^T X)^T\)). This symmetry ensures that when taking the derivative, we don’t need to explicitly add or remove transposes; they naturally align.</p> </li> <li><strong>Consistency of Vector-Matrix Multiplication</strong>:<br/> When differentiating terms like \(y^T X \theta\), we use the rule \(\frac{\partial (a^T b)}{\partial a} = b\), ensuring dimensions match. This often introduces or removes a transpose based on the structure of the derivative. For example: <ul> <li>\(\nabla_\theta (-2y^T X \theta) = -2X^T y\), where \(X^T\) arises naturally to align dimensions.</li> </ul> </li> <li><strong>Gradient Conventions</strong>:<br/> The transpose changes are necessary to ensure the resulting gradient is a column vector (matching \(\theta\)’s shape), as gradients are typically represented in the same dimensionality as the parameter being differentiated.</li> </ol> <h5 id="what-happens-if-xt-x-is-not-invertible"><strong>What happens if \(X^T X\) is not invertible?</strong></h5> <p>If \(X^T X\) is not invertible (also called singular or degenerate), the normal equations do not have a unique solution. This happens in cases such as:</p> <ul> <li><strong>Linearly dependent features</strong>: Some columns of \(X\) are linear combinations of others.</li> <li><strong>Too few data points</strong>: If \(N &lt; d\) (more features than samples), \(X^T X\) will not be full rank.</li> </ul> <p>To address this issue, we can:</p> <ol> <li><strong>Add regularization</strong>: Use techniques like Ridge Regression, which modifies the normal equation to include a penalty term:<br/> \(\theta = (X^T X + \lambda I)^{-1} X^T y,\)<br/> where \(\lambda &gt; 0\) is the regularization parameter.</li> <li><strong>Remove redundant features</strong>: Perform feature selection or dimensionality reduction (e.g., PCA) to eliminate linear dependencies.</li> <li><strong>Use pseudo-inverse</strong>: Compute the Moore-Penrose pseudo-inverse of \(X^T X\) to find a solution.</li> </ol> <hr/> <h4 id="understanding-linear-regression-through-a-probabilistic-lens"><strong>Understanding Linear Regression Through a Probabilistic Lens</strong></h4> <p>So far, we’ve discussed how linear regression can be understood as minimizing the squared loss. But why is the squared loss a reasonable choice for regression problems? To answer this, we need to think about the assumptions we are making on the data.</p> <p>Let’s approach linear regression from a <strong>probabilistic modeling perspective</strong>.</p> <h5 id="assumptions-in-linear-regression"><strong>Assumptions in Linear Regression</strong></h5> <p>In this framework, we assume that the target \(y\) and the features \(x\) are related through a linear function, with an added error term \(\epsilon\):</p> \[y = \theta^T x + \epsilon\] <p>Here, \(\epsilon\) represents the residual error that accounts for all unmodeled effects, such as noise or other sources of variation in the data. We assume that these errors \(\epsilon\) are independent and identically distributed (iid) and follow a normal distribution:</p> \[\epsilon \sim \mathcal{N}(0, \sigma^2)\] <p>Given this assumption, the conditional distribution of \(y\) given \(x\) is a normal distribution with mean \(\theta^T x\) and variance \(\sigma^2\):</p> \[p(y | x; \theta) = \mathcal{N}(\theta^T x, \sigma^2)\] <h5 id="intuition-behind-the-gaussian-distribution"><strong>Intuition Behind the Gaussian Distribution</strong></h5> <p>This distribution suggests that, for each value of \(x\), the output \(y\) is normally distributed around the value predicted by the linear model \(\theta^T x\), with a fixed variance \(\sigma^2\) that captures the uncertainty or noise in the data. In other words, we place a Gaussian “bump” around the output of the linear predictor, reflecting the uncertainty in our prediction.</p> <p>With this, we’ve laid the groundwork for our discussion on Maximum Likelihood Estimation.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>In this post, we introduced how probabilistic modeling can be used for understanding and estimating machine learning models, such as linear regression. By thinking of learning as statistical inference, we can incorporate our prior beliefs about the data-generating process and make more informed predictions.</p> <p>Next, we’ll dive into <strong>Maximum Likelihood Estimation (MLE)</strong> and examine how it can be applied to solve probabilistic linear regression and other machine learning algorithms. We’ll also explore how to formalize this understanding—stay tuned!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.]]></summary></entry><entry><title type="html">SVM Solution in the Span of the Data</title><link href="https://monishver11.github.io/blog/2025/svm-solution-span-of-data/" rel="alternate" type="text/html" title="SVM Solution in the Span of the Data"/><published>2025-01-16T18:13:00+00:00</published><updated>2025-01-16T18:13:00+00:00</updated><id>https://monishver11.github.io/blog/2025/svm-solution-span-of-data</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/svm-solution-span-of-data/"><![CDATA[<p>Previously, we explored the <strong>kernel trick</strong>, a powerful concept that allows Support Vector Machines (SVMs) to operate efficiently in high-dimensional feature spaces without explicitly computing the coordinates. Building on that foundation, we now turn our attention to an intriguing property of SVM solutions: they lie in the <strong>span of the data</strong>. This observation not only deepens our understanding of the connection between the dual and primal formulations of SVM but also provides a unifying perspective on how solutions in machine learning are inherently tied to the training data.</p> <hr/> <h4 id="svm-dual-problem-a-quick-recap"><strong>SVM Dual Problem: A Quick Recap</strong></h4> <p>To understand this property, let’s first revisit the SVM dual problem. It is formulated as:</p> \[\sup_{\alpha \in \mathbb{R}^n} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_j^T x_i\] <p>subject to the constraints:</p> <ol> <li> \[\sum_{i=1}^n \alpha_i y_i = 0\] </li> <li> \[\alpha_i \in [0, \frac{c}{n}], \quad i = 1, \dots, n\] </li> </ol> <p>Here, \(\alpha_i\) are the dual variables that correspond to the Lagrange multipliers, and \(c\) is the regularization parameter that controls the margin.</p> <p>The dual problem focuses on maximizing this quadratic function, which involves pairwise interactions between training samples. Once the optimal dual solution \(\alpha^*\) is obtained, it can be used to compute the primal solution as:</p> \[w^* = \sum_{i=1}^n \alpha^*_i y_i x_i\] <p>This equation reveals a critical insight: the primal solution \(w^*\) is expressed as a <strong>linear combination of the training inputs</strong> \(x_1, x_2, \dots, x_n\). This means that \(w^*\) is confined to the span of these inputs, or mathematically:</p> \[w^* \in \text{span}(x_1, \dots, x_n)\] <p>We refer to this phenomenon as “the SVM solution lies in the span of the data.” It underscores the dependency of \(w^*\) on the training data, aligning it with the geometric intuition of SVMs: the decision boundary is shaped by a subset of data points (the support vectors).</p> <hr/> <h4 id="ridge-regression-another-perspective-on-span-of-the-data"><strong>Ridge Regression: Another Perspective on Span of the Data</strong></h4> <p>Interestingly, this concept is not unique to SVMs. A similar property emerges in <strong>ridge regression</strong>, a linear regression method that incorporates \(\ell_2\) regularization to prevent overfitting. Let’s delve into this and see how the ridge regression solution also resides in the span of the data.</p> <h5 id="ridge-regression-objective"><strong>Ridge Regression Objective</strong></h5> <p>The objective function for ridge regression, with a regularization parameter \(\lambda &gt; 0\), is given by:</p> \[w^* = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \{ w^T x_i - y_i \}^2 + \lambda \|w\|_2^2\] <p>Here, \(w\) is the weight vector, \(x_i\) are the input data points, and \(y_i\) are the corresponding target values. The regularization term \(\lambda \|w\|_2^2\) penalizes large weights to improve generalization.</p> <h6 id="the-closed-form-solution"><strong>The Closed-Form Solutio</strong>n</h6> <p>The ridge regression problem has a closed-form solution:</p> \[w^* = \left( X^T X + \lambda I \right)^{-1} X^T y\] <p>where \(X\) is the design matrix (rows are \(x_1, \dots, x_n\)), and \(y\) is the vector of target values.</p> <p>At first glance, this expression might seem abstract. However, by rearranging it, we can show that the solution also lies in the span of the training data.</p> <h5 id="showing-the-span-property-for-ridge-regression"><strong>Showing the Span Property for Ridge Regression</strong></h5> <p>To reveal the span property, let’s rewrite the ridge regression solution. Using matrix algebra:</p> \[w^* = \left( X^T X + \lambda I \right)^{-1} X^T y\] <p>We can express \(w^*\) as:</p> \[w^* = X^T \left[ \frac{1}{\lambda} y - \frac{1}{\lambda} X w^* \right] \tag{1}\] <p>Now, define:</p> \[\alpha^* = \frac{1}{\lambda} y - \frac{1}{\lambda} X w^*\] <p>Substituting this back, we get:</p> \[w^* = X^T \alpha^*\] <p>Expanding further, it becomes:</p> \[w^* = \sum_{i=1}^n \alpha^*_i x_i\] <p>This clearly shows that the ridge regression solution \(w^*\) is also a linear combination of the training inputs. Thus, like SVMs, ridge regression solutions lie in the span of \(x_1, x_2, \dots, x_n\):</p> \[w^* \in \text{span}(x_1, \dots, x_n)\] <p>You may wonder how we arrived at this specific form for \(w^*\) at the start \((1)\). To understand this, we utilized the following lemma and a series of transformations to reframe it accordingly.</p> <h6 id="the-lemma-matrix-inverse-decomposition"><strong>The Lemma: Matrix Inverse Decomposition</strong></h6> <p>We use the following lemma:</p> <p>If \(A\) and \(A + B\) are non-singular, then:</p> \[(A + B)^{-1} = A^{-1} - A^{-1} B (A + B)^{-1}\] <p>This allows us to break down the inverse of a sum of matrices into manageable parts. Let’s apply it to our problem.</p> <h6 id="applying-the-lemma-to-ridge-regression"><strong>Applying the Lemma to Ridge Regression</strong></h6> <p>Let:</p> <ul> <li>\(A = \lambda I\) (scaled identity matrix),</li> <li>\(B = X^T X\) (Gram matrix).</li> </ul> <p>Substituting into the ridge regression solution:</p> \[w^* = (X^T X + \lambda I)^{-1} X^T y\] <p>Using the lemma, we expand the inverse:</p> \[w^* = \left( \lambda^{-1} - \lambda^{-1} X^T X (X^T X + \lambda I)^{-1} \right) X^T y\] <p>We simplify the terms step by step:</p> <ol> <li> <p><strong>Expand the first term:</strong></p> \[w^* = X^T \lambda^{-1} y - \lambda^{-1} X^T X (X^T X + \lambda I)^{-1} X^T y\] </li> <li> <p><strong>Notice the recursive structure:</strong></p> \[w^* = X^T \lambda^{-1} y - \lambda^{-1} X^T X w^*\] </li> <li> <p><strong>Rearrange to highlight the span of data:</strong></p> \[w^* = X^T \left( \frac{1}{\lambda} y - \frac{1}{\lambda} X w^* \right)\] </li> </ol> <h6 id="supporting-details"><strong>Supporting Details</strong></h6> <p>To solidify our understanding, here’s how the <strong>Matrix Sum Inverse Lemma</strong> is derived using the Woodbury identity:</p> <p><strong>The Woodbury Identity:</strong></p> \[(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}\] <p>Substituting:</p> <ul> <li>\(C = I\), \(V = I\), and \(U = B\), we get:</li> </ul> \[(A + B)^{-1} = A^{-1} - A^{-1} B (I + A^{-1} B)^{-1} A^{-1}\] <p>Simplify:</p> \[(A + B)^{-1} = A^{-1} - A^{-1} B (A (I + A^{-1} B))^{-1}\] \[(A + B)^{-1} = A^{-1} - A^{-1} B (A + B)^{-1}\] <p>This completes the proof of the lemma and justifies its use in our derivation.</p> <h5 id="core-takeaway"><strong>Core Takeaway:</strong></h5> <p>Both SVMs and ridge regression share the property that their solutions lie in the span of the training data. For SVMs, this emerges naturally from the dual-primal connection, highlighting how support vectors define the decision boundary. In ridge regression, the span property arises through matrix algebra and the closed-form solution.</p> <p>This unifying view provides a deeper understanding of how machine learning models leverage training data to construct solutions. Next, we’ll explore <strong>how this property influences kernelized methods</strong> and its implications for scalability and interpretability in machine learning.</p> <hr/> <h4 id="reparameterizing-optimization-problems-building-on-the-span-property"><strong>Reparameterizing Optimization Problems: Building on the Span Property</strong></h4> <p>In the previous section, we established that both SVM and ridge regression solutions lie in the <strong>span of the training data</strong>. This insight opens up a new avenue: we can <strong>reparameterize the optimization problem</strong> by restricting our search space to this span. Let’s explore how this simplifies the optimization process and why it’s particularly useful in high-dimensional settings.</p> <h5 id="reparameterization-of-ridge-regression"><strong>Reparameterization of Ridge Regression</strong></h5> <p>To recap, the ridge regression problem for regularization parameter \(\lambda &gt; 0\) is:</p> \[w^* = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \{ w^T x_i - y_i \}^2 + \lambda \|w\|_2^2\] <p>We know that \(w^* \in \text{span}(x_1, \dots, x_n) \subset \mathbb{R}^d\). Therefore, instead of minimizing over all of \(\mathbb{R}^d\), we can restrict our optimization to the span of the training data:</p> \[w^* = \arg\min_{w \in \text{span}(x_1, \dots, x_n)} \frac{1}{n} \sum_{i=1}^n \{ w^T x_i - y_i \}^2 + \lambda \|w\|_2^2\] <p>Now, let’s <strong>reparameterize</strong> the objective function. Since \(w \in \text{span}(x_1, \dots, x_n)\), we can express \(w\) as a linear combination of the inputs:</p> \[w = X^T \alpha, \quad \alpha \in \mathbb{R}^n\] <p>Substituting this into the optimization problem gives:</p> <h5 id="reparameterized-objective"><strong>Reparameterized Objective</strong></h5> <p>The original formulation becomes:</p> \[\alpha^* = \arg\min_{\alpha \in \mathbb{R}^n} \frac{1}{n} \sum_{i=1}^n \{ (X^T \alpha)^T x_i - y_i \}^2 + \lambda \|X^T \alpha\|_2^2\] <p>Once \(\alpha^*\) is obtained, the optimal weight vector \(w^*\) can be recovered as:</p> \[w^* = X^T \alpha^*\] <h5 id="why-does-this-matter"><strong>Why Does This Matter?</strong></h5> <p>By reparameterizing, we’ve effectively reduced the dimension of the optimization problem:</p> <ul> <li><strong>Original Problem</strong>: Optimize over \(\mathbb{R}^d\) (where \(d\) is the feature space dimension).</li> <li><strong>Reparameterized Problem</strong>: Optimize over \(\mathbb{R}^n\) (where \(n\) is the number of training examples).</li> </ul> <p>This reduction is significant in scenarios where \(d \gg n\). For instance:</p> <ul> <li><strong>Very Large Feature Space</strong>: Suppose \(d = 300 \, \text{million}\) (e.g., using high-order polynomial interactions).</li> <li><strong>Moderate Training Set Size</strong>: Suppose \(n = 300,000\) examples.</li> </ul> <p>In the original formulation, we solve a \(300 \, \text{million}\)-dimensional optimization problem. After reparameterization, we solve a much smaller \(300,000\)-dimensional problem. This simplification highlights why the span property is crucial, particularly when the number of features vastly exceeds the number of training examples.</p> <hr/> <h4 id="generalization-the-representer-theorem"><strong>Generalization: The Representer Theorem</strong></h4> <p>The span property is not unique to SVM and ridge regression. A powerful result known as the <strong>Representer Theorem</strong> shows that this property applies broadly to all norm-regularized linear models.Here’s how it works:</p> <h5 id="generalized-objective"><strong>Generalized Objective</strong></h5> <p>We start with a generalized objective for a norm-regularized model:</p> \[w^* = \arg \min_{w \in \mathcal{H}} R(\|w\|) + L\big((\langle w, x_1 \rangle), \dots, (\langle w, x_n \rangle)\big).\] <p>Here:</p> <ul> <li>\(R(\|w\|)\): Regularization term to control model complexity.</li> <li>\(L\): Loss function that measures the fit of the model to the data.</li> <li>\(\mathcal{H}\): Hypothesis space where \(w\) resides.</li> </ul> <h5 id="key-insight-from-the-representer-theorem"><strong>Key Insight from the Representer Theorem</strong></h5> <p>The Representer Theorem tells us that instead of searching for \(w^*\) in the entire hypothesis space \(\mathcal{H}\), we can restrict our search to the span of the training data. Mathematically:</p> \[w^* = \arg \min_{w \in \text{span}(x_1, \dots, x_n)} R(\|w\|) + L\big((\langle w, x_1 \rangle), \dots, (\langle w, x_n \rangle)\big).\] <p>This dramatically reduces the complexity of the optimization problem.</p> <h5 id="reparameterization"><strong>Reparameterization</strong></h5> <p>Using this insight, we can reparameterize the optimization problem as before. Let \(w = \sum_{i=1}^n \alpha_i x_i\), where \(\alpha = (\alpha_1, \dots, \alpha_n) \in \mathbb{R}^n\). Substituting this into the objective:</p> \[\alpha^* = \arg \min_{\alpha \in \mathbb{R}^n} R\left(\left\| \sum_{i=1}^n \alpha_i x_i \right\|\right) + L\Big(\big\langle \sum_{i=1}^n \alpha_i x_i, x_1 \big\rangle, \dots, \big\langle \sum_{i=1}^n \alpha_i x_i, x_n \big\rangle\Big).\] <h5 id="why-this-matters"><strong>Why This Matters</strong></h5> <p>By reparameterizing the problem, we transform the optimization from a potentially infinite-dimensional space \(\mathcal{H}\) to a finite-dimensional space (spanned by the data points). This makes the problem computationally feasible and reveals why the solution lies in the span of the data.</p> <h4 id="implications-kernelization-and-the-kernel-trick"><strong>Implications: Kernelization and the Kernel Trick</strong></h4> <p>The Representer Theorem plays a pivotal role in enabling <strong>kernelization</strong>. Here’s how it connects:</p> <p>Using the Representer Theorem, we know that the solution \(w^*\) resides in the span of the data. This insight allows us to replace the feature space \(\phi(x)\) with a kernel function \(K(x, x')\), where:</p> \[K(x, x') = \langle \phi(x), \phi(x') \rangle.\] <p>The kernel function computes the inner product in the transformed feature space without explicitly constructing \(\phi(x)\). This process is called <strong>kernelization</strong>.</p> <h5 id="kernelized-representer-theorem"><strong>Kernelized Representer Theorem</strong></h5> <p>The Representer Theorem in the context of kernels can be expressed as:</p> \[w^* = \sum_{i=1}^n \alpha_i \phi(x_i),\] <p>where the coefficients \(\alpha\) are obtained by solving an optimization problem that depends only on the kernel \(K(x_i, x_j)\).</p> <p>The Representer Theorem provides a unifying framework for kernelization. By recognizing that solutions lie in the span of the data, we can seamlessly replace explicit feature mappings with kernel functions. This powerful insight underpins many modern machine learning techniques, making high-dimensional learning tasks computationally feasible.</p> <hr/> <h5 id="summary"><strong>Summary</strong></h5> <ol> <li><strong>Reparameterization</strong>: If a solution lies in the span of the training data, we can reparameterize the optimization problem to reduce its dimensionality, simplifying the computation.</li> <li><strong>High-Dimensional Settings</strong>: This approach is especially useful when \(d \gg n\), where the feature space dimension far exceeds the number of training examples.</li> <li><strong>Representer Theorem</strong>: The span property generalizes to all norm-regularized linear models, forming the theoretical foundation for kernelization and advocates that linear models can be kernelized.</li> <li><strong>Kernel Trick</strong>: By kernelizing models, we can solve complex problems in high-dimensional spaces efficiently and without the need to represent \(\phi(x)\) explicitly.</li> </ol> <p>Understanding the span property and its implications is not just a mathematical curiosity—it’s a foundational principle that unifies many machine learning models and opens up practical avenues for efficient computation in challenging scenarios.</p> <p>Next, we’ll delve into specific topics related to SVM that we’ve touched on briefly. We’ll explore them in more depth to build an intuitive understanding of each concept, as many of these form the foundation for more advanced ML techniques. Mastering them is well worth the effort. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Representer Theorem</li> <li>Visualization elements</li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.]]></summary></entry><entry><title type="html">Understanding the Kernel Trick</title><link href="https://monishver11.github.io/blog/2025/kernel-trick/" rel="alternate" type="text/html" title="Understanding the Kernel Trick"/><published>2025-01-13T23:03:00+00:00</published><updated>2025-01-13T23:03:00+00:00</updated><id>https://monishver11.github.io/blog/2025/kernel-trick</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/kernel-trick/"><![CDATA[<p>When working with machine learning models, especially Support Vector Machines (SVMs), the idea of mapping data into a higher-dimensional space often comes into play. This mapping helps transform non-linearly separable data into a space where linear decision boundaries can be applied. But what happens when the dimensionality of the feature space becomes overwhelmingly large? This is where the <strong>kernel trick</strong> saves the day. In this post, we will explore the kernel trick, starting with SVMs, their reliance on feature mappings, and how inner products in feature space can be computed without ever explicitly constructing that space.</p> <hr/> <h4 id="svms-with-explicit-feature-maps"><strong>SVMs with Explicit Feature Maps</strong></h4> <p>To understand the kernel trick, let’s begin with SVMs. In the simplest case, an SVM aims to find a hyperplane that separates data into classes with the largest possible margin. To handle more complex data, we map the input data \(\mathbf{x}\) into a higher-dimensional feature space using a feature map \(\psi: X \to \mathbb{R}^d\). In this space, the SVM optimization problem can be written as:</p> \[\min_{\mathbf{w} \in \mathbb{R}^d} \frac{1}{2} \|\mathbf{w}\|^2 + \frac{c}{n} \sum_{i=1}^n \max(0, 1 - y_i \mathbf{w}^T \psi(\mathbf{x}_i)).\] <p>Here, \(\mathbf{w}\) is the weight vector, \(c\) is a regularization parameter, and \(y_i\) are the labels of the data points. While this approach works well for small \(d\), it becomes computationally expensive as \(d\) increases, especially when using high-degree polynomial mappings.</p> <p>To address this issue, we turn to a reformulation of the SVM problem, derived from <strong>Lagrangian duality</strong>.</p> <h4 id="the-svm-dual-problem"><strong>The SVM Dual Problem</strong></h4> <p>Through Lagrangian duality, the SVM optimization problem can be re-expressed as a dual problem:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \psi(\mathbf{x}_j)^T \psi(\mathbf{x}_i),\] <p>subject to:</p> \[\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \in \left[ 0, \frac{c}{n} \right] \quad \forall i.\] <p>Here, \(\alpha_i\) are the dual variables (Lagrange multipliers). Once the optimal \(\boldsymbol{\alpha}^*\) is obtained, the weight vector in the feature space can be reconstructed as:</p> \[\mathbf{w}^* = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i).\] <p>The decision function for a new input \(\mathbf{x}\) is given by:</p> \[\hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i)^T \psi(\mathbf{x}).\] <h5 id="observing-the-role-of-inner-products"><strong>Observing the Role of Inner Products</strong></h5> <p>An important observation here is that the feature map \(\psi(\mathbf{x})\) appears only through inner products of the form \(\psi(\mathbf{x}_j)^T \psi(\mathbf{x}_i)\). This means we don’t actually need the explicit feature representation \(\psi(\mathbf{x})\); instead, we just need the ability to compute these inner products efficiently.</p> <hr/> <h4 id="computing-inner-products-in-practice"><strong>Computing Inner Products in Practice</strong></h4> <p>Let’s explore the kernel trick with an example.</p> <h5 id="example-degree-2-monomials"><strong>Example: Degree-2 Monomials</strong></h5> <p>Suppose we are working with 2D data points \(\mathbf{x} = (x_1, x_2)\). If we map the data into a space of degree-2 monomials, the feature map becomes:</p> \[\psi: \mathbb{R}^2 \to \mathbb{R}^6, \quad (x_1, x_2) \mapsto (1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, \sqrt{2}x_1x_2, x_2^2).\] <p>The inner product in the feature space is:</p> \[\psi(\mathbf{x})^T \psi(\mathbf{x}') = 1 + 2x_1x_1' + 2x_2x_2' + (x_1x_1')^2 + 2x_1x_2x_1'x_2' + (x_2x_2')^2.\] <p>Simplifying, we observe:</p> \[\psi(\mathbf{x})^T \psi(\mathbf{x}') = (1 + x_1x_1' + x_2x_2')^2 = (1 + \mathbf{x}^T \mathbf{x}')^2.\] <p>This shows that we can compute \(\psi(\mathbf{x})^T \psi(\mathbf{x}')\) directly from the original input space without explicitly constructing \(\psi(\mathbf{x})\)—a key insight behind the kernel trick.</p> <h5 id="general-case-monomials-up-to-degree-p"><strong>General Case: Monomials Up to Degree \(p\)</strong></h5> <p>For feature maps that produce monomials up to degree \(p\), the inner product generalizes as:</p> \[\psi(x)^T \psi(x') = (1 + x^T x')^p.\] <p>It is worth noting that the coefficients of the monomials in \(\psi(x)\) may vary depending on the specific feature map.</p> <hr/> <h4 id="efficiency-of-the-kernel-trick-from-exponential-to-linear-complexity"><strong>Efficiency of the Kernel Trick: From Exponential to Linear Complexity</strong></h4> <p>One of the key advantages of the kernel trick is its ability to reduce the computational complexity of working with high-dimensional feature spaces. Let’s break this down:</p> <h5 id="explicit-computation-complexity"><strong>Explicit Computation Complexity</strong></h5> <p>When we map an input vector \(\mathbf{x} \in \mathbb{R}^d\) to a feature space with monomials up to degree \(p\), the dimensionality of the feature space increases significantly. Specifically:</p> <ul> <li> <p><strong>Feature Space Dimension</strong>: The number of features in the expansion is:</p> \[\binom{d + p}{p} = \frac{(d + p)!}{d! \, p!}.\] <p>For large \(p\) or \(d\), this grows rapidly and can quickly become computationally prohibitive.</p> </li> <li> <p><strong>Explicit Inner Product</strong>: Computing the inner product directly in this expanded space has a complexity of:</p> \[O\left(\binom{d + p}{p}\right),\] <p>which is exponential in \(p\) for fixed \(d\).</p> </li> </ul> <h5 id="implicit-computation-complexity"><strong>Implicit Computation Complexity</strong></h5> <p>Using the kernel trick, we avoid explicitly constructing the feature space. For a kernel function like:</p> \[k(\mathbf{x}, \mathbf{x}') = (1 + \mathbf{x}^T \mathbf{x}')^p,\] <p>the computation operates directly in the input space.</p> <ul> <li> <p><strong>Input Space Computation</strong>: Computing the kernel function involves:</p> <ol> <li><strong>Dot Product</strong>: \(\mathbf{x}^T \mathbf{x}'\) is computed in \(O(d)\).</li> <li><strong>Polynomial Evaluation</strong>: Raising this result to power \(p\) is done in constant time, independent of \(d\).</li> </ol> </li> </ul> <p>Thus, the complexity is reduced to:</p> \[O(d),\] <p>which is <strong>linear</strong> in the input dimensionality \(d\), regardless of \(p\).</p> <h5 id="why-this-matters"><strong>Why This Matters</strong></h5> <ul> <li><strong>Explicit Features</strong>: For high \(p\), the feature space grows exponentially, leading to a <strong>curse of dimensionality</strong> if explicit computation is used.</li> <li><strong>Implicit Kernel Computation</strong>: The kernel trick sidesteps the explicit feature space, allowing efficient computation even when the feature space is high-dimensional or infinite (e.g., with RBF kernels).</li> </ul> <p>This transformation from <strong>exponential</strong> to <strong>linear complexity</strong> is one of the core reasons kernel methods are powerful tools in machine learning.</p> <p><strong>Key Takeaway</strong> : The kernel trick enables efficient computation in high-dimensional feature spaces by directly working in the input space. This reduces the complexity from \(O\left(\binom{d + p}{p}\right)\) to \(O(d)\), making it feasible to apply machine learning methods to problems with high-degree polynomial or infinite-dimensional feature spaces.</p> <hr/> <h4 id="exploring-the-kernel-function"><strong>Exploring the Kernel Function</strong></h4> <p>To fully appreciate the kernel trick, we need to formalize the concept of the <strong>kernel function</strong>. In our earlier discussion, we introduced the idea of a feature map \(\psi: X \to \mathcal{H}\), which maps input data from the original space \(X\) to a higher-dimensional feature space \(\mathcal{H}\). The kernel function \(k\) corresponding to this feature map is defined as:</p> \[k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle,\] <p>where \(\langle \cdot, \cdot \rangle\) represents the inner product in \(\mathcal{H}\).</p> <h5 id="why-use-kernel-functions"><strong>Why Use Kernel Functions?</strong></h5> <p>At first glance, this notation might seem like a trivial restatement of the inner product, but it’s far more powerful. The key insight is that we can often evaluate \(k(\mathbf{x}, \mathbf{x}')\) directly, without explicitly computing \(\psi(\mathbf{x})\) and \(\psi(\mathbf{x}')\). This is crucial for efficiently working with high-dimensional or infinite-dimensional feature spaces. But this efficiency only applies to certain methods — those that can be <strong>kernelized</strong>.</p> <h4 id="kernelized-methods"><strong>Kernelized Methods</strong></h4> <p>A method is said to be <strong>kernelized</strong> if it uses the feature vectors \(\psi(\mathbf{x})\) only inside inner products of the form \(\langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle\). For such methods, we can replace these inner products with a kernel function \(k(\mathbf{x}, \mathbf{x}')\), avoiding explicit feature computation. This applies to both the optimization problem and the prediction function. Let’s revisit the SVM example to see kernelization in action.</p> <h5 id="kernelized-svm-dual-formulation"><strong>Kernelized SVM Dual Formulation</strong></h5> <p>Recall the dual problem for SVMs:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle,\] <p>subject to:</p> \[\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \in \left[ 0, \frac{c}{n} \right] \quad \forall i.\] <p><strong>Here’s the key</strong>: because every occurrence of \(\psi(\mathbf{x})\) is inside an inner product, we can replace \(\langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle\) with \(k(\mathbf{x}_i, \mathbf{x}_j)\), the kernel function. The resulting dual optimization problem becomes:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j),\] <p>subject to the same constraints.</p> <p>For predictions, the decision function can also be written in terms of the kernel:</p> \[\hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i k(\mathbf{x}_i, \mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i)^T \psi(\mathbf{x})\] <p>This reformulation is what allows SVMs to operate efficiently in high-dimensional spaces.</p> <h5 id="the-kernel-matrix"><strong>The Kernel Matrix</strong></h5> <p>A key component in kernelized methods is the <strong>kernel matrix</strong>, which encapsulates the pairwise kernel values for all data points. For a dataset \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}\), the kernel matrix \(\mathbf{K}\) is defined as:</p> \[\mathbf{K} = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}.\] <p>This \(n \times n\) matrix, also known as the <strong>Gram matrix</strong> in machine learning, summarizes all the information about the training data necessary for solving the kernelized optimization problem.</p> <p>For the kernelized SVM, we can replace \(\langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle\) with \(K_{ij}\), reducing the dual problem to:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K_{ij},\] <p>subject to the same constraints.</p> <p><strong>So, Given a kernelized ML algorithm</strong> (i.e., all \(\psi(x)\)’s show up as \(\langle \psi(x), \psi(x') \rangle\)) :</p> <ol> <li><strong>Flexibility</strong>: By substituting the kernel function, we can implicitly use very high-dimensional or even infinite-dimensional feature spaces.</li> <li><strong>Scalability</strong>: Once the kernel matrix is computed, the computational cost depends on the number of data points \(n\), rather than the dimension of the feature space \(d\).</li> <li><strong>Efficiency</strong>: For many kernels, \(k(\mathbf{x}, \mathbf{x}')\) can be computed without directly accessing the high-dimensional feature representation \(\psi(\mathbf{x})\), avoiding the \(O(d)\) dependence.</li> </ol> <p>These properties make kernelized methods invaluable when \(d \gg n\), a common scenario in machine learning tasks.</p> <p>The kernel trick revolutionizes how we think about high-dimensional data. Next, we will delve into popular kernel functions, their interpretations, and how to choose the right one for your problem.</p> <hr/> <h4 id="example-kernels"><strong>Example Kernels</strong></h4> <p>In many cases, it’s useful to think of the kernel function \(k(x, x')\) as a <strong>similarity score</strong> between the data points \(x\) and \(x'\). This perspective allows us to design similarity functions without explicitly considering the feature map.</p> <p>For example, we can create <strong>string kernels</strong> or <strong>graph kernels</strong>—functions that define similarity based on the structure of strings or graphs, respectively. The key question, however, is: <strong>How do we know that our kernel functions truly correspond to inner products in some feature space?</strong></p> <p>This is an essential consideration, as it ensures that the kernel method preserves the properties necessary for various machine learning algorithms to work effectively. Let’s break this down.</p> <h5 id="how-to-obtain-kernels"><strong>How to Obtain Kernels?</strong></h5> <p>There are two primary ways to define kernels:</p> <ol> <li> <p><strong>Explicit Construction</strong>: Define the feature map \(\psi(\mathbf{x})\) and use it to compute the kernel: \(k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle.\) (e.g. monomials)</p> </li> <li> <p><strong>Direct Definition</strong>: Directly define the kernel \(k(\mathbf{x}, \mathbf{x}')\) as a similarity score and verify that it corresponds to an inner product for some \(\psi\). This verification is often guided by mathematical theorems.</p> </li> </ol> <p>To understand this better, let’s first equip ourselves with some essential linear algebra concepts.</p> <h5 id="positive-semidefinite-matrices-and-kernels"><strong>Positive Semidefinite Matrices and Kernels</strong></h5> <p>To verify if a kernel corresponds to a valid inner product, we rely on the concept of <strong>positive semidefinite (PSD) matrices</strong>. Here’s a quick refresher:</p> <ul> <li> <p>A matrix \(\mathbf{M} \in \mathbb{R}^{n \times n}\) is positive semidefinite if: \(\mathbf{x}^\top \mathbf{M} \mathbf{x} \geq 0, \quad \forall \mathbf{x} \in \mathbb{R}^n.\)</p> </li> <li> <p>Equivalent conditions, each necessary and sufficient for a symmetric matrixfor \(\mathbf{M}\) being <strong>PSD</strong>:</p> <ul> <li>\(\mathbf{M} = \mathbf{R}^\top \mathbf{R}\), for some matrix \(\mathbf{R}\).</li> <li>All eigenvalues of \(\mathbf{M}\) are non-negative or \(\geq 0\).</li> </ul> </li> </ul> <p>Next, we define a <strong>positive definite (PD) kernel</strong>:</p> <h5 id="positive-definite-kernel"><strong>Positive Definite Kernel</strong></h5> <p><strong>Definition:</strong></p> <p>A symmetric function \(k: X \times X \to \mathbb{R}\) is a <strong>PD</strong> kernel if, for any finite set \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\} \subset X\), the kernel matrix:</p> \[\mathbf{K} = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}\] <p>is positive semidefinite.</p> <ol> <li>Symmetry: \(k(\mathbf{x}, \mathbf{x}') = k(\mathbf{x}', \mathbf{x})\).</li> <li>The kernel matrix needs to be positive semidefinite for any finite set of points.</li> <li>Equivalently: \(\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j k(\mathbf{x}_i, \mathbf{x}_j) \geq 0\), for all \(\alpha_i \in \mathbb{R}\) \(\forall i\).</li> </ol> <h6 id="think-of-it-this-way"><strong>Think of it this way:</strong></h6> <ol> <li> <p><strong>Symmetry</strong>:<br/> Symmetry ensures the kernel measures similarity consistently between any two points: \(k(\mathbf{x}, \mathbf{x}') = k(\mathbf{x}', \mathbf{x}).\)<br/> For example, the similarity between \(\mathbf{x}\) and \(\mathbf{x}'\) is the same as that between \(\mathbf{x}'\) and \(\mathbf{x}\).</p> </li> <li> <p><strong>Positive Semidefiniteness</strong>:<br/> Positive semidefiniteness ensures the kernel corresponds to a <strong>valid inner product</strong> in some (possibly high-dimensional or infinite-dimensional) feature space.</p> <ul> <li>Think of the kernel as a measure of similarity: this property ensures that the relationships it captures are geometrically valid in that feature space.</li> <li>Intuitively, this means the kernel matrix does not produce “negative energy,” ensuring a consistent representation of the data.</li> </ul> </li> </ol> <p><strong>Simpler Way to State It:</strong></p> <ul> <li>A kernel is <strong>PD</strong> if it acts like an inner product in some feature space.</li> <li>For any set of points, the kernel matrix must be symmetric and positive semidefinite.</li> <li>Symmetry ensures the similarity is consistent in both directions, while positive semidefiniteness guarantees geometrically valid relationships in the feature space.</li> </ul> <h5 id="mercers-theorem"><strong>Mercer’s Theorem</strong></h5> <p>Mercer’s Theorem provides a foundational result for kernels. It states:</p> <ul> <li> <p>A symmetric function \(k(\mathbf{x}, \mathbf{x}')\) can be expressed as an inner product</p> \[k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle\] <p>if and only if \(k(\mathbf{x}, \mathbf{x}')\) is positive definite.</p> </li> </ul> <p>While proving that a kernel is <strong>PD</strong> can be challenging, we can use known kernels to construct new ones.</p> <h5 id="constructing-new-kernels-from-existing-ones"><strong>Constructing New Kernels from Existing Ones</strong></h5> <p>Given valid PD kernels \(k_1\) and \(k_2\), we can create new kernels using the following operations:</p> <ol> <li><strong>Non-Negative Scaling</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = \alpha k(\mathbf{x}, \mathbf{x}')\), where \(\alpha \geq 0\).</li> <li><strong>Addition</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') + k_2(\mathbf{x}, \mathbf{x}')\).</li> <li><strong>Multiplication</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')\).</li> <li><strong>Recursion</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k(\psi(\mathbf{x}), \psi(\mathbf{x}'))\), for any function \(\psi(\cdot)\).</li> <li><strong>Feature Mapping</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = f(\mathbf{x}) f(\mathbf{x}')\), for any function \(f(\cdot)\).</li> </ol> <p>And, Lot more ways to help you construct new kernels from old.</p> <p>It should be noted that Mercer’s theorem only tells us when a candidate similarity function is admissible for use. It tells nothing about how good such a kernel function is.</p> <p>Next, we’ll dive into some of the most widely used kernel functions.</p> <hr/> <h4 id="the-linear-kernel"><strong>The Linear Kernel</strong></h4> <p>The linear kernel is the simplest and most intuitive kernel function. Imagine working with data in an input space represented as \(X = \mathbb{R}^d\). Here, the feature space, denoted as \(\mathcal{H}\), is the same as the input space \(\mathbb{R}^d\). The feature map for this kernel is straightforward: \(\psi(x) = x\).</p> <p>The kernel function itself is defined as:</p> \[k(x, x') = \langle x, x' \rangle = x^\top x',\] <p>where \(\langle x, x' \rangle\) represents the standard inner product. This simplicity makes the linear kernel computationally efficient and ideal for linear models.</p> <h4 id="the-quadratic-kernel"><strong>The Quadratic Kernel</strong></h4> <p>The quadratic kernel takes us a step further by mapping the input space \(X = \mathbb{R}^d\) into a higher-dimensional feature space \(\mathcal{H} = \mathbb{R}^D\), where \(D\) is approximately \(d + \binom d2 \approx \frac{d^2}{2}\). This expanded feature space enables the kernel to capture quadratic relationships in the data.</p> <p>The feature map for the quadratic kernel is given by:</p> \[\psi(x) = \left(x_1, \dots, x_d, x_1^2, \dots, x_d^2, \sqrt{2}x_1x_2, \dots, \sqrt{2}x_ix_j, \dots, \sqrt{2}x_{d-1}x_d\right)^\top.\] <p>To compute the kernel function, we use the inner product of the feature maps:</p> \[k(x, x') = \langle \psi(x), \psi(x') \rangle.\] <p>Expanding this yields:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2.\] <p><strong>Derivation of the Quadratic Kernel form:</strong></p> <p>The quadratic kernel is defined as the inner product in a higher-dimensional feature space. The feature map \(\psi(x)\) includes:</p> <ol> <li>Original features: \(x_1, x_2, \dots, x_d\)</li> <li>Squared features: \(x_1^2, x_2^2, \dots, x_d^2\)</li> <li>Cross-product terms: \(\sqrt{2}x_i x_j\) for \(i \neq j\)</li> </ol> <p>Thus:</p> \[\psi(x) = \left(x_1, x_2, \dots, x_d, x_1^2, x_2^2, \dots, x_d^2, \sqrt{2}x_1x_2, \sqrt{2}x_1x_3, \dots, \sqrt{2}x_{d-1}x_d\right)^\top\] <p>The kernel is computed as:</p> \[k(x, x') = \langle \psi(x), \psi(x') \rangle\] <p>Expanding this, we have:</p> <ol> <li> <p><strong>Linear terms</strong>:<br/> \(\langle x, x' \rangle = \sum_{i} x_i x_i'\)</p> </li> <li> <p><strong>Squared terms</strong>:<br/> \(\sum_{i} x_i^2 x_i'^2\)</p> </li> <li> <p><strong>Cross-product terms</strong>: \(2 \sum_{i \neq j} x_i x_j x_i' x_j'\)</p> </li> </ol> <p>Combining these, the kernel becomes:</p> \[k(x, x') = \langle x, x' \rangle + \sum_{i} x_i^2 x_i'^2 + 2 \sum_{i \neq j} x_i x_j x_i' x_j'\] <p>Recognizing that:</p> \[\langle x, x' \rangle^2 = \left( \sum_{i} x_i x_i' \right)^2 = \sum_{i} x_i^2 x_i'^2 + 2 \sum_{i \neq j} x_i x_j x_i' x_j'\] <p>The kernel simplifies to:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2\] <p>One of the key advantages of kernel methods is computational efficiency. While the explicit computation of the inner product in the feature space requires \(O(d^2)\) operations, the implicit kernel calculation only requires \(O(d)\) operations.</p> <p>A good example will make it much clearer.</p> <p>Let \(x = [1, 2]\) and \(x' = [3, 4]\).</p> <p>The quadratic kernel is defined as:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2\] <p><strong>Step 1</strong>: Compute \(\langle x, x' \rangle\) \(\langle x, x' \rangle = (1)(3) + (2)(4) = 3 + 8 = 11\)</p> <p><strong>Step 2</strong>: Compute \(\langle x, x' \rangle^2\) \(\langle x, x' \rangle^2 = 11^2 = 121\)</p> <p><strong>Step 3</strong>: Compute \(k(x, x')\) \(k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2 = 11 + 121 = 132\)</p> <p><strong>Step 4</strong>: Verify with the Feature Map</p> <p>The feature map for the quadratic kernel is:</p> \[\psi(x) = [x_1, x_2, x_1^2, x_2^2, \sqrt{2}x_1x_2]\] <p>For \(x = [1, 2]\): \(\psi(x) = [1, 2, 1^2, 2^2, \sqrt{2}(1)(2)] = [1, 2, 1, 4, 2\sqrt{2}]\)</p> <p>For \(x' = [3, 4]\): \(\psi(x') = [3, 4, 3^2, 4^2, \sqrt{2}(3)(4)] = [3, 4, 9, 16, 12\sqrt{2}]\)</p> <p>Compute the inner product:</p> <p>\(\langle \psi(x), \psi(x') \rangle = (1)(3) + (2)(4) + (1)(9) + (4)(16) + (2\sqrt{2})(12\sqrt{2})\) \(= 3 + 8 + 9 + 64 + 48 = 132\)</p> <p>Thus, the quadratic kernel gives: \(k(x, x') = 132\)</p> <h4 id="the-polynomial-kernel"><strong>The Polynomial Kernel</strong></h4> <p>Building on the quadratic kernel, the polynomial kernel generalizes the concept by introducing a degree parameter \(M\). The kernel function is defined as:</p> \[k(x, x') = (1 + \langle x, x' \rangle)^M.\] <p>This kernel corresponds to a feature space that includes all monomials of the input features up to degree \(M\). Notably, the computational cost of evaluating the kernel function remains constant, regardless of \(M\). However, explicitly computing the inner product in the feature space grows rapidly as \(M\) increases.</p> <hr/> <h4 id="the-radial-basis-function-rbf-kernel"><strong>The Radial Basis Function (RBF) Kernel</strong></h4> <p>The <strong>Radial Basis Function (RBF) kernel</strong>, also known as the <strong>Gaussian kernel</strong>, is one of the most widely used kernels for solving nonlinear problems. Unlike the linear and polynomial kernels, the RBF kernel maps data into an <strong>infinite-dimensional feature space</strong>, enabling it to capture highly complex relationships.</p> <p>The RBF kernel function is mathematically expressed as:</p> \[k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right),\] <p>where:</p> <ul> <li>\(x, x' \in \mathbb{R}^d\) are data points in the input space,</li> <li>\(\|x - x'\|\) is the Euclidean distance between \(x\) and \(x'\),</li> <li>\(\sigma^2\) (the <strong>bandwidth</strong>) controls how quickly the kernel value decays with distance.</li> </ul> <h5 id="how-the-rbf-kernel-works"><strong>How the RBF Kernel Works</strong></h5> <p>The RBF kernel measures <strong>similarity</strong> between two points based on their distance. Here’s a breakdown:</p> <ol> <li><strong>Close Points</strong>: <ul> <li>If \(x\) and \(x'\) are close, \(\|x - x'\|\) is small. The exponential term \(\exp(-\|x - x'\|^2 / 2\sigma^2)\) is close to 1, meaning the points are highly similar.</li> </ul> </li> <li><strong>Distant Points</strong>: <ul> <li>When \(x\) and \(x'\) are far apart, \(\|x - x'\|\) becomes large, and the kernel value approaches 0. This indicates little to no similarity.</li> </ul> </li> <li><strong>Smoothness Control with \(\sigma^2\)</strong>: <ul> <li>A smaller \(\sigma^2\) leads to sharper drops in similarity, making the kernel more sensitive to nearby points.</li> <li>A larger \(\sigma^2\) smooths the decay, creating broader generalization.</li> </ul> </li> </ol> <p>The RBF kernel is powerful because it implicitly maps data into an <strong>infinite-dimensional feature space</strong>. However, thanks to the <strong>kernel trick</strong>, we don’t need to compute these features explicitly. Instead, the kernel function \(k(x, x')\) directly computes the equivalent of the dot product in this space.</p> <h6 id="what-does-this-mean"><strong>What Does This Mean?</strong></h6> <ul> <li>In this infinite space, even simple algorithms (like linear classifiers) can create highly complex and nonlinear decision boundaries in the original input space.</li> </ul> <h5 id="intuition-behind-the-rbf-kernel"><strong>Intuition Behind the RBF Kernel</strong></h5> <p>To understand the RBF kernel, let’s break it down with some simple analogies.</p> <h6 id="1-a-bubble-of-influence"><strong>1. A Bubble of Influence</strong></h6> <p>Imagine every data point in your dataset creates an invisible “bubble” around itself. The size and shape of this bubble depend on the kernel’s parameter \(\sigma^2\) (the bandwidth):</p> <ul> <li><strong>Small \(\sigma^2\)</strong>: The bubble is tight and localized, meaning each point influences only its immediate neighbors. This captures fine-grained details.</li> <li><strong>Large \(\sigma^2\)</strong>: The bubble is wide and smooth, allowing points to influence data further away. This leads to broader generalization.</li> </ul> <p>When we compute \(k(x, x')\), we’re essentially asking, <em>“How much does the bubble around \(x\) overlap with the bubble around \(x'\)?”</em> The more overlap, the higher the similarity score.</p> <h6 id="2-analogy-dropping-pebbles-in-a-pond"><strong>2. Analogy: Dropping Pebbles in a Pond</strong></h6> <p>Imagine dropping pebbles into a still pond:</p> <ul> <li>Each pebble creates ripples that spread outward.</li> <li>The strength of the ripples diminishes as they travel further from the pebble.</li> </ul> <p>The kernel function \(k(x, x')\) measures how much the ripples from one pebble (data point \(x\)) interfere or overlap with those from another pebble (\(x'\)).</p> <ul> <li><strong>Close pebbles</strong>: Their ripples interfere constructively (high similarity, \(k(x, x')\) close to 1).</li> <li><strong>Distant pebbles</strong>: Their ripples barely touch (low similarity, \(k(x, x')\) close to 0).</li> </ul> <p>The parameter \(\sigma^2\) controls the rate at which the ripples fade:</p> <ul> <li><strong>Small \(\sigma^2\)</strong>: Ripples fade quickly, leading to sharp, localized interference.</li> <li><strong>Large \(\sigma^2\)</strong>: Ripples fade slowly, allowing broader interference.</li> </ul> <h6 id="3-the-infinite-dimensional-perspective"><strong>3. The Infinite-Dimensional Perspective</strong></h6> <p>Now imagine these ripples aren’t confined to the surface of the pond but instead exist in an infinite-dimensional space. Each data point generates a unique “wave” in this space.</p> <p>The RBF kernel computes the similarity between these waves without explicitly constructing them. It’s like a shortcut for comparing the interference patterns of ripples in an infinitely deep and wide pond.</p> <h6 id="4-why-does-this-matter"><strong>4. Why Does This Matter?</strong></h6> <p>This ripple analogy helps explain why the RBF kernel is so effective:</p> <ul> <li><strong>Localized Influence</strong>: Points that are closer together naturally exert more influence on each other.</li> <li><strong>Nonlinear Relationships</strong>: The ripple effect in the transformed feature space allows the kernel to capture intricate patterns in data.</li> <li><strong>Flexibility</strong>: By tuning \(\sigma^2\), you can adjust the model to balance between fine details (small \(\sigma^2\)) and broad generalization (large \(\sigma^2\)).</li> </ul> <p><strong>Key Takeaway:</strong> The RBF kernel creates a ripple effect around every data point and measures how much these ripples overlap. This process enables us to handle nonlinear relationships and create complex decision boundaries, all while staying computationally efficient.</p> <h5 id="why-is-the-rbf-kernel-infinite-dimensional"><strong>Why is the RBF Kernel Infinite-Dimensional?</strong></h5> <p>The RBF kernel maps data into an <strong>infinite-dimensional feature space</strong> because of its connection to the <strong>Taylor series expansion</strong> of the exponential function. The kernel is expressed as:</p> \[k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)\] <p>The exponential function can be expanded as a Taylor series:</p> \[\exp(-t) = \sum_{n=0}^{\infty} \frac{(-t)^n}{n!}\] <p>Applying this to the RBF kernel:</p> \[k(x, x') = \sum_{n=0}^{\infty} \frac{1}{n!} \left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)^n\] <p>Each term in this infinite series corresponds to a basis function in the feature space. Since the series includes terms of all powers \(n\), the feature space has an <strong>infinite number of dimensions</strong>.</p> <h6 id="key-intuition"><strong>Key Intuition:</strong></h6> <ol> <li> <p><strong>Infinite Series</strong>: The kernel includes contributions from all possible degrees of interaction between features (e.g., quadratic, cubic, quartic terms, etc.), up to infinity.</p> </li> <li> <p><strong>Feature Representation</strong>: The mapping \(\phi(x)\) to the feature space involves infinitely many components derived from the series expansion.</p> </li> <li> <p><strong>Kernel Trick</strong>: Instead of explicitly constructing these infinite features, the RBF kernel directly computes their inner product, \(\langle \phi(x), \phi(x') \rangle\), through \(k(x, x')\).</p> </li> </ol> <p>This infinite-dimensional nature is what gives the RBF kernel its remarkable flexibility to model complex, nonlinear patterns.</p> <hr/> <h4 id="kernelization-the-recipe"><strong>Kernelization: The Recipe</strong></h4> <p>To effectively leverage kernel methods, follow this general recipe:</p> <ol> <li>Recognize problems that can benefit from kernelization. These are cases where the feature map \(\psi(x)\) only appears in inner products \(\langle \psi(x), \psi(x') \rangle\).</li> <li>Select an appropriate kernel function(‘similarity score’) that suits the data and the task at hand.</li> <li>Compute the kernel matrix, a symmetric matrix of size \(n \times n\) for a dataset with \(n\) data points.</li> <li>Use the kernel matrix to optimize the model and make predictions.</li> </ol> <p>This approach allows us to solve problems in high-dimensional feature spaces without the computational burden of explicit mappings.</p> <h5 id="whats-next"><strong>What’s Next?</strong></h5> <p>We explored the theoretical foundations of kernel functions, how to construct valid kernels, and the properties of popular kernels. But, a key question remains: <strong>under what conditions can we apply kernelization effectively?</strong> Understanding this requires exploring one more crucial concept: how the solution to certain optimization problems is spanned by the input data itself. Next, we’ll delve into this idea and explore how it connects with kernels to solve the SVM problem we’ve been discussing. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Add some visualization for kernels intuition</li> <li><a href="https://www.cs.princeton.edu/~bee/courses/scribe/lec_10_09_2013.pdf">Kernels and Kernel Methods - Princeton University</a></li> <li><a href="https://towardsdatascience.com/radial-basis-function-rbf-kernel-the-go-to-kernel-acf0d22c798a">Radial Basis Function (RBF) Kernel: The Go-To Kernel</a></li> <li><a href="https://medium.com/@suvigya2001/the-gaussian-rbf-kernel-in-non-linear-svm-2fb1c822aae0">The Gaussian RBF Kernel in Non Linear SVM</a></li> <li><a href="https://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf">The Radial Basis Function Kernel: University of Wisconsin–Madison</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.]]></summary></entry><entry><title type="html">Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps</title><link href="https://monishver11.github.io/blog/2025/feature-maps/" rel="alternate" type="text/html" title="Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps"/><published>2025-01-13T21:26:00+00:00</published><updated>2025-01-13T21:26:00+00:00</updated><id>https://monishver11.github.io/blog/2025/feature-maps</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/feature-maps/"><![CDATA[<h4 id="understanding-the-input-space-mathcalx"><strong>Understanding the Input Space \(\mathcal{X}\)</strong></h4> <p>In machine learning, the input data we work with often originates from domains far removed from the mathematical structures we typically rely on. Text documents, image files, sound recordings, and even DNA sequences all serve as examples of such diverse input spaces. While these data types can be represented numerically, their raw form often lacks the structure necessary for effective analysis.</p> <p>Consider text data: each sequence might consist of words or characters, but the numerical representation of one text sequence may not align with another. The \(i\)-th entry of one sequence might hold a different meaning compared to the same position in another. Moreover, sequences like these often vary in length, which makes it even more challenging to align them into a consistent numerical format.</p> <p>This lack of structure highlights a fundamental challenge: we need a way to standardize and represent inputs in a meaningful way. The solution lies in a process called <strong>feature extraction</strong>.</p> <h5 id="feature-extraction-bridging-the-gap-between-input-and-model"><strong>Feature Extraction: Bridging the Gap Between Input and Model</strong></h5> <p>Feature extraction, also known as <strong>featurization</strong>, is the process of transforming inputs from their raw forms into structured numerical vectors that can be processed by machine learning models. Think of it as translating the data into a language that models can understand and interpret.</p> <p>Mathematically, we define feature extraction as a mapping: \(\phi: \mathcal{X} \to \mathbb{R}^d\) Here, \(\phi(x)\) takes an input \(x\) from the space \(\mathcal{X}\) and maps it into a \(d\)-dimensional vector in \(\mathbb{R}^d\). This transformation is crucial because it creates a consistent numerical structure that machine learning algorithms require.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Feature_Maps_1-480.webp 480w,/assets/img/Feature_Maps_1-800.webp 800w,/assets/img/Feature_Maps_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Feature_Maps_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Feature_Maps_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h4 id="linear-models-and-explicit-feature-maps"><strong>Linear Models and Explicit Feature Maps</strong></h4> <p>Let’s delve into how feature extraction integrates with linear models. In this setup, we make no assumptions about the input space \(\mathcal{X}\). Instead, we introduce a <strong>feature map</strong>: \(\phi: \mathcal{X} \to \mathbb{R}^d\)</p> <p>This feature map transforms inputs into a feature space \(\mathbb{R}^d\), enabling the use of standard linear model frameworks. Once in the feature space, the hypothesis space of affine functions is defined as:</p> \[F = \left\{ x \mapsto w^T \phi(x) + b \mid w \in \mathbb{R}^d, \, b \in \mathbb{R} \right\}\] <p>In this formulation:</p> <ul> <li>\(w\) represents the weights assigned to each feature.</li> <li>\(b\) is the bias term.</li> <li>\(\phi(x)\) is the feature-transformed representation of the input.</li> </ul> <p>This approach allows linear models to leverage the structured feature space effectively.</p> <h5 id="geometric-intuition-solving-nonlinear-problems-with-featurization"><strong>Geometric Intuition: Solving Nonlinear Problems with Featurization</strong></h5> <p>Imagine a two-class classification problem where the decision boundary is nonlinear. Using the <strong>identity feature map</strong> \(\phi(x) = (x_1, x_2)\), a linear model fails because the data points cannot be separated by a straight line in the original input space.</p> <p>However, by applying an appropriate feature map, such as: \(\phi(x) = (x_1, x_2, x_1^2 + x_2^2)\) we can transform the data into a higher-dimensional space. In this transformed space, the previously nonlinear boundary becomes linearly separable.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Feature_Maps_2-480.webp 480w,/assets/img/Feature_Maps_2-800.webp 800w,/assets/img/Feature_Maps_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Feature_Maps_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Feature_Maps_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This geometric perspective is a powerful way to understand how feature maps enhance the capability of machine learning models. If you’d like to visualize this concept, consider watching <a href="https://www.youtube.com/watch?v=3liCbRZPrZA">this illustrative video</a>.</p> <h5 id="expanding-the-hypothesis-space-the-role-of-features"><strong>Expanding the Hypothesis Space: The Role of Features</strong></h5> <p>The expressivity of a linear model—its ability to capture complex relationships—depends directly on the features it has access to. To increase the hypothesis space’s expressivity, we must introduce additional features.</p> <p>For instance, moving from basic linear features to polynomial or interaction features enables the model to capture more intricate patterns in the data. This expanded hypothesis space is often described as more <strong>expressive</strong> because it can fit a broader range of input-output relationships.</p> <p>However, with great expressivity comes the challenge of ensuring these features are meaningful and contribute to the task at hand. The art of feature design lies in striking the right balance: creating features that enhance the model’s capacity without overfitting or adding noise.</p> <hr/> <h4 id="handling-nonlinearity-with-linear-methods"><strong>Handling Nonlinearity with Linear Methods</strong></h4> <p>In machine learning, linear models are often preferred for their simplicity, interpretability, and efficiency. However, real-world problems rarely exhibit purely linear relationships, and this is where the challenge arises. How can we handle nonlinearity while retaining the advantages of linear methods?</p> <p>Let’s take an example task: predicting an individual’s health score. At first glance, this might seem straightforward—after all, we can list plenty of features relevant to medical diagnosis, such as:</p> <ul> <li>Height</li> <li>Weight</li> <li>Body temperature</li> <li>Blood pressure</li> </ul> <p>While these features are clearly useful, their relationships with the health score may not be linear. Furthermore, complex dependencies among these features can make predictions challenging. To address this, we must carefully consider the nature of nonlinearity and how it affects linear predictors.</p> <p>Nonlinearities in data can broadly be categorized into three types:</p> <ol> <li><strong>Non-monotonicity</strong>: When the relationship between a feature and the label does not follow a single increasing or decreasing trend.</li> <li><strong>Saturation</strong>: When the effect of a feature diminishes beyond a certain point, despite continuing to grow.</li> <li><strong>Interactions</strong>: When the effect of one feature depends on the value of another.</li> </ol> <p>Each of these presents unique challenges for linear models. Let’s explore them in detail.</p> <h5 id="non-monotonicity-when-extremes-behave-differently"><strong>Non-monotonicity: When Extremes Behave Differently</strong></h5> <p>Imagine we want to predict a health score \(y\) (where higher is better) based on body temperature \(t\). A simple feature map \(\phi(x) = [1, t(x)]\) assumes an affine relationship between temperature and health, meaning it can only model cases where:</p> <ul> <li>Higher temperatures are better, or</li> <li>Lower temperatures are better.</li> </ul> <p>But in reality, both extremes of temperature are harmful, and health is best around a “normal” temperature (e.g., 37°C). This non-monotonic relationship poses a problem.</p> <h6 id="solution-1-domain-knowledge"><strong>Solution 1: Domain Knowledge</strong></h6> <p>One approach is to manually transform the input to account for the non-monotonicity. For instance: \(\phi(x) = \left[1, \left(t(x) - 37\right)^2 \right]\) Here, we explicitly encode the deviation from normal temperature. While effective, this solution relies heavily on domain knowledge and manual feature engineering.</p> <h6 id="solution-2-let-the-model-decide"><strong>Solution 2: Let the Model Decide</strong></h6> <p>An alternative approach is to include additional features, such as: \(\phi(x) = \left[1, t(x), t(x)^2 \right]\) This makes the model more expressive, allowing it to learn the non-monotonic relationship directly from the data. As a general rule, features should be simple, modular building blocks that can adapt to various patterns.</p> <h5 id="saturation-when-effects-diminish"><strong>Saturation: When Effects Diminish</strong></h5> <p>Consider a recommendation system that scores products based on their relevance to a user query. One feature might be \(N(x)\), the number of people who purchased the product \(x\). Intuitively, relevance increases with \(N(x)\), but the relationship is not linear—beyond a certain point, each additional purchase contributes progressively less to relevance, reflecting diminishing returns.</p> <h6 id="the-solution"><strong>The Solution:</strong></h6> <p>To address saturation, we can apply nonlinear transformations to the feature. Two common methods are:</p> <ol> <li> <p><strong>Smooth nonlinear transformation</strong>:</p> \[\phi(x) = [1, \log(1 + N(x))]\] <p>The logarithm is particularly effective for features with large dynamic ranges, as it captures diminishing returns naturally.</p> </li> <li> <p><strong>Discretization</strong>:</p> \[\phi(x) = [1[0 \leq N(x) &lt; 10], 1[10 \leq N(x) &lt; 100], \ldots]\] <p>By bucketing \(N(x)\) into intervals, this method provides flexibility while maintaining interpretability.</p> </li> </ol> <h5 id="interactions-when-features-work-together"><strong>Interactions: When Features Work Together</strong></h5> <p>Suppose we want to predict a health score based on a patient’s height \(h\) and weight \(w\). Using a feature map \(\phi(x) = [h(x), w(x)]\) assumes these features independently influence the outcome. However, it’s the relationship between height and weight (e.g., body mass index) that matters most.</p> <h6 id="approach-1-domain-specific-features"><strong>Approach 1: Domain-Specific Features</strong></h6> <p>One way to capture this interaction is to use domain knowledge, such as Robinson’s ideal weight formula: \(\text{weight(kg)} = 52 + 1.9 \cdot (\text{height(in)} - 60)\)</p> <p>We can then score the deviation between actual weight \(w\) and ideal weight: \(f(x) = \left(52 + 1.9 \cdot [h(x) - 60] - w(x)\right)^2\)</p> <p>While precise, this approach depends heavily on external knowledge and is less adaptable to other problems.</p> <h6 id="approach-2-general-interaction-terms"><strong>Approach 2: General Interaction Terms</strong></h6> <p>A more flexible solution is to include all second-order features: \(\phi(x) = [1, h(x), w(x), h(x)^2, w(x)^2, h(x)w(x)]\) This approach eliminates the need for predefined formulas, letting the model discover relationships on its own. By using interaction terms like \(h(x)w(x)\), we can model complex dependencies in the data.</p> <hr/> <h4 id="monomial-interaction-terms-a-building-block-for-nonlinearity"><strong>Monomial Interaction Terms: A Building Block for Nonlinearity</strong></h4> <p>Interaction terms are fundamental for modeling nonlinearities. Starting with an input \(x = [1, x_1, \ldots, x_d]\), we can add monomials of degree \(M\), such as:</p> \[x_1^{p_1} \cdot x_2^{p_2} \cdots x_d^{p_d}, \quad \text{where } p_1 + \cdots + p_d = M\] <p>For example, in a 2D space with \(M = 2\), the interaction terms would include \(x_1^2, x_2^2, \text{and } x_1x_2\).</p> <h6 id="big-feature-spaces-challenges-and-solutions"><strong>Big Feature Spaces: Challenges and Solutions</strong></h6> <p>Adding interaction terms and monomials rapidly increases the size of the feature space. For \(d = 40\) and \(M = 8\), the number of features grows to an astronomical \(314,457,495\). Such large feature spaces bring two major challenges:</p> <ol> <li><strong>Overfitting</strong>: Addressed through regularization techniques like \(L1/L2\) penalties.</li> <li><strong>Memory and Computational Costs</strong>: Kernel methods can help handle high (or infinite) dimensional spaces efficiently by computing feature interactions implicitly.</li> </ol> <p>By leveraging feature maps and understanding the nuances of nonlinearities, we can significantly enhance the performance of linear models. In the next part, we’ll explore kernel methods and their role in handling complex feature spaces efficiently. Stay tuned, Bye 👋!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><summary type="html"><![CDATA[Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.]]></summary></entry><entry><title type="html">Demystifying SVMs - Understanding Complementary Slackness and Support Vectors</title><link href="https://monishver11.github.io/blog/2025/svm-dual-problem/" rel="alternate" type="text/html" title="Demystifying SVMs - Understanding Complementary Slackness and Support Vectors"/><published>2025-01-10T21:02:00+00:00</published><updated>2025-01-10T21:02:00+00:00</updated><id>https://monishver11.github.io/blog/2025/svm-dual-problem</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/svm-dual-problem/"><![CDATA[<p>At the heart of SVMs lies a fascinating optimization framework that balances maximizing the margin between classes and minimizing classification errors. This post dives into the dual formulation of the SVM optimization problem, exploring its mathematical underpinnings, derivation, and insights.</p> <hr/> <h4 id="the-svm-primal-problem"><strong>The SVM Primal Problem</strong></h4> <p>To understand the dual problem, we first start with the <strong>primal optimization problem</strong> of SVMs. It aims to find the optimal hyperplane that separates two classes while allowing for some misclassification through slack variables. The primal problem is expressed as:</p> \[\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + \frac{c}{n} \sum_{i=1}^n \xi_i\] <p>subject to the constraints:</p> \[-\xi_i \leq 0 \quad \text{for } i = 1, \dots, n\] \[1 - y_i (w^T x_i + b) - \xi_i \leq 0 \quad \text{for } i = 1, \dots, n\] <p>Here:</p> <ul> <li>\(w\) is the weight vector defining the hyperplane,</li> <li>\(b\) is the bias term,</li> <li>\(\xi_i\) are slack variables that allow some points to violate the margin, and</li> <li>\(C\) is the regularization parameter controlling the trade-off between maximizing the margin and minimizing errors.</li> </ul> <h4 id="lagrangian-formulation"><strong>Lagrangian Formulation</strong></h4> <p>To solve this constrained optimization problem, we use the method of <strong>Lagrange multipliers</strong>. Introducing \(\alpha_i\) and \(\lambda_i\) as multipliers for the inequality constraints, the <strong>Lagrangian</strong> becomes:</p> \[L(w, b, \xi, \alpha, \lambda) = \frac{1}{2} \|w\|^2 + \frac{c}{n} \sum_{i=1}^n \xi_i + \sum_{i=1}^n \alpha_i \left( 1 - y_i (w^T x_i + b) - \xi_i \right) + \sum_{i=1}^n \lambda_i (-\xi_i)\] <p>Here, the terms involving \(\alpha_i\) and \(\lambda_i\) enforce the constraints, while the first term captures the objective of maximizing the margin.</p> <table> <thead> <tr> <th>Lagrange Multiplier</th> <th>Constraint</th> </tr> </thead> <tbody> <tr> <td>\(\lambda_i\)</td> <td>\(-\xi_i \leq 0\)</td> </tr> <tr> <td>\(\alpha_i\)</td> <td>\((1 - y_i[w^T x_i + b]) - \xi_i \leq 0\)</td> </tr> </tbody> </table> <hr/> <h4 id="strong-duality-and-slaters-condition"><strong>Strong Duality and Slater’s Condition</strong></h4> <p>The next step is to leverage <strong>strong duality</strong>, which states that for certain optimization problems, the dual problem provides the same optimal value as the primal. For SVMs, strong duality holds due to <strong>Slater’s constraint qualification</strong>, which requires the problem to:</p> <ul> <li>Have a convex objective function,</li> <li>Include affine constraints, and</li> <li>Possess feasible points.</li> </ul> <p>In the context of <strong>Slater’s constraint qualification</strong> and <strong>strong duality</strong> for SVMs, <strong>feasible points</strong> refer to points in the feasible region that satisfy all the constraints of the primal optimization problem. Specifically, for SVMs, these points are:</p> <ol> <li> <p><strong>Convex Objective Function</strong>: The objective of the SVM (maximizing the margin, which is a quadratic optimization problem) is convex, meaning it has a global minimum.</p> </li> <li> <p><strong>Affine Constraints</strong>: These constraints are linear equations (or inequalities) that define the feasible region, such as ensuring that all data points are correctly classified. In mathematical form, for each data point \(y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1\).</p> </li> <li> <p><strong>Existence of Feasible Points</strong>: There must be at least one point in the domain that satisfies all of these constraints. In SVMs, this is satisfied when the data is linearly separable, meaning there exists a hyperplane that can perfectly separate the positive and negative classes. Slater’s condition requires that there be strictly feasible points, where the constraints are strictly satisfied (i.e., not just touching the boundary of the feasible region).</p> </li> </ol> <p>For SVMs, the feasible points are those that satisfy: \(y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 \quad \text{for all data points}\)</p> <p>These points are strictly inside the feasible region, meaning there is a margin between the hyperplane and the data points, ensuring a gap.</p> <p>In practical terms, <strong>Slater’s condition</strong> implies that there exists a hyperplane that not only separates the two classes but also satisfies the strict inequalities for the margin (i.e., it does not lie on the boundary). This strict feasibility is critical for the <strong>strong duality</strong> theorem to hold.</p> <h4 id="deriving-the-svm-dual-function"><strong>Deriving the SVM Dual Function</strong></h4> <p>The dual function is obtained by minimizing the Lagrangian over the primal variables \(w\), \(b\), and \(\xi\):</p> \[g(\alpha, \lambda) = \inf_{w, b, \xi} L(w, b, \xi, \alpha, \lambda)\] <p>This can be simplified to (after shuffling and grouping):</p> \[g(\alpha, \lambda) = \inf_{w, b, \xi} \left[ \frac{1}{2} w^T w + \sum_{i=1}^n \xi_i \left( \frac{c}{n} - \alpha_i - \lambda_i \right) + \sum_{i=1}^n \alpha_i \left( 1 - y_i \left[ w^T x_i + b \right] \right) \right]\] <p>This minimization leads to the following <strong>first-order optimality conditions</strong>:</p> <ol> <li> <p><strong>Gradient with respect to \(w\):</strong> Differentiating \(L\) with respect to \(w\), we get:</p> \[\frac{\partial L}{\partial w} = w - \sum_{i=1}^n \alpha_i y_i x_i = 0\] <p>Solving for \(w\), we find:</p> \[w = \sum_{i=1}^n \alpha_i y_i x_i\] </li> <li> <p><strong>Gradient with respect to \(b\):</strong> Differentiating \(L\) with respect to \(b\), we obtain:</p> \[\frac{\partial L}{\partial b} = -\sum_{i=1}^n \alpha_i y_i = 0\] <p>This implies the constraint:</p> \[\sum_{i=1}^n \alpha_i y_i = 0\] </li> <li> <p><strong>Gradient with respect to \(\xi_i\):</strong> Differentiating \(L\) with respect to \(\xi_i\), we have:</p> \[\frac{\partial L}{\partial \xi_i} = \frac{c}{n} - \alpha_i - \lambda_i = 0\] <p>This leads to the relationship:</p> \[\alpha_i + \lambda_i = \frac{c}{n}\] </li> </ol> <h4 id="the-svm-dual-problem"><strong>The SVM Dual Problem</strong></h4> <p>Substituting these conditions back into \(L\)(Lagrangian), the second term disappears.</p> <p>First and third terms become:</p> \[\frac{1}{2}w^T w = \frac{1}{2}\sum_{i,j} \alpha_i \alpha_j y_i y_j x_j^T x_i\] \[\sum_{i=1}^n \alpha_i \left( 1 - y_i \left[ w^T x_i + b \right] \right) = \sum_i \alpha_i - \sum_{i,j} \alpha_i \alpha_j y_i y_j x_j^T x_i - b \sum_{i=1}^n \alpha_i y_i\] <p>Putting it together, the dual function is:</p> \[g(\alpha, \lambda) = \begin{cases} \sum_{i=1}^{n} \alpha_i - \frac{1}{2}\sum_{i,j=1}^{n} \alpha_i \alpha_j y_i y_j x_j^T x_i &amp; \text{if } \sum_{i=1}^{n} \alpha_i y_i = 0 \text{ and } \alpha_i + \lambda_i = \frac{c}{n}, \text{ all } i \\ -\infty &amp; \text{otherwise} \end{cases}\] <p><strong>Quick tip</strong>: Go ahead and write the derivation yourself to see what cancels out. It’s much easier to follow the flow this way, and you’ll better understand how the second term in the equation above is derived.</p> <p><strong>The dual problem is</strong></p> \[\sup_{\alpha, \lambda \geq 0} g(\alpha, \lambda)\] \[\text{s.t. } \begin{cases} \sum_{i=1}^{n} \alpha_i y_i = 0 \\ \alpha_i + \lambda_i = \frac{c}{n}, \text{ } \alpha_i, \lambda_i \geq 0, \text{ } i = 1, ..., n \end{cases}\] <p>Don’t stress over this complex equation; we’ll break down its meaning and significance as we continue. Keep reading!</p> <h5 id="insights-from-the-dual-problem"><strong>Insights from the Dual Problem</strong></h5> <p>The dual problem offers several key insights into the optimization process of SVMs:</p> <ol> <li> <p><strong>Duality and Optimality:</strong><br/> Strong duality ensures that the primal and dual problems yield the same optimal value, provided conditions like Slater’s are met.</p> </li> <li> <p><strong>Dual Variables:</strong><br/> The variables \(\alpha_i\) and \(\lambda_i\) are Lagrange multipliers, indicating how sensitive the objective function is to the constraints. Large \(\alpha_i\) values correspond to constraints that are most violated.</p> </li> <li> <p><strong>Constraint Interpretation:</strong><br/> The constraint \(\sum_{i=1}^{n} \alpha_i y_i = 0\) ensures the hyperplane passes through the origin, while \(\alpha_i + \lambda_i = \frac{c}{n}\) connects the dual variables with the regularization parameter \(c\).</p> </li> <li> <p><strong>Support Vectors:</strong><br/> Non-zero \(\alpha_i\) values indicate support vectors, which are the data points closest to the decision boundary and crucial for defining the margin.</p> </li> <li> <p><strong>Weight Vector Representation:</strong><br/> The weight vector \(w\) lies in the space spanned by the support vectors: \(w = \sum_{i=1}^n \alpha_i y_i x_i\)</p> </li> </ol> <p>In essence, the dual problem simplifies the primal by focusing on constraints and provides insights into how data points affect the model’s decision boundary.</p> <h4 id="kkt-conditions"><strong>KKT Conditions</strong></h4> <p>For convex problems, if Slater’s condition is satisfied, the <strong>Karush-Kuhn-Tucker (KKT) conditions</strong> provide necessary and sufficient conditions for the optimal solution. For the <strong>SVM dual problem</strong>, these conditions can be expressed as:</p> <ul> <li> <p><strong>Primal Feasibility:</strong> \(f_i(x) \leq 0 \quad \forall i\)<br/> This condition ensures that the constraints of the primal problem are satisfied. In the context of SVM, this means that all data points are correctly classified by the hyperplane, i.e., for each data point \(i\), the constraint \(y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1\) is satisfied.</p> </li> <li> <p><strong>Dual Feasibility:</strong> \(\lambda_i \geq 0\)<br/> This condition ensures that the dual variables (Lagrange multipliers) are non-negative. For SVMs, it means the Lagrange multipliers \(\alpha_i\) associated with the classification constraints must be non-negative, i.e., \(\alpha_i \geq 0\).</p> </li> <li> <p><strong>Complementary Slackness:</strong> \(\lambda_i f_i(x) = 0\)<br/> This condition means that either the constraint is <strong>active</strong> (i.e., the constraint is satisfied with equality) or the dual variable is zero. For SVMs, it implies that if a data point is a support vector (i.e., it lies on the margin), then the corresponding \(\alpha_i\) is positive. Otherwise, for non-support vectors, \(\alpha_i = 0\).</p> </li> <li> <p><strong>First-Order Condition:</strong> \(\frac{\partial}{\partial x} L(x, \lambda) = 0\)<br/> The first-order condition ensures that the Lagrangian \(L(x, \lambda)\) is minimized with respect to the optimization variables. In SVMs, this condition leads to the optimal weights \(\mathbf{w}\) and bias \(b\) that define the separating hyperplane.</p> </li> </ul> <p><strong>To summarize</strong>:</p> <ul> <li><strong>Slater’s Condition</strong> ensures strong duality.</li> <li><strong>KKT Conditions</strong> ensure the existence of the optimal solution and give the specific conditions under which the solution occurs.</li> </ul> <h4 id="the-svm-dual-solution"><strong>The SVM Dual Solution</strong></h4> <p>We can express the <strong>SVM dual problem</strong> as follows:</p> \[\sup_{\alpha} \sum_{i=1}^{n} \alpha_{i} - \frac{1}{2} \sum_{i,j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} x_{j}^{T} x_{i}\] <p>subject to:</p> \[\sum_{i=1}^{n} \alpha_{i} y_{i} = 0\] \[\alpha_{i} \in [0, \frac{c}{n}], \quad i = 1, ..., n\] <p>In this formulation, \(\alpha_i\) are the Lagrange multipliers, which must satisfy the constraints. The dual problem maximizes the objective function involving these multipliers, while ensuring that the constraints are met.</p> <p>Once we have the optimal solution \(\alpha^*\) to the dual problem, the primal solution \(w^*\) can be derived as:</p> \[w^{*} = \sum_{i=1}^{n} \alpha_{i}^{*} y_{i} x_{i}\] <p>This shows that the optimal weight vector \(w^*\) is a linear combination of the input vectors \(x_i\), weighted by the corresponding \(\alpha_i^*\) and \(y_i\).</p> <p>It’s important to note that the solution is in the <strong>space spanned by the inputs</strong>. This means the decision boundary is influenced by the data points that lie closest to the hyperplane, i.e., the <strong>support vectors</strong>.</p> <p>The constraints \(\alpha_{i} \in [0, c/n]\) indicate that \(c\) controls the maximum weight assigned to each example. In other words, \(c\) acts as a regularization parameter, controlling the trade-off between achieving a large margin and minimizing classification errors. A larger \(c\) leads to less regularization, allowing the model to fit more closely to the training data, while a smaller \(c\) introduces more regularization, promoting a simpler model that may generalize better.</p> <p>Think of \(c\) as a <strong>“penalty meter”</strong> that controls how much you care about fitting the training data:</p> <ul> <li>A <strong>high \(c\)</strong> means you are <strong>less tolerant of mistakes</strong>. The model will try to fit the data perfectly, even if it leads to overfitting (less regularization).</li> <li>A <strong>low \(c\)</strong> means you’re more focused on <strong>simplicity and generalization</strong>. The model will allow some mistakes in the training data to avoid overfitting and create a smoother decision boundary (more regularization).</li> </ul> <p>Next, we will explore how the <strong>Complementary Slackness</strong> condition in the SVM dual formulation extends to <strong>kernel trick</strong>, enabling SVMs to handle non-linear decision boundaries effectively.</p> <hr/> <h4 id="understanding-complementary-slackness-in-svms"><strong>Understanding Complementary Slackness in SVMs</strong></h4> <p>In this section, we will focus on <strong>complementary slackness</strong>, a key property of optimization problems, and its implications for SVMs. Then we will explore how it connects with the margin, slack variables, and the role of support vectors.</p> <h5 id="revisiting-constraints-and-lagrange-multipliers"><strong>Revisiting Constraints and Lagrange Multipliers</strong></h5> <p>To understand complementary slackness, let’s start by recalling the constraints and Lagrange multipliers in the SVM problem:</p> <ol> <li> <p>The constraint on the slack variables:</p> \[-\xi_i \leq 0,\] <p>with Lagrange multiplier \(\lambda_i\).</p> </li> <li> <p>The margin constraint:</p> \[1 - y_i f(x_i) - \xi_i \leq 0,\] <p>with Lagrange multiplier \(\alpha_i\).</p> </li> </ol> <p>From the <strong>first-order condition</strong> with respect to \(\xi_i\), we derived the relationship:</p> \[\lambda_i^* = \frac{c}{n} - \alpha_i^*,\] <p>where \(c\) is the regularization parameter.</p> <p>By <strong>strong duality</strong>, the complementary slackness conditions must hold, which state:</p> \[\alpha_i^* \left( 1 - y_i f^*(x_i) - \xi_i^* \right) = 0\] <p>and,</p> \[\lambda_i^* \xi_i^* = \left( \frac{c}{n} - \alpha_i^* \right) \xi_i^* = 0\] <p>These conditions essentially enforce that either the constraints are satisfied exactly or their corresponding Lagrange multipliers vanish.</p> <h5 id="what-does-complementary-slackness-tell-us"><strong>What Does Complementary Slackness Tell Us?</strong></h5> <p>Complementary slackness provides crucial insights into the relationship between the dual variables \(\alpha_i^*\), the slack variables \(\xi_i^*\), and the margin \(1 - y_i f^*(x_i)\). Let’s break this down:</p> <ul> <li><strong>When \(y_i f^*(x_i) &gt; 1\):</strong> <ul> <li>The margin loss is zero (\(\xi_i^* = 0\)), meaning the data point is correctly classified and lies outside the margin.</li> <li>As a result, \(\alpha_i^* = 0\). Since the dual variable \(\alpha_i^*\) only applies to active constraints, a zero value indicates that this example has no effect on the decision boundary. These are non-support vectors that do not influence the margin or hyperplane.</li> </ul> </li> <li><strong>When \(y_i f^*(x_i) &lt; 1\):</strong> <ul> <li>The margin loss is positive (\(\xi_i^* &gt; 0\)), meaning the data point either lies inside the margin or is misclassified.</li> <li>In this case, \(\alpha_i^* = \frac{c}{n}\), assigning the maximum weight to these examples. These points are critical as they represent either boundary violations or significant misclassifications, making them influential in determining the hyperplane.</li> </ul> </li> <li><strong>When \(\alpha_i^* = 0\):</strong> <ul> <li>This implies \(\xi_i^* = 0\), meaning the margin loss is zero and the data point satisfies \(y_i f^*(x_i) \geq 1\).</li> <li>Such examples are correctly classified and lie well outside the margin, contributing nothing to the optimization. They remain irrelevant to the final decision boundary.</li> </ul> </li> <li><strong>When \(\alpha_i^* \in (0, \frac{c}{n})\):</strong> <ul> <li>This implies \(\xi_i^* = 0\), so the example lies exactly on the margin, where \(1 - y_i f^*(x_i) = 0\).</li> <li>These are the <strong>support vectors</strong>, the critical points that define the hyperplane. Their non-zero \(\alpha_i^*\) values indicate their contribution to maximizing the margin while satisfying the constraints.</li> </ul> </li> </ul> <p><strong>Why It Matters?</strong> Complementary slackness essentially acts as a filter, identifying which examples influence the decision boundary (support vectors) and which do not. It helps focus only on the most relevant points, reducing computational complexity and enhancing the interpretability of the model.</p> <p>We can summarize these relationships(between margin and example weights) as follows:</p> <ol> <li><strong>If \(\alpha_i^* = 0\):</strong> The example satisfies \(y_i f^*(x_i) \geq 1\), indicating no margin loss.</li> <li><strong>If \(\alpha_i^* \in (0, \frac{c}{n})\):</strong> The example lies exactly on the margin, with \(y_i f^*(x_i) = 1\).</li> <li><strong>If \(\alpha_i^* = \frac{c}{n}\):</strong> The example incurs a margin loss, with \(y_i f^*(x_i) \leq 1\).</li> </ol> <p>and the other way is:</p> \[y_if^*(x_i) &lt; 1 \Rightarrow α_i^* = \frac{c}{n}\] \[y_if^*(x_i) = 1 \Rightarrow α_i^* \in [0, \frac{c}{n}]\] \[y_if^*(x_i) &gt; 1 \Rightarrow α_i^* = 0\] <p>These relationships are foundational to understanding how SVMs allocate weights to examples and define the decision boundary.</p> <h5 id="analogy-tug-of-war-with-a-rope"><strong>Analogy: Tug of War with a Rope</strong></h5> <p>Imagine a tug-of-war game where each data point is trying to pull a rope (the decision boundary) towards itself. The strength of the pull (weight \(\alpha_i^*\)) depends on how far the point is from the ideal margin position:</p> <ol> <li><strong>If the point is far outside the margin (\(y_i f^*(x_i) &gt; 1\)):</strong> <ul> <li><strong>No pull (\(\alpha_i^* = 0\)):</strong><br/> The point is satisfied with its position and doesn’t exert any force on the rope. It’s correctly classified and irrelevant to defining the decision boundary.</li> </ul> </li> <li><strong>If the point is exactly on the margin (\(y_i f^*(x_i) = 1\)):</strong> <ul> <li><strong>Light pull (\(\alpha_i^* \in [0, \frac{c}{n}]\)):</strong><br/> The point contributes just enough force to keep the rope in its place. These are the <strong>support vectors</strong>, the critical points holding the boundary in position.</li> </ul> </li> <li><strong>If the point is inside the margin or misclassified (\(y_i f^*(x_i) &lt; 1\)):</strong> <ul> <li><strong>Maximum pull (\(\alpha_i^* = \frac{c}{n}\)):</strong><br/> The point exerts its full force, pulling the boundary to correct the violation. These points dominate the optimization problem because they need the most adjustment.</li> </ul> </li> </ol> <p>Looking at it from the perspective of \(y_i f^*(x_i)\):</p> <ul> <li><strong>\(y_i f^*(x_i) &gt; 1\):</strong> No pull (\(\alpha_i^* = 0\)) – the point is far and satisfied.</li> <li><strong>\(y_i f^*(x_i) = 1\):</strong> Light pull (\(\alpha_i^* \in [0, \frac{c}{n}]\)) – the point is holding the margin.</li> <li><strong>\(y_i f^*(x_i) &lt; 1\):</strong> Maximum pull (\(\alpha_i^* = \frac{c}{n}\)) – the point is violating the margin.</li> </ul> <p>This helps you remember which data points influence the decision boundary and how they do so.</p> <hr/> <h4 id="support-vectors-the-pillars-of-svms"><strong>Support Vectors: The Pillars of SVMs</strong></h4> <p>The dual formulation of SVMs reveals that the weight vector \(w^*\) can be expressed as:</p> \[w^* = \sum_{i=1}^n \alpha_i^* y_i x_i.\] <p>Here, the examples \(x_i\) with \(\alpha_i^* &gt; 0\) (Few margin errors or “on the margin”) are known as <strong>support vectors</strong>. These are the critical data points that determine the hyperplane. Examples with \(\alpha_i^* = 0\) do not influence the solution, leading to <strong>sparsity</strong> in the SVM model. This sparsity is one of the key reasons why SVMs are computationally efficient for large datasets.</p> <h5 id="the-role-of-inner-products-in-the-dual-problem"><strong>The Role of Inner Products in the Dual Problem</strong></h5> <p>An intriguing aspect of the dual problem is that it depends on the input data \(x_i\) and \(x_j\) only through their <strong>inner product</strong>:</p> \[\langle x_j, x_i \rangle = x_j^T x_i.\] <p>This dependence on inner products allows us to generalize SVMs using <strong>kernel methods</strong>, where the inner product \(x_j^T x_i\) is replaced with a kernel function \(K(x_j, x_i)\). Kernels enable SVMs to implicitly operate in high-dimensional feature spaces without explicitly transforming the data, making it possible to model complex, non-linear decision boundaries.</p> <p>The kernelized dual problem is written as:</p> \[\sup_{\alpha} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i,j=1}^{n} \alpha_i \alpha_j y_i y_j K(x_j, x_i),\] <p>subject to:</p> <ul> <li>\(\sum_{i=1}^n \alpha_i y_i = 0\),</li> <li>\(0 \leq \alpha_i \leq \frac{c}{n}\), for \(i = 1, \dots, n\).</li> </ul> <p>We’ll dive into kernels next and explore how this powerful trick enhances the usefulness of SVMs.</p> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>Complementary slackness conditions reveal much about the structure and workings of SVMs. They show how the margin, slack variables, and dual variables interact and highlight the pivotal role of support vectors. Moreover, the reliance on inner products paves the way for kernel methods, unlocking the power of SVMs for non-linear classification problems.</p> <p>In the next post, we’ll explore kernel functions in depth, including popular choices like Gaussian and polynomial kernels, and see how they influence SVM performance. See you!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Math parts verification</li> <li><a href="https://www.stat.cmu.edu/~ryantibs/convexopt-F16/scribes/kkt-scribed.pdf"> KKT conditions - KKT conditions</a></li> <li><a href="https://math.stackexchange.com/questions/2162932/big-picture-behind-how-to-use-kkt-conditions-for-constrained-optimization">Big picture behind how to use KKT conditions for constrained optimization</a></li> <li><a href="https://davidrosenberg.github.io/mlcourse/Archive/2019/Notes/SVM-main-points.pdf">SVM: Main Takeaways from Duality</a></li> <li><a href="https://davidrosenberg.github.io/mlcourse/Archive/2019/Notes/convex-optimization.pdf">Extreme Abridgment of Boyd and Vandenberghe’s Convex Optimization</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.]]></summary></entry><entry><title type="html">Subgradient and Subgradient Descent</title><link href="https://monishver11.github.io/blog/2025/subgradient/" rel="alternate" type="text/html" title="Subgradient and Subgradient Descent"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://monishver11.github.io/blog/2025/subgradient</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/subgradient/"><![CDATA[<p>Optimization is a cornerstone of machine learning, as it allows us to fine-tune models and minimize error. For smooth, differentiable functions, gradient descent is the go-to method. However, in the real world, many functions are not differentiable. This is where <strong>subgradients</strong> come into play. In this post, we’ll explore subgradients, understand their properties, and see how they are used in subgradient descent. Finally, we’ll dive into their application in support vector machines (SVMs).</p> <hr/> <h4 id="first-order-condition-for-convex-differentiable-functions"><strong>First-Order Condition for Convex, Differentiable Functions</strong></h4> <p>Let’s start with the basics of convex functions. For a function \(f : \mathbb{R}^d \to \mathbb{R}\) that is convex and differentiable, the <strong>first-order condition</strong> states that:</p> \[f(y) \geq f(x) + \nabla f(x)^\top (y - x), \quad \forall x, y \in \mathbb{R}^d\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Subgradient_1-480.webp 480w,/assets/img/Subgradient_1-800.webp 800w,/assets/img/Subgradient_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Subgradient_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Subgradient_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This equation tells us something profound: the linear approximation to \(f\) at \(x\) is a global underestimator of the function. In other words, the gradient provides a plane below the function, ensuring the convexity property holds. A direct implication is that if \(\nabla f(x) = 0\), then \(x\) is a global minimizer of \(f\). This serves as the foundation for gradient-based optimization.</p> <h4 id="subgradients-a-generalization-of-gradients"><strong>Subgradients: A Generalization of Gradients</strong></h4> <p>While gradients work well for differentiable functions, what happens when a function has kinks or sharp corners? This is where <strong>subgradients</strong> step in. A subgradient is a generalization of the gradient, defined as follows:</p> <p>A vector \(g \in \mathbb{R}^d\) is a subgradient of a convex function \(f : \mathbb{R}^d \to \mathbb{R}\) at \(x\) if:</p> \[f(z) \geq f(x) + g^\top (z - x), \quad \forall z \in \mathbb{R}^d\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Subgradient_2-480.webp 480w,/assets/img/Subgradient_2-800.webp 800w,/assets/img/Subgradient_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Subgradient_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Subgradient_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Subgradients essentially maintain the same idea as gradients: they provide a linear function that underestimates \(f\). However, while the gradient is unique, a subgradient can belong to a set of possible vectors called the <strong>subdifferential</strong>, denoted \(\partial f(x)\).</p> <p>For convex functions, the subdifferential is always non-empty. If \(f\) is differentiable at \(x\), the subdifferential collapses to a single point: \(\{ \nabla f(x) \}\). Importantly, for a convex function, a point \(x\) is a global minimizer if and only if \(0 \in \partial f(x)\). This property allows us to apply subgradients even when gradients don’t exist.</p> <hr/> <p><strong>To build an understanding for subgradients</strong>, think of them as a safety net for optimization when the function isn’t smooth. Imagine you’re skiing down a mountain, and instead of a smooth slope, you encounter a flat plateau or a jagged edge. A regular gradient would fail to guide you because there’s no single steepest slope at such points. Subgradients, however, provide a range of valid directions — all the paths you could safely take without “climbing uphill.”</p> <p>In mathematical terms, subgradients are generalizations of gradients. For a non-differentiable function, the subdifferential \(\partial f(x)\) at a point \(x\) is the set of all possible subgradients. Each of these subgradients \(g\) satisfies:</p> \[f(z) \geq f(x) + g^\top (z - x), \quad \forall z \in \mathbb{R}^d\] <p>This means that every subgradient defines a hyperplane that lies below the function \(f(x)\). It acts as a valid approximation of the function’s behavior, even at sharp corners or flat regions.</p> <p>Here’s a useful way to interpret subgradients: if you were to try and “push” the function \(f(x)\) upward using any of the subgradients at \(x\), you would never exceed the true value of \(f(z)\). Subgradients provide all the slopes that respect this property, even when the function isn’t smooth.</p> <p>Finally, the key insight: if the zero vector is in the subdifferential, \(0 \in \partial f(x)\), it means that there’s no descent direction left — you’ve reached the global minimum. Subgradients help us navigate these tricky, non-smooth terrains where gradients fail, making them a versatile tool in optimization.</p> <h5 id="subgradient-example-absolute-value-function"><strong>Subgradient Example: Absolute Value Function</strong></h5> <p>Let’s consider a classic example: \(f(x) = \lvert x \rvert\). This function is non-differentiable at \(x = 0\), making it an ideal candidate to illustrate subgradients. The subdifferential of \(f(x)\) is:</p> \[\partial f(x) = \begin{cases} \{-1\} &amp; \text{if } x &lt; 0, \\ \{1\} &amp; \text{if } x &gt; 0, \\ [-1, 1] &amp; \text{if } x = 0. \end{cases}\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Subgradient_3-480.webp 480w,/assets/img/Subgradient_3-800.webp 800w,/assets/img/Subgradient_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Subgradient_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Subgradient_3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The plot on the right shows: $$ \{(x, g) \mid x \in \mathbb{R}, g \in \partial f(x)\} $$ </div> <p>At \(x = 0\), the subgradient set contains all values in the interval \([-1, 1]\), which corresponds to all possible slopes of lines that underapproximate \(f(x)\) at that point.</p> <h4 id="subgradient-descent-the-optimization-method"><strong>Subgradient Descent: The Optimization Method</strong></h4> <p>Subgradient descent extends gradient descent to non-differentiable convex functions. The update rule is simple:</p> \[x_{t+1} = x_t - \eta_t g,\] <p>where \(g \in \partial f(x_t)\) is a subgradient, and \(\eta_t &gt; 0\) is the step size.</p> <p>Unlike gradients, subgradients do not always converge to zero as the algorithm progresses. <strong>Why?</strong> This is because subgradients are not as tightly coupled to the function’s local geometry as gradients are. In gradient descent, the gradient vanishes at critical points (e.g., minima, maxima, or saddle points), forcing the updates to slow down as the algorithm approaches a minimum. However, in subgradient descent, the subgradients remain non-zero even near a minimum due to the nature of subdifferentiability.</p> <p>For example, consider the absolute value function, \(f(x) = \lvert x \rvert\), which has a sharp corner at \(x = 0\). The subdifferential at \(x = 0\) is the interval \([-1, 1]\), meaning that even at the minimizer \(x^\ast = 0\), valid subgradients exist (e.g., \(g = 0.5\)). Consequently, the algorithm does not inherently “slow down” as it approaches the minimum, unlike gradient descent.</p> <p>This behavior requires us to carefully choose a decreasing step size \(\eta_t\) to make sure that the updates shrink over time, leading to convergence.</p> <p>For convex functions, subgradient descent ensures that the iterates move closer to the minimizer:</p> \[\|x_{t+1} - x^\ast\| &lt; \|x_t - x^\ast\|,\] <p>provided that the step size is small enough. However, subgradient methods tend to be slower than gradient descent because they rely on weaker information about the function’s structure.</p> <h4 id="subgradient-descent-for-svms-the-pegasos-algorithm"><strong>Subgradient Descent for SVMs: The Pegasos Algorithm</strong></h4> <p>Subgradients are particularly useful in optimizing the objective function of support vector machines (SVMs). The SVM objective is:</p> \[J(w) = \frac{1}{n} \sum_{i=1}^n \max(0, 1 - y_i w^\top x_i) + \frac{\lambda}{2} \|w\|^2,\] <p>where the first term represents the hinge loss, and the second term penalizes large weights to prevent overfitting. Optimizing this objective using gradient-based methods is tricky due to the non-differentiability of the hinge loss. Enter <strong>Pegasos</strong>, a stochastic subgradient descent algorithm.</p> <h5 id="the-pegasos-algorithm"><strong>The Pegasos Algorithm</strong></h5> <p>The Pegasos algorithm follows these steps:</p> <ol> <li>Initialize \(w_1 = 0\), \(t = 0\) and \(\lambda &gt; 0\)</li> <li>Randomly permute the data at the beginning of each epoch.</li> <li>While termination condition not met: <ul> <li>For each data point \((x_j, y_j)\), update the parameters: <ul> <li>Increment \(t\): \(t = t + 1\).</li> <li>Compute the step size: \(\eta_t = \frac{1}{t \lambda}\).</li> <li>If \(y_j w_t^\top x_j &lt; 1\), update: \(w_{t+1} = (1 - \eta_t \lambda) w_t + \eta_t y_j x_j.\)</li> <li>Otherwise, update: \(w_{t+1} = (1 - \eta_t \lambda) w_t.\)</li> </ul> </li> </ul> </li> </ol> <p>This algorithm cleverly combines subgradient updates with a decreasing step size to ensure convergence. The hinge loss ensures that the model maintains a margin of separation, while the regularization term prevents overfitting. By carefully adjusting the step size \(\eta_t = \frac{1}{t \lambda}\), Pegasos ensures convergence to the optimal \(w\) while handling the non-differentiable hinge loss.</p> <h6 id="derivation-for-both-cases"><strong>Derivation for Both Cases:</strong></h6> <ol> <li> <p><strong>Case 1: \(y_j w_t^\top x_j &lt; 1\) (misclassified or on the margin)</strong></p> <p>The subgradient of the hinge loss for this case is:</p> \[\nabla \text{hinge loss} = -y_j x_j.\] <p>Adding the subgradient of the regularization term:</p> \[\nabla J(w) = -y_j x_j + \lambda w_t.\] <p>Using the subgradient descent update rule:</p> \[w_{t+1} = w_t - \eta_t \nabla J(w),\] <p>we get:</p> \[w_{t+1} = w_t - \eta_t (-y_j x_j + \lambda w_t).\] <p>Simplifying:</p> \[w_{t+1} = (1 - \eta_t \lambda) w_t + \eta_t y_j x_j.\] </li> <li> <p><strong>Case 2: \(y_j w_t^\top x_j \geq 1\) (correctly classified)</strong></p> <p>In this case, the hinge loss is zero, so the subgradient is purely from the regularization term:</p> \[\nabla J(w) = \lambda w_t.\] <p>The subgradient descent update rule becomes:</p> \[w_{t+1} = w_t - \eta_t \nabla J(w).\] <p>Substituting \(\nabla J(w) = \lambda w_t\):</p> \[w_{t+1} = w_t - \eta_t \lambda w_t.\] <p>Simplifying:</p> \[w_{t+1} = (1 - \eta_t \lambda) w_t.\] </li> </ol> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>Subgradients are a powerful tool for dealing with non-differentiable convex functions, and subgradient descent provides a straightforward yet effective way to optimize such functions. While slower than gradient descent, subgradient descent shines in scenarios where gradients are unavailable.</p> <p>In the next part of this series, we’ll delve into the <strong>dual problem</strong> and uncover its connection to the primal SVM formulation. Stay tuned for more insights into the fascinating world of ML!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.]]></summary></entry><entry><title type="html">The Dual Problem of SVM</title><link href="https://monishver11.github.io/blog/2025/dual-problem/" rel="alternate" type="text/html" title="The Dual Problem of SVM"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://monishver11.github.io/blog/2025/dual-problem</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/dual-problem/"><![CDATA[<p>In machine learning, optimization problems often present themselves as key challenges. For example, when training a <strong>Support Vector Machine (SVM)</strong>, we are tasked with finding the optimal hyperplane that separates two classes in a high-dimensional feature space. While we can solve this directly using methods like subgradient descent, we can also leverage a more analytical approach through <strong>Quadratic Programming (QP)</strong> solvers.</p> <p>Moreover, for convex optimization problems, there is a powerful technique known as solving the <strong>dual problem</strong>. Understanding this duality is not only essential for theory, but it also offers computational advantages. In this blog, we’ll dive into the dual formulation of SVM and its implications.</p> <hr/> <h4 id="svm-as-a-quadratic-program"><strong>SVM as a Quadratic Program</strong></h4> <p>To understand the dual problem of SVM, let’s first revisit the primal optimization problem for SVMs. The goal of an SVM is to find the hyperplane that maximizes the margin between two classes. The optimization problem can be written as:</p> \[\begin{aligned} \min_{w, b, \xi} \quad &amp; \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\ \text{subject to} \quad &amp; -\xi_i \leq 0 \quad \text{for } i = 1, \dots, n, \\ &amp; 1 - y_i (w^T x_i + b) - \xi_i \leq 0 \quad \text{for } i = 1, \dots, n, \end{aligned}\] <p><strong>Primal</strong>—sounds technical, right? Here’s what it means: The <strong>primal problem</strong> refers to the original formulation of the optimization problem in terms of the decision variables (in this case, \(w\), \(b\), and \(\xi\)) and the objective function. It is called “primal” because it directly represents the problem without transformation. The primal form of SVM is concerned with finding the optimal hyperplane parameters that minimize the classification error while balancing the margin size.</p> <h5 id="breakdown-of-the-problem"><strong>Breakdown of the Problem</strong></h5> <ul> <li> <p><strong>Objective Function</strong>: The term \(\frac{1}{2} \|w\|^2\) is a regularization term that aims to minimize the complexity of the model, ensuring that the hyperplane is as “wide” as possible. The second term, \(C \sum_{i=1}^n \xi_i\), penalizes the violations of the margin (through slack variables \(\xi_i\)) and controls the trade-off between margin size and misclassification.</p> </li> <li> <p><strong>Constraints</strong>: The constraints consist of two parts:</p> <ul> <li>The first part ensures that slack variables are non-negative: \(-\xi_i \leq 0\), meaning that each slack variable must be at least zero. <strong>Why?</strong> The slack variables represent the margin violations, and they must be non-negative because they quantify how much a data point violates the margin, which cannot be negative.</li> <li>The second part enforces that the data points are correctly classified or their margin violations are captured by \(\xi_i\). For each data point \(i\), the condition \(1 - y_i (w^T x_i + b) - \xi_i \leq 0\) must hold. Here, \(y_i\) is the true label of the data point, and \(w^T x_i + b\) represents the signed distance of the point from the hyperplane. If the point is correctly classified and lies outside the margin (i.e., its signed distance from the hyperplane is greater than 1), the constraint holds true. If the point is misclassified or falls inside the margin, the slack variable \(\xi_i\) will account for this violation.</li> </ul> </li> </ul> <p>This problem has a <strong>differentiable objective function</strong>, <strong>affine constraints</strong>, and includes a number of <strong>unknowns</strong> that can be solved using Quadratic Programming (QP) solvers.</p> <p>So, <strong>Quadratic Programming (QP)</strong> is an optimization problem where the objective function is quadratic (i.e., includes squared terms like \(\|w\|^2\)), and the constraints are linear. In the context of SVM, QP is utilized because the objective function involves the squared norm of \(w\) (which is quadratic), and the constraints are linear inequalities.</p> <p>The QP formulation for SVM involves minimizing a quadratic objective function (with respect to \(w\), \(b\), and \(\xi\)) subject to linear constraints. Now, while QP solvers provide an efficient way to tackle this problem, let’s explore the <strong>dual problem(next)</strong> to gain further insights.</p> <p>But, why bother with the <strong>dual problem</strong>? Here’s why it’s worth the dive:</p> <ol> <li> <p><strong>Efficient Computation with Kernels</strong>: The dual formulation focuses on Lagrange multipliers and inner products between data points, rather than directly optimizing \(w\) and \(b\). This is particularly beneficial when using kernels, as it avoids explicit computation in high-dimensional spaces. For non-linear problems or datasets where relationships are better captured in transformed spaces, the dual approach enables efficient computation while leveraging the kernel trick.</p> </li> <li> <p><strong>Geometrical Insights</strong>: The dual formulation emphasizes the relationship between support vectors and the margin, offering a clearer geometrical interpretation. It shows that only the support vectors (the points closest to the decision boundary) determine the optimal hyperplane.</p> </li> <li> <p><strong>Convexity and Global Optimality</strong>: The dual problem is convex, ensuring that solving it leads to the global optimal solution. This is particularly beneficial when the primal problem has a large number of variables and constraints.</p> </li> </ol> <p>In short, while QP solvers can efficiently solve the primal problem, the dual problem formulation offers computational benefits, the potential for kernel methods, and a clearer understanding of the SVM model’s properties. This makes the dual approach a powerful tool in SVM optimization.</p> <p>No need to worry about the details above—we’ll cover them step by step. For now, keep reading!</p> <hr/> <h4 id="the-lagrangian"><strong>The Lagrangian</strong></h4> <p>To begin understanding the dual problem, we need to define the <strong>Lagrangian</strong> of the optimization problem. For general inequality-constrained optimization problems, the goal is:</p> \[\begin{aligned} \min_x \quad &amp; f_0(x) \\ \text{subject to} \quad &amp; f_i(x) \leq 0, \quad i = 1, \dots, m. \end{aligned}\] <p>The corresponding <strong>Lagrangian</strong> is defined as:</p> \[L(x, \lambda) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x),\] <p>where:</p> <ul> <li>\(\lambda_i\) are the <strong>Lagrange multipliers</strong> (also known as <strong>dual variables</strong>).</li> <li>The Lagrangian function combines the objective function \(f_0(x)\) with the constraints \(f_i(x)\), weighted by the Lagrange multipliers \(\lambda_i\). These multipliers represent how much the objective function will change if we relax or tighten the corresponding constraint.</li> </ul> <h5 id="why-do-we-use-the-lagrangian"><strong>Why Do We Use the Lagrangian?</strong></h5> <p>The Lagrangian serves as a bridge between constrained and unconstrained optimization. Here’s the intuition behind its design and necessity:</p> <ol> <li><strong>Softening Hard Constraints</strong>: <ul> <li>In constrained optimization, the solution must strictly satisfy all constraints, which can make direct optimization challenging.</li> <li>By introducing Lagrange multipliers, the constraints are “softened” into penalties. This means that instead of strictly enforcing constraints during every step of optimization, we penalize deviations from the constraints in the objective function.</li> </ul> </li> <li><strong>Unified Objective</strong>: <ul> <li>The Lagrangian integrates the objective function and constraints into a single function. This allows us to handle both aspects of the problem (maximizing or minimizing while respecting constraints) in one unified framework.</li> </ul> </li> <li><strong>Flexibility</strong>: <ul> <li>The Lagrange multipliers \(\lambda_i\) provide a mechanism to adjust the influence of each constraint. If a constraint is more critical, its corresponding multiplier will have a larger value, increasing its contribution to the Lagrangian.</li> </ul> </li> <li><strong>Theoretical Insights</strong>: <ul> <li>The Lagrangian formulation is foundational to deriving the <strong>dual problem</strong>, which can sometimes simplify the original (primal) problem. It also provides deeper insights into the sensitivity of the solution to changes in the constraints.</li> </ul> </li> </ol> <p><strong>Think of it this way:</strong></p> <p>Imagine you’re managing a budget to purchase items for a project. Your primary goal is to minimize costs (the objective function), but you have constraints like a maximum budget and a minimum quality requirement for each item.</p> <ul> <li>Without the Lagrangian: You’d need to find solutions that satisfy the budget and quality constraints explicitly, which could be cumbersome.</li> <li>With the Lagrangian: You assign a penalty (via Lagrange multipliers) to every dollar you overspend or every unit of quality you fail to meet. Now, your goal is to minimize the combined cost (original cost + penalties), which naturally leads you to solutions that respect the constraints.</li> </ul> <p>In optimization terms, the Lagrangian lets us trade off between violating constraints and optimizing the objective function. This trade-off is critical in complex problems where perfect feasibility might not always be achievable during intermediate steps.</p> <h5 id="the-role-of-lagrange-multipliers"><strong>The Role of Lagrange Multipliers</strong></h5> <p>The multipliers \(\lambda_i\) play a dual role:</p> <ol> <li> <p>They measure the <strong>sensitivity</strong> of the objective function to the corresponding constraint. For instance, if increasing the limit of a constraint by a small amount significantly improves the objective, its multiplier will have a large value.</p> </li> <li> <p>They ensure that the optimal solution respects the constraints. If a constraint is not active at the solution (i.e., it is satisfied without being tight), its multiplier will be zero. This is formalized in the concept of <strong>complementary slackness</strong>, which we’ll explore later.</p> </li> </ol> <p>In essence, the Lagrangian is not just a mathematical tool; it reflects the natural balance between objectives and constraints, making it indispensable in optimization theory.</p> <hr/> <h4 id="lagrange-dual-function"><strong>Lagrange Dual Function</strong></h4> <p>Next, we define the <strong>Lagrange dual function</strong>, which plays a crucial role in deriving the dual problem. The dual function is obtained by minimizing the Lagrangian with respect to the primal variables (denoted as \(x\)):</p> \[g(\lambda) = \inf_x L(x, \lambda) = \inf_x \left[ f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) \right].\] <h5 id="what-does-this-mean"><strong>What Does This Mean?</strong></h5> <p>The Lagrange dual function \(g(\lambda)\) gives us the smallest possible value of the Lagrangian \(L(x, \lambda)\) for a given set of Lagrange multipliers \(\lambda_i \geq 0\). It represents the “best” value of the Lagrangian when we optimize over the primal variables \(x\).</p> <p>In simpler terms:</p> <ul> <li>The primal variables \(x\) are those we’re directly trying to optimize in the original problem.</li> <li>By minimizing over \(x\) for fixed \(\lambda\), we explore how well the Lagrangian balances the objective function \(f_0(x)\) and the weighted constraints \(f_i(x)\).</li> </ul> <h5 id="why-do-we-minimize-over-x"><strong>Why Do We Minimize Over \(x\)?</strong></h5> <p>The reason for this step is that it helps us decouple the influence of the primal variables \(x\) and the dual variables \(\lambda\). By focusing only on \(x\), we shift our attention to understanding the properties of the dual variables, which simplifies the problem and provides valuable insights.</p> <p>Minimizing the Lagrangian with respect to \(x\) ensures that:</p> <ol> <li><strong>Feasibility of Constraints</strong>: <ul> <li>The minimization respects the constraints \(f_i(x) \leq 0\) through the penalization mechanism introduced by \(\lambda_i\).</li> </ul> </li> <li><strong>Dual Representation</strong>: <ul> <li>The dual function \(g(\lambda)\) captures how “good” a particular choice of \(\lambda\) is in approximating the original problem.</li> </ul> </li> <li><strong>Foundation for the Dual Problem</strong>: <ul> <li>The minimization step builds the foundation for solving the optimization problem via its <strong>dual formulation</strong>, which is often simpler than the primal.</li> </ul> </li> </ol> <h5 id="why-do-we-need-this"><strong>Why Do We Need This?</strong></h5> <p>The goal of introducing the dual function is to exploit the following properties, which help us solve the original optimization problem more efficiently:</p> <ul> <li> <p><strong>Lower Bound Property</strong>: The dual function \(g(\lambda)\) provides a lower bound on the optimal value of the primal problem. If \(p^*\) is the optimal value of the primal problem, then: \(g(\lambda) \leq p^*, \quad \forall \lambda \geq 0.\) This property is useful because even if we cannot solve the primal problem directly, we can approximate its solution by maximizing \(g(\lambda)\).</p> </li> <li> <p><strong>Convexity of \(g(\lambda)\)</strong>: The dual function is always concave, regardless of whether the primal problem is convex. This makes the dual problem easier to solve using convex optimization techniques.</p> </li> </ul> <h5 id="how-does-this-work-why-does-it-work"><strong>How Does This Work? Why Does It Work?</strong></h5> <ol> <li><strong>Lower Bound Property</strong>: <ul> <li>When we minimize the Lagrangian \(L(x, \lambda)\) with respect to the primal variables \(x\), we’re essentially finding the “best possible value” of the objective function \(f_0(x)\) for a fixed choice of \(\lambda\).</li> <li>Since the dual function \(g(\lambda)\) is derived from a relaxed version of the primal problem (allowing \(\lambda_i \geq 0\) to penalize constraint violations), it cannot exceed the true optimal value \(p^*\) of the primal problem. This creates the lower bound.</li> </ul> <p><strong>Intuition</strong>: Think of the dual function as a “proxy” for the primal problem. By maximizing \(g(\lambda)\), we try to approach the primal solution as closely as possible from below.</p> </li> <li><strong>Convexity of \(g(\lambda)\)</strong>: <ul> <li>The concavity of \(g(\lambda)\) follows from its definition as the infimum (or greatest lower bound) of a family of affine functions. In optimization, operations involving infima tend to preserve convexity (or result in concavity for maximization problems).</li> <li>This property ensures that maximizing \(g(\lambda)\) is computationally efficient, even if the original primal problem is non-convex.</li> </ul> <p><strong>Intuition</strong>: The concave structure of \(g(\lambda)\) creates an inverted “bowl-shaped” surface, making it easier to find the maximum using gradient-based optimization methods.</p> </li> </ol> <h5 id="what-is-inf-and-how-is-it-different-from-min"><strong>What is \(\inf\) and How is it Different from \(\min\)?</strong></h5> <p>The <strong>infimum</strong> (denoted as \(\inf\)) is a generalization of the minimum (\(\min\)) in optimization and analysis. Here’s the distinction:</p> <ol> <li><strong>Minimum (\(\min\))</strong>: <ul> <li>The minimum is the smallest value attained by a function within its domain. It must be achieved by some point \(x\) in the domain.</li> <li>Example: For \(f(x) = x^2\) on \([0, 2]\), the minimum is \(f(0) = 0\).</li> </ul> </li> <li><strong>Infimum (\(\inf\))</strong>: <ul> <li>The infimum is the greatest lower bound of a function, but it may not be attained by any point in the domain. It represents the “smallest possible value” the function can approach, even if it doesn’t reach it.</li> <li>Example: For \(f(x) = 1/x\) on \((0, 2]\), the infimum is \(\inf f(x) = 0\), but \(f(x)\) never actually equals \(0\) within the domain.</li> </ul> </li> </ol> <h5 id="why-use-inf-instead-of-min"><strong>Why Use \(\inf\) Instead of \(\min\)?</strong></h5> <p>In the context of the dual function:</p> <ul> <li>The infimum \(\inf_x L(x, \lambda)\) is used because the Lagrangian \(L(x, \lambda)\) might not achieve a true minimum for certain values of \(\lambda\) (e.g., the domain could be open or unbounded).</li> <li>Using \(\inf\) ensures that the dual function \(g(\lambda)\) is well-defined, even in cases where \(\min_x L(x, \lambda)\) doesn’t exist.</li> </ul> <h5 id="a-lighter-take"><strong>A Lighter Take</strong></h5> <p>Think of this as “playing with math language” to get to the result we want. Just as you might rephrase a sentence to make it clearer or more persuasive, in mathematics, we transform the problem into a new form (the dual) that’s easier to work with.</p> <p>For now, trust the process. This step might seem abstract, but it leads us to a form of the problem where powerful mathematical tools can come into play. Once we see the bigger picture, the reasoning behind these transformations will become clear.</p> <p>In essence, the Lagrange dual function \(g(\lambda)\) gives us a way to shift our perspective on the optimization problem, helping us solve it through duality principles. As we proceed, you’ll see how this approach simplifies the original problem and why it’s such a powerful concept.</p> <hr/> <h4 id="the-primal-and-the-dual"><strong>The Primal and the Dual</strong></h4> <p>For any general primal optimization problem:</p> \[\begin{aligned} \min_x \quad &amp; f_0(x) \\ \text{subject to} \quad &amp; f_i(x) \leq 0, \quad i = 1, \dots, m, \end{aligned}\] <p>we can formulate the corresponding <strong>dual problem</strong> as:</p> \[\begin{aligned} \max_\lambda \quad &amp; g(\lambda) \\ \text{subject to} \quad &amp; \lambda_i \geq 0, \quad i = 1, \dots, m. \end{aligned}\] <p>The dual problem has some remarkable properties:</p> <ul> <li><strong>Convexity</strong>: The dual problem is always a <strong>convex optimization problem</strong>, even if the primal problem is not convex.</li> <li><strong>Simplification</strong>: In some cases, solving the dual problem is easier than solving the primal problem directly. This is particularly true when the primal problem is difficult to solve or the number of constraints is large.</li> </ul> <h5 id="contract-negotiation-analogy"><strong>Contract Negotiation Analogy</strong></h5> <ul> <li> <p><strong>Primal Problem (Your Terms)</strong>: Think of the primal problem as negotiating a contract where you aim to minimize costs while respecting certain constraints (like budget or timelines). You’re focused on getting the best deal for yourself under these limitations.</p> </li> <li> <p><strong>Lagrangian (Penalties for Violations)</strong>: During the negotiation, you introduce penalties — if you can’t meet a certain term, you adjust other aspects of the contract. This is similar to using Lagrange multipliers in the Lagrangian function to penalize constraint violations.</p> </li> <li> <p><strong>Dual Problem (Value Assessment)</strong>: In the dual problem, you step into the other party’s shoes and assess how much value they assign to the contract’s terms, maximizing the value they place on the constraints (such as how much they’d pay for more time or resources).</p> </li> <li> <p><strong>Duality</strong>: The primal (minimizing costs) and dual (maximizing value) problems balance each other. Weak duality means the dual value is a lower bound to the primal, and strong duality means the best deal is the same for both sides when the optimal values align.</p> </li> </ul> <p>Below are the next two important results derived from the above formulation.</p> <h4 id="weak-and-strong-duality"><strong>Weak and Strong Duality</strong></h4> <ol> <li> <p><strong>Weak Duality</strong>: This property tells us that the optimal value of the primal problem is always greater than or equal to the optimal value of the dual problem:</p> \[p^* \geq d^*,\] <p>where \(p^*\) and \(d^*\) are the optimal values of the primal and dual problems, respectively. This is a fundamental result in optimization theory. <strong>Why so?</strong> Because the dual problem is designed to provide a lower bound on the primal objective through the Lagrangian.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Lagrangian_1-480.webp 480w,/assets/img/Lagrangian_1-800.webp 800w,/assets/img/Lagrangian_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Lagrangian_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lagrangian_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li> <p><strong>Strong Duality</strong>: In some special cases (such as when the problem satisfies <strong>Slater’s condition</strong>), <strong>strong duality</strong> holds, meaning the optimal values of the primal and dual problems are equal:</p> \[p^* = d^*.\] <p>Strong duality is particularly useful because it allows us to solve the dual problem instead of the primal one, often simplifying the problem or reducing computational complexity.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Lagrangian_2-480.webp 480w,/assets/img/Lagrangian_2-800.webp 800w,/assets/img/Lagrangian_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Lagrangian_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lagrangian_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="slaters-conditions"><strong>Slater’s Conditions</strong></h5> <ul> <li>They are a set of conditions that <strong>guarantee strong duality</strong> in certain types of constrained optimization problems (particularly convex problems).</li> <li>They state that for strong duality to hold, there must exist a <strong>strictly feasible point</strong> (a point where all inequality constraints are strictly satisfied).</li> <li>In simpler terms, Slater’s conditions ensure that there is at least one point where all the constraints are strictly satisfied, which <strong>enables strong duality</strong> and ensures that the primal and dual solutions align.</li> <li>These conditions are crucial in convex optimization as they help guarantee the optimal solutions for both the primal and dual problems are the same.</li> </ul> <h4 id="complementary-slackness"><strong>Complementary Slackness</strong></h4> <p>When <strong>strong duality</strong> holds, we can derive the <strong>complementary slackness</strong> condition. This condition provides deeper insight into the relationship between the primal and dual solutions. Specifically, if \(x^*\) is the optimal primal solution and \(\lambda^*\) is the optimal dual solution, we have:</p> \[f_0(x^*) = g(\lambda^*) = \inf_x L(x, \lambda^*) \leq L(x^*, \lambda^*) = f_0(x^*) + \sum_{i=1}^m \lambda_i^* f_i(x^*).\] <p>For this equality to hold, the term \(\sum_{i=1}^m \lambda_i^* f_i(x^*)\) must be zero for each constraint. This leads to the following <strong>complementary slackness condition</strong>:</p> <ol> <li>If \(\lambda_i^* &gt; 0\), then \(f_i(x^*) = 0\), meaning that the corresponding constraint is <strong>active</strong> at the optimal point. Also, it means the constraint is tight and directly affects the optimal solution.</li> <li>If \(f_i(x^*) &lt; 0\), then \(\lambda_i^* = 0\), meaning that the corresponding constraint is <strong>inactive</strong>. This indicates that the constraint doesn’t influence the optimal solution at all.</li> </ol> <p>This condition tells us which constraints are binding (active) at the optimal solution and which are not, providing critical information about the structure of the optimal solution.</p> <h5 id="contract-negotiation-analogy-duality-and-complementary-slackness"><strong>Contract Negotiation Analogy: Duality and Complementary Slackness</strong></h5> <h6 id="strong-duality-and-weak-duality">Strong Duality and Weak Duality:</h6> <ul> <li> <p><strong>Weak Duality</strong>: In a contract negotiation, <strong>weak duality</strong> is like the <strong>minimum acceptable price</strong> for the other party. The price (dual value) they would accept for your offer will always be a <strong>lower bound</strong> to what you are willing to pay (primal cost). The other party cannot ask for more than what you’re offering, but they may accept less.</p> </li> <li> <p><strong>Strong Duality</strong>: <strong>Strong duality</strong> happens when both parties agree on the same <strong>optimal terms</strong> for the contract. Both your <strong>best offer</strong> (primal) and the <strong>value assigned</strong> to the terms (dual) align perfectly, resulting in the best possible contract for both sides.</p> </li> </ul> <h6 id="complementary-slackness-1">Complementary Slackness:</h6> <ul> <li><strong>Complementary Slackness</strong> tells us which terms of the contract are really influencing the negotiation outcome. <ul> <li>If a <strong>dual variable</strong> (e.g., the price or terms the other party values) is <strong>positive</strong>, then the corresponding <strong>primal constraint</strong> (e.g., a term you care about, like the delivery time or cost) is <strong>active</strong> — it <strong>must</strong> be part of the final agreement.</li> <li>If a <strong>primal constraint</strong> (e.g., a timeline or budget) is not strict enough (it’s not a deal-breaker), then the <strong>dual variable</strong> (e.g., the value the other party assigns to it) is <strong>zero</strong>, meaning it doesn’t impact the outcome.</li> </ul> </li> </ul> <p><strong>Example:</strong></p> <ul> <li>Suppose you’re negotiating a <strong>project deadline</strong>. If the other party <strong>values</strong> the deadline highly (dual value is positive), it <strong>must</strong> be part of the contract (the deadline is <strong>active</strong>). If they don’t care much about it, then <strong>it doesn’t matter</strong> to the final deal (dual value is zero), and you can relax that constraint.</li> </ul> <hr/> <p>Finally, there might be one question left—this is the one I had, so here’s the explanation:</p> <h5 id="if-the-lagrangian-dual-function-is-concave-how-is-the-lagrangian-dual-problem-always-a-convex-optimization-problem"><strong>If the Lagrangian dual function is concave, how is the Lagrangian dual problem always a convex optimization problem?</strong></h5> <p>To understand why the dual problem is always a convex optimization problem, let’s revisit its structure: \(\max_{\lambda \geq 0} \; g(\lambda).\)</p> <ol> <li> <p><strong>Maximizing a Concave Function:</strong><br/> The dual function \(g(\lambda)\) is <strong>concave</strong> by construction. In optimization, maximizing a concave function is equivalent to minimizing a convex function (negating the objective). Therefore, the objective of the dual problem aligns with the structure of a convex optimization problem.</p> </li> <li> <p><strong>Convex Feasible Region:</strong><br/> The constraints in the dual problem (\(\lambda \geq 0\)) define a <strong>convex set</strong>, as the non-negative orthant in \(\mathbb{R}^m\) is convex.</p> </li> </ol> <h6 id="definition-of-a-convex-optimization-problem"><strong>Definition of a Convex Optimization Problem:</strong></h6> <p>A problem is a convex optimization problem if:</p> <ul> <li>The objective function is <strong>convex</strong> (for minimization) or <strong>concave</strong> (for maximization).</li> <li>The feasible region is a <strong>convex set</strong>.</li> </ul> <p>The dual problem satisfies these conditions because:</p> <ul> <li>The objective function \(g(\lambda)\) is concave.</li> <li>The constraint \(\lambda \geq 0\) defines a convex feasible region.</li> </ul> <p>Thus, the dual problem is always a <strong>convex optimization problem</strong>, regardless of whether the primal problem is convex or not.</p> <h6 id="intuition-behind-convexity-of-the-dual-problem"><strong>Intuition Behind Convexity of the Dual Problem</strong></h6> <p>The dual problem’s convexity comes from the way it is constructed:</p> <ol> <li>The Lagrangian combines the primal objective and constraints into a single function that penalizes constraint violations.</li> <li>By minimizing the Lagrangian over the primal variables \(x\), the dual function \(g(\lambda)\) captures the <strong>tightest lower bound</strong> of the primal objective.</li> <li>The pointwise infimum of affine functions (as in \(g(\lambda)\)) is guaranteed to be concave.</li> <li>The dual problem maximizes this concave function over a convex set (\(\lambda \geq 0\)), making it a convex optimization problem.</li> </ol> <p>This ensures that solving the dual problem is computationally efficient and well-structured, even when the primal problem is non-convex.</p> <hr/> <h4 id="conclusion"><strong>Conclusion</strong></h4> <p>By exploring the <strong>dual problem</strong> of SVM, we gain both theoretical insights and practical benefits. The dual formulation provides a new perspective on the original optimization problem, and solving it can sometimes be more efficient or insightful. The duality between the primal and dual problems underpins many of the optimization techniques used in machine learning, particularly in the context of support vector machines.</p> <p>Pat yourselves on the back for making it to the end of this blog! Take a well-deserved break, and stay tuned for the next one, where we’ll apply everything we’ve learned so far to formulate the SVM dual problem.</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://www.youtube.com/watch?v=thuYiebq1cE&amp;t=136s">https://www.youtube.com/watch?v=thuYiebq1cE&amp;t=136s - David S. Rosenberg</a></li> <li><a href="https://www.youtube.com/watch?v=8mjcnxGMwFo">Lagrange Multipliers | Geometric Meaning &amp; Full Example - Dr. Trefor Bazett</a></li> <li><a href="https://www.youtube.com/watch?v=d0CF3d5aEGc&amp;t=216s">Convexity and The Principle of Duality - Visually Explained</a></li> <li><a href="https://math.stackexchange.com/questions/515812/pointwise-infimum-of-affine-functions-is-concave?noredirect=1&amp;lq=1">Pointwise infimum of affine functions is concave</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.]]></summary></entry></feed>