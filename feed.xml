<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-24T18:51:36+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A clear, theory-focused approach to machine learning, designed to take you beyond the basics. </subtitle><entry><title type="html">Bayesian Machine Learning - Mathematical Foundations</title><link href="https://monishver11.github.io/blog/2025/Bayesian-ML/" rel="alternate" type="text/html" title="Bayesian Machine Learning - Mathematical Foundations"/><published>2025-01-24T14:56:00+00:00</published><updated>2025-01-24T14:56:00+00:00</updated><id>https://monishver11.github.io/blog/2025/Bayesian-ML</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/Bayesian-ML/"><![CDATA[<p>When working with machine learning models, it’s crucial to understand the underlying statistical principles that drive our methods. Whether you’re a frequentist or a Bayesian, the starting point often involves a <strong>parametric family of densities</strong>. This concept forms the foundation for inference and is used to model the data we observe.</p> <h4 id="parametric-family-of-densities"><strong>Parametric Family of Densities</strong></h4> <p>A <strong>parametric family of densities</strong> is defined as a set</p> \[\{p(y \mid \theta) : \theta \in \Theta\},\] <p>where \(p(y \mid \theta)\) is a density function over some sample space \(Y\), and \(\theta\) represents a parameter in a finite-dimensional parameter space \(\Theta\).</p> <p>In simpler terms, this is a collection of probability distributions, each associated with a specific value of the parameter \(\theta\). When we refer to “density,” it’s worth noting that this can be replaced with “mass function” if we’re dealing with discrete random variables. Similarly, integrals can be replaced with summations in such cases.</p> <p>This framework is the common starting point for both <strong>classical statistics</strong> and <strong>Bayesian statistics</strong>, as it provides a structured way to think about modeling the data.</p> <h5 id="frequentist-or-classical-statistics"><strong>Frequentist or “Classical” Statistics</strong></h5> <p>In frequentist statistics, we also work with the parametric family of densities \(\{p(y \mid \theta) : \theta \in \Theta\}\), assuming that the true distribution \(p(y \mid \theta)\) governs the world we observe. This means there exists some unknown parameter \(\theta \in \Theta\) that determines the true nature of the data.</p> <p>If we had direct access to this true parameter \(\theta\), we wouldn’t need statistics at all! However, in practice, we only have a dataset, denoted as</p> \[D = \{y_1, y_2, \dots, y_n\},\] <p>where each \(y_i\) is sampled independently from the true distribution \(p(y \mid \theta)\).</p> <p>This brings us to the heart of statistics: <strong>how do we make inferences about the unknown parameter \(\theta\) using only the observed data \(D\)?</strong></p> <h5 id="point-estimation"><strong>Point Estimation</strong></h5> <p>One fundamental problem in statistics is <strong>point estimation</strong>, where the goal is to estimate the true value of the parameter \(\theta\) as accurately as possible.</p> <p>To do this, we use a <strong>statistic</strong>, denoted as \(s = s(D)\), which is simply a function of the observed data. When this statistic is designed to estimate \(\theta\), we call it a <strong>point estimator</strong>, represented as \(\hat{\theta} = \hat{\theta}(D)\).</p> <p>A <strong>good point estimator</strong> is one that is both:</p> <ul> <li><strong>Consistent</strong>: As the sample size \(n\) grows larger, the estimator \(\hat{\theta}_n\) converges to the true parameter \(\theta\).</li> <li><strong>Efficient</strong>: The estimator \(\hat{\theta}_n\) extracts the maximum amount of information about \(\theta\) from the data, achieving the best possible accuracy for a given sample size.</li> </ul> <p>One of the most popular methods for point estimation is the <strong>maximum likelihood estimator (MLE)</strong>. While we’ve already covered it, let’s revisit it through a concrete example to reinforce our understanding.</p> <h5 id="example-coin-flipping-and-maximum-likelihood-estimation"><strong>Example: Coin Flipping and Maximum Likelihood Estimation</strong></h5> <p>Let’s consider the simple yet illustrative problem of estimating the probability of a coin landing on heads.</p> <p>Here, the parametric family of mass functions is given by:</p> \[p(\text{Heads} \mid \theta) = \theta, \quad \text{where } \theta \in \Theta = (0, 1).\] <p>The parameter \(\theta\) represents the probability of the coin landing on heads. Our goal is to estimate this parameter based on observed data.</p> <p>If this seems a bit confusing, seeing \(\theta\) in two places, let’s clarify it first.</p> <p>Imagine you have a coin, and you’re curious about how “fair” it is. A perfectly fair coin has a 50% chance of landing heads or tails, but your coin might be biased. To capture this bias mathematically, you introduce a parameter, \(\theta\), which represents the probability of the coin landing on heads.</p> <p>We write this as:</p> \[p(\text{Heads} \mid \theta) = \theta\] <p>Let’s break this down with intuition:</p> <ol> <li><strong>What does \(\theta\) mean?</strong><br/> \(\theta\) is the coin’s “personality.” For example: <ul> <li>If \(\theta = 0.8\), it means the coin “loves” heads, and there’s an 80% chance it will land heads on any given flip.</li> <li>If \(\theta = 0.3\), the coin is biased toward tails, and there’s only a 30% chance of heads.</li> </ul> </li> <li> <p><strong>What does \(p(\text{Heads} \mid \theta) = \theta\) mean?</strong><br/> This equation ties the probability of getting heads to the parameter \(\theta\). It’s like saying: “The parameter \(\theta\) <em>is</em> the probability of heads.” For every coin flip, \(\theta\) directly determines the likelihood of heads.</p> </li> <li><strong>Why is this useful?</strong><br/> It simplifies modeling. Instead of treating each flip as random and unconnected, we assume there’s a fixed bias, \(\theta\), that governs the coin’s behavior. Once we observe enough flips (data), we can estimate \(\theta\) and predict future outcomes.</li> </ol> <p><strong>A relatable example might be…</strong></p> <p>Imagine a factory making coins with varying biases. Each coin is labeled with its bias, \(\theta\), ranging between 0 (always tails) and 1 (always heads). If you’re handed a coin without a label, your job is to figure out its bias by flipping it multiple times and observing the outcomes.</p> <p>This is the setup for the equation \(p(\text{Heads} \mid \theta) = \theta\). It tells us the coin’s behavior is entirely controlled by its bias, \(\theta\), and allows us to estimate it from observed data. <strong>Data and Likelihood Function</strong></p> <p>I hope that clears things up, and we’re good to proceed!</p> <hr/> <p>Suppose we observe the outcomes of \(n\) independent coin flips, represented as:</p> \[D = (\text{H, H, T, T, T, T, T, H, ... , T}),\] <p>where \(n_h\) is the number of heads, and \(n_t\) is the number of tails. Since each flip is independent, the likelihood function for the observed data is:</p> \[L_D(\theta) = p(D \mid \theta) = \theta^{n_h} (1 - \theta)^{n_t}.\] <p><strong>Log-Likelihood and Optimization</strong></p> <p>Rather than working directly with the likelihood function, which involves products and can become cumbersome, we typically maximize the <strong>log-likelihood function</strong> for computational simplicity. The log-likelihood is:</p> \[\log L_D(\theta) = n_h \log \theta + n_t \log (1 - \theta).\] <p>The <strong>maximum likelihood estimate (MLE)</strong> of \(\theta\) is the value that maximizes this log-likelihood:</p> \[\hat{\theta}_{\text{MLE}} = \underset{\theta \in \Theta}{\text{argmax}} \, \log L_D(\theta).\] <p><strong>Derivation of the MLE</strong></p> <p>To find the MLE, we compute the derivative of the log-likelihood with respect to \(\theta\), set it to zero, and solve for \(\theta\):</p> \[\frac{\partial}{\partial \theta} \big[ n_h \log \theta + n_t \log (1 - \theta) \big] = \frac{n_h}{\theta} - \frac{n_t}{1 - \theta}.\] <p>Setting this derivative to zero:</p> \[\frac{n_h}{\theta} = \frac{n_t}{1 - \theta}.\] <p>Simplifying this equation gives:</p> \[\theta = \frac{n_h}{n_h + n_t}.\] <p>Thus, the MLE for \(\theta\) is:</p> \[\hat{\theta}_{\text{MLE}} = \frac{n_h}{n_h + n_t}.\] <p><strong>Intuition Behind the MLE</strong></p> <p>The result makes intuitive sense: the MLE simply calculates the proportion of heads observed in the data. It uses the empirical frequency as the best estimate of the true probability of heads, given the observed outcomes.</p> <hr/> <p>While frequentist approaches like MLE provide a single “best” estimate for \(\theta\), Bayesian methods take a different perspective. Instead of finding a point estimate, Bayesian inference quantifies uncertainty about \(\theta\) using probability distributions. This leads to the concepts of <strong>prior distributions</strong> and <strong>posterior inference</strong>, which is what we’re going to explore next.</p> <h4 id="bayesian-statistics-an-introduction"><strong>Bayesian Statistics: An Introduction</strong></h4> <p>In the frequentist framework, the goal is to estimate the true parameter \(\theta\) using the observed data. However, <strong>Bayesian statistics</strong> takes a fundamentally different approach by introducing an important concept: the <strong>prior distribution</strong>. This addition allows us to explicitly incorporate prior beliefs about the parameter into our analysis and update them rationally as we observe new data.</p> <h5 id="the-prior-distribution-reflecting-prior-beliefs"><strong>The Prior Distribution: Reflecting Prior Beliefs</strong></h5> <p>A <strong>prior distribution</strong>, denoted as \(p(\theta)\), is a probability distribution over the parameter space \(\Theta\). It represents our belief about the value of \(\theta\) <strong>before</strong> observing any data. For instance, if we believe that \(\theta\) is more likely to lie in a specific range, we can encode this belief directly into the prior.</p> <h5 id="a-bayesian-model-combining-prior-and-data"><strong>A Bayesian Model: Combining Prior and Data</strong></h5> <p>A <strong>[parametric] Bayesian model</strong> is constructed from two key components:</p> <ol> <li>A <strong>parametric family of densities</strong> \(\{p(D \mid \theta) : \theta \in \Theta\}\) that models the likelihood of the observed data \(D\) given \(\theta\).</li> <li>A <strong>prior distribution</strong> \(p(\theta)\) on the parameter space \(\Theta\).</li> </ol> <p>These two components combine to form a <strong>joint density</strong> over \(\theta\) and \(D\):</p> \[p(D, \theta) = p(D \mid \theta) p(\theta).\] <p>This joint density encapsulates both the likelihood of the data and our prior beliefs about the parameter.</p> <h5 id="posterior-distribution-updating-beliefs"><strong>Posterior Distribution: Updating Beliefs</strong></h5> <p>The real power of Bayesian statistics lies in the ability to <strong>update prior beliefs</strong> after observing data. This is achieved through the <strong>posterior distribution</strong>, denoted as \(p(\theta \mid D)\).</p> <ul> <li>The <strong>prior distribution</strong> \(p(\theta)\) captures our initial beliefs about \(\theta\).</li> <li>The <strong>posterior distribution</strong> \(p(\theta \mid D)\) reflects our updated beliefs after observing the data \(D\).</li> </ul> <p>By applying <strong>Bayes’ rule</strong>, we can express the posterior distribution as:</p> \[p(\theta \mid D) = \frac{p(D \mid \theta) p(\theta)}{p(D)},\] <p>where:</p> <ul> <li>\(p(D \mid \theta)\) is the <strong>likelihood</strong>, capturing how well \(\theta\) explains the observed data.</li> <li>\(p(\theta)\) is the <strong>prior</strong>, encoding our initial beliefs about \(\theta\).</li> <li>\(p(D)\) is a normalizing constant, ensuring the posterior integrates to 1.</li> </ul> <h5 id="simplifying-the-posterior"><strong>Simplifying the Posterior</strong></h5> <p>When analyzing the posterior distribution, we often focus on terms that depend on \(\theta\). Dropping constant factors that are independent of \(\theta\), we write:</p> \[p(\theta \mid D) \propto p(D \mid \theta) \cdot p(\theta),\] <p>where \(\propto\) denotes proportionality.</p> <p>In practice, this allows us to analyze and work with the posterior distribution more efficiently. For instance, the <strong>maximum a posteriori (MAP) estimate</strong> of \(\theta\) is given by:</p> \[\hat{\theta}_{\text{MAP}} = \underset{\theta \in \Theta}{\text{argmax}} \, p(\theta \mid D).\] <p><strong>A Way to Think About It:</strong></p> <p>A helpful way to think of Bayesian methods is to imagine you’re trying to predict the outcome of an event, but you have some prior knowledge (or beliefs) about it. For example, let’s say you’re predicting whether a student will pass an exam, and you have prior knowledge that most students in the class have been doing well. This prior belief can be represented as a probability distribution, which reflects how confident you are about the parameter (like the likelihood of passing).</p> <p>As you collect more data (say, the student’s past performance or study hours), Bayesian methods update your belief (the prior) to form a new, updated belief, called the <strong>posterior distribution</strong>. The more data you have, the more confident the posterior becomes about the true outcome.</p> <p>So, in essence:</p> <ul> <li><strong>Prior distribution</strong> = What you believe before observing data (your initial guess).</li> <li><strong>Likelihood</strong> = How the observed data might be related to your belief.</li> <li><strong>Posterior distribution</strong> = Your updated belief after observing the data.</li> </ul> <p>In Bayesian inference, the goal is to calculate the posterior, which balances the prior belief with the observed data.</p> <hr/> <h4 id="example-bayesian-coin-flipping"><strong>Example: Bayesian Coin Flipping</strong></h4> <p>Let’s revisit the coin-flipping example, but this time from a Bayesian perspective. We start with the parametric family of mass functions:</p> \[p(\text{Heads} \mid \theta) = \theta, \quad \text{where } \theta \in \Theta = (0, 1).\] <p>To complete our Bayesian model, we also need to specify a <strong>prior distribution</strong> over \(\theta\). One common choice is the <strong>Beta distribution</strong>, which is particularly convenient for this problem.</p> <h5 id="beta-prior-distribution"><strong>Beta Prior Distribution</strong></h5> <p>The Beta distribution, denoted as \(\text{Beta}(\alpha, \beta)\), is a flexible family of distributions defined on the interval \((0, 1)\). Its density function is:</p> \[p(\theta) \propto \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}.\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Beta_Distribution-480.webp 480w,/assets/img/Beta_Distribution-800.webp 800w,/assets/img/Beta_Distribution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Beta_Distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Beta_Distribution" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For our coin-flipping example, we can use:</p> \[p(\theta) \propto \theta^{h - 1} (1 - \theta)^{t - 1},\] <p>where \(h\) and \(t\) represent our prior “counts” of heads and tails, respectively.</p> <p>The <strong>mean</strong> of the Beta distribution is:</p> \[\mathbb{E}[\theta] = \frac{h}{h + t},\] <p>and its <strong>mode</strong> (for \(h, t &gt; 1\)) is:</p> \[\text{Mode} = \frac{h - 1}{h + t - 2}.\] <p><strong>A Way to Think of This Distribution:</strong></p> <p>Imagine you’re trying to estimate the probability of rain on a given day in a city you’ve never visited. You don’t have any direct weather data yet, but you do have some general knowledge about the region. Based on this, you form an initial belief about how likely it is to rain—maybe you’re unsure, so you assume it’s equally likely to rain or not, or maybe you’ve heard that it’s usually dry there.</p> <ul> <li><strong>The Beta distribution</strong> helps you represent this uncertainty. It’s like a flexible tool that encodes your prior beliefs about the probability of rain, and you can adjust these beliefs based on what you know or expect. <ul> <li>If you’re totally uncertain, you might use a <strong>uniform prior</strong> (where \(\alpha = \beta = 1\)), meaning you’re equally unsure whether rain is more likely or not.</li> <li>If you’ve already heard that it tends to rain more often, say 70% of the time, you could choose \(\alpha = 7\) and \(\beta = 3\) to reflect this prior information.</li> </ul> </li> </ul> <p>As you gather more data—say, after several days of weather observations—you can update your beliefs about the likelihood of rain. Each new observation (rain or no rain) “shapes” your distribution.</p> <ul> <li> <p><strong>The mean</strong> \(\mathbb{E}[\theta] = \frac{h}{h + t}\) represents the average likelihood of rain after considering all your prior knowledge and the observed days. This is your updated best guess about how likely it is to rain on any given day.</p> </li> <li> <p><strong>The mode</strong> \(\text{Mode} = \frac{h - 1}{h + t - 2}\), which reflects the most probable value of \(\theta\) after observing data, might give you a better estimate if the weather has shown a clear tendency over time (e.g., if it’s rained most days).</p> </li> </ul> <p>In essence, the Beta distribution allows you to start with an initial belief (or no belief) about the probability of rain, and as you observe more data, you continuously refine that belief. This is what makes Bayesian inference powerful—it enables you to <strong>update</strong> your beliefs rationally based on new evidence.</p> <p><strong>Why Use the Beta Prior Distribution in this Coin Flipping Problem?</strong></p> <p>The <strong>Beta distribution</strong> is particularly well-suited for modeling probabilities in Bayesian statistics, especially in problems like coin flipping. Here are a few reasons why it’s a good choice:</p> <ol> <li> <p><strong>Support on (0, 1):</strong> The Beta distribution is defined over the interval \(\theta \in (0, 1)\), which matches the range of possible values for \(\theta\) in the coin-flipping example. Since \(\theta\) represents the probability of getting heads, it must lie between 0 and 1.</p> </li> <li><strong>Flexibility:</strong> The Beta distribution is very flexible in shaping its probability density. By adjusting the parameters \(\alpha\) and \(\beta\), we can model a wide variety of prior beliefs about \(\theta\): <ul> <li>When \(\alpha = \beta = 1\), the Beta distribution is uniform, indicating that we have no strong prior belief about whether heads or tails is more likely.</li> <li>When \(\alpha &gt; \beta\), the distribution is biased towards heads, and when \(\alpha &lt; \beta\), it is biased towards tails.</li> <li>The parameters can also reflect <strong>observed data</strong>: if you’ve already seen \(h\) heads and \(t\) tails, the Beta distribution can be chosen with \(\alpha = h + 1\) and \(\beta = t + 1\), which matches the idea of “updating” your beliefs based on the data you observe.</li> </ul> </li> <li><strong>Intuitive Interpretation:</strong> The Beta distribution is easy to interpret in terms of prior knowledge. The parameters \(\alpha\) and \(\beta\) can be seen as counts of prior observations of heads and tails, respectively. This makes it a natural choice when we have prior information or beliefs about the likelihood of different outcomes, and want to update them as new data comes in.</li> </ol> <p><strong>Note:</strong> I highly suggest taking a look at the Beta distribution graph. As \(\alpha\) increases, the distribution tends to skew towards higher values of \(\theta\) (closer to 1), reflecting a higher likelihood of success. On the other hand, as \(\beta\) increases, the distribution skews towards lower values of \(\theta\) (closer to 0), indicating a higher likelihood of failure. If \(\alpha\) and \(\beta\) are equal, the distribution is symmetric and uniform, reflecting no prior preference between the two outcomes.</p> <hr/> <p>After observing data \(D = (\text{H, H, T, T, T, H, ...})\), where \(n_h\) is the number of heads and \(n_t\) is the number of tails, we combine the <strong>prior</strong> and <strong>likelihood</strong> to obtain the <strong>posterior distribution</strong>.</p> <p>The likelihood function, based on the observed data, is:</p> \[L(\theta) = p(D \mid \theta) = \theta^{n_h} (1 - \theta)^{n_t}.\] <p>Combining the prior and likelihood, the posterior density is:</p> \[p(\theta \mid D) \propto p(\theta) \cdot L(\theta),\] <p>which simplifies to:</p> \[p(\theta \mid D) \propto \theta^{h - 1} (1 - \theta)^{t - 1} \cdot \theta^{n_h} (1 - \theta)^{n_t}.\] <p>Simplifying further, we get:</p> \[p(\theta \mid D) \propto \theta^{h - 1 + n_h} (1 - \theta)^{t - 1 + n_t}.\] <p>This posterior distribution is also a Beta distribution:</p> \[\theta \mid D \sim \text{Beta}(h + n_h, t + n_t).\] <h5 id="interpreting-the-posterior"><strong>Interpreting the Posterior</strong></h5> <p>The posterior distribution shows how our prior beliefs are updated by the observed data:</p> <ul> <li>The prior \(\text{Beta}(h, t)\) initializes our counts with \(h\) heads and \(t\) tails.</li> <li>The posterior \(\text{Beta}(h + n_h, t + n_t)\) updates these counts by adding the observed \(n_h\) heads and \(n_t\) tails.</li> </ul> <p>For example, if our prior belief was \(\text{Beta}(2, 2)\) (a uniform prior), and we observed \(n_h = 3\) heads and \(n_t = 1\) tails, the posterior would be:</p> \[\text{Beta}(2 + 3, 2 + 1) = \text{Beta}(5, 3).\] <p>This reflects our updated belief about the probability of heads after observing the data.</p> <hr/> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>In this blog, we explored the essence of Bayesian statistics, focusing on how priors, likelihoods, and posteriors interact to update our beliefs. Using the coin-flipping example, we demonstrated key Bayesian tools like the Beta distribution and how to compute posterior updates. Also, as we mentioned, there’s one more important reason for choosing the Beta distribution—its technical term is <strong>conjugate priors</strong>. In the next blog, we’ll dive deeper into this concept and explore Bayesian point estimates, comparing them to the frequentist MLE estimate. Stay tuned as we continue to build intuition and delve further into Bayesian inference! 👋</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Bayesian Statistics</li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.]]></summary></entry><entry><title type="html">Understanding the Weighted Majority Algorithm in Online Learning</title><link href="https://monishver11.github.io/blog/2025/WMA/" rel="alternate" type="text/html" title="Understanding the Weighted Majority Algorithm in Online Learning"/><published>2025-01-23T16:33:00+00:00</published><updated>2025-01-23T16:33:00+00:00</updated><id>https://monishver11.github.io/blog/2025/WMA</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/WMA/"><![CDATA[<p><strong>The Weighted Majority Algorithm: A Powerful Online Learning Technique</strong></p> <p>In the continuation of our exploration into online learning, we turn to the <strong>Weighted Majority Algorithm (WMA)</strong>, an influential approach introduced by Littlestone and Warmuth in 1988. This algorithm builds upon the foundational principles of online learning and offers remarkable theoretical guarantees for handling adversarial scenarios.</p> <p>Let’s dive into the workings of the Weighted Majority Algorithm, analyze its performance, and understand its strengths and limitations.</p> <h4 id="the-weighted-majority-algorithm"><strong>The Weighted Majority Algorithm</strong></h4> <p>The Weighted Majority Algorithm operates in a framework where predictions are made by combining the advice of multiple experts. Unlike the Halving Algorithm, which outright eliminates incorrect hypotheses, WMA assigns and updates weights to experts based on their performance, ensuring a more adaptive approach.</p> <h5 id="the-algorithm-steps"><strong>The Algorithm Steps</strong></h5> <ol> <li><strong>Initialization</strong>: <ul> <li>Start with \(N\) experts, each assigned an initial weight of 1:<br/> \(w_{1,i} = 1 \quad \text{for } i = 1, 2, \dots, N\)</li> </ul> </li> <li><strong>Prediction</strong>: <ul> <li>At each time step \(t\): <ul> <li>Receive the instance \(x_t\).</li> <li>Predict the label \(\hat{y}_t\) using a <strong>weighted majority vote</strong>: \(\hat{y}_t = \begin{cases} 1 &amp; \text{if } \sum_{i: y_{t,i}=1} w_{t,i} &gt; \sum_{i: y_{t,i}=0} w_{t,i}, \\ 0 &amp; \text{otherwise.} \end{cases}\)</li> </ul> </li> </ul> </li> <li><strong>Update Weights</strong>: <ul> <li>After receiving the true label \(y_t\), update the weights of the experts: <ul> <li>For each expert \(i\): \(w_{t+1,i} = \begin{cases} \beta w_{t,i} &amp; \text{if } y_{t,i} \neq y_t, \\ w_{t,i} &amp; \text{otherwise,} \end{cases}\) where \(\beta \in [0,1)\) is a parameter that reduces the weight of experts who make incorrect predictions.</li> </ul> </li> </ul> </li> <li><strong>Termination</strong>: <ul> <li>After \(T\) iterations, return the final weights of all experts.</li> </ul> </li> </ol> <h4 id="theoretical-performance-of-the-weighted-majority-algorithm"><strong>Theoretical Performance of the Weighted Majority Algorithm</strong></h4> <h5 id="mistake-bound"><strong>Mistake Bound</strong></h5> <p>One of the key results of the Weighted Majority Algorithm is its <strong>mistake bound</strong>, which ensures that the algorithm performs nearly as well as the best expert in hindsight.</p> <p><strong>Theorem</strong>:<br/> Let \(m_t\) denote the total number of mistakes made by the Weighted Majority Algorithm up to time \(t\), and let \(m_t^*\) denote the number of mistakes made by the best expert. Then: \(m_t \leq m_t^* \log\left(\frac{1}{\beta}\right) + \log(N)\)</p> <h3 id="special-cases">Special Cases</h3> <ul> <li> <p><strong>Realizable Case</strong>: When there exists an expert who makes zero mistakes, the mistake bound simplifies to: \(m_t \leq \log(N)\) This matches the bound of the Halving Algorithm.</p> </li> <li> <p><strong>General Case</strong>: The additional term involving \(m_t^*\) reflects the cost of adapting to the worst-case scenario, where no expert perfectly predicts all labels.</p> </li> </ul> <h5 id="proof-of-the-mistake-bound"><strong>Proof of the Mistake Bound</strong></h5> <p>The analysis of WMA involves defining a <strong>potential function</strong> to measure the aggregate weight of all experts: \(W_t = \sum_{i=1}^N w_{t,i}\)</p> <p><strong>Upper Bound on Potential</strong></p> <ul> <li>After each mistake, the potential decreases by at least a factor of \(\beta\): \(W_{t+1} \leq \beta W_t\)</li> </ul> <p><strong>Lower Bound on Potential</strong></p> <ul> <li>The weight of the best expert, \(w_{t,i}^*\), remains unchanged if they predict correctly: \(w_{t,i}^* \leq W_t\)</li> </ul> <p><strong>Combining Bounds</strong> By comparing these upper and lower bounds, it can be shown that: \(m_t \leq m_t^* \log\left(\frac{1}{\beta}\right) + \log(N)\)</p> <p>This logarithmic dependence on the number of experts, \(N\), ensures that WMA performs efficiently even in the presence of many experts.</p> <h5 id="strengths-and-weaknesses-of-the-weighted-majority-algorithm"><strong>Strengths and Weaknesses of the Weighted Majority Algorithm</strong></h5> <p><strong>Advantages</strong></p> <ul> <li><strong>Remarkable Theoretical Guarantees</strong>: <ul> <li>The logarithmic mistake bound requires no assumptions about the data distribution or expert performance.</li> </ul> </li> <li><strong>Flexibility</strong>: <ul> <li>WMA can be applied in a variety of adversarial and dynamic environments.</li> </ul> </li> </ul> <p><strong>Disadvantages</strong></p> <ul> <li><strong>Binary Loss</strong>: <ul> <li>No deterministic algorithm, including WMA, can achieve zero regret with binary loss.</li> </ul> </li> <li><strong>Improvements with Randomization</strong>: <ul> <li>Randomized versions of WMA can achieve better regret bounds.</li> </ul> </li> <li><strong>Extensions for Convex Losses</strong>: <ul> <li>When applied to convex loss functions, WMA can yield improved theoretical guarantees.</li> </ul> </li> </ul> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>The Weighted Majority Algorithm is a cornerstone of online learning, offering an elegant and efficient approach to handling adversarial settings. By adaptively updating the weights of experts, it ensures robust performance with minimal assumptions. While it has certain limitations, such as its inability to guarantee zero regret with binary loss, these can often be addressed through randomization or extensions to convex losses.</p> <p>In the next post, we’ll explore randomized versions of WMA and delve into other powerful algorithms for online learning. Stay tuned!</p> <hr/> <p><strong>Exponential Weighted Average Algorithm: A Regret-Minimization Powerhouse</strong></p> <p>The <strong>Exponential Weighted Average Algorithm (EWAA)</strong> is a fundamental online learning algorithm that provides elegant guarantees for minimizing regret in adversarial settings. It extends the principles of the Weighted Majority Algorithm by incorporating exponential weight updates, making it particularly effective for handling convex loss functions.</p> <h4 id="how-the-exponential-weighted-average-algorithm-works"><strong>How the Exponential Weighted Average Algorithm Works</strong></h4> <p>At its core, the EWAA maintains and updates weights for a set of experts, similar to the Weighted Majority Algorithm. However, it uses an <strong>exponential weighting scheme</strong> to achieve better bounds on regret, especially for convex losses.</p> <h5 id="steps-of-the-algorithm"><strong>Steps of the Algorithm</strong></h5> <ol> <li><strong>Weight Update</strong>:<br/> At each time step \(t\), the weights are updated as follows:<br/> \(w_{t+1,i} = w_{t,i} \cdot e^{-\eta L(\hat{y}_{t,i}, y_t)}\) where: <ul> <li>\(\eta &gt; 0\) is the learning rate.</li> <li>\(L(\hat{y}_{t,i}, y_t)\) is the loss incurred by expert \(i\) at time \(t\).</li> </ul> <p>This update exponentially penalizes experts based on the loss they incur, ensuring that poorly performing experts are rapidly down-weighted.</p> </li> <li><strong>Prediction</strong>:<br/> The algorithm predicts by averaging the predictions of all experts, weighted by their current weights: \(\hat{y}_t = \sum_{i=1}^N w_{t,i} \cdot \hat{y}_{t,i}\)</li> </ol> <h5 id="theorem-regret-bound-for-ewaa"><strong>Theorem: Regret Bound for EWAA</strong></h5> <p>Let \(L(y, y')\) be a convex loss function in its first argument, taking values in \([0, 1]\). For any \(\eta &gt; 0\) and any sequence of labels \(y_1, \dots, y_T \in \mathcal{Y}\), the regret of the Exponential Weighted Average Algorithm satisfies: \(\text{Regret}(T) \leq \frac{\log N}{\eta} + \frac{\eta T}{8}.\)</p> <p>Here, regret is defined as the difference between the total loss of the algorithm and the loss of the best expert: \(\text{Regret}(T) = \sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i \in [N]} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t).\)</p> <h5 id="optimizing-the-learning-rate"><strong>Optimizing the Learning Rate</strong></h5> <p>Choosing \(\eta = \sqrt{\frac{8 \log N}{T}}\) minimizes the regret bound, yielding: \(\text{Regret}(T) \leq \sqrt{\frac{T}{2} \log N}.\)</p> <p>This result demonstrates the efficiency of the EWAA, with regret growing only logarithmically with the number of experts \(N\) and sublinearly with the number of time steps \(T\).</p> <h4 id="proof-sketch-regret-bound-of-ewaa"><strong>Proof Sketch: Regret Bound of EWAA</strong></h4> <h5 id="key-idea-potential-function"><strong>Key Idea: Potential Function</strong></h5> <p>The proof relies on analyzing the potential function: \(\Phi_t = \log \left( \sum_{i=1}^N w_{t,i} \right).\)</p> <p><strong>Upper Bound</strong> Using the weight update rule and the convexity of the loss function, the following upper bound is derived: \(\Phi_{t+1} - \Phi_t \leq -\eta L(\hat{y}_t, y_t) + \frac{\eta^2}{8}.\)</p> <p><strong>Lower Bound</strong> By considering the contribution of the best expert, the potential function satisfies: \(\Phi_T - \Phi_0 \geq -\eta \min_{i \in [N]} L_{T,i} - \log N,\) where \(L_{T,i} = \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\) is the cumulative loss of expert \(i\).</p> <p><strong>Combining Bounds</strong> Summing the inequalities and comparing the two expressions for \(\Phi_T - \Phi_0\) gives the regret bound: \(\sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i \in [N]} L_{T,i} \leq \frac{\log N}{\eta} + \frac{\eta T}{8}.\)</p> <h5 id="advantages-and-disadvantages-of-ewaa"><strong>Advantages and Disadvantages of EWAA</strong></h5> <p><strong>Advantages</strong></p> <ol> <li><strong>Strong Theoretical Guarantees</strong>: <ul> <li>The regret bound is logarithmic in the number of experts \(N\) and sublinear in the number of time steps \(T\).</li> </ul> </li> <li><strong>Applicability to Convex Losses</strong>: <ul> <li>Unlike binary loss-focused algorithms, EWAA works seamlessly with convex loss functions, making it more versatile.</li> </ul> </li> <li><strong>Weight Adaptivity</strong>: <ul> <li>Exponential weight updates ensure that poor-performing experts are penalized efficiently.</li> </ul> </li> </ol> <p><strong>Disadvantages</strong></p> <ol> <li><strong>Dependence on Horizon</strong>: <ul> <li>Choosing the optimal learning rate \(\eta\) requires prior knowledge of the time horizon \(T\), which is not always feasible in practice.</li> </ul> </li> <li><strong>Increased Computational Complexity</strong>: <ul> <li>Exponential weight updates and averaging can be computationally expensive for a large number of experts.</li> </ul> </li> </ol> <h5 id="conclusion-1"><strong>Conclusion</strong></h5> <p>The Exponential Weighted Average Algorithm is a cornerstone of online learning, offering robust guarantees for regret minimization under convex loss functions. Its reliance on exponential weight updates makes it adaptable and theoretically elegant, but its dependence on the time horizon \(T\) can pose practical challenges.</p> <p>In the next post, we’ll explore randomized extensions and their impact on improving regret bounds. Stay tuned for more insights into online learning!</p> <hr/> <p><strong>Doubling Trick: A Clever Strategy to Handle Unknown Horizons</strong></p> <p>In many online learning algorithms, selecting the learning rate (\(\eta\)) or other parameters often requires knowledge of the total time horizon \(T\). However, in practical scenarios, \(T\) may not be known in advance. The <strong>Doubling Trick</strong> is a simple yet powerful method to overcome this limitation by dividing the time horizon into exponentially growing intervals and resetting parameters at the start of each interval.</p> <h4 id="the-idea-behind-the-doubling-trick"><strong>The Idea Behind the Doubling Trick</strong></h4> <p>The Doubling Trick works by splitting time into <strong>periods of exponentially increasing length</strong>. For example, the \(k\)-th period spans the range: \(I_k = [2^k, 2^{k+1} - 1].\)</p> <p>In each period:</p> <ol> <li>A new learning rate or parameter is chosen based on the length of the period.</li> <li>The algorithm is reset for that period.</li> </ol> <p>By progressively increasing the size of the intervals, the algorithm ensures that the total regret remains within acceptable bounds.</p> <h4 id="regret-bound-with-the-doubling-trick"><strong>Regret Bound with the Doubling Trick</strong></h4> <h5 id="theorem"><strong>Theorem</strong></h5> <p>Assume the same conditions as those in the Exponential Weighted Average Algorithm. For any total time horizon \(T\), the regret achieved by the Doubling Trick satisfies: \(\text{Regret}(T) \leq \sqrt{2} \cdot \sqrt{\frac{T}{2} \log N} + \sqrt{\log N / 2}.\)</p> <p>This result demonstrates that the Doubling Trick achieves regret bounds that are only slightly worse (by a constant factor) than if the total time horizon \(T\) were known in advance.</p> <h5 id="proof-sketch"><strong>Proof Sketch</strong></h5> <p>To derive the regret bound, let:</p> <ul> <li>\(L_{I_k}\) be the cumulative loss of the algorithm in interval \(I_k\).</li> <li>\(L_{I_k, i}\) be the cumulative loss of expert \(i\) in interval \(I_k\).</li> </ul> <p>By the regret bound for a single interval, we know: \(L_{I_k} - \min_{i \in [N]} L_{I_k, i} \leq \sqrt{\frac{2^k}{2} \log N}.\)</p> <p><strong>Total Loss Over All Periods</strong> Summing the regret over all intervals \(I_k\) for \(k = 0, \dots, n\), where \(n\) is the number of intervals up to time \(T\), gives: \(L_T = \sum_{k=0}^n L_{I_k} \leq \sum_{k=0}^n \min_{i \in [N]} L_{I_k, i} + \sum_{k=0}^n \sqrt{\frac{2^k}{2} \log N}.\)</p> <p>Using the geometric sum \(\sum_{k=0}^n \sqrt{2^k}\), we bound the second term: \(\sum_{k=0}^n \sqrt{2^k} \leq \sqrt{2} \cdot \sqrt{T}.\)</p> <p>Thus, the total regret is bounded as: \(\text{Regret}(T) \leq \sqrt{2} \cdot \sqrt{\frac{T}{2} \log N} + \sqrt{\log N / 2}.\)</p> <h5 id="why-the-doubling-trick-works"><strong>Why the Doubling Trick Works</strong></h5> <p>The beauty of the Doubling Trick lies in its ability to simulate knowing \(T\) without actually requiring it:</p> <ul> <li>The intervals grow exponentially, so the algorithm spends more time in later intervals, where the length of the period aligns better with the total horizon \(T\).</li> <li>By resetting parameters for each interval, the algorithm effectively adapts its learning rate to the length of the interval.</li> </ul> <h5 id="applications-of-the-doubling-trick"><strong>Applications of the Doubling Trick</strong></h5> <p>The Doubling Trick is not limited to regret minimization. It finds applications in various areas of machine learning and optimization:</p> <ol> <li><strong>Hyperparameter Tuning</strong>: <ul> <li>Learning rates or regularization parameters can be adjusted dynamically using the Doubling Trick.</li> </ul> </li> <li><strong>Bandit Problems</strong>: <ul> <li>It helps adapt exploration-exploitation strategies over unknown time horizons.</li> </ul> </li> <li><strong>General Parameter Updates</strong>: <ul> <li>Any scenario requiring parameter tuning over an unknown horizon can leverage this method.</li> </ul> </li> </ol> <h5 id="advantages-and-limitations"><strong>Advantages and Limitations</strong></h5> <p><strong>Advantages</strong></p> <ol> <li><strong>Handles Unknown Horizons</strong>: <ul> <li>No prior knowledge of \(T\) is needed, making it highly practical.</li> </ul> </li> <li><strong>Minimal Regret Penalty</strong>: <ul> <li>The regret bound is only worse by a constant factor compared to the known \(T\) case.</li> </ul> </li> </ol> <p><strong>Limitations</strong></p> <ol> <li><strong>Reset Overhead</strong>: <ul> <li>Resetting parameters at the start of each interval may introduce computational overhead.</li> </ul> </li> <li><strong>Suboptimal for Short Horizons</strong>: <ul> <li>For small \(T\), the initial intervals may dominate, leading to suboptimal performance.</li> </ul> </li> </ol> <hr/> <h5 id="conclusion-2"><strong>Conclusion</strong></h5> <p>The Doubling Trick is an ingenious and versatile technique that alleviates the need to know the time horizon in advance. By dividing time into exponentially growing intervals, it achieves regret bounds that are competitive with the best possible bounds for known \(T\). Its applicability to various domains makes it a fundamental tool in the arsenal of machine learning practitioners and theorists.</p> <p>In the next section, we’ll explore how this idea extends to randomized algorithms and convex loss functions for improved guarantees. Stay tuned for more!</p> <hr/> <p>For Content Piece 1: Halving Algorithm Title: “Understanding the Halving Algorithm: A Foundational Tool in Online Learning” Description: “Explore the Halving Algorithm, its mistake bounds, and how it minimizes regret in adversarial online learning scenarios.”</p> <p>For Content Piece 2: Weighted Majority Algorithm Title: “The Weighted Majority Algorithm: Combining Expert Predictions Effectively” Description: “Learn how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.”</p> <p>For Content Piece 3: Exponential Weighted Average Algorithm Title: “Exponential Weighted Average Algorithm: Optimizing Online Learning with Convex Losses” Description: “Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.”</p> <p>For Content Piece 4: Doubling Trick Title: “Doubling Trick in Online Learning: Tackling Unknown Time Horizons” Description: “Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.”</p>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.]]></summary></entry><entry><title type="html">Online Learning in ML - A Beginner’s Guide to Adaptive Learning</title><link href="https://monishver11.github.io/blog/2025/Online-Learning/" rel="alternate" type="text/html" title="Online Learning in ML - A Beginner’s Guide to Adaptive Learning"/><published>2025-01-23T14:45:00+00:00</published><updated>2025-01-23T14:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/Online-Learning</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/Online-Learning/"><![CDATA[<p>In this post, we’ll dive into one of the foundational topics that was discussed in the first class of my advanced machine learning course: online learning. This course is centered on theoretical insights and encourages students to think critically, experiment fearlessly, and embrace confusion as a stepping stone toward innovation. Let’s explore how online learning fits into the broader landscape of machine learning and why it’s such a powerful concept.</p> <h5 id="the-advanced-ml-course-a-brief-overview"><strong>The Advanced ML Course: A Brief Overview</strong></h5> <p>The course takes a deeply theoretical approach, focusing on critical analysis and research to build innovative machine learning algorithms. It’s not just about solving problems but about challenging established ideas, learning from mistakes, and having the courage to be wrong. Confusion, in this context, is not a roadblock—it’s a catalyst for deeper thinking.</p> <p>Two primary domains form the core of present ML/AI space:</p> <ol> <li><strong>Neural Networks</strong>: The cornerstone of modern machine learning.</li> <li><strong>Online Learning</strong>: A versatile and influential approach with deep connections to game theory and optimization.</li> </ol> <h4 id="what-is-online-learning"><strong>What is Online Learning?</strong></h4> <p>Online learning stands out as an area of machine learning with rich literature and numerous practical applications. It bridges the gap between supervised learning and game-theoretic optimization while offering efficient solutions for large-scale problems.</p> <p>Unlike traditional batch learning, where algorithms process the entire dataset at once, online learning operates iteratively, processing one sample at a time. This makes it computationally efficient and ideal for large datasets. Moreover, online learning does not rely on the common assumption that data points are independent and identically distributed (i.i.d.). Instead, it is designed to handle adversarial scenarios, making it incredibly flexible and applicable to situations where data distributions are unknown or variable.</p> <p>Even though these algorithms are inherently designed for adversarial settings, they can, under specific conditions, yield accurate predictions in scenarios where data does follow a distribution.</p> <p><strong>What do we mean by adversarial in the context of online learning?</strong></p> <p>In online learning, “adversarial” refers to settings where data is not assumed to follow a fixed probabilistic distribution. Instead, the data sequence might be unpredictable, dependent, or deliberately chosen to challenge the algorithm. This flexibility makes online learning particularly robust in real-world applications.</p> <ul> <li><strong>Real-world Examples</strong>: <ul> <li><strong>Financial Models</strong>: Adapting to volatile or externally influenced stock price movements.</li> <li><strong>Recommendation Systems</strong>: Managing biased or strategically influenced user feedback.</li> <li><strong>Security Systems</strong>: Responding to data manipulations or malicious attacks.</li> </ul> </li> </ul> <p>By focusing on resilience and adaptability, online learning algorithms excel in handling evolving data and challenging environments, making them indispensable for a wide range of applications.</p> <h5 id="why-online-learning"><strong>Why Online Learning?</strong></h5> <p>Traditional machine learning approaches often rely on the PAC (Probably Approximately Correct) framework, where:</p> <ul> <li>The data distribution remains fixed over time.</li> <li>Both training and testing data are assumed to follow the same i.i.d. distribution.</li> </ul> <p><strong>What is the PAC Learning Framework?</strong></p> <p>The PAC learning framework provides a theoretical foundation for understanding the feasibility of learning in a probabilistic setting. Under this framework:</p> <ul> <li>The algorithm’s goal is to find a hypothesis that is <em>probably approximately correct</em>, meaning it performs well on the training data and generalizes to unseen data with high probability.</li> <li>It assumes that data points are drawn independently and identically distributed (i.i.d.) from a fixed, unknown distribution.</li> <li>Key metrics include the error rate of the hypothesis on future samples and its convergence to the true distribution as more data is provided.</li> </ul> <p>While this framework is powerful for traditional batch learning, it relies on strong assumptions about the stability and predictability of the data distribution, making it less suitable for dynamic or adversarial scenarios.</p> <p>In contrast, online learning assumes no such distributional stability. It operates under the following key principles:</p> <ul> <li><strong>No Assumptions on Data Distribution</strong>: The data can follow any sequence, including adversarially generated ones. This flexibility allows online learning to adapt to real-world scenarios where data patterns may shift unpredictably.</li> <li><strong>Mixed Training and Testing</strong>: Training and testing are not separate phases but occur simultaneously, enabling the algorithm to continuously learn and improve from new data.</li> <li><strong>Worst-Case Analysis</strong>: Algorithms are designed to perform well even under the most challenging conditions, ensuring robustness in unpredictable environments.</li> <li><strong>Performance Metrics</strong>: Instead of accuracy or loss functions commonly used in batch learning, online learning evaluates performance using measures like: <ul> <li><strong><em>Mistake Model</em></strong>: The total number of incorrect predictions made during the learning process.</li> <li><strong><em>Regret</em></strong>: The difference between the cumulative loss of the algorithm and the loss of the best possible strategy in hindsight.</li> </ul> </li> </ul> <p><strong>What are some practical applications of online learning?</strong></p> <p>Online learning has proven invaluable in a variety of real-world domains where data is dynamic, unpredictable, or arrives sequentially:</p> <ol> <li><strong>Stock Market Predictions</strong>: Continuously adapting to ever-changing financial data, helping traders and financial systems make real-time decisions.</li> <li><strong>Online Advertising</strong>: Personalizing ads based on user behavior that evolves with every click or interaction.</li> <li><strong>Recommendation Systems</strong>: Adapting suggestions in real time as users interact with platforms like Netflix, Amazon, or YouTube.</li> <li><strong>Autonomous Systems</strong>: Enabling self-driving cars or robots to learn and adapt to new scenarios as they encounter them.</li> <li><strong>Spam Filtering</strong>: Continuously updating filters to catch new spam types as they emerge.</li> <li><strong>Security Systems</strong>: Responding to cyberattacks or new threats by learning and adapting on the fly.</li> </ol> <p>This shift in perspective allows online learning to address a broader range of real-world problems. For now, if all of this feels a bit abstract, don’t worry—hang tight! We’ll dive deeper and make sure to explore it thoroughly, leaving no stone unturned.</p> <h4 id="the-general-online-learning-framework"><strong>The General Online Learning Framework</strong></h4> <p>The online learning process follows a simple yet powerful framework. At each step:</p> <ol> <li>The algorithm receives an instance, denoted as \(x_t\).</li> <li>It makes a prediction, \(\hat{y}_t\).</li> <li>The true label, \(y_t\), is revealed.</li> <li>A loss is incurred, calculated as \(L(\hat{y}_t, y_t)\), which quantifies the prediction error.</li> </ol> <p>The overarching goal of online learning is to minimize the total loss over a sequence of predictions: \(\sum_{t=1}^T L(\hat{y}_t, y_t)\)</p> <p>For classification tasks, a common choice of loss is the 0-1 loss: \(L(\hat{y}_t, y_t) = \mathbb{1}(\hat{y}_t \neq y_t) \; or \; \vert \hat{y}_t - y_t \vert\) For regression tasks, the squared loss is often used: \(L(\hat{y}_t, y_t) = (\hat{y}_t - y_t)^2\)</p> <hr/> <h4 id="prediction-with-expert-advice"><strong>Prediction with Expert Advice</strong></h4> <p>One particularly compelling framework in online learning is <strong>Prediction with Expert Advice</strong>. Imagine you have multiple “experts,” each providing advice on how to predict the label for a given instance. The challenge lies in aggregating their advice to make accurate predictions while minimizing the regret associated with poor decisions.</p> <p>The process unfolds as follows:</p> <ol> <li>At each time step, the algorithm receives an instance, \(x_t\), and predictions from \(N\) experts, \(\{y_{t,1}, y_{t,2}, \dots, y_{t,N}\}\).</li> <li>Based on this advice, the algorithm predicts \(\hat{y}_t\).</li> <li>The true label, \(y_t\), is revealed, and the loss, \(L(\hat{y}_t, y_t)\), is incurred.</li> </ol> <p>The performance of the algorithm is measured by its <strong><em>regret</em></strong>, which is the difference between the total loss incurred by the algorithm and the total loss of the best-performing expert:</p> \[\text{Regret}(T) = \sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i=1, \dots, N} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p>Minimizing regret ensures that the algorithm’s predictions improve over time and closely approximate the performance of the best expert.</p> <p><strong>What does the regret equation convey and how do we interpret it?</strong></p> <p>The regret equation provides a way to evaluate the algorithm’s performance in hindsight by comparing it to the best expert. Here’s what each term means:</p> <ol> <li> <p><strong>Algorithm’s Loss</strong> (\(\sum_{t=1}^T L(\hat{y}_t, y_t)\)):<br/> This is the cumulative loss incurred by the algorithm over \(T\) time steps. It reflects how well the algorithm performs when making predictions based on the aggregated advice of all experts.</p> </li> <li> <p><strong>Best Expert’s Loss</strong> (\(\min_{i=1, \dots, N} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\)):<br/> This represents the cumulative loss of the single best-performing expert in hindsight. Note that the best expert is identified after observing all \(T\) instances, which gives it an advantage over the algorithm that has to predict in real time.</p> </li> <li> <p><strong>Regret</strong>:<br/> The difference between these two terms quantifies how much worse the algorithm performs compared to the best expert.</p> <ul> <li><strong>Low regret</strong> indicates that the algorithm’s predictions are close to those of the best expert, demonstrating effective learning.</li> <li><strong>High regret</strong> suggests that the algorithm is failing to learn effectively from the experts’ advice.</li> </ul> </li> </ol> <p><strong>Why is regret important?</strong></p> <p>Regret is a crucial metric in online learning because:</p> <ul> <li>It provides a measure of how well the algorithm adapts to the expert advice over time.</li> <li>It ensures that, as the number of time steps \(T\) increases, the algorithm’s performance converges to that of the best expert (ideally achieving sublinear regret, such as \(O(\sqrt{T})\) or better).</li> <li>It accounts for the dynamic nature of predictions, focusing on learning improvement rather than static accuracy.</li> </ul> <p>A few more questions to make our understadning better.</p> <p><strong>How to Calculate the Best Expert’s Loss?</strong></p> <p>The <strong>Best Expert’s Loss</strong> is the cumulative loss of the single expert that performs best over the entire sequence of predictions, \(T\). Here’s how to calculate it:</p> <ol> <li> <p><strong>Track each expert’s cumulative loss</strong>:<br/> For each expert \(i\), maintain a running sum of their losses over the rounds:</p> \[L_{\text{expert } i} = \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p>Here, \(\hat{y}_{t,i}\) is the prediction made by expert \(i\) at time \(t\), and \(y_t\) is the true label. \(L\) could represent any loss function, such as zero-one loss or squared loss.</p> </li> <li> <p><strong>Find the expert with the minimum cumulative loss</strong>:<br/> After summing the losses for all \(N\) experts over \(T\) rounds, identify the expert whose cumulative loss is the smallest:</p> \[\min_{i=1, \dots, N} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)\] <p>This value represents the <strong>Best Expert’s Loss</strong>, which serves as the benchmark for evaluating the algorithm’s regret.</p> </li> </ol> <p><strong>Do We Pick the Best Expert After Each Round?</strong></p> <p>No, the <strong>Best Expert’s Loss</strong> is determined in hindsight, <strong><em>after</em></strong> observing the entire sequence of \(T\) rounds. The algorithm does not know in advance which expert is the best. Instead, it aggregates predictions from all experts during the process (e.g., using techniques like weighted averaging).</p> <ul> <li>The <strong>best expert</strong> is identified retrospectively after all rounds.</li> <li>The cumulative loss of this best expert is used to compute regret.</li> </ul> <p><strong>Example:</strong></p> <p>Suppose we have 3 experts, and their losses over 5 rounds are:</p> <table> <thead> <tr> <th>Round (t)</th> <th>Expert 1 Loss</th> <th>Expert 2 Loss</th> <th>Expert 3 Loss</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>0.2</td> <td>0.3</td> <td>0.1</td> </tr> <tr> <td>2</td> <td>0.1</td> <td>0.4</td> <td>0.2</td> </tr> <tr> <td>3</td> <td>0.3</td> <td>0.2</td> <td>0.3</td> </tr> <tr> <td>4</td> <td>0.4</td> <td>0.1</td> <td>0.3</td> </tr> <tr> <td>5</td> <td>0.2</td> <td>0.5</td> <td>0.1</td> </tr> </tbody> </table> <ol> <li><strong>Calculate the cumulative loss for each expert</strong>: <ul> <li><strong>Expert 1</strong>: \(0.2 + 0.1 + 0.3 + 0.4 + 0.2 = 1.2\)</li> <li><strong>Expert 2</strong>: \(0.3 + 0.4 + 0.2 + 0.1 + 0.5 = 1.5\)</li> <li><strong>Expert 3</strong>: \(0.1 + 0.2 + 0.3 + 0.3 + 0.1 = 1.0\)</li> </ul> </li> <li> <p><strong>Find the minimum cumulative loss</strong>: \(\min(1.2, 1.5, 1.0) = 1.0\)</p> <p>Hence, the <strong>Best Expert’s Loss</strong> is <strong>1.0</strong>, achieved by Expert 3.</p> </li> </ol> <p>Prediction with Expert Advice is a powerful framework for dynamic environments where multiple sources of information or strategies need to be combined effectively. It ensures robustness and adaptability by iteratively improving predictions while minimizing regret.</p> <hr/> <h4 id="the-halving-algorithm-simple-and-powerful"><strong>The Halving Algorithm: Simple and Powerful</strong></h4> <p>The <strong>Halving Algorithm</strong> is a simple yet effective online learning algorithm designed to minimize mistakes. It works by maintaining a set of hypotheses (or experts) and systematically eliminating those that make incorrect predictions.</p> <p>Here’s how it works:</p> <ol> <li><strong>Initialization</strong>: Start with a set of hypotheses, \(H_1 = H\).</li> <li><strong>Iteration</strong>: At each time step, \(t\): <ul> <li>Receive an instance, \(x_t\).</li> <li>Predict the label, \(\hat{y}_t\), using majority voting among the hypotheses in \(H_t\).</li> <li>Receive the true label, \(y_t\).</li> <li>If \(\hat{y}_t \neq y_t\), update the hypothesis set: \(H_{t+1} = \{h \in H_t : h(x_t) = y_t\}\)</li> </ul> </li> <li><strong>Termination</strong>: After all iterations, return the final hypothesis set, \(H_{T+1}\).</li> </ol> <h5 id="mistake-bound-for-the-halving-algorithm"><strong>Mistake Bound for the Halving Algorithm</strong></h5> <p><strong>Theorem</strong>: If the initial hypothesis set \(H\) is finite, the number of mistakes made by the Halving Algorithm is bounded by:</p> \[M_{Halving(H)} \leq \log_2 |H|\] <p><strong>Proof Outline</strong>:</p> <ul> <li>Each mistake reduces the size of the hypothesis set by at least half: \(|H_{t+1}| \leq \frac{|H_t|}{2}\)</li> <li>Initially, \(|H_1| = |H|\). After \(M\) mistakes: \(|H_{M+1}| \leq \frac{|H|}{2^M}\)</li> <li>To ensure \(|H_{M+1}| \geq 1\) (at least one hypothesis remains), we require: \(M \leq \log_2 |H|\)</li> </ul> <p>This logarithmic bound demonstrates the efficiency of the Halving Algorithm, even in adversarial settings.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>Online learning offers a powerful framework for making predictions in dynamic and adversarial environments. Its ability to adapt, operate under minimal assumptions, and deliver robust performance makes it a cornerstone of modern machine learning research. The Halving Algorithm provides a concrete example of how online learning methods can be both intuitive and theoretically grounded.</p> <p>In upcoming posts, we’ll delve deeper into other online learning algorithms and explore their theoretical guarantees, practical applications, and connections to broader machine learning principles. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>PAC</li> <li>Online Learning Intro</li> </ul>]]></content><author><name></name></author><category term="ADV-ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.]]></summary></entry><entry><title type="html">Multivariate Gaussian Distribution and Naive Bayes</title><link href="https://monishver11.github.io/blog/2025/Multivariate-GNB/" rel="alternate" type="text/html" title="Multivariate Gaussian Distribution and Naive Bayes"/><published>2025-01-23T03:01:00+00:00</published><updated>2025-01-23T03:01:00+00:00</updated><id>https://monishver11.github.io/blog/2025/Multivariate-GNB</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/Multivariate-GNB/"><![CDATA[<p>When analyzing data in higher dimensions, we often encounter scenarios where input features are not independent. In such cases, the <strong>Multivariate Gaussian Distribution</strong> provides a robust probabilistic framework to model these relationships. It extends the familiar univariate Gaussian distribution to multiple dimensions, enabling us to capture dependencies and correlations between variables effectively.</p> <h4 id="understanding-the-multivariate-gaussian-distribution"><strong>Understanding the Multivariate Gaussian Distribution</strong></h4> <p>A multivariate Gaussian distribution is defined as:</p> \[x \sim \mathcal{N}(\mu, \Sigma),\] <p>where \(\mu\) is the mean vector, and \(\Sigma\) is the covariance matrix. Its probability density function is given by:</p> \[p(x) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu)^\top \Sigma^{-1} (x - \mu)\right),\] <p>Here, \(d\) represents the dimensionality of the input \(x\), \(\vert \Sigma \vert\) denotes the determinant of the covariance matrix, and \(\Sigma^{-1}\) is its inverse.</p> <p>The term \((x - \mu)^\top \Sigma^{-1} (x - \mu)\) is referred to as the <strong>Mahalanobis distance</strong>, which measures the distance of a point \(x\) from the mean \(\mu\). Unlike the Euclidean distance, the Mahalanobis distance normalizes for differences in variances and accounts for correlations between the dimensions. This normalization makes it particularly useful in multivariate data analysis.</p> <h5 id="intuition-and-analogy-for-multivariate-gaussian"><strong>Intuition and Analogy for Multivariate Gaussian</strong></h5> <p>Think of the multivariate Gaussian distribution as a <strong>3D bell-shaped curve</strong> (or higher-dimensional equivalent) where:</p> <ul> <li>The <strong>peak</strong> of the bell is at \(\mu\), the mean vector.</li> <li>The <strong>spread</strong> of the bell in different directions is determined by \(\Sigma\), the covariance matrix. It stretches or compresses the curve along certain axes depending on the variances and correlations.</li> </ul> <h6 id="analogy-a-weighted-balloon"><strong>Analogy: A Weighted Balloon</strong></h6> <p>Imagine a balloon filled with air. If the balloon is perfectly spherical, it represents a distribution where all dimensions are independent and have the same variance (this corresponds to \(\Sigma\) being a diagonal matrix with equal entries).</p> <p>Now, if you squeeze the balloon in one direction:</p> <ul> <li>It elongates in one direction and compresses in another. This reflects <strong>correlations</strong> between dimensions in the data, encoded by the off-diagonal elements of \(\Sigma\).</li> <li>The shape of the balloon changes, and distances (like Mahalanobis distance) now account for these correlations, unlike Euclidean distance.</li> </ul> <h5 id="how-to-think-about-mahalanobis-distance"><strong>How to Think About Mahalanobis Distance</strong></h5> <p>The Mahalanobis distance:</p> \[d_M(x) = \sqrt{(x - \mu)^\top \Sigma^{-1} (x - \mu)}\] <p>can be understood as the distance from a point \(x\) to the center \(\mu\), scaled by the shape and orientation of the distribution:</p> <ol> <li> <p><strong>Scaling by Variance</strong>: In directions where the variance is large (the distribution is “spread out”), the Mahalanobis distance will consider points farther from the mean as less unusual. Conversely, in directions where variance is small, even small deviations from the mean are considered significant.</p> </li> <li> <p><strong>Accounting for Correlations</strong>: If two dimensions are correlated, the Mahalanobis distance adjusts for this by using the covariance matrix \(\Sigma\). The covariance matrix captures both the variances of individual dimensions and the relationships (correlations) between them.</p> </li> </ol> <p><strong>Role of the Inverse Covariance Matrix:</strong></p> <p>The term \(\Sigma^{-1}\) (the inverse of the covariance matrix) in the Mahalanobis distance ensures that the contribution of each dimension is scaled appropriately. For example:</p> <ul> <li>If two dimensions are strongly correlated, deviations along one dimension are partially “explained” by deviations along the other. The Mahalanobis distance reduces the weight of such deviations, treating them as less unusual.</li> <li>Conversely, if two dimensions are uncorrelated, the deviations are treated independently.</li> </ul> <p><strong>Example:</strong></p> <p>In a dataset of height and weight, a taller-than-average person is likely to weigh more than average. The covariance matrix captures this relationship, and \(\Sigma^{-1}\) adjusts the distance calculation to reflect that such deviations are expected. Without this adjustment (as in Euclidean distance), the relationship would be ignored, leading to an overestimation of the “unusualness” of the point.</p> <p><strong>Returning to the Balloon Analogy,</strong></p> <p>The Mahalanobis distance incorporates the “shape” of the balloon (determined by \(\Sigma\)) to measure distances:</p> <p><strong>Shape and Scaling:</strong></p> <ul> <li>A spherical balloon corresponds to a covariance matrix where all dimensions are independent and have equal variance. In this case, the Mahalanobis distance reduces to the Euclidean distance.</li> <li>A stretched or compressed balloon reflects correlations or differences in variance. The Mahalanobis distance scales the contribution of each dimension based on the covariance structure, ensuring that distances are measured relative to the shape of the distribution.</li> </ul> <p><strong>How It Works:</strong></p> <ul> <li>Points on the surface of the balloon correspond to a Mahalanobis distance of 1, regardless of the balloon’s shape. This is because the Mahalanobis distance normalizes for the stretching or compressing of the balloon in different directions.</li> <li>Mathematically, this is achieved by transforming the space using \(\Sigma^{-1}\), effectively “flattening” the correlations and variances. In this transformed space, the balloon becomes a perfect sphere, and distances are measured uniformly.</li> </ul> <p>These adjustments make the Mahalanobis distance a powerful metric for detecting outliers and understanding the distribution of data in a multivariate context.</p> <hr/> <p>If you’re still unsure about the concept, let’s walk through an example and explore it together.</p> <h6 id="1-covariance-matrix"><strong>1. Covariance Matrix</strong>:</h6> <p>For a dataset with two variables, say <strong>height</strong> (\(x_1\)) and <strong>weight</strong> (\(x_2\)), the covariance matrix \(\Sigma\) looks like this:</p> \[\Sigma = \begin{pmatrix} \sigma_{11} &amp; \sigma_{12} \\ \sigma_{21} &amp; \sigma_{22} \end{pmatrix}\] <p>Where:</p> <ul> <li>\(\sigma_{11}\) is the variance of height (\(x_1\)).</li> <li>\(\sigma_{22}\) is the variance of weight (\(x_2\)).</li> <li>\(\sigma_{12} = \sigma_{21}\) is the covariance between height and weight.</li> </ul> <h6 id="2-inverse-covariance-matrix"><strong>2. Inverse Covariance Matrix</strong>:</h6> <p>The inverse of the covariance matrix \(\Sigma^{-1}\) is used to “normalize” the data and account for correlations. The inverse of a 2x2 matrix is given by:</p> \[\Sigma^{-1} = \frac{1}{\text{det}(\Sigma)} \begin{pmatrix} \sigma_{22} &amp; -\sigma_{12} \\ -\sigma_{21} &amp; \sigma_{11} \end{pmatrix}\] <p>Where the determinant of the covariance matrix is:</p> \[\text{det}(\Sigma) = \sigma_{11} \sigma_{22} - \sigma_{12}^2\] <h6 id="3-example-correlated-data-height-and-weight"><strong>3. Example: Correlated Data (Height and Weight)</strong></h6> <p>Suppose we have a dataset of heights and weights, and the covariance matrix looks like this:</p> \[\Sigma = \begin{pmatrix} 100 &amp; 80 \\ 80 &amp; 200 \end{pmatrix}\] <p>This means:</p> <ul> <li>The variance of height (\(\sigma_{11}\)) is 100.</li> <li>The variance of weight (\(\sigma_{22}\)) is 200.</li> <li>The covariance between height and weight (\(\sigma_{12} = \sigma_{21}\)) is 80, indicating a strong positive correlation between height and weight.</li> </ul> <p>Now, let’s say we have a data point:</p> \[x = \begin{pmatrix} 180 \\ 75 \end{pmatrix}\] <p>This means the person is 180 cm tall and weighs 75 kg. The mean of the dataset is:</p> \[\mu = \begin{pmatrix} 170 \\ 70 \end{pmatrix}\] <h6 id="31-euclidean-distance-without-accounting-for-correlation"><strong>3.1. Euclidean Distance</strong> (Without Accounting for Correlation)</h6> <p>The Euclidean distance between the data point \(x\) and the mean \(\mu\) is:</p> \[D_E(x) = \sqrt{(x_1 - \mu_1)^2 + (x_2 - \mu_2)^2}\] <p>Substituting the values:</p> \[D_E(x) = \sqrt{(180 - 170)^2 + (75 - 70)^2} = \sqrt{10^2 + 5^2} = \sqrt{100 + 25} = \sqrt{125} \approx 11.18\] <p>This distance doesn’t account for the correlation between height and weight. It treats the two dimensions as if they are independent, and gives a straightforward measure of how far the point is from the mean in Euclidean space.</p> <h6 id="32-mahalanobis-distance-with-covariance-adjustment"><strong>3.2. Mahalanobis Distance</strong> (With Covariance Adjustment)</h6> <p>Now, let’s compute the Mahalanobis distance. First, we need to compute the inverse of the covariance matrix \(\Sigma^{-1}\).</p> <p>The determinant of \(\Sigma\) is:</p> \[\text{det}(\Sigma) = 100 \times 200 - 80^2 = 20000 - 6400 = 13600\] <p>So, the inverse covariance matrix is:</p> \[\Sigma^{-1} = \frac{1}{13600} \begin{pmatrix} 200 &amp; -80 \\ -80 &amp; 100 \end{pmatrix} = \begin{pmatrix} 0.0147 &amp; -0.0059 \\ -0.0059 &amp; 0.0074 \end{pmatrix}\] <p>Now, we compute the Mahalanobis distance:</p> \[D_M(x) = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)}\] <p>Substituting the values:</p> \[x - \mu = \begin{pmatrix} 180 - 170 \\ 75 - 70 \end{pmatrix} = \begin{pmatrix} 10 \\ 5 \end{pmatrix}\] <p>Now, calculate the Mahalanobis distance:</p> \[D_M(x) = \sqrt{\begin{pmatrix} 10 &amp; 5 \end{pmatrix} \begin{pmatrix} 0.0147 &amp; -0.0059 \\ -0.0059 &amp; 0.0074 \end{pmatrix} \begin{pmatrix} 10 \\ 5 \end{pmatrix}}\] <p>First, multiply the vectors:</p> \[\begin{pmatrix} 10 &amp; 5 \end{pmatrix} \begin{pmatrix} 0.0147 &amp; -0.0059 \\ -0.0059 &amp; 0.0074 \end{pmatrix} = \begin{pmatrix} 10 \times 0.0147 + 5 \times (-0.0059) \\ 10 \times (-0.0059) + 5 \times 0.0074 \end{pmatrix} = \begin{pmatrix} 0.147 - 0.0295 \\ -0.059 + 0.037 \end{pmatrix} = \begin{pmatrix} 0.1175 \\ -0.022 \end{pmatrix}\] <p>Now, multiply this result by the vector \(\begin{pmatrix} 10 \\ 5 \end{pmatrix}\):</p> \[\begin{pmatrix} 0.1175 &amp; -0.022 \end{pmatrix} \begin{pmatrix} 10 \\ 5 \end{pmatrix} = 0.1175 \times 10 + (-0.022) \times 5 = 1.175 - 0.11 = 1.065\] <p>Thus, the Mahalanobis distance is:</p> \[D_M(x) = \sqrt{1.065} \approx 1.03\] <h6 id="4-interpretation-of-the-results"><strong>4. Interpretation of the Results</strong>:</h6> <ul> <li>The <strong>Euclidean distance</strong> between the point and the mean was approximately <strong>11.18</strong>. This suggests that the point is far from the mean, without considering the correlation between height and weight.</li> <li>The <strong>Mahalanobis distance</strong> is <strong>1.03</strong>, which is much smaller. This is because the Mahalanobis distance accounts for the fact that height and weight are correlated. The deviation in weight is expected given the deviation in height, so the Mahalanobis distance treats this as less “unusual.”</li> </ul> <h6 id="takeaways"><strong>Takeaways:</strong></h6> <ul> <li><strong>Euclidean distance</strong> treats each dimension as independent, ignoring correlations, which can lead to an overestimation of how unusual a point is.</li> <li><strong>Mahalanobis distance</strong>, by using the inverse covariance matrix \(\Sigma^{-1}\), adjusts for correlations and scales the deviations accordingly. This results in a more accurate measure of how far a point is from the mean, considering the underlying structure of the data (e.g., the correlation between height and weight in this example).</li> </ul> <hr/> <h5 id="grasping-better-with-bivariate-normal-distributions"><strong>Grasping Better with Bivariate Normal Distributions</strong></h5> <p>To build a deeper understanding, let’s focus on a specific case: the two-dimensional Gaussian, commonly referred to as the <strong>bivariate normal distribution</strong>.</p> <h6 id="case-1-identity-covariance-matrix"><strong>Case 1: Identity Covariance Matrix</strong></h6> <p>Suppose the covariance matrix is given as:</p> \[\Sigma = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}.\] <p>In this scenario, the contours of the distribution are circular. This indicates that there is no correlation between the two variables, and both have equal variances. The shape of the contours reflects the isotropic nature of the distribution.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-1-480.webp 480w,/assets/img/Multivariate-GNB-1-800.webp 800w,/assets/img/Multivariate-GNB-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h6 id="case-2-scaled-identity-covariance"><strong>Case 2: Scaled Identity Covariance</strong></h6> <p>If we scale the covariance matrix, say:</p> \[\Sigma = 0.5 \cdot \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix},\] <p>the variances of both variables decrease, resulting in smaller circular contours. Conversely, if we scale it up:</p> \[\Sigma = 2 \cdot \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix},\] <p>the variances increase, leading to larger circular contours. This demonstrates how scaling the covariance matrix affects the spread of the distribution.</p> <h6 id="case-3-anisotropic-variance"><strong>Case 3: Anisotropic Variance</strong></h6> <p>When the variances of the variables are different, such as when \(\text{var}(x_1) \neq \text{var}(x_2)\), the contours take on an elliptical shape. The orientation and eccentricity of the ellipse are determined by the relative variances along each axis.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-2-480.webp 480w,/assets/img/Multivariate-GNB-2-800.webp 800w,/assets/img/Multivariate-GNB-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h6 id="case-4-correlated-variables"><strong>Case 4: Correlated Variables</strong></h6> <p>Correlation between variables introduces an additional layer of complexity.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-3-480.webp 480w,/assets/img/Multivariate-GNB-3-800.webp 800w,/assets/img/Multivariate-GNB-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For instance, if:</p> \[\Sigma = \begin{bmatrix} 1 &amp; \rho \\ \rho &amp; 1 \end{bmatrix},\] <p>where \(\rho\) is the correlation coefficient:</p> <ul> <li>When \(\rho &gt; 0\), the variables are positively correlated, and the ellipse tilts along the diagonal.</li> <li>When \(\rho &lt; 0\), the variables are negatively correlated, and the ellipse tilts in the opposite direction.</li> <li>When \(\rho = 0\), the variables remain uncorrelated, resulting in circular or axis-aligned ellipses.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-4-480.webp 480w,/assets/img/Multivariate-GNB-4-800.webp 800w,/assets/img/Multivariate-GNB-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h4 id="gaussian-bayes-classifier"><strong>Gaussian Bayes Classifier</strong></h4> <p>The <strong>Gaussian Bayes Classifier (GBC)</strong> extends the Gaussian framework to classification tasks. It assumes that the conditional distribution \(p(x \vert y)\) follows a multivariate Gaussian distribution. Mathematically, for a class \(k\):</p> \[p(x|t = k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_k)^\top \Sigma_k^{-1} (x - \mu_k)\right),\] <p>where each class \(k\) has its own mean vector \(\mu_k\) and covariance matrix \(\Sigma_k\). The determinant \(\vert \Sigma_k \vert\) and the inverse \(\Sigma_k^{-1}\) are crucial components for computing probabilities.</p> <p>Estimating the parameters for each class becomes computationally challenging in high dimensions, as the covariance matrix has \(O(d^2)\) parameters. This complexity often necessitates simplifying assumptions to make the model tractable.</p> <p><strong>How do we arrive at this complexity, and why is it considered computationally challenging?</strong></p> <p>In the Gaussian Bayes Classifier, each class \(k\) has its own covariance matrix \(\Sigma_k\), which is a \(d \times d\) matrix where \(d\) is the dimensionality of the feature space. For a single class, this covariance matrix has \(\frac{d(d+1)}{2}\) unique parameters. This is because a covariance matrix is symmetric, meaning that the upper and lower triangular portions are mirrors of each other. Specifically, the diagonal elements represent the variances, while the off-diagonal elements represent the covariances between different features.</p> <p>For \(K\) classes, the total number of parameters required to estimate all the covariance matrices would be: \(K \times \frac{d(d+1)}{2}\)</p> <p>This can become computationally expensive, especially when \(d\) (the number of features) is large.</p> <p>This large number of parameters is the reason why the Gaussian Bayes Classifier faces challenges in high-dimensional settings, as the model needs to estimate these parameters from data, and estimating a large number of parameters requires a substantial amount of data. Moreover, when the dimensionality \(d\) is large relative to the number of training samples, the covariance matrix can become ill-conditioned or singular, which might lead to poor performance.</p> <p>To handle this, simplifications such as assuming diagonal covariance matrices (where off-diagonal covariances are set to zero) or sharing a common covariance matrix across all classes are often made, which reduces the number of parameters that need to be estimated.</p> <hr/> <h5 id="special-cases-of-gaussian-bayes-classifier"><strong>Special Cases of Gaussian Bayes Classifier</strong></h5> <p>To address the computational challenges, we consider the following special cases of the Gaussian Bayes Classifier:</p> <ol> <li> <p><strong>Full Covariance Matrix</strong><br/> Each class has its own covariance matrix \(\Sigma_k\). This allows for flexible modeling of the class distributions, as it can capture correlations between different features. The decision boundary, however, is quadratic, as the posterior probability depends on the quadratic term involving \((x - \mu_k)^\top \Sigma_k^{-1} (x - \mu_k)\).</p> <p><strong>Decision Boundary Derivation</strong>:<br/> The decision rule is based on the ratio of the posterior probabilities:</p> \[\frac{p(x \vert t = k)}{p(x \vert t = l)} &gt; 1\] <p>which leads to a quadratic expression involving the covariance matrices \(\Sigma_k\) and \(\Sigma_l\). This quadratic form creates a curved decision boundary.</p> <p><strong>Insight</strong>:<br/> Since each class can have a different covariance matrix, the decision boundary can bend and adapt to the data’s true distribution, allowing for accurate classification even in complex scenarios. However, the computational cost is high because each class requires estimating a full covariance matrix with \(O(d^2)\) parameters.</p> </li> <li> <p><strong>Shared Covariance Matrix</strong><br/> If all classes share a common covariance matrix \(\Sigma\), the decision boundary becomes linear. This assumption simplifies the model by treating all classes as having the same spread, reducing the number of parameters to estimate.</p> <p><strong>Decision Boundary Derivation</strong>:<br/> In this case, the likelihood for each class is given by:</p> \[p(x \vert t = k) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_k)^\top \Sigma^{-1} (x - \mu_k)\right)\] <p>The decision rule between two classes \(k\) and \(l\) simplifies to:</p> \[(x - \mu_k)^\top \Sigma^{-1} (x - \mu_k) - (x - \mu_l)^\top \Sigma^{-1} (x - \mu_l) = \text{constant}\] <p>which is linear in \(x\). This results in a linear decision boundary between the classes.</p> <p><strong>Insight</strong>:<br/> By assuming a common covariance matrix, we treat the class distributions as having the same shape and orientation. This simplifies the model and makes the decision boundary linear, leading to reduced computational cost and faster training. However, this may be less flexible if the true distributions of the classes are significantly different.</p> </li> <li> <p><strong>Naive Bayes Assumption</strong><br/> The Naive Bayes classifier assumes that the features are conditionally independent given the class, meaning that the covariance matrix is diagonal. This leads to a model where each feature is treated independently when making class predictions.</p> <p><strong>Decision Boundary Derivation</strong>:<br/> Under the Naive Bayes assumption, the covariance matrix is diagonal, so the likelihood for each class becomes:</p> \[p(x \vert t = k) = \prod_{i=1}^{d} \frac{1}{\sqrt{2\pi \sigma_{k,i}^2}} \exp\left(-\frac{(x_i - \mu_{k,i})^2}{2\sigma_{k,i}^2}\right)\] <p>The decision rule between two classes \(k\) and \(l\) leads to a quadratic expression for each feature, but since the features are independent, the decision boundary remains quadratic overall, as the product of exponentials leads to terms that depend on the squares of the feature values.</p> <p><strong>Insight</strong>:<br/> Even though the features are assumed to be independent, the decision boundary remains quadratic because of the feature-wise variances. The strong independence assumption makes the model computationally efficient, as it reduces the number of parameters to estimate (each class only requires \(d\) variances), but it limits the model’s flexibility to capture interactions between features.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Multivariate-GNB-5-480.webp 480w,/assets/img/Multivariate-GNB-5-800.webp 800w,/assets/img/Multivariate-GNB-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Multivariate-GNB-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Multivariate-GNB-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="gaussian-bayes-classifier-vs-logistic-regression"><strong>Gaussian Bayes Classifier vs. Logistic Regression</strong></h5> <p>One interesting connection between GBC and logistic regression arises when the data is truly Gaussian. If we assume shared covariance matrices, the decision boundaries produced by GBC become identical to those of logistic regression. However, logistic regression is more versatile since it does not rely on Gaussian assumptions and can learn other types of decision boundaries.</p> <p><strong>Note:</strong> Even though both methods produce the same linear decision boundary under the Gaussian assumption with shared covariance, the actual parameter values (weights and means) will be different because they are derived from different models and assumptions.</p> <hr/> <h5 id="final-thoughts"><strong>Final Thoughts</strong></h5> <p>The multivariate Gaussian distribution provides a probabilistic framework for understanding data with correlated features. By extending this to classification tasks, the Gaussian Bayes Classifier offers an elegant and interpretable approach to modeling. However, its reliance on assumptions like Gaussianity and the complexity of covariance estimation in high dimensions present practical challenges.</p> <p>Generative models, like GBC, aim to model the joint distribution \(p(x, y)\), which contrasts with discriminative models, such as logistic regression, that focus directly on \(p(y \vert x)\). While generative models offer a principled way to derive loss functions via maximum likelihood, they can struggle with small datasets, where estimating the joint distribution becomes difficult. <strong>Why?</strong></p> <p>Generative models typically use the product of the likelihood and the prior. Specifically, the likelihood \(p(x \mid y)\), can become challenging with small datasets because:</p> <ul> <li><strong>Insufficient data for accurate parameter estimation</strong>: With limited data, the model may not have enough examples to accurately estimate the parameters of the distribution (such as the means and variances in the case of Gaussian distributions).</li> <li><strong>Overfitting risk</strong>: With small datasets, the model may overfit to noise or specific patterns that don’t generalize well, leading to poor estimates of the joint distribution.</li> </ul> <p>In contrast, discriminative models like logistic regression focus directly on \(p(y \mid x)\) and are less affected by small sample sizes because they only need to model the decision boundary between classes, making them more robust in such situations.</p> <p>As you delve deeper into probabilistic frameworks, a question worth pondering is: Do generative models have an equivalent form of regularization to mitigate overfitting? This opens up avenues for exploring how these models can be made more robust in practice.</p> <p>To address this question, we’ll next explore <strong>Bayesian Inference</strong>, where generative models can incorporate priors over their parameters. For instance, in a Gaussian Bayes Classifier, instead of relying on point estimates for means and variances, Bayesian methods treat these parameters as distributions. This approach naturally regularizes the model by spreading probability mass over plausible parameter values, reducing the risk of overfitting. Stay tuned—we’ll dive into this in the next post!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.]]></summary></entry><entry><title type="html">Gaussian Naive Bayes - A Natural Extension</title><link href="https://monishver11.github.io/blog/2025/NB-continuous-features/" rel="alternate" type="text/html" title="Gaussian Naive Bayes - A Natural Extension"/><published>2025-01-20T20:39:00+00:00</published><updated>2025-01-20T20:39:00+00:00</updated><id>https://monishver11.github.io/blog/2025/NB-continuous-features</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/NB-continuous-features/"><![CDATA[<p>In the previous blog, we explored the Naive Bayes (NB) model for binary features and how it works under the assumption of conditional independence. However, real-world datasets often include continuous features. How can we extend the NB framework to handle such cases? Let’s dive into Gaussian Naive Bayes (GNB), a variant of NB that uses Gaussian distributions to model continuous inputs.</p> <p><strong>Before we start:</strong> I know this might be challenging to follow just by reading through, especially for this part. So, grab a pen and paper and work through it yourself. You’ll notice that within the summations, all terms except the one you’re differentiating with respect to are constants and will drop out (i.e., become zero). As you write it out, you’ll also understand why certain steps involve a change in sign. Working through it once will make everything much clearer and easier to grasp.</p> <hr/> <p>Consider a multiclass classification problem where each input feature \(x_i\) is continuous. To model \(p(x_i \mid y)\), we assume that the feature values follow a Gaussian (normal) distribution:</p> \[p(x_i \mid y = k) \sim \mathcal{N}(\mu_{i,k}, \sigma^2_{i,k}),\] <p>where \(\mu_{i,k}\) and \(\sigma^2_{i,k}\) are the mean and variance of \(x_i\) for class \(y = k\), respectively. Additionally, we model the class prior probabilities as:</p> \[p(y = k) = \theta_k.\] <p>With these assumptions, the likelihood of the dataset becomes:</p> \[p(D) = \prod_{n=1}^N p_\theta(x^{(n)}, y^{(n)})\] \[p(D) = \prod_{n=1}^N p(y^{(n)}) \prod_{i=1}^d p(x_i^{(n)} \mid y^{(n)}).\] <p>Substituting the Gaussian distribution for \(p(x_i \mid y)\), we get:</p> \[p(D) = \prod_{n=1}^N \theta_{y^{(n)}} \prod_{i=1}^d \frac{1}{\sqrt{2\pi\sigma_{i,y^{(n)}}^2}} \exp\left(-\frac{\left(x_i^{(n)} - \mu_{i,y^{(n)}}\right)^2}{2\sigma_{i,y^{(n)}}^2}\right).\] <p>It may seem complex at first, but if you look closely, you’ll see that we’re applying the same principle. The only difference is in the distribution. To visualize this, we’ve essentially applied the distribution to a familiar form \((1)\) once again to obtain the result. Take a moment to reflect on this.</p> \[\hat{y} = \arg\max_{y \in \mathcal{Y}} p(x, y; \theta) = \arg\max_{y} p(y \mid x; \theta) = \arg\max_{y} p(x \mid y; \theta) p(y; \theta) \tag{1}\] <hr/> <h4 id="learning-parameters-with-maximum-likelihood-estimation-mle"><strong>Learning Parameters with Maximum Likelihood Estimation (MLE)</strong></h4> <p>To train the Gaussian Naive Bayes model, we estimate the parameters \(\mu_{i,k}\), \(\sigma^2_{i,k}\), and \(\theta_k\) using MLE.</p> <h5 id="mean-mu_ik"><strong>Mean (\(\mu_{i,k}\)):</strong></h5> <p>The log-likelihood of the data is:</p> \[\ell = \sum_{n=1}^N \log \theta_{y^{(n)}} + \sum_{n=1}^N \sum_{i=1}^d \left[-\frac{1}{2} \log (2\pi \sigma_{i,y^{(n)}}^2) - \frac{\left(x_i^{(n)} - \mu_{i,y^{(n)}}\right)^2}{2\sigma_{i,y^{(n)}}^2}\right]\] <p>Taking the derivative with respect to \(\mu_{j,k}\) and setting it to zero gives:</p> \[\mu_{j,k} = \frac{\sum_{n:y^{(n)}=k} x_j^{(n)}}{\sum_{n:y^{(n)}=k} 1}\] <p>This is simply the sample mean of \(x_j\) for class \(k\).</p> <h5 id="derivation-of-mu_jk-for-gaussian-naive-bayes"><strong>Derivation of \(\mu_{j,k}\) for Gaussian Naive Bayes</strong></h5> <p>To estimate the parameter \(\mu_{j,k}\), the mean of feature \(x_j\) for class \(k\), we maximize the log-likelihood with respect to \(\mu_{j,k}\).</p> <p><strong>Step 1: Compute the Derivative of the Log-Likelihood</strong></p> <p>The log-likelihood is differentiated with respect to \(\mu_{j,k}\):</p> \[\frac{\partial}{\partial \mu_{j,k}} \ell = \frac{\partial}{\partial \mu_{j,k}} \sum_{n: y^{(n)} = k} \left( -\frac{1}{2 \sigma_{j,k}^2} \left( x_j^{(n)} - \mu_{j,k} \right)^2 \right)\] <p>Ignoring irrelevant terms (constants that do not depend on \(\mu_{j,k}\)), this simplifies to:</p> \[\frac{\partial}{\partial \mu_{j,k}} \ell = \sum_{n: y^{(n)} = k} \frac{1}{\sigma_{j,k}^2} \left( x_j^{(n)} - \mu_{j,k} \right)\] <p><strong>Step 2: Set the Derivative to Zero</strong></p> <p>To find the maximum likelihood estimate, set the derivative to zero:</p> \[\sum_{n: y^{(n)} = k} \frac{1}{\sigma_{j,k}^2} \left( x_j^{(n)} - \mu_{j,k} \right) = 0\] <p><strong>Step 3: Solve for \(\mu_{j,k}\)</strong></p> <p>Rearranging terms:</p> \[\sum_{n: y^{(n)} = k} x_j^{(n)} = \mu_{j,k} \sum_{n: y^{(n)} = k} 1\] <p>Divide both sides by \(\sum_{n: y^{(n)} = k} 1\):</p> \[\mu_{j,k} = \frac{\sum_{n: y^{(n)} = k} x_j^{(n)}}{\sum_{n: y^{(n)} = k} 1}\] <p><strong>Final Expression</strong></p> <p>The maximum likelihood estimate of \(\mu_{j,k}\) is:</p> \[\mu_{j,k} = \frac{\sum_{n: y^{(n)} = k} x_j^{(n)}}{\sum_{n: y^{(n)} = k} 1}\] <p><strong>Interpretation:</strong></p> <ul> <li>\(\mu_{j,k}\) is the sample mean of \(x_j\) for all data points in class \(k\).</li> <li>This parameter is essential for defining the Gaussian distribution for feature \(x_j\) given class \(k\) in Gaussian Naive Bayes.</li> </ul> <h5 id="variance-sigma2_ik"><strong>Variance (\(\sigma^2_{i,k}\)):</strong></h5> <p>Similarly, the variance for feature \(x_j\) in class \(k\) is:</p> \[\sigma^2_{j,k} = \frac{\sum_{n:y^{(n)}=k} \left(x_j^{(n)} - \mu_{j,k}\right)^2}{\sum_{n:y^{(n)}=k} 1}\] <h5 id="class-prior-theta_k"><strong>Class Prior (\(\theta_k\)):</strong></h5> <p>The class prior \(\theta_k\) is estimated as the proportion of data points belonging to class \(k\):</p> \[\theta_k = \frac{\sum_{n:y^{(n)}=k} 1}{N}\] <h5 id="derivation-of-sigma_jk2-sample-variance-and-theta_k-class-prior"><strong>Derivation of \(\sigma_{j,k}^2\) (Sample Variance) and \(\theta_k\) (Class Prior)</strong></h5> <p><strong>1. Derivation of \(\sigma_{j,k}^2\) (Sample Variance)</strong></p> <p>To derive the sample variance \(\sigma_{j,k}^2\), we start from the log-likelihood of the Gaussian distribution for feature \(x_j\) within class \(k\):</p> \[\ell = \sum_{n: y^{(n)} = k} \left[ -\frac{1}{2} \log(2\pi \sigma_{j,k}^2) - \frac{\left( x_j^{(n)} - \mu_{j,k} \right)^2}{2\sigma_{j,k}^2} \right]\] <p>We take the derivative of \(\ell\) with respect to \(\sigma_{j,k}^2\) and set it to zero:</p> \[\frac{\partial \ell}{\partial \sigma_{j,k}^2} = \sum_{n: y^{(n)} = k} \left[ -\frac{1}{2\sigma_{j,k}^2} + \frac{\left( x_j^{(n)} - \mu_{j,k} \right)^2}{2\sigma_{j,k}^4} \right] = 0\] <p>Simplify the equation:</p> \[\sum_{n: y^{(n)} = k} \left[ -\sigma_{j,k}^2 + \left( x_j^{(n)} - \mu_{j,k} \right)^2 \right] = 0\] <p>Divide by \(\sigma_{j,k}^2\) and rearrange:</p> \[\sigma_{j,k}^2 = \frac{\sum_{n: y^{(n)} = k} \left( x_j^{(n)} - \mu_{j,k} \right)^2}{\sum_{n: y^{(n)} = k} 1}\] <p>Thus, the MLE for \(\sigma_{j,k}^2\) is:</p> \[\sigma_{j,k}^2 = \frac{\sum_{n: y^{(n)} = k} \left( x_j^{(n)} - \mu_{j,k} \right)^2}{\sum_{n: y^{(n)} = k} 1}\] <p><strong>2. Derivation of \(\theta_k\) (Class Prior)</strong></p> <p>The class prior \(\theta_k\) represents the proportion of data points belonging to class \(k\) in the dataset. It is given by:</p> \[\theta_k = \frac{\sum_{n: y^{(n)} = k} 1}{N}\] <p><strong>Steps:</strong></p> <ol> <li><strong>Numerator</strong>: \(\sum_{n: y^{(n)} = k} 1\) counts the total number of data points that belong to class \(k\).</li> <li><strong>Denominator</strong>: \(N\) is the total number of data points in the entire dataset.</li> </ol> <p><strong>Finally,</strong></p> <ol> <li> <p><strong>Sample Variance</strong>:</p> \[\sigma_{j,k}^2 = \frac{\sum_{n: y^{(n)} = k} \left( x_j^{(n)} - \mu_{j,k} \right)^2}{\sum_{n: y^{(n)} = k} 1}\] </li> <li> <p><strong>Class Prior</strong>:</p> \[\theta_k = \frac{\sum_{n: y^{(n)} = k} 1}{N}\] </li> </ol> <ul> <li>The sample variance \(\sigma_{j,k}^2\) measures the spread of feature \(x_j\) for class \(k\), derived using MLE.</li> <li>The class prior \(\theta_k\) represents the proportion of data points in class \(k\), computed directly from the dataset.</li> </ul> <hr/> <h4 id="decision-boundary-of-the-gaussian-naive-bayes-gnb-model"><strong>Decision Boundary of the Gaussian Naive Bayes (GNB) Model</strong></h4> <p><strong>General Formulation of the Decision Boundary:</strong></p> <p>For binary classification (\(y \in \{0, 1\}\)), the <strong>log odds ratio</strong> is expressed as:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \log \frac{p(x \mid y=1)p(y=1)}{p(x \mid y=0)p(y=0)}.\] <p>If you’re unclear about what the log odds ratio is, it represents the logarithm of the ratio of the probabilities of the two classes. By setting the log odds ratio to zero, we identify the points where the model is equally likely to classify a sample as belonging to either class.</p> <p>In Gaussian Naive Bayes, this involves substituting the Gaussian distributions for \(p(x \mid y)\), simplifying the expression, and determining whether the resulting decision boundary is quadratic or linear based on the assumptions about the variances.</p> <p>Thus, the log odds ratio serves as a straightforward mathematical tool to derive the decision boundary by locating the regions where the probabilities of the two classes are equal.</p> <p>So, the conditional distributions \(p(x_i \mid y)\) are Gaussian:</p> \[p(x_i \mid y) = \frac{1}{\sqrt{2\pi \sigma_{i,y}^2}} \exp\left(-\frac{(x_i - \mu_{i,y})^2}{2\sigma_{i,y}^2}\right).\] <p>Substituting this into the log odds equation, we get:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \log \frac{\theta_1}{\theta_0} + \sum_{i=1}^d \left[\log \sqrt{\frac{\sigma_{i,0}^2}{\sigma_{i,1}^2}} + \frac{(x_i - \mu_{i,0})^2}{2\sigma_{i,0}^2} - \frac{(x_i - \mu_{i,1})^2}{2\sigma_{i,1}^2}\right].\] <p>This equation represents the <strong>general case</strong> of the GNB decision boundary.</p> <h5 id="linear-vs-quadratic-decision-boundaries"><strong>Linear vs. Quadratic Decision Boundaries</strong></h5> <h6 id="quadratic-decision-boundary"><strong>Quadratic Decision Boundary:</strong></h6> <p>In the general case, where the variances \(\sigma_{i,0}^2\) and \(\sigma_{i,1}^2\) differ between classes, the decision boundary is <strong>quadratic</strong>. This is due to the presence of quadratic terms in the numerator:</p> \[\frac{(x_i - \mu_{i,0})^2}{2\sigma_{i,0}^2} - \frac{(x_i - \mu_{i,1})^2}{2\sigma_{i,1}^2}.\] <h6 id="linear-decision-boundary"><strong>Linear Decision Boundary:</strong></h6> <p>When we assume the variances are equal for both classes \((\sigma_{i,0}^2 = \sigma_{i,1}^2 = \sigma_i^2)\), the quadratic terms cancel out. Simplifying the log odds equation yields:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \sum_{i=1}^d \frac{\mu_{i,1} - \mu_{i,0}}{\sigma_i^2} x_i + \sum_{i=1}^d \frac{\mu_{i,0}^2 - \mu_{i,1}^2}{2\sigma_i^2}.\] <p>In matrix form, this becomes:</p> \[\log \frac{p(y=1 \mid x)}{p(y=0 \mid x)} = \theta^\top x + \theta_0,\] <p>where:</p> <ul> <li> \[\theta_i = \frac{\mu_{i,1} - \mu_{i,0}}{\sigma_i^2}, \quad i \in [1, d]\] </li> <li> \[\theta_0 = \sum_{i=1}^d \frac{\mu_{i,0}^2 - \mu_{i,1}^2}{2\sigma_i^2}.\] </li> </ul> <p>Thus, under the shared variance assumption, the decision boundary is <strong>linear</strong>.</p> <p><strong>Takeaways:</strong></p> <ul> <li><strong>Quadratic Boundary</strong>: The difference in variances between the two classes introduces curvature, resulting in a nonlinear boundary.</li> <li><strong>Linear Boundary</strong>: Equal variances lead to a linear boundary, making the model behave similarly to logistic regression.</li> </ul> <p>This derivation connects Gaussian Naive Bayes to logistic regression and helps to understand its behavior under different assumptions.</p> <hr/> <h4 id="naive-bayes-vs-logistic-regression"><strong>Naive Bayes vs. Logistic Regression</strong></h4> <p>Both Naive Bayes and logistic regression are popular classifiers, but they differ fundamentally in their approach:</p> <hr/> <table> <thead> <tr> <th> </th> <th><strong>Logistic Regression</strong></th> <th><strong>Gaussian Naive Bayes</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Model Type</strong></td> <td>Conditional/Discriminative</td> <td>Generative</td> </tr> <tr> <td><strong>Parametrization</strong></td> <td>\(p(y \mid x)\)</td> <td>\(p(x \mid y), p(y)\)</td> </tr> <tr> <td><strong>Assumptions on Y</strong></td> <td>Bernoulli</td> <td>Bernoulli</td> </tr> <tr> <td><strong>Assumptions on X</strong></td> <td>—</td> <td>Gaussian</td> </tr> <tr> <td><strong>Decision Boundary</strong></td> <td>\(\theta_{LR}^\top x\)</td> <td>\(\theta_{GNB}^\top x\)</td> </tr> </tbody> </table> <hr/> <ul> <li> <p><strong>Logistic Regression (LR)</strong> is a discriminative model that directly models the conditional probability \(p(y \mid x)\). It does not make assumptions about the distribution of features \(X\) but instead focuses on finding a decision boundary that separates the classes based on the observed data.</p> </li> <li> <p><strong>Gaussian Naive Bayes (GNB)</strong>, on the other hand, is a generative model that explicitly models the joint distribution \(p(x, y)\) by assuming that the features \(X\) are conditionally Gaussian given the class \(y\).</p> </li> </ul> <p>A few questions to address before we wrap up.</p> <p><strong>Question 1:</strong> Given the same training data, is \(\theta_{LR} = \theta_{GNB}\)?</p> <ul> <li>This is a critical question to explore the relationship between discriminative and generative models. While the forms of the decision boundary (e.g., linear) may look similar under certain assumptions (e.g., shared variance in GNB), the parameters \(\theta_{LR}\) and \(\theta_{GNB}\) are generally not the same due to differences in how the two models approach the learning process.</li> </ul> <p><strong>Question 2:</strong> Relationship Between LR and GNB</p> <ul> <li>Logistic regression and Gaussian naive Bayes <strong>converge to the same classifier asymptotically</strong>, assuming the GNB assumptions hold: <ol> <li>Data points are generated from Gaussian distributions for each class.</li> <li>Each dimension of the feature vector is generated independently.</li> <li>Both classes share the same variance for each feature (shared variance assumption).</li> </ol> </li> <li>Under these conditions, the decision boundary derived from GNB becomes identical to that of logistic regression as the amount of data increases.</li> </ul> <p><strong>Question 3:</strong> What Happens if the GNB Assumptions Are Not True?</p> <ul> <li>If the assumptions of GNB are violated (e.g., features are not Gaussian, dimensions are not independent, or variances are not shared), the decision boundary derived by GNB can deviate significantly from the optimal boundary. In such cases: <ul> <li><strong>Logistic Regression</strong> is likely to perform better, as it does not rely on specific assumptions about the feature distributions.</li> <li><strong>GNB</strong> may produce suboptimal results because its assumptions are hardcoded into the model and do not adapt to the true data distribution.</li> </ul> </li> </ul> <p>Thus, the choice between LR and GNB depends heavily on whether the data aligns with GNB’s assumptions.</p> <hr/> <h4 id="generative-vs-discriminative-models-trade-offs"><strong>Generative vs. Discriminative Models: Trade-offs</strong></h4> <p>The contrast between Naive Bayes and logistic regression highlights the differences between <strong>generative</strong> and <strong>discriminative</strong> models. Generative models like Naive Bayes model the joint distribution \(p(x, y)\), allowing them to generate data as well as make predictions. In contrast, discriminative models like logistic regression focus directly on \(p(y \mid x)\), optimizing for classification accuracy.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Generative_vs_Discriminative_models-480.webp 480w,/assets/img/Generative_vs_Discriminative_models-800.webp 800w,/assets/img/Generative_vs_Discriminative_models-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Generative_vs_Discriminative_models.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Generative_vs_Discriminative_models" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This tradeoff is explored in the classic paper by Ng, A. and Jordan, M. (2002), On discriminative versus generative classifiers: A comparison of logistic regression and naive Bayes., which shows that generative models converge faster but may have higher asymptotic error compared to their discriminative counterparts.</p> <hr/> <p>In the next section, we’ll explore the Multivariate Gaussian Distribution and the Gaussian Bayes Classifier in greater detail. Stay tuned👋!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.]]></summary></entry><entry><title type="html">An Introduction to Generative Models - Naive Bayes for Binary Features</title><link href="https://monishver11.github.io/blog/2025/generative-models/" rel="alternate" type="text/html" title="An Introduction to Generative Models - Naive Bayes for Binary Features"/><published>2025-01-20T20:29:00+00:00</published><updated>2025-01-20T20:29:00+00:00</updated><id>https://monishver11.github.io/blog/2025/generative-models</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/generative-models/"><![CDATA[<p>Generative models represent a powerful class of machine learning techniques. Unlike methods that directly map inputs \(x\) to outputs \(y\), such as generalized linear models or perceptrons, generative models take a broader approach. They aim to model the <strong>joint distribution</strong> \(p(x, y; \theta)\), which allows us to capture the underlying relationships between inputs and outputs in a holistic manner.</p> <h4 id="generalized-linear-models-vs-generative-models"><strong>Generalized Linear Models vs. Generative Models</strong></h4> <p>To recall, generalized linear models focus on the conditional distribution \(p(y \mid x; \theta)\). By contrast, generative models model \(p(x, y; \theta)\). Once we have the joint distribution, we can predict labels for new data by leveraging the following rule:</p> \[\hat{y} = \arg\max_{y \in \mathcal{Y}} p(x, y; \theta)\] <p>This prediction process connects naturally to conditional distributions. Using <strong>Bayes’ Rule</strong>, we can rewrite \(p(y \mid x)\) as:</p> \[p(y \mid x) = \frac{p(x \mid y) p(y)}{p(x)}\] <p>In practice, we often bypass computing \(p(x)\), as it is independent of \(y\). Instead, predictions simplify to:</p> \[\hat{y} = \arg\max_{y} p(y \mid x) = \arg\max_{y} p(x \mid y) p(y)\] <p>With this foundation, let us explore one of the most straightforward and widely used generative models: <strong>Naive Bayes (NB)</strong>.</p> <p>If you’re unable to follow the above formulation, here’s a quick refresher on Bayes’ Rule to help you out.</p> <p>Bayes’ Rule relates conditional probabilities to joint and marginal probabilities. It can be expressed as:</p> \[p(y \mid x) = \frac{p(x, y)}{p(x)} = \frac{p(x \mid y) p(y)}{p(x)},\] <p>where:</p> <ul> <li>\(p(y \mid x)\): Posterior probability of \(y\) given \(x\),</li> <li>\(p(x, y)\): Joint probability of \(x\) and \(y\),</li> <li>\(p(x \mid y)\): Likelihood of \(x\) given \(y\),</li> <li>\(p(y)\): Prior probability of \(y\),</li> <li>\(p(x)\): Marginal probability of \(x\), which ensures proper normalization.</li> </ul> <hr/> <h4 id="naive-bayes-a-simple-and-effective-generative-model"><strong>Naive Bayes: A Simple and Effective Generative Model</strong></h4> <p>To understand Naive Bayes, consider a simple yet practical problem: binary text classification. Imagine we want to classify a document as either a <strong>fake review</strong> or a <strong>genuine review</strong>. This setup offers a clear context to explore the mechanics of generative modeling.</p> <h5 id="representing-documents-as-features"><strong>Representing Documents as Features</strong></h5> <p>To make this task computationally feasible, we use a <strong>bag-of-words representation</strong>. A document is expressed as a binary vector \(x\), where:</p> \[x = [x_1, x_2, \dots, x_d].\] <p>Here, \(d\) represents the vocabulary size, and each \(x_i\) indicates whether the \(i\)-th word in the vocabulary exists in the document (\(x_i = 1\)) or not (\(x_i = 0\)).</p> <h5 id="modeling-the-joint-probability-of-documents-and-labels"><strong>Modeling the Joint Probability of Documents and Labels</strong></h5> <p>For a document \(x\) with label \(y\), the joint probability \(p(x, y)\) can be expressed using the <strong>chain rule of probability</strong>:</p> \[p(x \mid y) = p(x_1, x_2, \dots, x_d \mid y) = p(x_1 \mid y) p(x_2 \mid y, x_1) \cdots p(x_d \mid y, x_{d-1}, \dots, x_1).\] \[p(x \mid y) = \prod_{i=1}^d p(x_i \mid y, x_{&lt;i}),\] <p>However, modeling the dependencies between features (\(x_1, x_2, \dots, x_d\)) becomes intractable as the number of features grows and hard to estimate. This is where Naive Bayes introduces its defining assumption.</p> <h5 id="the-naive-bayes-assumption"><strong>The Naive Bayes Assumption</strong></h5> <p>Naive Bayes simplifies the problem by assuming that <strong>features are conditionally independent given the label \(y\)</strong>. Mathematically, this means:</p> \[p(x \mid y) = \prod_{i=1}^d p(x_i \mid y)\] <p>This assumption significantly reduces computational complexity while often delivering excellent results in practice. While the assumption of conditional independence may not hold in all cases, it is surprisingly effective in many real-world applications.</p> <hr/> <h4 id="parameterizing-the-naive-bayes-model"><strong>Parameterizing the Naive Bayes Model</strong></h4> <p>To make predictions, we need to parameterize the probabilities \(p(x_i \mid y)\) and \(p(y)\).</p> <p><strong><em>Why?</em></strong> Parameterizing these distributions allows us to learn the necessary values (e.g., \(\theta\)) from data in a structured way.</p> <h5 id="binary-features"><strong>Binary Features</strong></h5> <p>For simplicity, let us assume the features \(x_i\) are binary (\(x_i \in \{0, 1\}\)). We model \(p(x_i \mid y)\) as Bernoulli distributions:</p> \[p(x_i = 1 \mid y = 1) = \theta_{i,1}, \quad p(x_i = 1 \mid y = 0) = \theta_{i,0}\] <p>Similarly, the label distribution is modeled as:</p> \[p(y = 1) = \theta_0\] <p><strong><em>How do we arrive at these definitions?</em></strong> These definitions arise from the following assumptions and modeling principles:</p> <ol> <li><strong>Binary Nature of Features</strong>: Since the features \(x_i\) are binary (\(x_i \in \{0, 1\}\)), we need a probability distribution that models the likelihood of binary outcomes. The Bernoulli distribution is a natural choice for this.</li> <li><strong>Parameterization with Bernoulli Distributions</strong>: <ul> <li>For \(p(x_i \mid y)\), the Bernoulli distribution models the probability that \(x_i = 1\) for each possible value of \(y\).</li> <li>We introduce parameters \(\theta_{i,1}\) and \(\theta_{i,0}\), which represent the probability of \(x_i = 1\) given \(y = 1\) and \(y = 0\), respectively.</li> </ul> </li> <li><strong>Label Distribution \(p(y)\)</strong>: <ul> <li>The label \(y\) is also binary (\(y \in \{0, 1\}\)), so we model \(p(y)\) using a Bernoulli distribution with a single parameter \(\theta_0\), where \(\theta_0 = p(y = 1)\).</li> <li>This parameter reflects the prior probability of the positive class.</li> </ul> </li> <li><strong>Learning from Data</strong>: These parameters (\(\theta_{i,1}, \theta_{i,0}, \theta_0\)) are learned from data using methods like Maximum Likelihood Estimation (MLE), ensuring that the model reflects the observed distribution of features and labels in the dataset.</li> </ol> <p>Thus, the definitions provide a straightforward and interpretable way to model binary features and labels within the Naive Bayes framework.</p> <p>With these definitions, the joint probability \(p(x, y)\) can be written as (<strong>with NB assumption</strong>):</p> \[p(x, y) = p(y) \prod_{i=1}^d p(x_i \mid y)\] <p>Substituting the probabilities for binary features:</p> \[p(x, y) = p(y) \prod_{i=1}^d \theta_{i,y}{\mathbb{I}\{x_i = 1\}} + (1 - \theta_{i,y}){\mathbb{I}\{x_i = 0\}}\] <p>Here, \(\mathbb{I}\{\text{condition}\}\) is an indicator function that evaluates to 1 if the condition is true and 0 otherwise.</p> <p><strong><em>How to intuitively understand this equation?</em></strong> This equation represents the joint probability \(p(x, y)\) by combining the prior probability \(p(y)\) with the product of the individual probabilities \(p(x_i \mid y)\) for each feature \(x_i\). Here:</p> <ol> <li>For each feature \(x_i\), the term \(\mathbb{I}\{x_i = 1\}\) ensures that the corresponding parameter \(\theta_{i,y}\) is used if \(x_i = 1\), while \(\mathbb{I}\{x_i = 0\}\) ensures that \((1 - \theta_{i,y})\) is used if \(x_i = 0\).</li> <li>The product \(\prod_{i=1}^d\) combines the contributions of all features under the Naive Bayes assumption of conditional independence.</li> <li>Finally, multiplying by \(p(y)\) incorporates the prior belief about the label \(y\), providing the full joint distribution \(p(x, y)\).</li> </ol> <p>By this decomposition, we can efficiently compute \(p(x, y)\) for classification tasks.</p> <hr/> <h4 id="learning-parameters-with-maximum-likelihood-estimation-mle"><strong>Learning Parameters with Maximum Likelihood Estimation (MLE)</strong></h4> <p>The parameters \(\theta\) of the Naive Bayes model are learned by maximizing the likelihood of the observed data. Given a dataset of \(N\) labeled examples \(\{(x^{(n)}, y^{(n)})\}_{n=1}^N\), the likelihood of the data is:</p> \[\prod_{n=1}^N p_\theta(x^{(n)}, y^{(n)})\] <p>Taking the logarithm of the likelihood to simplify optimization, we obtain the log-likelihood:</p> \[\ell(\theta) = \sum_{n=1}^N \log p_\theta(x^{(n)}, y^{(n)})\] <p>For binary features, substituting the joint probability \(p_\theta(x, y)\) (as defined earlier) gives:</p> \[\ell(\theta) = \sum_{n=1}^N \left[ \sum_{i=1}^d \log ( \mathbb{I}\{x_i^{(n)} = 1\} \theta_{i,y^{(n)}} + \mathbb{I}\{x_i^{(n)} = 0\} (1 - \theta_{i,y^{(n)}}) ) \right] + \log p_\theta(y^{(n)})\] <p>Focusing on a specific feature \(x_j\) and label \(y = 1\), the relevant portion of the log-likelihood is:</p> \[\ell(\theta) = \sum_{n=1}^N \log \left[ \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\} \theta_{j,1} + \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\} (1 - \theta_{j,1}) \right] \tag{1}\] <p><strong>Step 1: Derivative of the Log-Likelihood</strong></p> <p>Taking the derivative of the log-likelihood with respect to \(\theta_{j,1}\):</p> \[\frac{\partial \ell}{\partial \theta_{j,1}} = \sum_{n=1}^N \left[ \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}}{\theta_{j,1}} - \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\}}{1 - \theta_{j,1}} \right] \tag{2}\] <p><strong>Did you follow the derivative?</strong> You might be wondering how derivative \(\log(a + b)\) can be written as \(\frac{1}{a} + \frac{1}{b}\), right? If so, that’s a completely valid question — I had the same thought myself. Here’s the explanation.</p> <p>The transition from equation (1) to equation (2) involves taking the derivative of the log-likelihood with respect to \(\theta_{j,1}\). Let’s break it down:</p> \[\frac{\partial}{\partial \theta_{j,1}} \ell = \frac{\partial}{\partial \theta_{j,1}} \sum_{n=1}^{N} \left[ \log \left( \theta_{j,1} I\{ x_j(n) = 1 \} + (1 - \theta_{j,1}) I\{ x_j(n) = 0 \} \right) \right]\] <p>Here, the derivative is applied to the logarithm term. Using the chain rule, we first compute the derivative of the logarithm, which is:</p> \[\frac{\partial}{\partial \theta_{j,1}} \log(f(\theta_{j,1})) = \frac{1}{f(\theta_{j,1})} \cdot \frac{\partial f(\theta_{j,1})}{\partial \theta_{j,1}},\] <p>where</p> \[f(\theta_{j,1}) = \theta_{j,1} I\{ x_j(n) = 1 \} + (1 - \theta_{j,1}) I\{ x_j(n) = 0 \}.\] <p>For a single \(n\), the term inside the logarithm is:</p> \[f(\theta_{j,1}) = \begin{cases} \theta_{j,1}, &amp; \text{if } x_j(n) = 1, \\ 1 - \theta_{j,1}, &amp; \text{if } x_j(n) = 0. \end{cases}\] <p>The derivative of \(f(\theta_{j,1})\) with respect to \(\theta_{j,1}\) is:</p> \[\frac{\partial f(\theta_{j,1})}{\partial \theta_{j,1}} = \begin{cases} 1, &amp; \text{if } x_j(n) = 1, \\ -1, &amp; \text{if } x_j(n) = 0. \end{cases}\] <p>Using the chain rule:</p> \[\frac{\partial}{\partial \theta_{j,1}} \log(f(\theta_{j,1})) = \begin{cases} \frac{1}{\theta_{j,1}}, &amp; \text{if } x_j(n) = 1, \\ \frac{-1}{1 - \theta_{j,1}}, &amp; \text{if } x_j(n) = 0. \end{cases}\] <p>Applying this to the summation over \(N\):</p> \[\frac{\partial}{\partial \theta_{j,1}} \ell = \sum_{n=1}^{N} \left[ I\{ y(n) = 1 \land x_j(n) = 1 \} \frac{1}{\theta_{j,1}} - I\{ y(n) = 1 \land x_j(n) = 0 \} \frac{1}{1 - \theta_{j,1}} \right].\] <p>This is exactly what equation (48) represents, showing the decomposition of the derivative into two terms for \(x_j(n) = 1\) and \(x_j(n) = 0\).</p> <p>The simplification uses the indicator functions \(I\) to select the appropriate cases, where:</p> \[I\{ x_j(n) = 1 \} \quad \text{contributes} \quad \frac{1}{\theta_{j,1}},\] <p>and</p> \[I\{ x_j(n) = 0 \} \quad \text{contributes} \quad \frac{1}{1 - \theta_{j,1}}.\] <p><strong>Key Insight:</strong></p> <p>At each step of the derivation, we are dealing with a <strong>single term</strong> inside the logarithm. As a result, when we take the derivative of the logarithm, the result is simply \(\frac{1}{\text{term}}\), where the term is either \(\theta_{j,1}\) or \(1 - \theta_{j,1}\), depending on the value of \(x_j(n)\).</p> <ul> <li>If \(x_j(n) = 1\), the term inside the log is \(\theta_{j,1}\), and its derivative is \(\frac{1}{\theta_{j,1}}\).</li> <li>If \(x_j(n) = 0\), the term inside the log is \(1 - \theta_{j,1}\), and its derivative is \(\frac{-1}{1 - \theta_{j,1}}\).</li> </ul> <p>Thus, at each \(n\), we compute the derivative as \(\frac{1}{\text{term}}\), with the specific term depending on the value of \(x_j(n)\). This makes the process more straightforward as we apply it term by term across all \(N\) data points.</p> <p>I hope that makes sense now. Let’s continue.</p> <p><strong>Step 2: Setting the Derivative to Zero</strong></p> <p>To find the maximum likelihood estimate, we set the derivative to zero:</p> \[\sum_{n=1}^N \left[ \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}}{\theta_{j,1}} \right] = \sum_{n=1}^N \left[ \frac{\mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\}}{1 - \theta_{j,1}} \right]\] <p>Simplifying:</p> \[\theta_{j,1} \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \} = \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}\] <p>The above simplification is quite straightforward. I encourage you to write it out for yourself and work through the steps. Simply <strong>multiply both sides</strong> by \(\theta_{j,1}(1 - \theta_{j,1})\) to eliminate the denominators, then expand both sides and <strong>isolate</strong> \(\theta_{j,1}\).</p> <p><strong>Note</strong>:</p> \[\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\} + \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 0\} = \sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \}\] <p><strong>Step 3: Solving for \(\theta_{j,1}\)</strong></p> <p>Rearranging to isolate \(\theta_{j,1}\):</p> \[\theta_{j,1} = \frac{\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1 \wedge x_j^{(n)} = 1\}}{\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1\}}\] <p><strong>Interpretation:</strong> This estimate corresponds to the fraction of examples with \(y = 1\) in which the \(j\)-th feature \(x_j\) is active (i.e., \(x_j = 1\)). Intuitively, it represents the conditional probability \(p(x_j = 1 \mid y = 1)\) under the Naive Bayes assumption.</p> <hr/> <h5 id="next-steps"><strong>Next Steps:</strong></h5> <ol> <li> <p><strong>Compute the other \(\theta_{i,y}\) values</strong>: You should calculate the parameters for all other features in the model (for example, \(\theta_{i,0}\) and \(\theta_{i,1}\) for binary features). These values represent the probability of a feature given a class, so you’ll continue by maximizing the likelihood for each \(i\) and class \(y\) to estimate these parameters.</p> </li> <li> <p><strong>Estimate \(p(y)\)</strong>: You’ll also need to compute the class prior probability \(p(y)\), which is simply the proportion of each class in the training data. This can be done by counting how many times each class label appears and normalizing by the total number of examples.</p> </li> </ol> \[\theta_0 = \frac{\sum_{n=1}^N \mathbb{I}\{y^{(n)} = 1\}}{N}\] <ul> <li>\(\theta_0\) is the proportion of samples in the dataset that belong to the class \(y = 1\).</li> <li>It serves as the prior probability of \(y = 1\).</li> </ul> <p><strong>Substituting the Probabilities for Binary Features:</strong></p> <p>The likelihood of the joint probability \(p(x, y)\) can be expressed as:</p> \[p(x, y) = p(y) \prod_{i=1}^d \theta_{i,y} {\mathbb{I}\{x_i = 1\}} + (1 - \theta_{i,y}) {\mathbb{I}\{x_i = 0\}}\] <p>Where \(\mathbb{I}\{x_i = 1\}\) is an indicator function that equals 1 when \(x_i = 1\), and \(\mathbb{I}\{x_i = 0\}\) equals 1 when \(x_i = 0\).</p> <p><strong>Remember this equation; it’s the one we started with.</strong></p> <p>Once all parameters are estimated, you will have a fully parameterized Naive Bayes model. The model can then be used for prediction by computing the posterior probabilities for each class \(y\) given an input \(x\). For prediction, you would use the formula:</p> \[\hat{y} = \arg\max_{y \in Y} p(y) \prod_{i=1}^d p(x_i \mid y)\] <p>Where \(p(x_i \mid y)\) are the feature likelihoods, and \(p(y)\) is the class prior. The class with the highest posterior probability is chosen as the predicted label. This approach allows Naive Bayes to make efficient, probabilistic predictions based on the learned parameters.</p> <p><strong>So, the fundamental idea is:</strong></p> <p>You are estimating the parameters \(\theta\) for all possible features and classes. Once the parameters are learned, you apply Bayes’ rule to compute the posterior probability for each class \(y\). Finally, you take the class that maximizes the posterior probability using:</p> \[\hat{y} = \arg\max_y p(y \mid x)\] <p>This gives you the predicted class \(\hat{y}\), based on the learned parameters from the training data.</p> <hr/> <h5 id="recipe-for-learning-a-naive-bayes-model"><strong>Recipe for Learning a Naive Bayes Model:</strong></h5> <ol> <li><strong>Choose \(p(x_i \mid y)\)</strong>: Select an appropriate distribution for the features, e.g., Bernoulli distribution for binary features \(x_i\).</li> <li><strong>Choose \(p(y)\)</strong>: Typically, use a categorical distribution for the class labels.</li> <li><strong>Estimate Parameters by MLE</strong>: Use Maximum Likelihood Estimation (MLE) to estimate the parameters, following the same strategy used in conditional models.</li> </ol> <h5 id="where-do-we-go-from-here"><strong>Where Do We Go From Here?</strong></h5> <p>So far, we have focused on modeling binary features. However, many real-world datasets involve continuous features. How can Naive Bayes be extended to handle such cases? In the next blog, we’ll explore Naive Bayes for continuous features and see how this simple model adapts to more complex data types. See you there!</p> ]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.]]></summary></entry><entry><title type="html">Generalized Linear Models Explained - Leveraging MLE for Regression and Classification</title><link href="https://monishver11.github.io/blog/2025/MLE/" rel="alternate" type="text/html" title="Generalized Linear Models Explained - Leveraging MLE for Regression and Classification"/><published>2025-01-18T15:45:00+00:00</published><updated>2025-01-18T15:45:00+00:00</updated><id>https://monishver11.github.io/blog/2025/MLE</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/MLE/"><![CDATA[<p>When building machine learning models, one of the most important tasks is estimating the parameters of a model in a way that best explains the observed data. This is where the principle of <strong>Maximum Likelihood Estimation (MLE)</strong> comes into play. MLE provides a rigorous framework for parameter estimation, grounded in probability theory, and is widely used across regression, classification, and beyond.</p> <p>Suppose we have a probabilistic model and a dataset \(D\). The central question is: how do we estimate the parameters \(\theta\) of the model? According to MLE, we should choose \(\theta\) to maximize the likelihood of the observed data. Formally, the likelihood function is defined as:</p> \[L(\theta) \stackrel{\text{def}}{=} p(D; \theta),\] <p>which captures how likely the dataset \(D\) is, given the model parameters \(\theta\).</p> <p>If the dataset consists of \(N\) independent and identically distributed (iid) examples, the likelihood simplifies to a product of individual data likelihoods:</p> \[L(\theta) = \prod_{n=1}^{N} p\left(y^{(n)} \mid x^{(n)}; \theta\right)\] <p>While this expression is mathematically correct, the product of many probabilities can be unwieldy. To simplify the computation, we typically work with the <strong>log-likelihood</strong>, \(\ell(\theta)\), which is simply the natural logarithm of the likelihood:</p> \[\ell(\theta) \stackrel{\text{def}}{=} \log L(\theta) = \sum_{n=1}^{N} \log p\left(y^{(n)} \mid x^{(n)}; \theta\right)\] <p>Maximizing \(\ell(\theta)\) is equivalent to maximizing \(L(\theta)\), as the logarithm is a monotonic function. Alternatively, minimizing the <strong>negative log-likelihood (NLL)</strong> is a common approach, as it frames the problem as a minimization task.</p> <hr/> <h4 id="mle-for-linear-regression"><strong>MLE for Linear Regression</strong></h4> <p>To make these concepts more concrete, let’s see how MLE applies to a simple and widely known model: linear regression.</p> <p>In linear regression, we assume the output \(Y\) is conditionally Gaussian given the input \(X\). Specifically,</p> \[Y \mid X = x \sim \mathcal{N}(\theta^\top x, \sigma^2),\] <p>where \(\theta^\top x\) is the mean and \(\sigma^2\) is the variance of the Gaussian distribution.</p> <p>The log-likelihood for this model can be written as:</p> \[\ell(\theta) \stackrel{\text{def}}{=} \log L(\theta) = \log \prod_{n=1}^{N} p\left(y^{(n)} \mid x^{(n)}; \theta\right) = \sum_{n=1}^{N} \log p\left(y^{(n)} \mid x^{(n)};\theta\right)\] <p>Substituting the Gaussian probability density function into the equation, we get:</p> \[\ell(\theta) = \sum_{n=1}^{N} \log \left( \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\frac{\left(y^{(n)} - \theta^\top x^{(n)}\right)^2}{2\sigma^2} \right) \right)\] <p>Simplifying further, the log-likelihood becomes:</p> \[\ell(\theta) = N \log \frac{1}{\sqrt{2\pi \sigma^2}} - \frac{1}{2\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right)^2\] <p>Notice that the first term, \(N \log \frac{1}{\sqrt{2\pi \sigma^2}}\), is independent of \(\theta\). This means that to maximize \(\ell(\theta)\), we only need to minimize the second term, which is proportional to the sum of squared residuals.</p> <p>This brings us to an important insight: <strong>maximizing the log-likelihood in linear regression is equivalent to minimizing the squared error.</strong></p> <h5 id="deriving-the-gradient-of-the-log-likelihood"><strong>Deriving the Gradient of the Log-Likelihood</strong></h5> <p>To find the parameters that maximize the log-likelihood, we compute its gradient with respect to \(\theta\) and set it to zero. From our earlier expression for \(\ell(\theta)\):</p> \[\ell(\theta) = N \log \frac{1}{\sqrt{2\pi \sigma^2}} - \frac{1}{2\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right)^2,\] <p>the gradient with respect to \(\theta_i\), the \(i\)-th parameter, is:</p> \[\frac{\partial \ell}{\partial \theta_i} = -\frac{1}{\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right) x_i^{(n)}\] <p>Setting \(\frac{\partial \ell}{\partial \theta_i} = 0\) gives us the familiar <strong>normal equations</strong> for linear regression, which are typically solved to find the optimal \(\theta\).</p> <p>This yields:</p> \[-\frac{1}{\sigma^2} \sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right) x_i^{(n)} = 0\] <p>Multiplying both sides by \(\sigma^2\) gives:</p> \[\sum_{n=1}^{N} \left(y^{(n)} - \theta^\top x^{(n)}\right) x_i^{(n)} = 0\] <p>We can write the equation in matrix form as:</p> \[\mathbf{X}^\top \left( \mathbf{y} - \mathbf{X} \theta \right) = 0\] <p>Rewriting the equation:</p> \[\mathbf{X}^\top \mathbf{y} - \mathbf{X}^\top \mathbf{X} \theta = 0\] <p>Rearranging gives the normal equation:</p> \[\mathbf{X}^\top \mathbf{X} \theta = \mathbf{X}^\top \mathbf{y}\] <p>Solving for \(\theta\):</p> \[\theta = \left( \mathbf{X}^\top \mathbf{X} \right)^{-1} \mathbf{X}^\top \mathbf{y}\] <p>Through this derivation, we’ve established a key connection between the probabilistic interpretation of linear regression and the classical squared error minimization. The principle of MLE not only provides a mathematically grounded way to estimate parameters but also reveals the assumptions underlying different models.</p> <p>What’s fascinating is that this approach generalizes beyond regression. For instance, in classification tasks, MLE leads to the cross-entropy loss. This will be the focus of the next section, where we’ll explore how MLE ties into classification problems and the role of log-loss in optimizing model parameters.</p> <hr/> <h4 id="from-linear-to-logistic-regression-expanding-the-scope-of-mle"><strong>From Linear to Logistic Regression: Expanding the Scope of MLE</strong></h4> <p>In the previous section, we explored how the Maximum Likelihood Estimation (MLE) principle naturally connects with linear regression. We saw that linear regression assumes the target \(Y \vert X = x\) follows a Gaussian distribution, and maximizing the likelihood aligns with minimizing the squared loss.</p> <p>But is the Gaussian assumption always valid? Not necessarily. For example, in classification tasks where \(Y\) takes on discrete values (e.g., 0 or 1), assuming a Gaussian distribution is inappropriate. This raises an important question: <strong>can we use the same MLE-based modeling approach for tasks beyond regression?</strong></p> <p>The answer is yes, and this brings us to <strong>logistic regression</strong>, which is tailored for classification tasks.</p> <h4 id="logistic-regression-assumptions-and-foundations"><strong>Logistic Regression: Assumptions and Foundations</strong></h4> <p>Consider a binary classification problem where the target \(Y \in \{0, 1\}\). What should the conditional distribution of \(Y\) given \(X = x\) look like? For logistic regression, we model \(p(y \mid x)\) using a <strong>Bernoulli distribution</strong>:</p> \[p(y \mid x) = h(x)^y (1 - h(x))^{1-y},\] <p>where \(h(x) \in (0, 1)\) represents the probability \(p(y = 1 \mid x)\).</p> <h5 id="parameterizing-hx"><strong>Parameterizing \(h(x)\):</strong></h5> <p>In linear regression, the mean \(\mathbb{E}[Y \mid X = x]\) was parameterized as \(\theta^\top x\). However, for classification, \(h(x)\) must map the linear predictor \(\theta^\top x\) (which lies in \(\mathbb{R}\)) to the interval \((0, 1)\). To achieve this, we use the <strong>logistic function</strong>:</p> \[f(\eta) = \frac{1}{1 + e^{-\eta}}, \quad \text{where } \eta = \theta^\top x\] <p>Thus, the probability \(p(y \mid x)\) becomes:</p> \[p(y \mid x) = \text{Bernoulli}(f(\theta^\top x)),\] <p>or equivalently:</p> \[p(y = 1 \mid x) = f(\theta^\top x), \quad p(y = 0 \mid x) = 1 - f(\theta^\top x)\] <p>[any reason why bernoulli?]</p> <p><strong>Why do we use the Bernoulli distribution in logistic regression?</strong></p> <p>In logistic regression, the target variable \(Y\) is binary, taking values in \(\{0, 1\}\), making the <strong>Bernoulli distribution</strong> a natural choice. The Bernoulli distribution models the probability of success (1) or failure (0) in a single trial.</p> <p>We model the conditional probability \(p(y \mid x)\) using the logistic function, which ensures that the predicted probabilities lie in the interval \((0, 1)\):</p> \[p(y = 1 \mid x) = f(\theta^\top x), \quad p(y = 0 \mid x) = 1 - f(\theta^\top x),\] <p>where \(f(\eta) = \frac{1}{1 + e^{-\eta}}\) is the logistic function. The Bernoulli distribution is then used to model the binary outcomes given these probabilities.</p> <h5 id="interpreting-the-logistic-function"><strong>Interpreting the Logistic Function</strong></h5> <p>The logistic function is smooth and monotonically increasing, mapping any real-valued input to a value in the range \((0, 1)\). It has the characteristic “S-shape” and is particularly useful for modeling probabilities.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Logistic_Fn-480.webp 480w,/assets/img/Logistic_Fn-800.webp 800w,/assets/img/Logistic_Fn-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Logistic_Fn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Logistic_Fn" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>One interesting property of the logistic function is its connection to the <strong>log-odds</strong>. For logistic regression:</p> \[\log \frac{p(y = 1 \mid x)}{p(y = 0 \mid x)} = \theta^\top x\] <p>This shows that the log-odds (or logit) of \(y = 1\) depend linearly on the input \(x\). Moreover, the decision boundary, where \(p(y = 1 \mid x) = p(y = 0 \mid x) = 0.5\), is defined by \(\theta^\top x = 0\), making it a <strong>linear decision boundary</strong>.</p> <h5 id="mle-for-logistic-regression"><strong>MLE for Logistic Regression</strong></h5> <p>As with linear regression, the parameters \(\theta\) in logistic regression are estimated by maximizing the conditional log-likelihood:</p> \[\ell(\theta) = \sum_{n=1}^{N} \log p\left(y^{(n)} \mid x^{(n)}; \theta\right)\] <p>For a Bernoulli-distributed \(y\), substituting \(p(y \mid x)\):</p> \[\ell(\theta) = \sum_{n=1}^{N} \left[ y^{(n)} \log f(\theta^\top x^{(n)}) + (1 - y^{(n)}) \log (1 - f(\theta^\top x^{(n)})) \right]\] <p>Unlike linear regression, this log-likelihood does not have a closed-form solution for \(\theta\). However, it is <strong>concave</strong>, meaning that optimization techniques like gradient ascent can efficiently find the unique optimal solution.</p> <h5 id="gradient-ascent-for-logistic-regression"><strong>Gradient Ascent for Logistic Regression</strong></h5> <p>The gradient of the log-likelihood \(\ell(\theta)\) with respect to \(\theta_i\) (the \(i\)-th parameter) is given by:</p> \[\frac{\partial \ell}{\partial \theta_i} = \sum_{n=1}^{N} \left( y^{(n)} - f(\theta^\top x^{(n)}) \right) x_i^{(n)}\] <p><strong>Derivation of the above form:</strong></p> <p>Math Review: Chain Rule</p> <p>If \(z\) depends on \(y\), which itself depends on \(x\), e.g., \(z = (y(x))^2\), then:</p> \[\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}\] <p>Likelihood for a Single Example:</p> \[\ell^n = y^{(n)} \log f(\theta^\top x^{(n)}) + (1 - y^{(n)}) \log(1 - f(\theta^\top x^{(n)}))\] <p>The gradient with respect to \(\theta_i\) is:</p> \[\frac{\partial \ell^n}{\partial \theta_i} = \frac{\partial \ell^n}{\partial f^n} \frac{\partial f^n}{\partial \theta_i}\] <p>Using the chain rule:</p> \[= \left(\frac{y^{(n)}}{f^n} - \frac{1 - y^{(n)}}{1 - f^n}\right) \frac{\partial f^n}{\partial \theta_i}\] <p>Simplify:</p> \[= \left(\frac{y^{(n)}}{f^n} - \frac{1 - y^{(n)}}{1 - f^n}\right) \left(f^n (1 - f^n) x_i^{(n)}\right)\] \[= \left(y^{(n)} - f^n\right) x_i^{(n)}.\] <p>The full gradient is thus:</p> \[\frac{\partial \ell}{\partial \theta_i} = \sum_{n=1}^{N} \left(y^{(n)} - f(\theta^\top x^{(n)})\right) x_i^{(n)}.\] <p>This gradient looks strikingly similar to that of linear regression, except for the presence of the logistic function \(f(\cdot)\).</p> <p>Using this gradient, we iteratively update the parameters using gradient ascent:</p> \[\theta \leftarrow \theta + \alpha \nabla_\theta \ell(\theta),\] <p>where \(\alpha\) is the learning rate.</p> <p><strong>Note the distinction:</strong> since the function is concave, we apply gradient ascent rather than descent.</p> <h5 id="a-comparison-linear-vs-logistic-regression"><strong>A Comparison: Linear vs Logistic Regression</strong></h5> <p>Here’s a side-by-side comparison to highlight the similarities and differences:</p> <hr/> <table> <thead> <tr> <th>Feature</th> <th>Linear Regression</th> <th>Logistic Regression</th> </tr> </thead> <tbody> <tr> <td>Input combination</td> <td>\(\theta^\top x\) (linear)</td> <td>\(\theta^\top x\) (linear)</td> </tr> <tr> <td>Output</td> <td>Real-valued</td> <td>Categorical (0 or 1)</td> </tr> <tr> <td>Conditional distribution</td> <td>Gaussian</td> <td>Bernoulli</td> </tr> <tr> <td>Transfer function \(f(\theta^\top x)\)</td> <td>Identity</td> <td>Logistic</td> </tr> <tr> <td>Mean \(\mathbb{E}[Y \mid X = x; \theta]\)</td> <td>\(f(\theta^\top x)\)</td> <td>\(f(\theta^\top x)\)</td> </tr> </tbody> </table> <hr/> <p>The main difference lies in the conditional distribution of \(Y\) and the transfer function \(f(\cdot)\), which maps \(\theta^\top x\) to the appropriate range for each model.</p> <h4 id="generalizing-logistic-regression"><strong>Generalizing Logistic Regression</strong></h4> <p>The principles behind logistic regression can be extended to handle other types of outputs, such as counts or probabilities for multiple classes. This generalization leads to the broader family of <strong>generalized linear models (GLMs)</strong>.</p> <h5 id="steps-for-generalized-regression-models"><strong>Steps for Generalized Regression Models</strong></h5> <ol> <li><strong>Task</strong>: Given \(x\), predict \(p(y \mid x)\).</li> <li><strong>Modeling</strong>: <ul> <li>Choose a parametric family of distributions \(p(y; \theta)\) with parameters \(\theta \in \Theta\).</li> <li>Choose a transfer function that maps a linear predictor in \(\mathbb{R}\) to \(\Theta\):</li> </ul> \[x \in \mathbb{R}^d \mapsto w^\top x \in \mathbb{R} \mapsto f(w^\top x) = \theta \in \Theta\] </li> <li> <p><strong>Learning</strong>: Use MLE to estimate the parameters:</p> \[\hat{\theta} = \arg\max_\theta \log p(D; \hat{\theta})\] </li> <li> <p><strong>Inference</strong>: For prediction, map \(x\) through the learned transfer function:</p> \[x \mapsto f(w^\top x)\] </li> </ol> <p>In the next section, we’ll dive deeper into these generalized models, exploring their flexibility and application to diverse prediction tasks.</p> <hr/> <h4 id="extending-generalized-linear-models-from-poisson-to-multinomial-logistic-regression"><strong>Extending Generalized Linear Models: From Poisson to Multinomial Logistic Regression</strong></h4> <p>In our journey through generalized linear models (GLMs), we’ve seen how logistic regression extends MLE principles to classification tasks. Now, let’s explore other use cases where GLMs shine, including <strong>Poisson regression</strong> for count-based predictions and <strong>multinomial logistic regression</strong> for multiclass classification.</p> <h4 id="example-poisson-regression"><strong>Example: Poisson Regression</strong></h4> <p>Imagine we want to predict the number of people entering a New York restaurant during lunchtime. What features could help? Time of day, day of the week, weather conditions, or nearby events might all be relevant. Importantly, the target variable \(Y\), representing the number of visitors, is a non-negative integer: \(Y \in \{0, 1, 2, \dots\}\).</p> <h5 id="why-use-the-poisson-distribution"><strong>Why Use the Poisson Distribution?</strong></h5> <p>The Poisson distribution is a natural choice for modeling count data. A random variable \(Y \sim \text{Poisson}(\lambda)\) has the probability mass function:</p> \[p(Y = k; \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k \in \{0, 1, 2, \dots\},\] <p>where \(\lambda &gt; 0\) is the rate parameter. The expected value \(\mathbb{E}[Y] = \lambda\), making \(\lambda\) both the mean and variance of \(Y\).</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Poisson_pmf.svg-480.webp 480w,/assets/img/Poisson_pmf.svg-800.webp 800w,/assets/img/Poisson_pmf.svg-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Poisson_pmf.svg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Poisson_pmf" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h5 id="what-does-the-poisson-distribution-mean-intuitively"><strong>What Does the Poisson Distribution Mean, Intuitively?</strong></h5> <p>The Poisson distribution can be understood through a simple analogy: imagine standing at a bus stop.</p> <ol> <li><strong>The Events</strong>: Each bus that arrives at the stop is an “event.”</li> <li><strong>Constant Rate</strong>: On average, buses arrive every 10 minutes, meaning we expect about 6 buses per hour. This average rate, \(\lambda = 6\), is constant.</li> <li><strong>Independence</strong>: The arrival of one bus doesn’t affect when the next one will come (events are independent).</li> </ol> <p>Now, if you wait at the bus stop for an hour, the Poisson distribution models the probability of seeing exactly 5, 6, or 7 buses in that time. While the average is 6 buses, randomness may cause the actual count to vary, with probabilities decreasing for more extreme deviations (e.g., 0 buses or 12 buses in an hour are unlikely).</p> <p>This shows how the Poisson distribution captures both the expected rate (\(\lambda\)) and the variability in the number of events.</p> <h5 id="constructing-the-poisson-regression-model"><strong>Constructing the Poisson Regression Model</strong></h5> <p>We assume \(Y \mid X = x \sim \text{Poisson}(\lambda)\). The challenge is to ensure \(\lambda\), the rate parameter, is positive. This is achieved using a transfer function \(f\):</p> \[x \mapsto w^\top x \quad \text{(linear predictor in } \mathbb{R} \text{)} \mapsto \lambda = f(w^\top x) \quad \text{(rate parameter in } (0, \infty) \text{)}.\] <p>The standard transfer function is the exponential function:</p> \[f(w^\top x) = e^{w^\top x}\] <h5 id="log-likelihood-for-poisson-regression"><strong>Log-Likelihood for Poisson Regression</strong></h5> <p>Given a dataset \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\), the log-likelihood is:</p> \[\log p(y_i; \lambda_i) = \left[y_i \log \lambda_i - \lambda_i - \log(y_i!)\right]\] \[\log p(D; w) = \sum_{i=1}^{n} \left[ y_i \log f(w^\top x_i) - f(w^\top x_i) - \log(y_i!) \right],\] <p>where \(f(w^\top x_i) = e^{w^\top x_i}\). Substituting \(\lambda_i\), we get:</p> \[\log p(D; w) = \sum_{i=1}^{n} \left[ y_i (w^\top x_i) - e^{w^\top x_i} - \log(y_i!) \right]\] <p>As with logistic regression, the likelihood is concave, so gradient-based methods can efficiently optimize \(w\).</p> <h5 id="gradient-for-poisson-regression"><strong>Gradient for Poisson Regression</strong></h5> <p>To optimize the log-likelihood, we compute its gradient with respect to the weight vector \(w\).</p> <p>The gradient is:</p> \[\frac{\partial \log p(D; w)}{\partial w} = \sum_{i=1}^{n} \left[ y_i x_i - e^{w^\top x_i} x_i \right]\] <p>Factoring out common terms, we get:</p> \[\frac{\partial \log p(D; w)}{\partial w} = \sum_{i=1}^{n} x_i \left[ y_i - e^{w^\top x_i} \right]\] <p>The gradient indicates the update direction for \(w\), with each term capturing the difference between the observed count \(y_i\) and the predicted count \(e^{w^\top x_i}\), weighted by the feature vector \(x_i\). Gradient ascent can then be used to maximize the log-likelihood, as the likelihood is concave for Poisson regression. Again, notice how similar the gradient is to the other problems we’ve explored so far—the transfer function differs in each case.</p> <hr/> <h4 id="example-multinomial-logistic-regression"><strong>Example: Multinomial Logistic Regression</strong></h4> <p>Next, let’s tackle multiclass classification, where the target \(Y \in \{1, 2, \dots, k\}\) spans multiple categories. Logistic regression’s Bernoulli distribution extends to the <strong>categorical distribution</strong>, which is parameterized by a probability vector \(\theta = (\theta_1, \dots, \theta_k)\). For valid probabilities:</p> \[\sum_{i=1}^{k} \theta_i = 1, \quad \theta_i \geq 0 \text{ for all } i.\] <p>For a given \(y \in \{1, \dots, k\}\):</p> \[p(y) = \theta_y.\] <h5 id="what-does-the-categorical-distribution-mean-intuitively"><strong>What Does the Categorical Distribution Mean, Intuitively?</strong></h5> <p>The categorical distribution assigns a probability to each class. For a given input \(x\), we want to predict the probability of each class \(y\) belonging to the target set. The probability of the class \(y\) is \(\theta_y\), where \(\theta_y\) is the component of the probability vector corresponding to the class \(y\). This allows us to perform multiclass classification by selecting the class with the highest probability.</p> \[p(y) = \theta_y.\] <h5 id="constructing-the-multinomial-logistic-regression-model"><strong>Constructing the Multinomial Logistic Regression Model</strong></h5> <p>The key idea in multinomial logistic regression is to compute a linear score for each class. For a given input vector \(x\), we compute a vector of scores \(s \in \mathbb{R}^k\) for all classes:</p> \[s = (w_1^\top x, \dots, w_k^\top x),\] <p>where \(w_i\) represents the weight vector associated with class \(i\). These scores are then transformed using the <strong>softmax function</strong> to produce valid probabilities. The softmax function is defined as:</p> \[\text{softmax}(s)_i = \frac{e^{s_i}}{\sum_{j=1}^{k} e^{s_j}} \quad \text{for } i = 1, \dots, k.\] <p>The softmax function ensures that the resulting probabilities form a valid probability distribution, satisfying:</p> \[\sum_{i=1}^{k} \theta_i = 1, \quad \theta_i \geq 0 \text{ for all } i.\] <h5 id="log-likelihood-for-multinomial-logistic-regression"><strong>Log-Likelihood for Multinomial Logistic Regression</strong></h5> <p>Given a dataset \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\), the log-likelihood of the model is the sum of the log probabilities for the true classes. The log-likelihood is given by:</p> \[\log p(D; W) = \sum_{i=1}^{n} \log \theta_{y_i},\] <p>where \(\theta_{y_i} = \text{softmax}(W^\top x_i)_{y_i}\). The parameters \(W\) are learned by maximizing the log-likelihood using gradient-based optimization methods.</p> <p><strong>Note:</strong> Don’t be misled by the use of theta and softmax notations. The way this is mentioned can be confusing. For now, let’s proceed with caution.</p> <h5 id="gradient-for-multinomial-logistic-regression"><strong>Gradient for Multinomial Logistic Regression</strong></h5> <p>To optimize the parameters \(W\), we compute the gradient of the log-likelihood with respect to \(W\).</p> <h6 id="step-by-step-derivation"><strong>Step-by-Step Derivation</strong></h6> <ol> <li> <p><strong>Log-Likelihood for Multinomial Logistic Regression:</strong></p> <p>The log-likelihood for the dataset \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\) is:</p> \[\log p(D; W) = \sum_{i=1}^{n} \log \theta_{y_i},\] <p>where \(\theta_{y_i} = \text{softmax}(W^\top x_i)_{y_i}\) is the predicted probability of the true class \(y_i\) for the \(i\)-th data point.</p> </li> <li> <p><strong>Softmax Function:</strong></p> <p>The softmax function for class \(j\) is given by:</p> \[\theta_j(x_i; W) = \frac{e^{w_j^\top x_i}}{\sum_{k=1}^{k} e^{w_k^\top x_i}}.\] </li> <li> <p><strong>Log-Likelihood Expansion:</strong></p> <p>Substituting the softmax expression into the log-likelihood:</p> \[\log p(D; W) = \sum_{i=1}^{n} \log \left( \frac{e^{w_{y_i}^\top x_i}}{\sum_{k=1}^{k} e^{w_k^\top x_i}} \right),\] <p>which simplifies to:</p> \[\log p(D; W) = \sum_{i=1}^{n} \left( w_{y_i}^\top x_i - \log \left( \sum_{k=1}^{k} e^{w_k^\top x_i} \right) \right).\] </li> <li> <p><strong>Gradient of the Log-Likelihood:</strong></p> <p>The gradient of the log-likelihood with respect to the weight vector \(w_j\) is computed by differentiating each term in the log-likelihood expression:</p> <ul> <li> <p>The derivative of the first term, \(w_{y_i}^\top x_i\), with respect to \(w_j\) is simply \(x_i\) when \(y_i = j\) and 0 otherwise.</p> </li> <li> <p>The second term involves the <strong>log-sum-exp</strong>, and its derivative with respect to \(w_j\) is:</p> \[\nabla_{w_j} \log \left( \sum_{k=1}^{k} e^{w_k^\top x_i} \right) = \theta_j(x_i; W) \cdot x_i.\] </li> </ul> </li> <li> <p><strong>Final Gradient Expression:</strong></p> <p>Combining the two terms, the gradient of the log-likelihood with respect to \(w_j\) is:</p> \[\nabla_{w_j} \log p(D; W) = \sum_{i=1}^{n} \left( \mathbf{1}_{\{y_i = j\}} - \theta_j(x_i; W) \right) x_i,\] <p>where \(\mathbf{1}_{\{y_i = j\}}\) is the indicator function that is 1 if the true class \(y_i\) equals \(j\), and 0 otherwise.</p> </li> </ol> <p>This gradient is used to adjust the weight vectors during training to improve the model’s predictions.</p> <hr/> <h4 id="review-recipe-for-conditional-models"><strong>Review: Recipe for Conditional Models</strong></h4> <p>GLMs provide a unified approach for constructing conditional models. Here’s a quick recipe:</p> <ol> <li><strong>Define the Input and Output Space</strong>: Start with the features \(x\) and target \(y\).</li> <li><strong>Choose an Output Distribution</strong>: Based on the nature of \(y\), select an appropriate probability distribution \(p(y \mid x; \theta)\).</li> <li><strong>Select a Transfer Function</strong>: Map the linear predictor \(w^\top x\) into the required range of the distribution parameters.</li> <li><strong>Optimize via MLE</strong>: Estimate \(\theta\) by maximizing the log-likelihood using gradient-based methods.</li> </ol> <p>This framework, called <strong>generalized linear models</strong>, can be adapted for a wide range of prediction tasks.</p> <hr/> <p>Before we wrap up, did we overlook something? Ah, yes—let’s revisit the log-sum-exp function and its significance.</p> <h4 id="log-sum-exp-function"><strong>Log-Sum-Exp Function</strong></h4> <p>The <strong>log-sum-exp</strong> (LSE) function is a mathematical expression used frequently in machine learning and statistics, particularly in contexts involving probabilities and normalization. It is defined as:</p> \[\text{LSE}(s) = \log \left( \sum_{k=1}^K e^{s_k} \right),\] <p>where \(s = (s_1, s_2, \dots, s_K)\) is a vector of real-valued scores.</p> <h5 id="key-properties-of-log-sum-exp"><strong>Key Properties of Log-Sum-Exp</strong></h5> <ol> <li> <p><strong>Smooth Maximum Approximation</strong><br/> The log-sum-exp function can be thought of as a “soft maximum” of the elements in \(s\) because, for large values of \(s_k\), the largest term dominates the sum:</p> \[\text{LSE}(s) \approx \max(s_k)\] </li> <li> <p><strong>Numerical Stability</strong><br/> To ensure numerical stability when computing \(e^{s_k}\) for large values of \(s_k\), the log-sum-exp function is often rewritten as:</p> \[\text{LSE}(s) = \log \left( \sum_{k=1}^K e^{s_k - \max(s)} \right) + \max(s),\] <p>where \(\max(s)\) is the maximum value in \(s\). This adjustment ensures that the exponentials remain within a manageable range. Check why it’s written this way in the referenced resource; I highly recommend it.</p> </li> </ol> <h5 id="log-sum-exp-in-multinomial-logistic-regression"><strong>Log-Sum-Exp in Multinomial Logistic Regression</strong></h5> <p>In multinomial logistic regression, the log-sum-exp term naturally arises when computing the log-likelihood. The predicted probabilities are computed using the softmax function:</p> \[\theta_j(x_i; W) = \frac{e^{w_j^\top x_i}}{\sum_{k=1}^{K} e^{w_k^\top x_i}}\] <p>The denominator of the softmax involves a sum of exponentials. Taking the logarithm of the denominator gives the log-sum-exp term:</p> \[\log \left( \sum_{k=1}^K e^{w_k^\top x_i} \right)\] <p>This term normalizes the probabilities so that they sum to 1 across all classes.</p> <h5 id="derivative-of-log-sum-exp"><strong>Derivative of Log-Sum-Exp</strong></h5> <p>The derivative of the log-sum-exp function with respect to a specific score \(s_j\) is:</p> \[\frac{\partial}{\partial s_j} \log \left( \sum_{k=1}^K e^{s_k} \right) = \frac{e^{s_j}}{\sum_{k=1}^K e^{s_k}}\] <p>This is equivalent to the probability assigned to class \(j\) by the softmax function:</p> \[\frac{\partial}{\partial s_j} \log \left( \sum_{k=1}^K e^{s_k} \right) = \text{softmax}(s)_j.\] <p>In the case of multinomial logistic regression, where \(s_j = w_j^\top x_i\), the derivative with respect to \(w_j\) is:</p> \[\nabla_{w_j} \log \left( \sum_{k=1}^K e^{w_k^\top x_i} \right) = \theta_j(x_i; W) \cdot x_i,\] <p>where \(\theta_j(x_i; W) = \text{softmax}(W^\top x_i)_j\) is the predicted probability for class \(j\).</p> <h5 id="still-why-the-use-of-log-sum-exp"><strong>Still, Why the Use of Log-Sum-Exp</strong></h5> <p>In multinomial logistic regression, we use the <strong>softmax function</strong> to convert the raw class scores (logits) into probabilities. The softmax function itself involves summing the exponentials of the scores, not the log-sum-exp.</p> <p>However, when calculating the <strong>log-likelihood</strong> of the model during optimization, we encounter the <strong>log-sum-exp</strong> function. The log-sum-exp is used in the log-likelihood to handle the sum of the exponentials in a numerically stable way. It’s primarily used to:</p> <ol> <li><strong>Avoid overflow and underflow</strong>: When working with large or small exponentiated values (as in the softmax function), exponentiation can cause numerical instability. The log-sum-exp helps stabilize these computations.</li> <li><strong>Ensure proper normalization</strong>: In the context of the softmax function, it ensures the sum of the probabilities is 1, making them valid probabilities for classification.</li> </ol> <p>In summary, while softmax uses the sum of exponentials, the <strong>log-sum-exp</strong> appears in the log-likelihood computation to stabilize the logarithmic transformation, enabling proper optimization.</p> <h5 id="analogy-for-the-log-sum-exp-function"><strong>Analogy for the Log-Sum-Exp Function</strong></h5> <p>Imagine you’re at a sports competition with several players, and their scores are exponentially amplified (think of \(e^{s_k}\) as the “hype” around each player’s performance). The log-sum-exp function acts like a judge summarizing all the scores into a single value that reflects the overall competition, but with a bias toward the top performers.</p> <ul> <li>The <strong>“log”</strong> compresses the scale, keeping the summary manageable.</li> <li>The <strong>“sum”</strong> captures the contributions of <em>all</em> players, not just the best one.</li> <li>The <strong>“exp”</strong> amplifies the impact of the highest scores, making it feel like a weighted average that leans toward the standout performers.</li> </ul> <p>In short, the <strong>log-sum-exp is like a “soft maximum”</strong>: it highlights the best, considers the rest, and ensures the result is stable and interpretable.</p> <p>If you’re having trouble with this analogy, go through the example in the reference. It’ll help clarify things.</p> <hr/> <p>Alright, it’s time to wrap this up. In the next section, we’ll dive into another type of probabilistic modeling: generative models. We’ll explore what they are and how they work. Stay tuned, and see you there!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://benhay.es/posts/exploring-distributions/">Exploring Probability Distributions</a></li> <li><a href="https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/">The Log-Sum-Exp Trick</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.]]></summary></entry><entry><title type="html">Unveiling Probabilistic Modeling</title><link href="https://monishver11.github.io/blog/2025/probabilistic-modeling/" rel="alternate" type="text/html" title="Unveiling Probabilistic Modeling"/><published>2025-01-17T04:51:00+00:00</published><updated>2025-01-17T04:51:00+00:00</updated><id>https://monishver11.github.io/blog/2025/probabilistic-modeling</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/probabilistic-modeling/"><![CDATA[<h4 id="why-probabilistic-modeling"><strong>Why Probabilistic Modeling?</strong></h4> <p>Probabilistic modeling offers a unified framework that underpins many machine learning methods, from linear regression to logistic regression and beyond. At its core, probabilistic modeling allows us to handle uncertainty and make informed decisions based on observed data. It provides a principled way to update our beliefs about the data-generating process as new information becomes available.</p> <p>In machine learning, we often think of learning as statistical inference, where the goal is to use data to draw conclusions about the underlying distribution or process that generated it, rather than simply fitting a model to the observed data. In this view, the goal is not just to fit a model to data, but to estimate the underlying distribution that best explains the observed data. Probabilistic methods give us a powerful tool to incorporate our beliefs about the world—often referred to as inductive biases—into the learning process. This allows us to make more informed predictions and gain deeper insights into the data.</p> <p>For example, in Bayesian inference, prior beliefs are combined with evidence from the data to update our understanding of the underlying process. This principled approach enables us to not only predict outcomes but also quantify our confidence in those predictions, making probabilistic modeling a powerful tool for developing robust, interpretable, and informed machine learning systems.</p> <h5 id="two-ways-of-generating-data"><strong>Two Ways of Generating Data</strong></h5> <p>When we think about how data is generated, there are two main perspectives to consider. The first is through <strong>conditional models</strong>, where we model the likelihood of the output \(y\) given the input \(x\). This is often denoted as \(p(y \vert x)\).</p> <p>The second perspective is through <strong>generative models</strong>, where we model the joint distribution of both the input \(x\) and the output \(y\), denoted as \(p(x, y)\).</p> <p>To understand the distinction between conditional models and generative models, let’s use the analogy of handwriting recognition:</p> <ul> <li> <p><strong>Conditional Models</strong>: Imagine you are given a handwritten letter and asked to identify the corresponding alphabet. Here, you focus only on the relationship between the handwriting (\(x\)) and the letter it represents (\(y\)). This corresponds to modeling \(p(y \mid x)\), where you predict the output \(y\) (the letter) conditioned on the input \(x\) (the handwriting). Essentially, you’re answering the question, <em>“What is the most likely letter given this handwriting?”</em></p> </li> <li> <p><strong>Generative Models</strong>: Now, imagine that instead of just recognizing handwriting, you also aim to generate realistic handwriting for any letter. To do this, you need to understand how the letters (\(y\)) and handwriting styles (\(x\)) are generated together. This involves modeling the joint distribution \(p(x, y)\), where you learn how inputs and outputs are related as part of a larger generative process. The question here becomes, <em>“How are handwriting (\(x\)) and letters (\(y\)) jointly produced?”</em></p> </li> </ul> <p>Each approach offers different advantages depending on the context. However, both share the common goal of estimating the parameters of the model, often using a technique called <strong>Maximum Likelihood Estimation (MLE)</strong>.</p> <hr/> <h4 id="conditional-models"><strong>Conditional Models</strong></h4> <p>Conditional models focus on predicting the output given the input. One of the most well-known and widely used conditional models is <strong>linear regression</strong>. Let’s take a closer look at linear regression and how it fits within this probabilistic framework.</p> <h4 id="linear-regression"><strong>Linear Regression</strong></h4> <p>Linear regression is a fundamental technique in both machine learning and statistics. Its primary goal is to predict a real-valued target \(y\) (also called the response variable) from a vector of features \(x\) (also known as covariates). Linear regression is often used in situations where we want to predict a continuous value, such as:</p> <ul> <li>Predicting house prices based on factors like location, condition, and age of the house.</li> <li>Estimating medical costs of a person based on their age, sex, region, and BMI.</li> <li>Predicting someone’s age from their photograph.</li> </ul> <h5 id="the-problem-setup"><strong>The Problem Setup</strong></h5> <p>In linear regression, we are given a set of training examples, \(D = \{(x^{(n)}, y^{(n)})\}_{n=1}^N\), where \(x^{(n)} \in \mathbb{R}^d\) represents the features and \(y^{(n)} \in \mathbb{R}\) represents the target. The task is to model the relationship between the features \(x\) and the target \(y\).</p> <p>To do this, we assume that there is a linear relationship between \(x\) and \(y\), which can be expressed as:</p> \[h(x) = \theta^T x = \sum_{i=0}^{d} \theta_i x_i\] <p>Here, \(\theta \in \mathbb{R}^d\) represents the parameters (also known as the weights) of the model, and \(x_0 = 1\) is the bias term. The goal is to find the values of \(\theta\) that best explain the observed data.</p> <blockquote> <p><em>“We use superscript to denote the example id and subscript to denote the dimension id”</em></p> </blockquote> <h5 id="unveiling-probabilistic-modeling"><strong>Unveiling Probabilistic Modeling</strong></h5> <p>To estimate the parameters \(\theta\), we use the <strong>least squares method</strong>, which involves minimizing the squared loss between the predicted and observed values. The loss function is defined as:</p> \[J(\theta) = \frac{1}{N} \sum_{n=1}^{N} \left( y^{(n)} - \theta^T x^{(n)} \right)^2\] <p>This function represents the <strong>empirical risk</strong>, which quantifies the difference between the predicted and actual values across all the training examples.</p> <h5 id="matrix-formulation"><strong>Matrix Formulation</strong></h5> <p>We can also express this problem in matrix form for efficiency. Let \(X \in \mathbb{R}^{N \times d}\) be the design matrix, whose rows represent the input features for each training example. Let \(y \in \mathbb{R}^N\) be the vector of all target values. The objective is to solve for the parameter vector \(\hat{\theta}\) that minimizes the loss:</p> \[\hat{\theta} = \arg\min_\theta \left( (X\theta - y)^T (X\theta - y) \right)\] <h5 id="closed-form-solution"><strong>Closed-Form Solution</strong></h5> <p>The closed-form solution to this optimization problem is:</p> \[\hat{\theta} = (X^T X)^{-1} X^T y\] <p>This gives us the values for \(\theta\) that minimize the squared loss, and hence, provide the best linear model for the data.</p> <hr/> <p>Before proceeding further, here are a few review questions. Ask yourself these and check the answers.</p> <h5 id="how-do-we-derive-the-solution-for-linear-regression"><strong>How do we derive the solution for linear regression?</strong></h5> <p>The squared loss function in matrix form is:</p> \[J(\theta) = \frac{1}{N} (X\theta - y)^T (X\theta - y)\] <p>To minimize \(J(\theta)\), we compute the gradient with respect to \(\theta\):</p> <ul> <li>Expand the quadratic term:</li> </ul> \[J(\theta) = \frac{1}{N} \left[ \theta^T X^T X \theta - 2y^T X \theta + y^T y \right]\] <ul> <li>Take the derivative with respect to \(\theta\): <ul> <li>Recall that for any vector \(a\), \(b\), and matrix \(A\), the following derivatives are useful: <ul> <li> \[\frac{\partial (a^T b)}{\partial a} = b\] </li> <li> \[\frac{\partial (a^T A a)}{\partial a} = 2A a\] </li> </ul> <p>(when \(A\) is symmetric).</p> </li> </ul> </li> </ul> <p>Applying these rules:</p> \[\nabla_\theta J(\theta) = \frac{1}{N} \left[ 2X^T X \theta - 2X^T y \right].\] <ul> <li>Set the gradient to zero to find the minimizer:</li> </ul> \[X^T X \theta = X^T y\] <ul> <li>Solve for \(\theta\):</li> </ul> \[\theta = (X^T X)^{-1} X^T y,\] <p>provided \(X^T X\) is invertible.</p> <p><strong>Note:</strong> The \(\frac{1}{N}\) normalization factor is constant and cancels out when setting the gradient to zero, so it does not affect the solution for \(\theta\).</p> <h5 id="why-do-transposes-appear-or-disappear"><strong>Why Do Transposes Appear or Disappear?</strong></h5> <ol> <li> <p><strong>Symmetry of Quadratic Terms</strong>:<br/> In the term \(\theta^T X^T X \theta\), note that \(X^T X\) is a symmetric matrix (because \(X^T X = (X^T X)^T\)). This symmetry ensures that when taking the derivative, we don’t need to explicitly add or remove transposes; they naturally align.</p> </li> <li><strong>Consistency of Vector-Matrix Multiplication</strong>:<br/> When differentiating terms like \(y^T X \theta\), we use the rule \(\frac{\partial (a^T b)}{\partial a} = b\), ensuring dimensions match. This often introduces or removes a transpose based on the structure of the derivative. For example: <ul> <li>\(\nabla_\theta (-2y^T X \theta) = -2X^T y\), where \(X^T\) arises naturally to align dimensions.</li> </ul> </li> <li><strong>Gradient Conventions</strong>:<br/> The transpose changes are necessary to ensure the resulting gradient is a column vector (matching \(\theta\)’s shape), as gradients are typically represented in the same dimensionality as the parameter being differentiated.</li> </ol> <h5 id="what-happens-if-xt-x-is-not-invertible"><strong>What happens if \(X^T X\) is not invertible?</strong></h5> <p>If \(X^T X\) is not invertible (also called singular or degenerate), the normal equations do not have a unique solution. This happens in cases such as:</p> <ul> <li><strong>Linearly dependent features</strong>: Some columns of \(X\) are linear combinations of others.</li> <li><strong>Too few data points</strong>: If \(N &lt; d\) (more features than samples), \(X^T X\) will not be full rank.</li> </ul> <p>To address this issue, we can:</p> <ol> <li><strong>Add regularization</strong>: Use techniques like Ridge Regression, which modifies the normal equation to include a penalty term:<br/> \(\theta = (X^T X + \lambda I)^{-1} X^T y,\)<br/> where \(\lambda &gt; 0\) is the regularization parameter.</li> <li><strong>Remove redundant features</strong>: Perform feature selection or dimensionality reduction (e.g., PCA) to eliminate linear dependencies.</li> <li><strong>Use pseudo-inverse</strong>: Compute the Moore-Penrose pseudo-inverse of \(X^T X\) to find a solution.</li> </ol> <hr/> <h4 id="understanding-linear-regression-through-a-probabilistic-lens"><strong>Understanding Linear Regression Through a Probabilistic Lens</strong></h4> <p>So far, we’ve discussed how linear regression can be understood as minimizing the squared loss. But why is the squared loss a reasonable choice for regression problems? To answer this, we need to think about the assumptions we are making on the data.</p> <p>Let’s approach linear regression from a <strong>probabilistic modeling perspective</strong>.</p> <h5 id="assumptions-in-linear-regression"><strong>Assumptions in Linear Regression</strong></h5> <p>In this framework, we assume that the target \(y\) and the features \(x\) are related through a linear function, with an added error term \(\epsilon\):</p> \[y = \theta^T x + \epsilon\] <p>Here, \(\epsilon\) represents the residual error that accounts for all unmodeled effects, such as noise or other sources of variation in the data. We assume that these errors \(\epsilon\) are independent and identically distributed (iid) and follow a normal distribution:</p> \[\epsilon \sim \mathcal{N}(0, \sigma^2)\] <p>Given this assumption, the conditional distribution of \(y\) given \(x\) is a normal distribution with mean \(\theta^T x\) and variance \(\sigma^2\):</p> \[p(y | x; \theta) = \mathcal{N}(\theta^T x, \sigma^2)\] <h5 id="intuition-behind-the-gaussian-distribution"><strong>Intuition Behind the Gaussian Distribution</strong></h5> <p>This distribution suggests that, for each value of \(x\), the output \(y\) is normally distributed around the value predicted by the linear model \(\theta^T x\), with a fixed variance \(\sigma^2\) that captures the uncertainty or noise in the data. In other words, we place a Gaussian “bump” around the output of the linear predictor, reflecting the uncertainty in our prediction.</p> <p>With this, we’ve laid the groundwork for our discussion on Maximum Likelihood Estimation.</p> <hr/> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>In this post, we introduced how probabilistic modeling can be used for understanding and estimating machine learning models, such as linear regression. By thinking of learning as statistical inference, we can incorporate our prior beliefs about the data-generating process and make more informed predictions.</p> <p>Next, we’ll dive into <strong>Maximum Likelihood Estimation (MLE)</strong> and examine how it can be applied to solve probabilistic linear regression and other machine learning algorithms. We’ll also explore how to formalize this understanding—stay tuned!</p>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.]]></summary></entry><entry><title type="html">SVM Solution in the Span of the Data</title><link href="https://monishver11.github.io/blog/2025/svm-solution-span-of-data/" rel="alternate" type="text/html" title="SVM Solution in the Span of the Data"/><published>2025-01-16T18:13:00+00:00</published><updated>2025-01-16T18:13:00+00:00</updated><id>https://monishver11.github.io/blog/2025/svm-solution-span-of-data</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/svm-solution-span-of-data/"><![CDATA[<p>Previously, we explored the <strong>kernel trick</strong>, a powerful concept that allows Support Vector Machines (SVMs) to operate efficiently in high-dimensional feature spaces without explicitly computing the coordinates. Building on that foundation, we now turn our attention to an intriguing property of SVM solutions: they lie in the <strong>span of the data</strong>. This observation not only deepens our understanding of the connection between the dual and primal formulations of SVM but also provides a unifying perspective on how solutions in machine learning are inherently tied to the training data.</p> <hr/> <h4 id="svm-dual-problem-a-quick-recap"><strong>SVM Dual Problem: A Quick Recap</strong></h4> <p>To understand this property, let’s first revisit the SVM dual problem. It is formulated as:</p> \[\sup_{\alpha \in \mathbb{R}^n} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_j^T x_i\] <p>subject to the constraints:</p> <ol> <li> \[\sum_{i=1}^n \alpha_i y_i = 0\] </li> <li> \[\alpha_i \in [0, \frac{c}{n}], \quad i = 1, \dots, n\] </li> </ol> <p>Here, \(\alpha_i\) are the dual variables that correspond to the Lagrange multipliers, and \(c\) is the regularization parameter that controls the margin.</p> <p>The dual problem focuses on maximizing this quadratic function, which involves pairwise interactions between training samples. Once the optimal dual solution \(\alpha^*\) is obtained, it can be used to compute the primal solution as:</p> \[w^* = \sum_{i=1}^n \alpha^*_i y_i x_i\] <p>This equation reveals a critical insight: the primal solution \(w^*\) is expressed as a <strong>linear combination of the training inputs</strong> \(x_1, x_2, \dots, x_n\). This means that \(w^*\) is confined to the span of these inputs, or mathematically:</p> \[w^* \in \text{span}(x_1, \dots, x_n)\] <p>We refer to this phenomenon as “the SVM solution lies in the span of the data.” It underscores the dependency of \(w^*\) on the training data, aligning it with the geometric intuition of SVMs: the decision boundary is shaped by a subset of data points (the support vectors).</p> <hr/> <h4 id="ridge-regression-another-perspective-on-span-of-the-data"><strong>Ridge Regression: Another Perspective on Span of the Data</strong></h4> <p>Interestingly, this concept is not unique to SVMs. A similar property emerges in <strong>ridge regression</strong>, a linear regression method that incorporates \(\ell_2\) regularization to prevent overfitting. Let’s delve into this and see how the ridge regression solution also resides in the span of the data.</p> <h5 id="ridge-regression-objective"><strong>Ridge Regression Objective</strong></h5> <p>The objective function for ridge regression, with a regularization parameter \(\lambda &gt; 0\), is given by:</p> \[w^* = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \{ w^T x_i - y_i \}^2 + \lambda \|w\|_2^2\] <p>Here, \(w\) is the weight vector, \(x_i\) are the input data points, and \(y_i\) are the corresponding target values. The regularization term \(\lambda \|w\|_2^2\) penalizes large weights to improve generalization.</p> <h6 id="the-closed-form-solution"><strong>The Closed-Form Solutio</strong>n</h6> <p>The ridge regression problem has a closed-form solution:</p> \[w^* = \left( X^T X + \lambda I \right)^{-1} X^T y\] <p>where \(X\) is the design matrix (rows are \(x_1, \dots, x_n\)), and \(y\) is the vector of target values.</p> <p>At first glance, this expression might seem abstract. However, by rearranging it, we can show that the solution also lies in the span of the training data.</p> <h5 id="showing-the-span-property-for-ridge-regression"><strong>Showing the Span Property for Ridge Regression</strong></h5> <p>To reveal the span property, let’s rewrite the ridge regression solution. Using matrix algebra:</p> \[w^* = \left( X^T X + \lambda I \right)^{-1} X^T y\] <p>We can express \(w^*\) as:</p> \[w^* = X^T \left[ \frac{1}{\lambda} y - \frac{1}{\lambda} X w^* \right] \tag{1}\] <p>Now, define:</p> \[\alpha^* = \frac{1}{\lambda} y - \frac{1}{\lambda} X w^*\] <p>Substituting this back, we get:</p> \[w^* = X^T \alpha^*\] <p>Expanding further, it becomes:</p> \[w^* = \sum_{i=1}^n \alpha^*_i x_i\] <p>This clearly shows that the ridge regression solution \(w^*\) is also a linear combination of the training inputs. Thus, like SVMs, ridge regression solutions lie in the span of \(x_1, x_2, \dots, x_n\):</p> \[w^* \in \text{span}(x_1, \dots, x_n)\] <p>You may wonder how we arrived at this specific form for \(w^*\) at the start \((1)\). To understand this, we utilized the following lemma and a series of transformations to reframe it accordingly.</p> <h6 id="the-lemma-matrix-inverse-decomposition"><strong>The Lemma: Matrix Inverse Decomposition</strong></h6> <p>We use the following lemma:</p> <p>If \(A\) and \(A + B\) are non-singular, then:</p> \[(A + B)^{-1} = A^{-1} - A^{-1} B (A + B)^{-1}\] <p>This allows us to break down the inverse of a sum of matrices into manageable parts. Let’s apply it to our problem.</p> <h6 id="applying-the-lemma-to-ridge-regression"><strong>Applying the Lemma to Ridge Regression</strong></h6> <p>Let:</p> <ul> <li>\(A = \lambda I\) (scaled identity matrix),</li> <li>\(B = X^T X\) (Gram matrix).</li> </ul> <p>Substituting into the ridge regression solution:</p> \[w^* = (X^T X + \lambda I)^{-1} X^T y\] <p>Using the lemma, we expand the inverse:</p> \[w^* = \left( \lambda^{-1} - \lambda^{-1} X^T X (X^T X + \lambda I)^{-1} \right) X^T y\] <p>We simplify the terms step by step:</p> <ol> <li> <p><strong>Expand the first term:</strong></p> \[w^* = X^T \lambda^{-1} y - \lambda^{-1} X^T X (X^T X + \lambda I)^{-1} X^T y\] </li> <li> <p><strong>Notice the recursive structure:</strong></p> \[w^* = X^T \lambda^{-1} y - \lambda^{-1} X^T X w^*\] </li> <li> <p><strong>Rearrange to highlight the span of data:</strong></p> \[w^* = X^T \left( \frac{1}{\lambda} y - \frac{1}{\lambda} X w^* \right)\] </li> </ol> <h6 id="supporting-details"><strong>Supporting Details</strong></h6> <p>To solidify our understanding, here’s how the <strong>Matrix Sum Inverse Lemma</strong> is derived using the Woodbury identity:</p> <p><strong>The Woodbury Identity:</strong></p> \[(A + UCV)^{-1} = A^{-1} - A^{-1} U (C^{-1} + V A^{-1} U)^{-1} V A^{-1}\] <p>Substituting:</p> <ul> <li>\(C = I\), \(V = I\), and \(U = B\), we get:</li> </ul> \[(A + B)^{-1} = A^{-1} - A^{-1} B (I + A^{-1} B)^{-1} A^{-1}\] <p>Simplify:</p> \[(A + B)^{-1} = A^{-1} - A^{-1} B (A (I + A^{-1} B))^{-1}\] \[(A + B)^{-1} = A^{-1} - A^{-1} B (A + B)^{-1}\] <p>This completes the proof of the lemma and justifies its use in our derivation.</p> <h5 id="core-takeaway"><strong>Core Takeaway:</strong></h5> <p>Both SVMs and ridge regression share the property that their solutions lie in the span of the training data. For SVMs, this emerges naturally from the dual-primal connection, highlighting how support vectors define the decision boundary. In ridge regression, the span property arises through matrix algebra and the closed-form solution.</p> <p>This unifying view provides a deeper understanding of how machine learning models leverage training data to construct solutions. Next, we’ll explore <strong>how this property influences kernelized methods</strong> and its implications for scalability and interpretability in machine learning.</p> <hr/> <h4 id="reparameterizing-optimization-problems-building-on-the-span-property"><strong>Reparameterizing Optimization Problems: Building on the Span Property</strong></h4> <p>In the previous section, we established that both SVM and ridge regression solutions lie in the <strong>span of the training data</strong>. This insight opens up a new avenue: we can <strong>reparameterize the optimization problem</strong> by restricting our search space to this span. Let’s explore how this simplifies the optimization process and why it’s particularly useful in high-dimensional settings.</p> <h5 id="reparameterization-of-ridge-regression"><strong>Reparameterization of Ridge Regression</strong></h5> <p>To recap, the ridge regression problem for regularization parameter \(\lambda &gt; 0\) is:</p> \[w^* = \arg\min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n \{ w^T x_i - y_i \}^2 + \lambda \|w\|_2^2\] <p>We know that \(w^* \in \text{span}(x_1, \dots, x_n) \subset \mathbb{R}^d\). Therefore, instead of minimizing over all of \(\mathbb{R}^d\), we can restrict our optimization to the span of the training data:</p> \[w^* = \arg\min_{w \in \text{span}(x_1, \dots, x_n)} \frac{1}{n} \sum_{i=1}^n \{ w^T x_i - y_i \}^2 + \lambda \|w\|_2^2\] <p>Now, let’s <strong>reparameterize</strong> the objective function. Since \(w \in \text{span}(x_1, \dots, x_n)\), we can express \(w\) as a linear combination of the inputs:</p> \[w = X^T \alpha, \quad \alpha \in \mathbb{R}^n\] <p>Substituting this into the optimization problem gives:</p> <h5 id="reparameterized-objective"><strong>Reparameterized Objective</strong></h5> <p>The original formulation becomes:</p> \[\alpha^* = \arg\min_{\alpha \in \mathbb{R}^n} \frac{1}{n} \sum_{i=1}^n \{ (X^T \alpha)^T x_i - y_i \}^2 + \lambda \|X^T \alpha\|_2^2\] <p>Once \(\alpha^*\) is obtained, the optimal weight vector \(w^*\) can be recovered as:</p> \[w^* = X^T \alpha^*\] <h5 id="why-does-this-matter"><strong>Why Does This Matter?</strong></h5> <p>By reparameterizing, we’ve effectively reduced the dimension of the optimization problem:</p> <ul> <li><strong>Original Problem</strong>: Optimize over \(\mathbb{R}^d\) (where \(d\) is the feature space dimension).</li> <li><strong>Reparameterized Problem</strong>: Optimize over \(\mathbb{R}^n\) (where \(n\) is the number of training examples).</li> </ul> <p>This reduction is significant in scenarios where \(d \gg n\). For instance:</p> <ul> <li><strong>Very Large Feature Space</strong>: Suppose \(d = 300 \, \text{million}\) (e.g., using high-order polynomial interactions).</li> <li><strong>Moderate Training Set Size</strong>: Suppose \(n = 300,000\) examples.</li> </ul> <p>In the original formulation, we solve a \(300 \, \text{million}\)-dimensional optimization problem. After reparameterization, we solve a much smaller \(300,000\)-dimensional problem. This simplification highlights why the span property is crucial, particularly when the number of features vastly exceeds the number of training examples.</p> <hr/> <h4 id="generalization-the-representer-theorem"><strong>Generalization: The Representer Theorem</strong></h4> <p>The span property is not unique to SVM and ridge regression. A powerful result known as the <strong>Representer Theorem</strong> shows that this property applies broadly to all norm-regularized linear models.Here’s how it works:</p> <h5 id="generalized-objective"><strong>Generalized Objective</strong></h5> <p>We start with a generalized objective for a norm-regularized model:</p> \[w^* = \arg \min_{w \in \mathcal{H}} R(\|w\|) + L\big((\langle w, x_1 \rangle), \dots, (\langle w, x_n \rangle)\big).\] <p>Here:</p> <ul> <li>\(R(\|w\|)\): Regularization term to control model complexity.</li> <li>\(L\): Loss function that measures the fit of the model to the data.</li> <li>\(\mathcal{H}\): Hypothesis space where \(w\) resides.</li> </ul> <h5 id="key-insight-from-the-representer-theorem"><strong>Key Insight from the Representer Theorem</strong></h5> <p>The Representer Theorem tells us that instead of searching for \(w^*\) in the entire hypothesis space \(\mathcal{H}\), we can restrict our search to the span of the training data. Mathematically:</p> \[w^* = \arg \min_{w \in \text{span}(x_1, \dots, x_n)} R(\|w\|) + L\big((\langle w, x_1 \rangle), \dots, (\langle w, x_n \rangle)\big).\] <p>This dramatically reduces the complexity of the optimization problem.</p> <h5 id="reparameterization"><strong>Reparameterization</strong></h5> <p>Using this insight, we can reparameterize the optimization problem as before. Let \(w = \sum_{i=1}^n \alpha_i x_i\), where \(\alpha = (\alpha_1, \dots, \alpha_n) \in \mathbb{R}^n\). Substituting this into the objective:</p> \[\alpha^* = \arg \min_{\alpha \in \mathbb{R}^n} R\left(\left\| \sum_{i=1}^n \alpha_i x_i \right\|\right) + L\Big(\big\langle \sum_{i=1}^n \alpha_i x_i, x_1 \big\rangle, \dots, \big\langle \sum_{i=1}^n \alpha_i x_i, x_n \big\rangle\Big).\] <h5 id="why-this-matters"><strong>Why This Matters</strong></h5> <p>By reparameterizing the problem, we transform the optimization from a potentially infinite-dimensional space \(\mathcal{H}\) to a finite-dimensional space (spanned by the data points). This makes the problem computationally feasible and reveals why the solution lies in the span of the data.</p> <h4 id="implications-kernelization-and-the-kernel-trick"><strong>Implications: Kernelization and the Kernel Trick</strong></h4> <p>The Representer Theorem plays a pivotal role in enabling <strong>kernelization</strong>. Here’s how it connects:</p> <p>Using the Representer Theorem, we know that the solution \(w^*\) resides in the span of the data. This insight allows us to replace the feature space \(\phi(x)\) with a kernel function \(K(x, x')\), where:</p> \[K(x, x') = \langle \phi(x), \phi(x') \rangle.\] <p>The kernel function computes the inner product in the transformed feature space without explicitly constructing \(\phi(x)\). This process is called <strong>kernelization</strong>.</p> <h5 id="kernelized-representer-theorem"><strong>Kernelized Representer Theorem</strong></h5> <p>The Representer Theorem in the context of kernels can be expressed as:</p> \[w^* = \sum_{i=1}^n \alpha_i \phi(x_i),\] <p>where the coefficients \(\alpha\) are obtained by solving an optimization problem that depends only on the kernel \(K(x_i, x_j)\).</p> <p>The Representer Theorem provides a unifying framework for kernelization. By recognizing that solutions lie in the span of the data, we can seamlessly replace explicit feature mappings with kernel functions. This powerful insight underpins many modern machine learning techniques, making high-dimensional learning tasks computationally feasible.</p> <hr/> <h5 id="summary"><strong>Summary</strong></h5> <ol> <li><strong>Reparameterization</strong>: If a solution lies in the span of the training data, we can reparameterize the optimization problem to reduce its dimensionality, simplifying the computation.</li> <li><strong>High-Dimensional Settings</strong>: This approach is especially useful when \(d \gg n\), where the feature space dimension far exceeds the number of training examples.</li> <li><strong>Representer Theorem</strong>: The span property generalizes to all norm-regularized linear models, forming the theoretical foundation for kernelization and advocates that linear models can be kernelized.</li> <li><strong>Kernel Trick</strong>: By kernelizing models, we can solve complex problems in high-dimensional spaces efficiently and without the need to represent \(\phi(x)\) explicitly.</li> </ol> <p>Understanding the span property and its implications is not just a mathematical curiosity—it’s a foundational principle that unifies many machine learning models and opens up practical avenues for efficient computation in challenging scenarios.</p> <p>Next, we’ll delve into specific topics related to SVM that we’ve touched on briefly. We’ll explore them in more depth to build an intuitive understanding of each concept, as many of these form the foundation for more advanced ML techniques. Mastering them is well worth the effort. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Representer Theorem</li> <li>Visualization elements</li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.]]></summary></entry><entry><title type="html">Understanding the Kernel Trick</title><link href="https://monishver11.github.io/blog/2025/kernel-trick/" rel="alternate" type="text/html" title="Understanding the Kernel Trick"/><published>2025-01-13T23:03:00+00:00</published><updated>2025-01-13T23:03:00+00:00</updated><id>https://monishver11.github.io/blog/2025/kernel-trick</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/kernel-trick/"><![CDATA[<p>When working with machine learning models, especially Support Vector Machines (SVMs), the idea of mapping data into a higher-dimensional space often comes into play. This mapping helps transform non-linearly separable data into a space where linear decision boundaries can be applied. But what happens when the dimensionality of the feature space becomes overwhelmingly large? This is where the <strong>kernel trick</strong> saves the day. In this post, we will explore the kernel trick, starting with SVMs, their reliance on feature mappings, and how inner products in feature space can be computed without ever explicitly constructing that space.</p> <hr/> <h4 id="svms-with-explicit-feature-maps"><strong>SVMs with Explicit Feature Maps</strong></h4> <p>To understand the kernel trick, let’s begin with SVMs. In the simplest case, an SVM aims to find a hyperplane that separates data into classes with the largest possible margin. To handle more complex data, we map the input data \(\mathbf{x}\) into a higher-dimensional feature space using a feature map \(\psi: X \to \mathbb{R}^d\). In this space, the SVM optimization problem can be written as:</p> \[\min_{\mathbf{w} \in \mathbb{R}^d} \frac{1}{2} \|\mathbf{w}\|^2 + \frac{c}{n} \sum_{i=1}^n \max(0, 1 - y_i \mathbf{w}^T \psi(\mathbf{x}_i)).\] <p>Here, \(\mathbf{w}\) is the weight vector, \(c\) is a regularization parameter, and \(y_i\) are the labels of the data points. While this approach works well for small \(d\), it becomes computationally expensive as \(d\) increases, especially when using high-degree polynomial mappings.</p> <p>To address this issue, we turn to a reformulation of the SVM problem, derived from <strong>Lagrangian duality</strong>.</p> <h4 id="the-svm-dual-problem"><strong>The SVM Dual Problem</strong></h4> <p>Through Lagrangian duality, the SVM optimization problem can be re-expressed as a dual problem:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \psi(\mathbf{x}_j)^T \psi(\mathbf{x}_i),\] <p>subject to:</p> \[\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \in \left[ 0, \frac{c}{n} \right] \quad \forall i.\] <p>Here, \(\alpha_i\) are the dual variables (Lagrange multipliers). Once the optimal \(\boldsymbol{\alpha}^*\) is obtained, the weight vector in the feature space can be reconstructed as:</p> \[\mathbf{w}^* = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i).\] <p>The decision function for a new input \(\mathbf{x}\) is given by:</p> \[\hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i)^T \psi(\mathbf{x}).\] <h5 id="observing-the-role-of-inner-products"><strong>Observing the Role of Inner Products</strong></h5> <p>An important observation here is that the feature map \(\psi(\mathbf{x})\) appears only through inner products of the form \(\psi(\mathbf{x}_j)^T \psi(\mathbf{x}_i)\). This means we don’t actually need the explicit feature representation \(\psi(\mathbf{x})\); instead, we just need the ability to compute these inner products efficiently.</p> <hr/> <h4 id="computing-inner-products-in-practice"><strong>Computing Inner Products in Practice</strong></h4> <p>Let’s explore the kernel trick with an example.</p> <h5 id="example-degree-2-monomials"><strong>Example: Degree-2 Monomials</strong></h5> <p>Suppose we are working with 2D data points \(\mathbf{x} = (x_1, x_2)\). If we map the data into a space of degree-2 monomials, the feature map becomes:</p> \[\psi: \mathbb{R}^2 \to \mathbb{R}^6, \quad (x_1, x_2) \mapsto (1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, \sqrt{2}x_1x_2, x_2^2).\] <p>The inner product in the feature space is:</p> \[\psi(\mathbf{x})^T \psi(\mathbf{x}') = 1 + 2x_1x_1' + 2x_2x_2' + (x_1x_1')^2 + 2x_1x_2x_1'x_2' + (x_2x_2')^2.\] <p>Simplifying, we observe:</p> \[\psi(\mathbf{x})^T \psi(\mathbf{x}') = (1 + x_1x_1' + x_2x_2')^2 = (1 + \mathbf{x}^T \mathbf{x}')^2.\] <p>This shows that we can compute \(\psi(\mathbf{x})^T \psi(\mathbf{x}')\) directly from the original input space without explicitly constructing \(\psi(\mathbf{x})\)—a key insight behind the kernel trick.</p> <h5 id="general-case-monomials-up-to-degree-p"><strong>General Case: Monomials Up to Degree \(p\)</strong></h5> <p>For feature maps that produce monomials up to degree \(p\), the inner product generalizes as:</p> \[\psi(x)^T \psi(x') = (1 + x^T x')^p.\] <p>It is worth noting that the coefficients of the monomials in \(\psi(x)\) may vary depending on the specific feature map.</p> <hr/> <h4 id="efficiency-of-the-kernel-trick-from-exponential-to-linear-complexity"><strong>Efficiency of the Kernel Trick: From Exponential to Linear Complexity</strong></h4> <p>One of the key advantages of the kernel trick is its ability to reduce the computational complexity of working with high-dimensional feature spaces. Let’s break this down:</p> <h5 id="explicit-computation-complexity"><strong>Explicit Computation Complexity</strong></h5> <p>When we map an input vector \(\mathbf{x} \in \mathbb{R}^d\) to a feature space with monomials up to degree \(p\), the dimensionality of the feature space increases significantly. Specifically:</p> <ul> <li> <p><strong>Feature Space Dimension</strong>: The number of features in the expansion is:</p> \[\binom{d + p}{p} = \frac{(d + p)!}{d! \, p!}.\] <p>For large \(p\) or \(d\), this grows rapidly and can quickly become computationally prohibitive.</p> </li> <li> <p><strong>Explicit Inner Product</strong>: Computing the inner product directly in this expanded space has a complexity of:</p> \[O\left(\binom{d + p}{p}\right),\] <p>which is exponential in \(p\) for fixed \(d\).</p> </li> </ul> <h5 id="implicit-computation-complexity"><strong>Implicit Computation Complexity</strong></h5> <p>Using the kernel trick, we avoid explicitly constructing the feature space. For a kernel function like:</p> \[k(\mathbf{x}, \mathbf{x}') = (1 + \mathbf{x}^T \mathbf{x}')^p,\] <p>the computation operates directly in the input space.</p> <ul> <li> <p><strong>Input Space Computation</strong>: Computing the kernel function involves:</p> <ol> <li><strong>Dot Product</strong>: \(\mathbf{x}^T \mathbf{x}'\) is computed in \(O(d)\).</li> <li><strong>Polynomial Evaluation</strong>: Raising this result to power \(p\) is done in constant time, independent of \(d\).</li> </ol> </li> </ul> <p>Thus, the complexity is reduced to:</p> \[O(d),\] <p>which is <strong>linear</strong> in the input dimensionality \(d\), regardless of \(p\).</p> <h5 id="why-this-matters"><strong>Why This Matters</strong></h5> <ul> <li><strong>Explicit Features</strong>: For high \(p\), the feature space grows exponentially, leading to a <strong>curse of dimensionality</strong> if explicit computation is used.</li> <li><strong>Implicit Kernel Computation</strong>: The kernel trick sidesteps the explicit feature space, allowing efficient computation even when the feature space is high-dimensional or infinite (e.g., with RBF kernels).</li> </ul> <p>This transformation from <strong>exponential</strong> to <strong>linear complexity</strong> is one of the core reasons kernel methods are powerful tools in machine learning.</p> <p><strong>Key Takeaway</strong> : The kernel trick enables efficient computation in high-dimensional feature spaces by directly working in the input space. This reduces the complexity from \(O\left(\binom{d + p}{p}\right)\) to \(O(d)\), making it feasible to apply machine learning methods to problems with high-degree polynomial or infinite-dimensional feature spaces.</p> <hr/> <h4 id="exploring-the-kernel-function"><strong>Exploring the Kernel Function</strong></h4> <p>To fully appreciate the kernel trick, we need to formalize the concept of the <strong>kernel function</strong>. In our earlier discussion, we introduced the idea of a feature map \(\psi: X \to \mathcal{H}\), which maps input data from the original space \(X\) to a higher-dimensional feature space \(\mathcal{H}\). The kernel function \(k\) corresponding to this feature map is defined as:</p> \[k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle,\] <p>where \(\langle \cdot, \cdot \rangle\) represents the inner product in \(\mathcal{H}\).</p> <h5 id="why-use-kernel-functions"><strong>Why Use Kernel Functions?</strong></h5> <p>At first glance, this notation might seem like a trivial restatement of the inner product, but it’s far more powerful. The key insight is that we can often evaluate \(k(\mathbf{x}, \mathbf{x}')\) directly, without explicitly computing \(\psi(\mathbf{x})\) and \(\psi(\mathbf{x}')\). This is crucial for efficiently working with high-dimensional or infinite-dimensional feature spaces. But this efficiency only applies to certain methods — those that can be <strong>kernelized</strong>.</p> <h4 id="kernelized-methods"><strong>Kernelized Methods</strong></h4> <p>A method is said to be <strong>kernelized</strong> if it uses the feature vectors \(\psi(\mathbf{x})\) only inside inner products of the form \(\langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle\). For such methods, we can replace these inner products with a kernel function \(k(\mathbf{x}, \mathbf{x}')\), avoiding explicit feature computation. This applies to both the optimization problem and the prediction function. Let’s revisit the SVM example to see kernelization in action.</p> <h5 id="kernelized-svm-dual-formulation"><strong>Kernelized SVM Dual Formulation</strong></h5> <p>Recall the dual problem for SVMs:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle,\] <p>subject to:</p> \[\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \in \left[ 0, \frac{c}{n} \right] \quad \forall i.\] <p><strong>Here’s the key</strong>: because every occurrence of \(\psi(\mathbf{x})\) is inside an inner product, we can replace \(\langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle\) with \(k(\mathbf{x}_i, \mathbf{x}_j)\), the kernel function. The resulting dual optimization problem becomes:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j),\] <p>subject to the same constraints.</p> <p>For predictions, the decision function can also be written in terms of the kernel:</p> \[\hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i k(\mathbf{x}_i, \mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i)^T \psi(\mathbf{x})\] <p>This reformulation is what allows SVMs to operate efficiently in high-dimensional spaces.</p> <h5 id="the-kernel-matrix"><strong>The Kernel Matrix</strong></h5> <p>A key component in kernelized methods is the <strong>kernel matrix</strong>, which encapsulates the pairwise kernel values for all data points. For a dataset \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}\), the kernel matrix \(\mathbf{K}\) is defined as:</p> \[\mathbf{K} = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}.\] <p>This \(n \times n\) matrix, also known as the <strong>Gram matrix</strong> in machine learning, summarizes all the information about the training data necessary for solving the kernelized optimization problem.</p> <p>For the kernelized SVM, we can replace \(\langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle\) with \(K_{ij}\), reducing the dual problem to:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K_{ij},\] <p>subject to the same constraints.</p> <p><strong>So, Given a kernelized ML algorithm</strong> (i.e., all \(\psi(x)\)’s show up as \(\langle \psi(x), \psi(x') \rangle\)) :</p> <ol> <li><strong>Flexibility</strong>: By substituting the kernel function, we can implicitly use very high-dimensional or even infinite-dimensional feature spaces.</li> <li><strong>Scalability</strong>: Once the kernel matrix is computed, the computational cost depends on the number of data points \(n\), rather than the dimension of the feature space \(d\).</li> <li><strong>Efficiency</strong>: For many kernels, \(k(\mathbf{x}, \mathbf{x}')\) can be computed without directly accessing the high-dimensional feature representation \(\psi(\mathbf{x})\), avoiding the \(O(d)\) dependence.</li> </ol> <p>These properties make kernelized methods invaluable when \(d \gg n\), a common scenario in machine learning tasks.</p> <p>The kernel trick revolutionizes how we think about high-dimensional data. Next, we will delve into popular kernel functions, their interpretations, and how to choose the right one for your problem.</p> <hr/> <h4 id="example-kernels"><strong>Example Kernels</strong></h4> <p>In many cases, it’s useful to think of the kernel function \(k(x, x')\) as a <strong>similarity score</strong> between the data points \(x\) and \(x'\). This perspective allows us to design similarity functions without explicitly considering the feature map.</p> <p>For example, we can create <strong>string kernels</strong> or <strong>graph kernels</strong>—functions that define similarity based on the structure of strings or graphs, respectively. The key question, however, is: <strong>How do we know that our kernel functions truly correspond to inner products in some feature space?</strong></p> <p>This is an essential consideration, as it ensures that the kernel method preserves the properties necessary for various machine learning algorithms to work effectively. Let’s break this down.</p> <h5 id="how-to-obtain-kernels"><strong>How to Obtain Kernels?</strong></h5> <p>There are two primary ways to define kernels:</p> <ol> <li> <p><strong>Explicit Construction</strong>: Define the feature map \(\psi(\mathbf{x})\) and use it to compute the kernel: \(k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle.\) (e.g. monomials)</p> </li> <li> <p><strong>Direct Definition</strong>: Directly define the kernel \(k(\mathbf{x}, \mathbf{x}')\) as a similarity score and verify that it corresponds to an inner product for some \(\psi\). This verification is often guided by mathematical theorems.</p> </li> </ol> <p>To understand this better, let’s first equip ourselves with some essential linear algebra concepts.</p> <h5 id="positive-semidefinite-matrices-and-kernels"><strong>Positive Semidefinite Matrices and Kernels</strong></h5> <p>To verify if a kernel corresponds to a valid inner product, we rely on the concept of <strong>positive semidefinite (PSD) matrices</strong>. Here’s a quick refresher:</p> <ul> <li> <p>A matrix \(\mathbf{M} \in \mathbb{R}^{n \times n}\) is positive semidefinite if: \(\mathbf{x}^\top \mathbf{M} \mathbf{x} \geq 0, \quad \forall \mathbf{x} \in \mathbb{R}^n.\)</p> </li> <li> <p>Equivalent conditions, each necessary and sufficient for a symmetric matrixfor \(\mathbf{M}\) being <strong>PSD</strong>:</p> <ul> <li>\(\mathbf{M} = \mathbf{R}^\top \mathbf{R}\), for some matrix \(\mathbf{R}\).</li> <li>All eigenvalues of \(\mathbf{M}\) are non-negative or \(\geq 0\).</li> </ul> </li> </ul> <p>Next, we define a <strong>positive definite (PD) kernel</strong>:</p> <h5 id="positive-definite-kernel"><strong>Positive Definite Kernel</strong></h5> <p><strong>Definition:</strong></p> <p>A symmetric function \(k: X \times X \to \mathbb{R}\) is a <strong>PD</strong> kernel if, for any finite set \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\} \subset X\), the kernel matrix:</p> \[\mathbf{K} = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}\] <p>is positive semidefinite.</p> <ol> <li>Symmetry: \(k(\mathbf{x}, \mathbf{x}') = k(\mathbf{x}', \mathbf{x})\).</li> <li>The kernel matrix needs to be positive semidefinite for any finite set of points.</li> <li>Equivalently: \(\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j k(\mathbf{x}_i, \mathbf{x}_j) \geq 0\), for all \(\alpha_i \in \mathbb{R}\) \(\forall i\).</li> </ol> <h6 id="think-of-it-this-way"><strong>Think of it this way:</strong></h6> <ol> <li> <p><strong>Symmetry</strong>:<br/> Symmetry ensures the kernel measures similarity consistently between any two points: \(k(\mathbf{x}, \mathbf{x}') = k(\mathbf{x}', \mathbf{x}).\)<br/> For example, the similarity between \(\mathbf{x}\) and \(\mathbf{x}'\) is the same as that between \(\mathbf{x}'\) and \(\mathbf{x}\).</p> </li> <li> <p><strong>Positive Semidefiniteness</strong>:<br/> Positive semidefiniteness ensures the kernel corresponds to a <strong>valid inner product</strong> in some (possibly high-dimensional or infinite-dimensional) feature space.</p> <ul> <li>Think of the kernel as a measure of similarity: this property ensures that the relationships it captures are geometrically valid in that feature space.</li> <li>Intuitively, this means the kernel matrix does not produce “negative energy,” ensuring a consistent representation of the data.</li> </ul> </li> </ol> <p><strong>Simpler Way to State It:</strong></p> <ul> <li>A kernel is <strong>PD</strong> if it acts like an inner product in some feature space.</li> <li>For any set of points, the kernel matrix must be symmetric and positive semidefinite.</li> <li>Symmetry ensures the similarity is consistent in both directions, while positive semidefiniteness guarantees geometrically valid relationships in the feature space.</li> </ul> <h5 id="mercers-theorem"><strong>Mercer’s Theorem</strong></h5> <p>Mercer’s Theorem provides a foundational result for kernels. It states:</p> <ul> <li> <p>A symmetric function \(k(\mathbf{x}, \mathbf{x}')\) can be expressed as an inner product</p> \[k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle\] <p>if and only if \(k(\mathbf{x}, \mathbf{x}')\) is positive definite.</p> </li> </ul> <p>While proving that a kernel is <strong>PD</strong> can be challenging, we can use known kernels to construct new ones.</p> <h5 id="constructing-new-kernels-from-existing-ones"><strong>Constructing New Kernels from Existing Ones</strong></h5> <p>Given valid PD kernels \(k_1\) and \(k_2\), we can create new kernels using the following operations:</p> <ol> <li><strong>Non-Negative Scaling</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = \alpha k(\mathbf{x}, \mathbf{x}')\), where \(\alpha \geq 0\).</li> <li><strong>Addition</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') + k_2(\mathbf{x}, \mathbf{x}')\).</li> <li><strong>Multiplication</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')\).</li> <li><strong>Recursion</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k(\psi(\mathbf{x}), \psi(\mathbf{x}'))\), for any function \(\psi(\cdot)\).</li> <li><strong>Feature Mapping</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = f(\mathbf{x}) f(\mathbf{x}')\), for any function \(f(\cdot)\).</li> </ol> <p>And, Lot more ways to help you construct new kernels from old.</p> <p>It should be noted that Mercer’s theorem only tells us when a candidate similarity function is admissible for use. It tells nothing about how good such a kernel function is.</p> <p>Next, we’ll dive into some of the most widely used kernel functions.</p> <hr/> <h4 id="the-linear-kernel"><strong>The Linear Kernel</strong></h4> <p>The linear kernel is the simplest and most intuitive kernel function. Imagine working with data in an input space represented as \(X = \mathbb{R}^d\). Here, the feature space, denoted as \(\mathcal{H}\), is the same as the input space \(\mathbb{R}^d\). The feature map for this kernel is straightforward: \(\psi(x) = x\).</p> <p>The kernel function itself is defined as:</p> \[k(x, x') = \langle x, x' \rangle = x^\top x',\] <p>where \(\langle x, x' \rangle\) represents the standard inner product. This simplicity makes the linear kernel computationally efficient and ideal for linear models.</p> <h4 id="the-quadratic-kernel"><strong>The Quadratic Kernel</strong></h4> <p>The quadratic kernel takes us a step further by mapping the input space \(X = \mathbb{R}^d\) into a higher-dimensional feature space \(\mathcal{H} = \mathbb{R}^D\), where \(D\) is approximately \(d + \binom d2 \approx \frac{d^2}{2}\). This expanded feature space enables the kernel to capture quadratic relationships in the data.</p> <p>The feature map for the quadratic kernel is given by:</p> \[\psi(x) = \left(x_1, \dots, x_d, x_1^2, \dots, x_d^2, \sqrt{2}x_1x_2, \dots, \sqrt{2}x_ix_j, \dots, \sqrt{2}x_{d-1}x_d\right)^\top.\] <p>To compute the kernel function, we use the inner product of the feature maps:</p> \[k(x, x') = \langle \psi(x), \psi(x') \rangle.\] <p>Expanding this yields:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2.\] <p><strong>Derivation of the Quadratic Kernel form:</strong></p> <p>The quadratic kernel is defined as the inner product in a higher-dimensional feature space. The feature map \(\psi(x)\) includes:</p> <ol> <li>Original features: \(x_1, x_2, \dots, x_d\)</li> <li>Squared features: \(x_1^2, x_2^2, \dots, x_d^2\)</li> <li>Cross-product terms: \(\sqrt{2}x_i x_j\) for \(i \neq j\)</li> </ol> <p>Thus:</p> \[\psi(x) = \left(x_1, x_2, \dots, x_d, x_1^2, x_2^2, \dots, x_d^2, \sqrt{2}x_1x_2, \sqrt{2}x_1x_3, \dots, \sqrt{2}x_{d-1}x_d\right)^\top\] <p>The kernel is computed as:</p> \[k(x, x') = \langle \psi(x), \psi(x') \rangle\] <p>Expanding this, we have:</p> <ol> <li> <p><strong>Linear terms</strong>:<br/> \(\langle x, x' \rangle = \sum_{i} x_i x_i'\)</p> </li> <li> <p><strong>Squared terms</strong>:<br/> \(\sum_{i} x_i^2 x_i'^2\)</p> </li> <li> <p><strong>Cross-product terms</strong>: \(2 \sum_{i \neq j} x_i x_j x_i' x_j'\)</p> </li> </ol> <p>Combining these, the kernel becomes:</p> \[k(x, x') = \langle x, x' \rangle + \sum_{i} x_i^2 x_i'^2 + 2 \sum_{i \neq j} x_i x_j x_i' x_j'\] <p>Recognizing that:</p> \[\langle x, x' \rangle^2 = \left( \sum_{i} x_i x_i' \right)^2 = \sum_{i} x_i^2 x_i'^2 + 2 \sum_{i \neq j} x_i x_j x_i' x_j'\] <p>The kernel simplifies to:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2\] <p>One of the key advantages of kernel methods is computational efficiency. While the explicit computation of the inner product in the feature space requires \(O(d^2)\) operations, the implicit kernel calculation only requires \(O(d)\) operations.</p> <p>A good example will make it much clearer.</p> <p>Let \(x = [1, 2]\) and \(x' = [3, 4]\).</p> <p>The quadratic kernel is defined as:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2\] <p><strong>Step 1</strong>: Compute \(\langle x, x' \rangle\) \(\langle x, x' \rangle = (1)(3) + (2)(4) = 3 + 8 = 11\)</p> <p><strong>Step 2</strong>: Compute \(\langle x, x' \rangle^2\) \(\langle x, x' \rangle^2 = 11^2 = 121\)</p> <p><strong>Step 3</strong>: Compute \(k(x, x')\) \(k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2 = 11 + 121 = 132\)</p> <p><strong>Step 4</strong>: Verify with the Feature Map</p> <p>The feature map for the quadratic kernel is:</p> \[\psi(x) = [x_1, x_2, x_1^2, x_2^2, \sqrt{2}x_1x_2]\] <p>For \(x = [1, 2]\): \(\psi(x) = [1, 2, 1^2, 2^2, \sqrt{2}(1)(2)] = [1, 2, 1, 4, 2\sqrt{2}]\)</p> <p>For \(x' = [3, 4]\): \(\psi(x') = [3, 4, 3^2, 4^2, \sqrt{2}(3)(4)] = [3, 4, 9, 16, 12\sqrt{2}]\)</p> <p>Compute the inner product:</p> <p>\(\langle \psi(x), \psi(x') \rangle = (1)(3) + (2)(4) + (1)(9) + (4)(16) + (2\sqrt{2})(12\sqrt{2})\) \(= 3 + 8 + 9 + 64 + 48 = 132\)</p> <p>Thus, the quadratic kernel gives: \(k(x, x') = 132\)</p> <h4 id="the-polynomial-kernel"><strong>The Polynomial Kernel</strong></h4> <p>Building on the quadratic kernel, the polynomial kernel generalizes the concept by introducing a degree parameter \(M\). The kernel function is defined as:</p> \[k(x, x') = (1 + \langle x, x' \rangle)^M.\] <p>This kernel corresponds to a feature space that includes all monomials of the input features up to degree \(M\). Notably, the computational cost of evaluating the kernel function remains constant, regardless of \(M\). However, explicitly computing the inner product in the feature space grows rapidly as \(M\) increases.</p> <hr/> <h4 id="the-radial-basis-function-rbf-kernel"><strong>The Radial Basis Function (RBF) Kernel</strong></h4> <p>The <strong>Radial Basis Function (RBF) kernel</strong>, also known as the <strong>Gaussian kernel</strong>, is one of the most widely used kernels for solving nonlinear problems. Unlike the linear and polynomial kernels, the RBF kernel maps data into an <strong>infinite-dimensional feature space</strong>, enabling it to capture highly complex relationships.</p> <p>The RBF kernel function is mathematically expressed as:</p> \[k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right),\] <p>where:</p> <ul> <li>\(x, x' \in \mathbb{R}^d\) are data points in the input space,</li> <li>\(\|x - x'\|\) is the Euclidean distance between \(x\) and \(x'\),</li> <li>\(\sigma^2\) (the <strong>bandwidth</strong>) controls how quickly the kernel value decays with distance.</li> </ul> <h5 id="how-the-rbf-kernel-works"><strong>How the RBF Kernel Works</strong></h5> <p>The RBF kernel measures <strong>similarity</strong> between two points based on their distance. Here’s a breakdown:</p> <ol> <li><strong>Close Points</strong>: <ul> <li>If \(x\) and \(x'\) are close, \(\|x - x'\|\) is small. The exponential term \(\exp(-\|x - x'\|^2 / 2\sigma^2)\) is close to 1, meaning the points are highly similar.</li> </ul> </li> <li><strong>Distant Points</strong>: <ul> <li>When \(x\) and \(x'\) are far apart, \(\|x - x'\|\) becomes large, and the kernel value approaches 0. This indicates little to no similarity.</li> </ul> </li> <li><strong>Smoothness Control with \(\sigma^2\)</strong>: <ul> <li>A smaller \(\sigma^2\) leads to sharper drops in similarity, making the kernel more sensitive to nearby points.</li> <li>A larger \(\sigma^2\) smooths the decay, creating broader generalization.</li> </ul> </li> </ol> <p>The RBF kernel is powerful because it implicitly maps data into an <strong>infinite-dimensional feature space</strong>. However, thanks to the <strong>kernel trick</strong>, we don’t need to compute these features explicitly. Instead, the kernel function \(k(x, x')\) directly computes the equivalent of the dot product in this space.</p> <h6 id="what-does-this-mean"><strong>What Does This Mean?</strong></h6> <ul> <li>In this infinite space, even simple algorithms (like linear classifiers) can create highly complex and nonlinear decision boundaries in the original input space.</li> </ul> <h5 id="intuition-behind-the-rbf-kernel"><strong>Intuition Behind the RBF Kernel</strong></h5> <p>To understand the RBF kernel, let’s break it down with some simple analogies.</p> <h6 id="1-a-bubble-of-influence"><strong>1. A Bubble of Influence</strong></h6> <p>Imagine every data point in your dataset creates an invisible “bubble” around itself. The size and shape of this bubble depend on the kernel’s parameter \(\sigma^2\) (the bandwidth):</p> <ul> <li><strong>Small \(\sigma^2\)</strong>: The bubble is tight and localized, meaning each point influences only its immediate neighbors. This captures fine-grained details.</li> <li><strong>Large \(\sigma^2\)</strong>: The bubble is wide and smooth, allowing points to influence data further away. This leads to broader generalization.</li> </ul> <p>When we compute \(k(x, x')\), we’re essentially asking, <em>“How much does the bubble around \(x\) overlap with the bubble around \(x'\)?”</em> The more overlap, the higher the similarity score.</p> <h6 id="2-analogy-dropping-pebbles-in-a-pond"><strong>2. Analogy: Dropping Pebbles in a Pond</strong></h6> <p>Imagine dropping pebbles into a still pond:</p> <ul> <li>Each pebble creates ripples that spread outward.</li> <li>The strength of the ripples diminishes as they travel further from the pebble.</li> </ul> <p>The kernel function \(k(x, x')\) measures how much the ripples from one pebble (data point \(x\)) interfere or overlap with those from another pebble (\(x'\)).</p> <ul> <li><strong>Close pebbles</strong>: Their ripples interfere constructively (high similarity, \(k(x, x')\) close to 1).</li> <li><strong>Distant pebbles</strong>: Their ripples barely touch (low similarity, \(k(x, x')\) close to 0).</li> </ul> <p>The parameter \(\sigma^2\) controls the rate at which the ripples fade:</p> <ul> <li><strong>Small \(\sigma^2\)</strong>: Ripples fade quickly, leading to sharp, localized interference.</li> <li><strong>Large \(\sigma^2\)</strong>: Ripples fade slowly, allowing broader interference.</li> </ul> <h6 id="3-the-infinite-dimensional-perspective"><strong>3. The Infinite-Dimensional Perspective</strong></h6> <p>Now imagine these ripples aren’t confined to the surface of the pond but instead exist in an infinite-dimensional space. Each data point generates a unique “wave” in this space.</p> <p>The RBF kernel computes the similarity between these waves without explicitly constructing them. It’s like a shortcut for comparing the interference patterns of ripples in an infinitely deep and wide pond.</p> <h6 id="4-why-does-this-matter"><strong>4. Why Does This Matter?</strong></h6> <p>This ripple analogy helps explain why the RBF kernel is so effective:</p> <ul> <li><strong>Localized Influence</strong>: Points that are closer together naturally exert more influence on each other.</li> <li><strong>Nonlinear Relationships</strong>: The ripple effect in the transformed feature space allows the kernel to capture intricate patterns in data.</li> <li><strong>Flexibility</strong>: By tuning \(\sigma^2\), you can adjust the model to balance between fine details (small \(\sigma^2\)) and broad generalization (large \(\sigma^2\)).</li> </ul> <p><strong>Key Takeaway:</strong> The RBF kernel creates a ripple effect around every data point and measures how much these ripples overlap. This process enables us to handle nonlinear relationships and create complex decision boundaries, all while staying computationally efficient.</p> <h5 id="why-is-the-rbf-kernel-infinite-dimensional"><strong>Why is the RBF Kernel Infinite-Dimensional?</strong></h5> <p>The RBF kernel maps data into an <strong>infinite-dimensional feature space</strong> because of its connection to the <strong>Taylor series expansion</strong> of the exponential function. The kernel is expressed as:</p> \[k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)\] <p>The exponential function can be expanded as a Taylor series:</p> \[\exp(-t) = \sum_{n=0}^{\infty} \frac{(-t)^n}{n!}\] <p>Applying this to the RBF kernel:</p> \[k(x, x') = \sum_{n=0}^{\infty} \frac{1}{n!} \left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)^n\] <p>Each term in this infinite series corresponds to a basis function in the feature space. Since the series includes terms of all powers \(n\), the feature space has an <strong>infinite number of dimensions</strong>.</p> <h6 id="key-intuition"><strong>Key Intuition:</strong></h6> <ol> <li> <p><strong>Infinite Series</strong>: The kernel includes contributions from all possible degrees of interaction between features (e.g., quadratic, cubic, quartic terms, etc.), up to infinity.</p> </li> <li> <p><strong>Feature Representation</strong>: The mapping \(\phi(x)\) to the feature space involves infinitely many components derived from the series expansion.</p> </li> <li> <p><strong>Kernel Trick</strong>: Instead of explicitly constructing these infinite features, the RBF kernel directly computes their inner product, \(\langle \phi(x), \phi(x') \rangle\), through \(k(x, x')\).</p> </li> </ol> <p>This infinite-dimensional nature is what gives the RBF kernel its remarkable flexibility to model complex, nonlinear patterns.</p> <hr/> <h4 id="kernelization-the-recipe"><strong>Kernelization: The Recipe</strong></h4> <p>To effectively leverage kernel methods, follow this general recipe:</p> <ol> <li>Recognize problems that can benefit from kernelization. These are cases where the feature map \(\psi(x)\) only appears in inner products \(\langle \psi(x), \psi(x') \rangle\).</li> <li>Select an appropriate kernel function(‘similarity score’) that suits the data and the task at hand.</li> <li>Compute the kernel matrix, a symmetric matrix of size \(n \times n\) for a dataset with \(n\) data points.</li> <li>Use the kernel matrix to optimize the model and make predictions.</li> </ol> <p>This approach allows us to solve problems in high-dimensional feature spaces without the computational burden of explicit mappings.</p> <h5 id="whats-next"><strong>What’s Next?</strong></h5> <p>We explored the theoretical foundations of kernel functions, how to construct valid kernels, and the properties of popular kernels. But, a key question remains: <strong>under what conditions can we apply kernelization effectively?</strong> Understanding this requires exploring one more crucial concept: how the solution to certain optimization problems is spanned by the input data itself. Next, we’ll delve into this idea and explore how it connects with kernels to solve the SVM problem we’ve been discussing. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Add some visualization for kernels intuition</li> <li><a href="https://www.cs.princeton.edu/~bee/courses/scribe/lec_10_09_2013.pdf">Kernels and Kernel Methods - Princeton University</a></li> <li><a href="https://towardsdatascience.com/radial-basis-function-rbf-kernel-the-go-to-kernel-acf0d22c798a">Radial Basis Function (RBF) Kernel: The Go-To Kernel</a></li> <li><a href="https://medium.com/@suvigya2001/the-gaussian-rbf-kernel-in-non-linear-svm-2fb1c822aae0">The Gaussian RBF Kernel in Non Linear SVM</a></li> <li><a href="https://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf">The Radial Basis Function Kernel: University of Wisconsin–Madison</a></li> </ul>]]></content><author><name></name></author><category term="ML-NYU"/><category term="ML"/><category term="Math"/><summary type="html"><![CDATA[A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.]]></summary></entry></feed>