<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-31T02:22:30+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Gradient Descent Convergence - Prerequisites and Detailed Derivation</title><link href="https://monishver11.github.io/blog/2024/gd-convergence/" rel="alternate" type="text/html" title="Gradient Descent Convergence - Prerequisites and Detailed Derivation"/><published>2024-12-29T01:44:00+00:00</published><updated>2024-12-29T01:44:00+00:00</updated><id>https://monishver11.github.io/blog/2024/gd-convergence</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/gd-convergence/"><![CDATA[<p>To understand the <strong>Convergence Theorem for Fixed Step Size</strong>, it is essential to grasp a few foundational concepts like <strong>Lipschitz continuity</strong> and <strong>convexity</strong>. This section introduces these concepts and establishes the necessary prerequisites.</p> <p><strong>Big Note:</strong> <code class="language-plaintext highlighter-rouge">If you find yourself struggling to understand a specific part or step, don’t worry—simply copy and paste it into ChatGPT/Perplexity for an explanation. More than 90% of the time, you'll be able to grasp the concept and move forward. If you’re still stuck, don’t hesitate to ask for help. The key is not to get bogged down by small hurdles—keep going and seek assistance when needed!</code></p> <h4 id="lipschitz-continuity"><strong>Lipschitz Continuity?</strong></h4> <p>At its core, Lipschitz continuity imposes a <strong>limit on how fast a function can change</strong>. Mathematically, a function \(g : \mathbb{R}^d \to \mathbb{R}\) is said to be <strong>Lipschitz continuous</strong> if there exists a constant \(L &gt; 0\) such that:</p> \[\|g(x) - g(x')\| \leq L \|x - x'\|, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This means the function’s rate of change is bounded by \(L\). For differentiable functions, Lipschitz continuity is often applied to the gradient. If \(\nabla f(x)\) is Lipschitz continuous with constant \(L &gt; 0\), then:</p> \[\|\nabla f(x) - \nabla f(x')\| \leq L \|x - x'\|, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This ensures the gradient does not change too rapidly, which is crucial for the convergence of optimization algorithms like gradient descent.</p> <h6 id="intuition-behind-lipschitz-continuity"><strong>Intuition Behind Lipschitz Continuity</strong></h6> <ol> <li><strong>Bounding the Slope</strong>: Lipschitz continuity ensures that the slope of the function (or the steepness of the graph) is bounded by \(L\). You can think of it as saying, “No part of the function can change too steeply.”</li> <li><strong>Gradient Smoothness</strong>: For \(\nabla f(x)\), Lipschitz continuity means the gradient varies smoothly between nearby points. This avoids abrupt jumps or erratic behavior in the optimization landscape.</li> </ol> <h6 id="visual-way-to-think-about-it"><strong>Visual Way to Think About It</strong></h6> <p>Imagine walking along a path represented by the graph of \(f(x)\). Lipschitz continuity guarantees:</p> <ul> <li>No sudden steep hills or cliffs.</li> <li>A smooth path where the steepness (gradient) is capped.</li> </ul> <p>Alternatively, picture a <strong>rubber band stretched smoothly over some pegs</strong>. The tension in the rubber band ensures there are no sharp kinks, making the graph smooth and predictable.</p> <h6 id="examples-of-lipschitz-continuous-functions"><strong>Examples of Lipschitz Continuous Functions</strong></h6> <ol> <li><strong>Linear Function</strong>: \(f(x) = mx + b\) is Lipschitz continuous because the slope \(m\) is constant, and \(|f'(x)| = |m|\) is bounded.</li> <li><strong>Quadratic Function</strong>: \(f(x) = x^2\) is \(L\)-smooth with \(L = 2\). Its gradient \(f'(x) = 2x\) satisfies:</li> </ol> \[|f'(x) - f'(x')| = |2x - 2x'| = 2|x - x'|.\] <ol> <li><strong>Non-Lipschitz Example</strong>: \(f(x) = \sqrt{x}\) (for \(x &gt; 0\)) is <strong>not Lipschitz continuous</strong> at \(x = 0\) because the slope becomes infinitely steep as \(x \to 0\). (If you’re not getting this, just plot \(\sqrt{x}\) function in <a href="https://www.desmos.com/">Desmos</a> and you’ll get it.)</li> </ol> <h6 id="why-does-lipschitz-continuity-matter"><strong>Why Does Lipschitz Continuity Matter?</strong></h6> <ol> <li><strong>Predictability</strong>: Lipschitz continuity ensures that a function behaves predictably, without sudden spikes or erratic changes.</li> <li><strong>Gradient Descent</strong>: If \(\nabla f(x)\) is Lipschitz continuous, we can choose a step size \(\eta \leq \frac{1}{L}\) to ensure gradient descent converges smoothly without overshooting the minimum.</li> </ol> <p>But Why? We’ll see that in the Convergence Theorem down below. For now, lets equip ourselves with the next important concept needed.</p> <hr/> <h4 id="2-convex-functions-and-convexity-condition"><strong>2. Convex Functions and Convexity Condition</strong></h4> <p>A function \(f : \mathbb{R}^d \to \mathbb{R}\) is <strong>convex</strong> if for any \(x, x' \in \mathbb{R}^d\) and \(\alpha \in [0, 1]\):</p> \[f(\alpha x + (1 - \alpha)x') \leq \alpha f(x) + (1 - \alpha)f(x').\] <p>Intuitively, the line segment between any two points on the graph of \(f\) lies above the graph itself.</p> <h6 id="convexity-condition-using-gradients"><strong>Convexity Condition Using Gradients</strong></h6> <p>If \(f\) is differentiable, convexity is equivalent to the following condition:</p> \[f(x') \geq f(x) + \langle \nabla f(x), x' - x \rangle, \quad \forall x, x' \in \mathbb{R}^d.\] <p>This means that the function lies above its tangent plane at any point.</p> <hr/> <h4 id="3-l-smoothness"><strong>3. \(L\)-Smoothness</strong></h4> <p>A function \(f\) is said to be \(L\)-smooth if its gradient is Lipschitz continuous. This implies the following inequality:</p> \[f(x') \leq f(x) + \langle \nabla f(x), x' - x \rangle + \frac{L}{2} \|x' - x\|^2.\] <p>This property bounds the change in the function value using the gradient and the distance between \(x\) and \(x'\).</p> <hr/> <h4 id="4-optimality-conditions-for-convex-functions"><strong>4. Optimality Conditions for Convex Functions</strong></h4> <p>For convex functions, the following is true:</p> <ul> <li>If \(x^*\) is a minimizer of \(f\), then:</li> </ul> \[\nabla f(x^*) = 0.\] <ul> <li>For any \(x\), the difference between \(f(x)\) and \(f(x^*)\) can be bounded using the gradient:</li> </ul> \[f(x) - f(x^*) \leq \langle \nabla f(x), x - x^* \rangle.\] <p>These conditions help in deriving the convergence results for gradient descent.</p> <hr/> <p><strong>To quickly summarize, before we proceed further:</strong></p> <ol> <li><strong>Lipschitz continuity</strong> ensures the gradient does not change too rapidly.</li> <li><strong>Convexity</strong> guarantees that the function behaves well, with no local minima other than the global minimum.</li> <li><strong>\(L\)-smoothness</strong> combines convexity and Lipschitz continuity to bound the function’s behavior using gradients.</li> </ol> <hr/> <p>With these concepts in place, we can now proceed to derive the <strong>Convergence Theorem for Fixed Step Size</strong>.</p> <h4 id="convergence-of-gradient-descent-with-fixed-step-size"><strong>Convergence of Gradient Descent with Fixed Step Size</strong></h4> <h5 id="theorem"><strong>Theorem:</strong></h5> <p>Suppose the function \(f : \mathbb{R}^n \to \mathbb{R}\) is convex and differentiable, and its gradient is Lipschitz continuous with constant \(L &gt; 0\), i.e.,</p> \[\|\nabla f(x) - \nabla f(y)\|_2 \leq L \|x - y\|_2 \quad \text{for any} \quad x, y.\] <p>Then, if we run gradient descent for \(k\) iterations with a fixed step size \(t \leq \frac{1}{L}\), the solution \(x^{(k)}\) satisfies:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t k},\] <p>where \(f(x^*)\) is the optimal value.</p> <h5 id="proof"><strong>Proof:</strong></h5> <h6 id="step-1-lipschitz-continuity-and-smoothness"><strong>Step 1: Lipschitz Continuity and Smoothness</strong></h6> <p>From the Lipschitz continuity of \(\nabla f\), the function \(f\) satisfies the following inequality for any \(x, y \in \mathbb{R}^n\):</p> \[f(y) \leq f(x) + \nabla f(x)^T (y - x) + \frac{L}{2} \|y - x\|_2^2.\] <p>This inequality allows us to bound how the function \(f\) changes as we move from \(x\) to \(y\), given the Lipschitz constant \(L\).</p> <h6 id="step-2-gradient-descent-update"><strong>Step 2: Gradient Descent Update</strong></h6> <p>The gradient descent update step is defined as:</p> \[x^{+} = x - t \nabla f(x),\] <p>where \(t\) is the step size. Letting \(y = x^+\) in the smoothness inequality gives:</p> \[f(x^+) \leq f(x) + \nabla f(x)^T (x^+ - x) + \frac{L}{2} \|x^+ - x\|_2^2.\] <h6 id="step-3-substituting-the-update-rule"><strong>Step 3: Substituting the Update Rule</strong></h6> <p>Substituting \(x^+ - x = -t \nabla f(x)\), we get:</p> \[f(x^+) \leq f(x) + \nabla f(x)^T (-t \nabla f(x)) + \frac{L}{2} \| -t \nabla f(x)\|_2^2.\] <p>Simplifying each term:</p> <ul> <li>The second term simplifies to:</li> </ul> \[\nabla f(x)^T (-t \nabla f(x)) = -t \|\nabla f(x)\|_2^2.\] <ul> <li>The third term simplifies to:</li> </ul> \[\frac{L}{2} \| -t \nabla f(x)\|_2^2 = \frac{L t^2}{2} \|\nabla f(x)\|_2^2.\] <p>Combining these, we have:</p> \[f(x^+) \leq f(x) - t \|\nabla f(x)\|_2^2 + \frac{L t^2}{2} \|\nabla f(x)\|_2^2.\] <p>Factoring out \(\|\nabla f(x)\|_2^2\):</p> \[f(x^+) \leq f(x) - \left( t - \frac{L t^2}{2} \right) \|\nabla f(x)\|_2^2.\] <h6 id="step-4-ensuring-decrease-in-fx"><strong>Step 4: Ensuring Decrease in \(f(x)\)</strong></h6> <p>To ensure that the function value decreases at each iteration, the coefficient \(t - \frac{L t^2}{2}\) must be non-negative. This holds when \(t \leq \frac{1}{L}\). Substituting \(t = \frac{1}{L}\), we verify:</p> \[t - \frac{L t^2}{2} = \frac{1}{L} - \frac{L}{2} \cdot \frac{1}{L^2} = \frac{1}{L} - \frac{1}{2L} = \frac{1}{2L}.\] <p>Thus, with \(t \leq \frac{1}{L}\), the function value strictly decreases:</p> \[f(x^+) \leq f(x) - \frac{t}{2} \|\nabla f(x)\|_2^2.\] <h6 id="step-5-bounding-fx---fx"><strong>Step 5: Bounding \(f(x^+) - f(x^*)\)</strong></h6> <p>From the convexity of \(f\), we know:</p> \[f(x^*) \geq f(x) + \nabla f(x)^T (x^* - x).\] <p>Rearranging: \(f(x) \leq f(x^*) + \nabla f(x)^T (x - x^*).\)</p> <p>Substituting this into the inequality for \(f(x^+)\):</p> \[f(x^+) \leq f(x^*) + \nabla f(x)^T (x - x^*) - \frac{t}{2} \|\nabla f(x)\|_2^2.\] <p>Rearranging terms:</p> \[f(x^+) - f(x^*) \leq \frac{1}{2t} \left( \|x - x^*\|_2^2 - \|x^+ - x^*\|_2^2 \right).\] <p>This shows how the objective value at \(x^+\) is related to the distance between \(x\) and the optimal solution \(x^*\).</p> <h6 id="step-6-summing-over-k-iterations"><strong>Step 6: Summing Over \(k\) Iterations</strong></h6> <p>Let \(x^{(i)}\) denote the iterate after \(i\) steps. Applying the inequality iteratively, we have:</p> \[f(x^{(i)}) - f(x^*) \leq \frac{1}{2t} \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right).\] <p>Summing over \(i = 1, 2, \dots, k\):</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \sum_{i=1}^k \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right).\] <h6 id="step-7-telescoping-sum"><strong>Step 7: Telescoping Sum</strong></h6> <p>The terms on the right-hand side form a telescoping sum:</p> \[\sum_{i=1}^k \left( \|x^{(i-1)} - x^*\|_2^2 - \|x^{(i)} - x^*\|_2^2 \right) = \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2.\] <p>Thus, we have:</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Since \(f(x^{(i)})\) is decreasing with each iteration, the largest term dominates the average:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right).\] <p>But, why is the above inequality right? Let’s find out:</p> <p>The inequality</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right)\] <p>is derived based on the property that \(f(x^{(i)})\) is <strong>monotonically decreasing</strong> during gradient descent. Let’s break it down step by step.</p> <ul> <li><strong>Key Property: Monotonic Decrease</strong>: In gradient descent, the function value decreases with each iteration due to the fixed step size \(t \leq \frac{1}{L}\). This means:</li> </ul> \[f(x^{(1)}) \geq f(x^{(2)}) \geq \cdots \geq f(x^{(k)}).\] <p>Thus, the latest value \(f(x^{(k)})\) is the smallest among all iterations.</p> <ul> <li><strong>Averaging the Function Values</strong>: The sum of the differences \(f(x^{(i)}) - f(x^*)\) over all \(k\) iterations can be written as:</li> </ul> \[\frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right),\] <p>which represents the average difference between the function values at each iteration and the optimal value \(f(x^*)\).</p> <ul> <li><strong>Bounding the Smallest Term by the Average</strong>: Since \(f(x^{(k)})\) is the smallest value (due to monotonic decrease), it cannot exceed the average value. In mathematical terms:</li> </ul> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right).\] <ul> <li> <p><strong>Intuition Behind the Inequality</strong>: This inequality reflects a simple fact: the smallest value in a decreasing sequence of numbers is less than or equal to their average. For example, if we have values \(10, 8, 7, 6\), the smallest value (6) will always be less than or equal to the average of these values.</p> </li> <li> <p><strong>Significance in Gradient Descent</strong>: This inequality is important because it allows us to bound the final iterate \(f(x^{(k)})\) using the sum of all previous iterations.</p> </li> </ul> <h6 id="step-8-final-substitution-to-derive-the-convergence-result"><strong>Step 8: Final Substitution to Derive the Convergence Result</strong></h6> <p>From the telescoping sum, we have:</p> \[\sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right) \leq \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Using the inequality:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \sum_{i=1}^k \left( f(x^{(i)}) - f(x^*) \right),\] <p>we substitute the bound on the sum into this expression:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{1}{k} \cdot \frac{1}{2t} \left( \|x^{(0)} - x^*\|_2^2 - \|x^{(k)} - x^*\|_2^2 \right).\] <p>Since \(\|x^{(k)} - x^*\|_2^2 \geq 0\), we drop this term to get the worst-case bound:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2tk}.\] <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>We have derived the convergence guarantee for gradient descent with a fixed step size \(t \leq \frac{1}{L}\). The final result:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t k},\] <p>shows that the function value \(f(x^{(k)})\) decreases towards the optimal value \(f(x^*)\) at a rate proportional to \(O(1/k)\). This rate depends on the step size \(t\) and the initial distance \(\|x^{(0)} - x^*\|_2^2\).</p> <p>The result highlights that gradient descent converges reliably under the conditions of convexity, differentiability, and Lipschitz continuity of the gradient. As \(k \to \infty\), the function value approaches the optimal value, demonstrating the effectiveness of gradient descent for optimization problems with these properties.</p> <hr/> <p>Next,</p> <ul> <li>Convergence of gradient descent with adaptive step size</li> <li>Strongly convex - “linear convergence” rate</li> </ul> <h4 id="convergence-of-gradient-descent-with-adaptive-step-size"><strong>Convergence of gradient descent with adaptive step size</strong></h4> <p>In the above section, we derived the convergence rate for gradient descent with a <strong>fixed step size</strong>. In this part, we extend this analysis to the case where the step size is chosen adaptively using a <strong>backtracking line search</strong>. This method ensures that the step size decreases as necessary to guarantee sufficient decrease in the objective function at each iteration.</p> <h6 id="step-1-setup-and-assumptions"><strong>Step 1: Setup and Assumptions</strong></h6> <p>Consider a differentiable convex function \(f: \mathbb{R}^n \to \mathbb{R}\) with a <strong>Lipschitz continuous gradient</strong>. That is, for any two points \(x, y \in \mathbb{R}^n\),</p> \[\|\nabla f(x) - \nabla f(y)\|_2 \leq L \|x - y\|_2,\] <p>where \(L\) is the <strong>Lipschitz constant</strong> of the gradient.</p> <p>Let \(x^*\) be the minimizer of \(f\), and let \(x^{(i)}\) represent the iterates of gradient descent. The update rule for gradient descent with backtracking line search is:</p> \[x^{(i+1)} = x^{(i)} - t_i \nabla f(x^{(i)}),\] <p>where \(t_i\) is the step size at iteration \(i\), chosen adaptively using the backtracking procedure.</p> <h6 id="step-2-descent-lemma"><strong>Step 2: Descent Lemma</strong></h6> <p>In the case of gradient descent with a <strong>fixed step size</strong> \(t\), we know from the <strong>descent lemma</strong> (for smooth convex functions) that:</p> \[f(x^{(i+1)}) \leq f(x^{(i)}) - t \|\nabla f(x^{(i)})\|_2^2 + \frac{L}{2} t^2 \|\nabla f(x^{(i)})\|_2^2.\] <p>This inequality states that at each iteration, the function value decreases by a term proportional to the gradient’s squared norm, and this decrease depends on the step size \(t\).</p> <h6 id="step-3-backtracking-line-search"><strong>Step 3: Backtracking Line Search</strong></h6> <p>With <strong>backtracking line search</strong>, the step size \(t_i\) is chosen at each iteration to ensure sufficient decrease in the function value. Specifically, the step size is selected such that:</p> \[f(x^{(i+1)}) \leq f(x^{(i)}) + \alpha t_i \nabla f(x^{(i)})^T \nabla f(x^{(i)}),\] <p>where \(0 &lt; \alpha &lt; 1\) is a constant. The backtracking line search ensures that \(t_i\) satisfies the condition:</p> \[t_i \leq \frac{1}{L}.\] <p>Thus, the step size at each iteration is bounded by \(\frac{1}{L}\), which prevents the gradient from changing too rapidly and ensures that the update does not overshoot the optimal point.</p> <p><strong>Why “Adaptive”?</strong></p> <p>The step size is called <strong>adaptive</strong> because it changes at each iteration depending on the function’s behavior. If the function is steep or the gradient is large, the backtracking line search may choose a smaller step size to avoid overshooting. If the function is shallow or the gradient is small, it might allow a larger step size. This adaptive process uses a parameter \(\beta\) to control how the step size is reduced when the decrease condition is not met.</p> <h6 id="step-4-backtracking-process-and-beta"><strong>Step 4: Backtracking Process and \(\beta\)</strong></h6> <p>The process of backtracking works as follows:</p> <ul> <li> <p><strong>Initial Step Size</strong>: Start with an initial guess for the step size, typically \(t_0 = 1\).</p> </li> <li> <p><strong>Condition Check</strong>: Check whether the condition</p> </li> </ul> \[f(x^{(i+1)}) \leq f(x^{(i)}) + \alpha t_i \nabla f(x^{(i)})^T \nabla f(x^{(i)})\] <p>holds. If it does, accept \(t_i\); if not, reduce the step size.</p> <ul> <li><strong>Reduce Step Size</strong>: If the condition is not satisfied, reduce the step size \(t_i\) by a factor \(\beta\):</li> </ul> \[t_{i+1} = \beta t_i,\] <p>where \(\beta\) is a constant between 0 and 1 (usually around 0.5 or 0.8). This step size reduction continues until the condition is met.</p> <ul> <li><strong>Accept the Step Size</strong>: Once the condition is satisfied, the current \(t_i\) is accepted for the update.</li> </ul> <p>The use of \(\beta\) helps to ensure that the step size does not become too large, allowing the algorithm to converge smoothly without overshooting.</p> <h6 id="step-5-bounding-the-convergence"><strong>Step 5: Bounding the Convergence</strong></h6> <p>Now, let’s derive the convergence bound for gradient descent with backtracking line search. From the descent lemma, the change in the function value at each iteration can be bounded as:</p> \[f(x^{(i+1)}) - f(x^{(i)}) \leq - t_i \|\nabla f(x^{(i)})\|_2^2 \left( 1 - \frac{L}{2} t_i \right).\] <p>Because the backtracking line search ensures that \(t_i \leq t_{\text{min}} = \min\left( 1, \frac{\beta}{L} \right)\), we can bound the function value decrease as:</p> \[f(x^{(i+1)}) - f(x^{(i)}) \leq - t_{\text{min}} \|\nabla f(x^{(i)})\|_2^2 \left( 1 - \frac{L}{2} t_{\text{min}} \right).\] <p>This shows that the function value decreases at each iteration, with the step size \(t_{\text{min}}\) controlling the rate of decrease.</p> <p>Now, if you observe carefully, the equation above closely resembles the one we encountered in the fixed step size proof. The only minor difference is that \(t\) has been replaced with \(t_{\text{min}}\). Therefore, we can follow the same steps as in the fixed step size case and eventually arrive at the following result:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|_2^2}{2 t_{\text{min}} k}.\] <p>This shows that by adaptively choosing the step size, we can achieve a convergence rate similar to that of the fixed step size approach, but without needing to manually set a fixed value for ( t ).</p> <p><strong>Quick Note:</strong> I’m still not completely satisfied with the proof for Adaptive Step Size. I’ll be working on refining the explanation further and will update you with any improvements.</p> <h5 id="and-finally"><strong>And finally…</strong></h5> <p>We’ve reached the end of this blog post! A huge kudos to you for making it all the way through and sticking with me. The reason we went through all of this is that understanding such proofs will lay the foundation for exploring the intricate details that drive machine learning and produce its remarkable results. To truly dive into ML research, we need to immerse ourselves in these depths and make it happen.</p> <p>So, take a well-deserved break, and in the next post, we’ll delve into the tips and tricks of SGD that are widely practiced in the industry. Until then, take care and see you soon!</p> <h5 id="references"><strong>References:</strong></h5> <ul> <li><a href="https://nyu-cs2565.github.io/mlcourse-public/2024-fall/lectures/lec02/gradient_descent_converge.pdf"> Gradient Descent: Convergence Analysis - Ryan Tibshirani</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.]]></summary></entry><entry><title type="html">Understanding Stochastic Gradient Descent (SGD)</title><link href="https://monishver11.github.io/blog/2024/SGD/" rel="alternate" type="text/html" title="Understanding Stochastic Gradient Descent (SGD)"/><published>2024-12-28T03:42:00+00:00</published><updated>2024-12-28T03:42:00+00:00</updated><id>https://monishver11.github.io/blog/2024/SGD</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/SGD/"><![CDATA[<p>In the last blog, we introduced <strong>Stochastic Gradient Descent (SGD)</strong> as a computationally efficient optimization method. In this post, we’ll dive deeper into the mechanics of SGD, exploring its nuances, trade-offs, and how it compares to other gradient descent variants. Let’s unravel the details and gain a comprehensive understanding of these optimization techniques.</p> <h4 id="noisy-gradient-descent"><strong>“Noisy” Gradient Descent</strong></h4> <p>Instead of computing the exact gradient at every step, <strong>noisy gradient descent</strong> estimates the gradient using random subsamples. Surprisingly, this approximation often works well.</p> <p><strong>Why Does It Work?</strong></p> <p>Gradient descent is inherently iterative, meaning it has the chance to recover from previous missteps at each step. Leveraging noisy estimates can speed up the process without significantly impacting the final results.</p> <hr/> <h4 id="mini-batch-gradient-descent"><strong>Mini-batch Gradient Descent</strong></h4> <p>The <strong>full gradient</strong> for a dataset \(D_n = (x_1, y_1), \dots, (x_n, y_n)\) is given by:</p> \[\nabla \hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \nabla_w \ell(f_w(x_i), y_i)\] <p>This requires the entire dataset, which can be computationally expensive. To mitigate this, we use a <strong>mini-batch</strong> of size \(N\), a random subset of the data:</p> \[\nabla \hat{R}_N(w) = \frac{1}{N} \sum_{i=1}^N \nabla_w \ell(f_w(x_{m_i}), y_{m_i})\] <p>Here, \((x_{m_1}, y_{m_1}), \dots, (x_{m_N}, y_{m_N})\) is the mini-batch.</p> <hr/> <h4 id="batch-vs-stochastic-methods"><strong>Batch vs. Stochastic Methods</strong></h4> <p><strong>Rule of Thumb:</strong></p> <ul> <li><strong>Stochastic methods</strong> perform well far from the optimum but struggle as we approach it.</li> <li><strong>Batch methods</strong> excel near the optimum due to more precise gradient calculations.</li> </ul> <h4 id="mini-batch-gradient-properties"><strong>Mini-batch Gradient Properties</strong></h4> <ul> <li>The mini-batch gradient is an <strong>unbiased estimator</strong> of the full gradient, meaning on average, the gradient computed using a minibatch (a small, random subset of the dataset) gives the same direction of descent as the gradient computed using the entire dataset.</li> </ul> \[\mathbb{E}[\nabla \hat{R}_N(w)] = \nabla \hat{R}_n(w)\] <ul> <li> <p>This implies that while individual minibatch gradients may vary due to the randomness of the sample, their expected value matches the full batch gradient. This property allows Stochastic Gradient Descent (SGD) to make consistent progress toward the optimum without requiring computation over the entire dataset in each iteration.</p> </li> <li> <p>Larger mini-batches result in better estimates but are slower to compute:</p> </li> </ul> \[\text{Var}[\nabla \hat{R}_N(w)] = \frac{1}{N} \text{Var}[\nabla \hat{R}_i(w)]\] <ul> <li>This is because averaging over more samples reduces randomness. Specifically, the variance is scaled by \(1/𝑁\), meaning larger minibatches produce more accurate and stable gradient estimates, closer to the full batch gradient.</li> </ul> <p><strong>Tradeoffs of minibatch size:</strong></p> <ul> <li><strong>Larger \(N\):</strong> Better gradient estimate, slower computation.</li> <li><strong>Smaller \(N\):</strong> Faster computation, noisier gradient estimates.</li> </ul> <hr/> <h4 id="convergence-of-sgd"><strong>Convergence of SGD</strong></h4> <p>To ensure convergence, <strong>diminishing step sizes</strong> like \(\eta_k = 1/k\) are often used. While gradient descent (GD) theoretically converges faster than SGD:</p> <ul> <li><strong>GD</strong> is efficient near the minimum due to higher accuracy.</li> <li><strong>SGD</strong> is more practical for large-scale problems where high accuracy is unnecessary.</li> </ul> <p>In practice, SGD with <strong>fixed step sizes</strong> works well and can be adjusted using techniques like <strong>staircase decay</strong> or <strong>inverse time decay</strong> (\(1/t\)).</p> <h6 id="sgd-algorithm-with-mini-batches"><strong>SGD Algorithm with Mini-batches</strong></h6> <ol> <li>Initialize \(w = 0\).</li> <li>Repeat: <ul> <li>Randomly sample \(N\) points from \(D_n\): \({(x_i, y_i)}_{i=1}^N\).</li> <li>Update weights:</li> </ul> \[w \leftarrow w - \eta \left( \frac{1}{N} \sum_{i=1}^N \nabla_w \ell(f_w(x_i), y_i) \right)\] </li> </ol> <hr/> <h5 id="why-diminishing-step-sizes-theoretical-aspects"><strong>Why Diminishing Step Sizes? (Theoretical Aspects)</strong></h5> <p>If \(f\) is \(L\)-smooth and convex, and the variance of \(\nabla f(x^{(k)})\) is bounded</p> \[\text{Var}(\nabla f(x^{(k)})) \leq \sigma^2\] <p>, then SGD with step size</p> \[\eta \leq \frac{1}{L}\] <p>satisfies:</p> \[\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2] \leq \frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k} + \frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\] <p><strong>Breaking it Down:</strong></p> <ol> <li><strong>L-Smooth and Convex Function</strong>: <ul> <li>A function ( f ) is <strong>smooth</strong> if its gradient doesn’t change too rapidly. Specifically, an \(L\)-smooth function means that the gradient’s rate of change is bounded by a constant \(L\).</li> <li>A <strong>convex</strong> function means that it has a single global minimum, making optimization easier because we don’t have to worry about getting stuck in local minima.</li> </ul> </li> <li><strong>Variance of Gradient</strong>: <ul> <li>The gradient at each step of SGD might not be exact. The variance \(\text{Var}(\nabla f(x^{(k)}))\) measures the “noise” or fluctuations in the gradient estimate. A smaller variance means the gradient is more stable.</li> </ul> </li> </ol> <p><strong>What Does the Formula Mean?</strong></p> <p>The formula provides an upper bound on the expected squared magnitude of the gradient:</p> \[\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2] \leq \frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k} + \frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\] <ul> <li> <p><strong>Left Side</strong>: \(\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2]\) represents the minimum expected squared gradient magnitude. A smaller value indicates that the gradient is approaching zero, meaning we’re getting closer to the optimal solution.</p> </li> <li> <p><strong>Right Side</strong>:</p> <ul> <li>The first term \(\frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k}\) reflects how the gap between the initial point \(x^{(0)}\) and the optimal solution \(x^*\) decreases over time. The more steps we take (i.e., the larger the sum of the step sizes \(\eta_k\)), the smaller the gap becomes.</li> <li>The second term \(\frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\) accounts for the variance in the gradient. If the step size doesn’t decrease over time, this variance term grows, which can destabilize the optimization process. The numerator \(\sum_k \eta_k^2\) grows faster than the denominator \(\sum_k \eta_k\), so increasing step sizes overall increases the second term. So, this term will dominate if the step size does not decrease.</li> </ul> </li> </ul> <p><strong>Intuition Behind Diminishing Step Sizes:</strong></p> <ul> <li> <p><strong>Without diminishing step sizes</strong> - If you keep taking large steps, especially when close to the minimum, you risk overshooting the optimal solution. Large gradients or noisy estimates can lead to erratic behavior.</p> </li> <li> <p><strong>With diminishing step sizes</strong> - As we get closer to the minimum, reducing the step size helps take smaller, more controlled steps. This reduces the variance (noise) in the gradient and makes the convergence process smoother and more stable.</p> </li> </ul> <p><strong>So, now why diminish step sizes?</strong></p> <p>Diminishing step sizes are important because:</p> <ul> <li>Early on, larger steps help explore the solution space and make significant progress.</li> <li>As you approach the optimal solution, smaller steps are needed to fine-tune the result and avoid overshooting. This balance helps the optimization process converge efficiently while maintaining stability.</li> </ul> <p>More on the mathematical details of convergence will be covered in a separate blog post. For now, the key intuition to keep in mind is that diminishing step sizes help strike a balance between exploration (larger steps) and stability (smaller steps), leading to smoother convergence.</p> <hr/> <h4 id="summary"><strong>Summary</strong></h4> <p>Gradient descent variants provide trade-offs in speed, accuracy, and computational cost:</p> <ul> <li><strong>Full-batch gradient descent:</strong> Uses the entire dataset for gradient computation, yielding precise updates but high computational cost.</li> <li><strong>Mini-batch gradient descent:</strong> Balances computational efficiency and gradient accuracy by using subsets of data.</li> <li><strong>Stochastic gradient descent (SGD):</strong> Uses a single data point (\(N = 1\)) for updates, making it highly efficient but noisy.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GD_Variations.webp" sizes="95vw"/> <img src="/assets/img/GD_Variations.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GD_Variations" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Batch Vs Stochastic Vs Mini-Batch GD </div> <p>When referring to SGD, always clarify the batch size to avoid ambiguity. Modern machine learning heavily relies on SGD due to its time and memory efficiency, especially for large-scale problems.</p> <hr/> <h5 id="example-logistic-regression-with-ell_2-regularization"><strong>Example: Logistic Regression with \(\ell_2\)-Regularization</strong></h5> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_1-480.webp 480w,/assets/img/SGD_Comp_1-800.webp 800w,/assets/img/SGD_Comp_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SGD_Comp_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_2-480.webp 480w,/assets/img/SGD_Comp_2-800.webp 800w,/assets/img/SGD_Comp_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SGD_Comp_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_3-480.webp 480w,/assets/img/SGD_Comp_3-800.webp 800w,/assets/img/SGD_Comp_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/SGD_Comp_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Convergence Speed(1), Computational Efficiency(2) and Rate at Near Optimum(3) of different GD approaches </div> <ul> <li><strong>Batch methods:</strong> Converge faster near the optimum.</li> <li><strong>Stochastic methods:</strong> Are computationally efficient, especially for large datasets.</li> </ul> <p>Understanding these trade-offs helps in choosing the right approach for different scenarios.</p> <hr/> <p>In the next blog, we’ll explore <strong>Gradient Descent Convergence Theorems</strong> and how to intuitively make sense out of it! See you.</p> <h6 id="image-credits"><strong>Image Credits:</strong></h6> <ul> <li><a href="https://alwaysai.co/blog/what-is-gradient-descent">Batch Vs Stochastic Vs Mini-Batch GD</a></li> <li><a href="https://www.stat.berkeley.edu/~ryantibs/">Example from Ryan Tibshirani</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).]]></summary></entry><entry><title type="html">Gradient Descent - A Detailed Walkthrough</title><link href="https://monishver11.github.io/blog/2024/gradient-descent/" rel="alternate" type="text/html" title="Gradient Descent - A Detailed Walkthrough"/><published>2024-12-25T19:01:00+00:00</published><updated>2024-12-25T19:01:00+00:00</updated><id>https://monishver11.github.io/blog/2024/gradient-descent</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/gradient-descent/"><![CDATA[<p>In our last blog post, we discussed Empirical Risk Minimization (ERM). Let’s build on that foundation by exploring a concrete example: <strong>Linear Least Squares Regression</strong>. This will help us understand how gradient descent fits into the optimization landscape.</p> <h4 id="linear-least-squares-regression"><strong>Linear Least Squares Regression</strong></h4> <p><strong>Problem Setup</strong></p> <p>We aim to minimize the empirical risk using linear regression. Here’s the setup:</p> <ul> <li><strong>Loss function</strong>: \(\ell(\hat{y}, y) = (\hat{y} - y)^2\)</li> <li><strong>Hypothesis space</strong>: \(\mathcal{F} = \{ f : \mathbb{R}^d \to \mathbb{R} \mid f(x) = w^\top x, \, w \in \mathbb{R}^d \}\)</li> <li><strong>Data set</strong>: \(\mathcal{D}_n = \{(x_1, y_1), \dots, (x_n, y_n)\}\) Our goal is to find the ERM solution: \(\hat{f} \in \mathcal{F}\). <strong>Objective Function</strong></li> </ul> <p>We want to find the function in \(\mathcal{F}\), parametrized by \(w \in \mathbb{R}^d\), that minimizes the empirical risk:</p> \[\hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n (w^\top x_i - y_i)^2\] <p>This leads to the optimization problem: \(\min_{w \in \mathbb{R}^d} \hat{R}_n(w)\)</p> <p>Although ordinary least squares (OLS) offers a closed-form solution (<strong>refer to the provided resource to understand OLS</strong>), gradient descent proves more versatile, particularly when closed-form solutions are not feasible.</p> <hr/> <h3 id="gradient-descent"><strong>Gradient Descent</strong></h3> <p>Gradient descent is a powerful optimization technique for unconstrained problems.</p> <h4 id="unconstrained-optimization-setting"><strong>Unconstrained Optimization Setting</strong></h4> <p>We assume the objective function \(f : \mathbb{R}^d \to \mathbb{R}\) is <strong>differentiable</strong>, and we aim to find:</p> \[x^* = \arg \min_{x \in \mathbb{R}^d} f(x)\] <h4 id="the-gradient"><strong>The Gradient</strong></h4> <p>The gradient is a fundamental concept in optimization. For a differentiable function \(f : \mathbb{R}^d \to \mathbb{R}\), the gradient at a point \(x_0 \in \mathbb{R}^d\) is denoted by \(\nabla f(x_0)\). It represents the vector of partial derivatives of \(f\) with respect to each dimension of \(x\):</p> \[\nabla f(x_0) = \left[ \frac{\partial f}{\partial x_1}(x_0), \frac{\partial f}{\partial x_2}(x_0), \dots, \frac{\partial f}{\partial x_d}(x_0) \right]\] <h6 id="key-points"><strong>Key points:</strong></h6> <ul> <li>The gradient points in the direction of the steepest <strong>increase</strong> of the function \(f(x)\) starting from \(x_0\).</li> <li>The <strong>magnitude</strong> of the gradient indicates how steep the slope is in that direction.</li> </ul> <p>For example, consider a 2D function \(f(x, y)\). The gradient at a point \((x_0, y_0)\) is:</p> \[\nabla f(x_0, y_0) = \left[ \frac{\partial f}{\partial x}(x_0, y_0), \frac{\partial f}{\partial y}(x_0, y_0) \right]\] <p>This tells us how \(f\) changes with respect to \(x\) and \(y\) near \((x_0, y_0)\).</p> <h6 id="importance-in-optimization"><strong>Importance in Optimization</strong></h6> <p>The gradient is crucial because it provides the direction in which the function \(f(x)\) increases most rapidly. To minimize \(f(x)\):</p> <ul> <li>We move in the <strong>opposite</strong> direction of the gradient, as this is where the function decreases most rapidly.</li> </ul> <h6 id="geometric-interpretation"><strong>Geometric Interpretation</strong></h6> <p>Imagine a 3D surface representing a function \(f(x, y)\). At any point on the surface:</p> <ul> <li>The gradient vector points <strong>uphill</strong>, perpendicular to the contour lines (or level curves) of the function.</li> <li>To find a minimum, we “descend” by moving in the <strong>opposite direction</strong> of the gradient vector.</li> </ul> <p>This understanding lays the foundation for applying gradient descent effectively in optimization problems.</p> <hr/> <h3 id="gradient-descent-algorithm"><strong>Gradient Descent Algorithm</strong></h3> <p>To iteratively minimize \(f(x)\), follow these steps:</p> <ol> <li><strong>Initialize</strong>: \(x \leftarrow 0\)</li> <li><strong>Repeat</strong>: \(x \leftarrow x - \eta \nabla f(x)\) <ul> <li>until a stopping criterion is met.</li> </ul> </li> </ol> <p>Here, \(\eta\) is the <strong>step size</strong> (or <strong>learning rate</strong>). Choosing \(\eta\) appropriately is critical to avoid divergence or slow convergence. “Step size” is also referred to as “learning rate” in neural networks literature.</p> <div align="center"> <img src="https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif" alt="Gradient Descent GIF" width="500"/> <p>Path of a Gradient Descent Algorithm</p> </div> <h4 id="insights-into-gradient-descent"><strong>Insights into Gradient Descent</strong></h4> <h6 id="step-size"><strong>Step Size</strong></h6> <ul> <li><strong>Fixed step size</strong>: Works if small enough.</li> <li>If \(\eta\) is too large, the process might diverge.</li> <li>Experimenting with multiple step sizes is often necessary.</li> <li>Big vs. Small Steps: <ul> <li><strong>Big steps</strong>: In flat regions where the gradient is small, larger steps accelerate convergence.</li> <li><strong>Small steps</strong>: In steep regions where the gradient is large, smaller steps ensure stability and prevent overshooting.</li> <li>Adaptive methods like Adam or RMSprop leverage this intuition by dynamically adjusting the step size based on the gradient’s magnitude or past behavior.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GD_Learning_Rate-480.webp 480w,/assets/img/GD_Learning_Rate-800.webp 800w,/assets/img/GD_Learning_Rate-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/GD_Learning_Rate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GD_Learning_Rate" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Step size variations. Image credit: https://medium.com/@yennhi95zz </div> <h6 id="convergence"><strong>Convergence</strong></h6> <p>Gradient descent converges to a stationary point (where the derivative is zero) for differentiable functions. These stationary points could be:</p> <ul> <li>Local minima</li> <li>Local maxima</li> <li>Saddle points</li> </ul> <p>For convex functions(Added Reference), gradient descent can converge to the global minimum.</p> <hr/> <p>The following theorems are often overlooked when learning about gradient descent. We’ll dive into them in detail in a separate blog, but for now, give them a quick read and continue.</p> <h4 id="theorem-convergence-of-gradient-descent-with-fixed-step-size"><strong>Theorem: Convergence of Gradient Descent with Fixed Step Size</strong></h4> <p>Suppose \(f : \mathbb{R}^d \to \mathbb{R}\) is convex and differentiable, and \(\nabla f\) is Lipschitz continuous with constant \(L &gt; 0\) (i.e., \(f\) is L-smooth). This means:</p> \[\|\nabla f(x) - \nabla f(x')\| \leq L \|x - x'\|\] <p>for any \(x, x' \in \mathbb{R}^d\).</p> <p><strong>Result:</strong></p> <p>If gradient descent uses a fixed step size \(\eta \leq \frac{1}{L}\), then:</p> \[f(x^{(k)}) - f(x^*) \leq \frac{\|x^{(0)} - x^*\|^2}{2\eta k}\] <p><strong>Implications:</strong></p> <ul> <li>Gradient descent is <strong>guaranteed to converge</strong> under these conditions.</li> <li>The convergence rate is \(O(1/k)\)</li> </ul> <hr/> <h4 id="strongly-convex-functions"><strong>Strongly Convex Functions</strong></h4> <p>A function \(f\) is \(\mu\)-strongly convex if:</p> \[f(x') \geq f(x) + \nabla f(x) \cdot (x' - x) + \frac{\mu}{2} \|x - x'\|^2\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Strongly_Convex-480.webp 480w,/assets/img/Strongly_Convex-800.webp 800w,/assets/img/Strongly_Convex-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/Strongly_Convex.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Strongly_Convex" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Convex and Strongly Convex Curve </div> <h4 id="convergence-theorem-for-strongly-convex-functions"><strong>Convergence Theorem for Strongly Convex Functions</strong></h4> <p>If \(f\) is both \(L\)-smooth and \(\mu\)-strongly convex, with step size \(0 &lt; \eta \leq \frac{1}{L}\), then gradient descent achieves convergence with the following inequality:</p> \[\|x^{(k)} - x^*\|^2 \leq (1 - \eta \mu)^k \|x^{(0)} - x^*\|^2\] <p>This implies <strong>linear convergence</strong>, but it depends on \(\mu\). If the estimate of µ is bad then the rate is not great.</p> <hr/> <h4 id="stopping-criterion"><strong>Stopping Criterion</strong></h4> <ul> <li>Stop when \(\|\nabla f(x)\|_2 \leq \epsilon\), where \(\epsilon\) is a small threshold(of our choice). <strong>Why?</strong> At a local minimum, \(\nabla f(x) = 0\). If the gradient becomes small and plateaus, further updates are unlikely to significantly reduce the objective function, so we can stop the gradient updates.</li> <li>Early Stopping <ul> <li>Evaluate the loss on validation data (unseen held-out data) after each iteration.</li> <li>Stop when the loss no longer improves or starts to worsen.</li> </ul> </li> </ul> <hr/> <h3 id="quick-recap-gradient-descent-for-erm"><strong>Quick recap: Gradient Descent for ERM</strong></h3> <p>Given a hypothesis space \(F = \{f_w : X \to Y \mid w \in \mathbb{R}^d\}\), we aim to minimize:</p> \[\hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \ell(f_w(x_i), y_i)\] <p>Gradient descent is applicable if \(\ell(f_w(x_i), y_i)\) is differentiable with respect to \(w\).</p> <h6 id="scalability"><strong>Scalability</strong></h6> <p>At each iteration, we compute the gradient at the current \(w\) as:</p> \[\nabla \hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \nabla_w \ell(f_w(x_i), y_i)\] <p>This requires \(O(n)\) computation per step, as we have to iterate over all n training points to take a single step. To scale better, alternative methods like <strong>stochastic gradient descent (SGD)</strong> can be considered.</p> <hr/> <p>Gradient descent is an indispensable tool for optimization, especially in machine learning. By understanding its principles, convergence properties, and practical considerations, we can effectively tackle a variety of optimization problems.</p> <p>Stay tuned for the next post, where we’ll explore stochastic gradient descent and its variations for scalability!</p> <hr/> <h5 id="references--good-resources-for-visualizing-gradient-descent"><strong>References &amp; Good Resources for Visualizing Gradient Descent:</strong></h5> <ul> <li><a href="https://aero-learn.imperial.ac.uk/vis/Machine%20Learning/gradient_descent_3d.html" target="_blank">3D Gradient Descent</a></li> <li><a href="https://kaggle.com/code/trolukovich/animating-gradien-descent" target="_blank">Animating Gradient Descent</a></li> <li><a href="https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c" target="_blank">A Visual Explanation of Gradient Descent Methods (Momentum, AdaGrad, RMSProp, Adam) – Towards Data Science</a> (Variations of Gradient Descent)</li> <li><a href="https://medium.com/@yennhi95zz/4-a-beginners-guide-to-gradient-descent-in-machine-learning-773ba7cd3dfe" target="_blank">Yennhi95zz, 2023. #4. A Beginner’s Guide to Gradient Descent in Machine Learning. Medium</a></li> <li><a href="https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif">Path of a Gradient Descent Algorithm</a> (Image Credit)</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[An in-depth exploration of gradient descent, including its convergence and step size considerations.]]></summary></entry><entry><title type="html">Empirical Risk Minimization (ERM)</title><link href="https://monishver11.github.io/blog/2024/ERM/" rel="alternate" type="text/html" title="Empirical Risk Minimization (ERM)"/><published>2024-12-24T23:50:00+00:00</published><updated>2024-12-24T23:50:00+00:00</updated><id>https://monishver11.github.io/blog/2024/ERM</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/ERM/"><![CDATA[<p>Continuing from our discussion of supervised learning, we now dive into <strong>Empirical Risk Minimization (ERM)</strong>. While the ultimate goal is to minimize the true risk, ERM provides a practical way to approximate this goal using finite data. Let’s break it down.</p> <hr/> <h3 id="definition-empirical-risk-minimization"><strong>Definition: Empirical Risk Minimization</strong></h3> <p>A function \(\hat{f}\) is an <strong>empirical risk minimizer</strong> if:</p> \[\hat{f} \in \arg\min_f \hat{R}_n(f),\] <p>where \(\hat{R}_n(f)\) is the empirical risk, defined over a finite sample and the minimum is taken over all functions\(f: X \to Y\). In an ideal scenario, we would want to minimize the true risk \(R(f)\). This raises a critical question:</p> <p><strong>Is the empirical risk minimizer close enough to the true risk minimizer?</strong></p> <h4 id="example-of-erm-in-action"><strong>Example of ERM in Action</strong></h4> <p>Let’s consider a simple case:</p> <ul> <li>Let \(P_X = \text{Uniform}[0,1]\) and \(Y \equiv 1\) (i.e., \(Y\) is always 1).</li> <li>A proposed prediction function:</li> </ul> \[\hat{f}(x) = \begin{cases} 1 &amp; \text{if } x \in \{0.25, 0.5, 0.75\}, \\ 0 &amp; \text{otherwise.} \end{cases}\] <p><strong>Loss Analysis:</strong></p> <p>Under both the square loss and the 0-1 loss:</p> <ul> <li><strong>Empirical Risk</strong>: 0</li> <li><strong>True Risk</strong>: 1</li> </ul> <p><strong>Explanation</strong>:</p> <ul> <li>The <strong>empirical risk</strong> measures the loss on the training data. Since \(\hat{f}(x)\) perfectly predicts the labels for the training points \(x \in \{0.25, 0.5, 0.75\}\), the empirical risk is zero. There are no errors on the observed data points. - The <strong>true risk</strong>, however, measures the loss over the entire distribution of \(P_X\). For all \(x \notin \{0.25, 0.5, 0.75\}\), \(\hat{f}(x)\) incorrectly predicts 0 instead of the correct label 1, resulting in significant errors. Since \(P_X\) is uniform over \([0,1]\), this means \(\hat{f}(x)\) is incorrect for all others data points of the domain, leading to a true risk of 1.</li> </ul> <p>This illustrates a key problem: <strong>ERM can lead to overfitting by simply memorizing the training data.</strong></p> <hr/> <h3 id="generalization-improving-erm"><strong>Generalization: Improving ERM</strong></h3> <p>In the above example, ERM failed to generalize to unseen data. To improve generalization, we must “smooth things out”—a process that spreads and extrapolates information from observed parts of the input space \(X\) to unobserved parts.</p> <p>One solution is <strong>Constrained ERM</strong>: Instead of minimizing empirical risk over all possible functions, we restrict our search to a subset of functions, known as a <strong>hypothesis space</strong>.</p> <h4 id="hypothesis-spaces"><strong>Hypothesis Spaces</strong></h4> <p>A <strong>hypothesis space</strong> \(\mathcal{F}\) is a set of prediction functions mapping \(X \to Y\) that we consider when applying ERM.</p> <p><strong>Desirable Properties of a Hypothesis Space</strong></p> <ul> <li>Includes only functions with the desired “regularity” (e.g., smoothness or simplicity).</li> <li>Is computationally tractable (efficient algorithms exist for finding the best function in \(\mathcal{F}\)).</li> </ul> <p>In practice, much of machine learning involves designing appropriate hypothesis spaces for specific tasks.</p> <h4 id="constrained-erm"><strong>Constrained ERM</strong></h4> <p>Given a hypothesis space \(\mathcal{F}\), the empirical risk minimizer in \(\mathcal{F}\) is defined as:</p> \[\hat{f}_n \in \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i),\] <p>where \(\ell(f(x), y)\) is the loss function.</p> <p>Similarly, the true risk minimizer in \(\mathcal{F}\) is:</p> \[f^*_{\mathcal{F}} \in \arg\min_{f \in \mathcal{F}} \mathbb{E}[\ell(f(x), y)].\] <hr/> <h3 id="excess-risk-decomposition"><strong>Excess Risk Decomposition</strong></h3> <p>We analyze the performance of ERM through <strong>excess risk decomposition</strong>, which breaks down the gap between the true risk(e.g., the function returned by ERM) and the risk of the Bayes optimal function:</p> <p><strong>Again, Definitions</strong></p> <ul> <li> <p><strong>Bayes Optimal Function</strong>:</p> \[f^* = \arg\min_f \mathbb{E}[\ell(f(x), y)]\] </li> <li> <p><strong>Risk Minimizer in \(\mathcal{F}\)</strong>:</p> \[f_{\mathcal{F}} = \arg\min_{f \in \mathcal{F}} \mathbb{E}[\ell(f(x), y)]\] </li> <li> <p><strong>ERM Solution</strong>:</p> \[\hat{f}_n = \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)\] </li> </ul> <h4 id="excess-risk-decomposition-for-erm"><strong>Excess Risk Decomposition for ERM</strong></h4> <p><strong>Definition</strong></p> <p>The <strong>excess risk</strong> measures how much worse the risk of a function \(f\) is compared to the risk of the Bayes optimal function \(f^*\), which minimizes the true risk. Mathematically, it is defined as:</p> \[\text{Excess Risk}(f) = R(f) - R(f^*)\] <p>where:</p> <ul> <li>\(R(f)\) is the true risk of the function \(f\).</li> <li>\(R(f^*)\) is the Bayes risk, i.e., the lowest achievable risk.</li> </ul> <p><strong>Can Excess Risk Be Negative?</strong> No, the excess risk can never be negative because the Bayes optimal function \(f^*\) achieves the minimum possible risk by definition. For any other function \(f\), the risk \(R(f)\) will be equal to or greater than \(R(f^*)\).</p> <h5 id="decomposition-of-excess-risk-for-erm"><strong>Decomposition of Excess Risk for ERM</strong></h5> <p>For the empirical risk minimizer \(\hat{f}_n\), the excess risk can be decomposed as follows:</p> \[\text{Excess Risk}(\hat{f}_n) = R(\hat{f}_n) - R(f^*) = \underbrace{R(\hat{f}_n) - R(f_\mathcal{F})}_{\text{Estimation Error}} + \underbrace{R(f_\mathcal{F}) - R(f^*)}_{\text{Approximation Error}}\] <p>where:</p> <ul> <li>\(f_\mathcal{F}\) is the best function within the chosen hypothesis space \(\mathcal{F}\).</li> <li><strong>Estimation Error</strong>: This term captures the error due to estimating the best function \(f_\mathcal{F}\) using finite training data.</li> <li><strong>Approximation Error</strong>: This term reflects the penalty for restricting the search to the hypothesis space \(\mathcal{F}\) instead of considering all possible functions.</li> </ul> <p><strong>Key Insight: Tradeoff Between Errors</strong> There is always a <strong>tradeoff</strong> between approximation and estimation errors:</p> <ul> <li>A larger hypothesis space \(\mathcal{F}\) reduces approximation error but increases estimation error (due to greater model complexity).</li> <li>A smaller hypothesis space \(\mathcal{F}\) reduces estimation error but increases approximation error (due to limited flexibility).</li> </ul> <p>This tradeoff is crucial when designing models and choosing hypothesis spaces.</p> <hr/> <h3 id="erm-in-practice"><strong>ERM in Practice</strong></h3> <p>In real-world machine learning, finding the exact ERM solution is challenging. We often settle for an approximate solution:</p> <ul> <li>Let \(\tilde{f}_n\) be the function returned by an optimization algorithm.</li> <li>The <strong>optimization error</strong> is:</li> </ul> \[\text{Optimization Error} = R(\tilde{f}_n) - R(\hat{f}_n)\] <p>where:</p> <ul> <li>\(\tilde{f}_n\)is the function returned by the optimization method.</li> <li>\(\hat{f}_n\)is the empirical risk minimizer.</li> </ul> <h5 id="practical-decomposition"><strong>Practical Decomposition</strong></h5> <p>For \(\tilde{f}_n\), the excess risk can be further decomposed as:</p> \[\text{Excess Risk}(\tilde{f}_n) = \underbrace{R(\tilde{f}_n) - R(\hat{f}_n)}_{\text{Optimization Error}} + \underbrace{R(\hat{f}_n) - R(f_{\mathcal{F}})}_{\text{Estimation Error}} + \underbrace{R(f_{\mathcal{F}}) - R(f^*)}_{\text{Approximation Error}}.\] <hr/> <h3 id="summary-erm-overview"><strong>Summary: ERM Overview</strong></h3> <p>To apply ERM in practice:</p> <ol> <li><strong>Choose a loss function</strong> \(\ell(f(x), y)\).</li> <li><strong>Define a hypothesis space</strong> \(\mathcal{F}\) that balances approximation and estimation error.</li> <li><strong>Use an optimization method</strong> to find \(\hat{f}_n = \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)\) (or an approximate solution \(\tilde{f}_n\)).</li> </ol> <p>As the size of training data increases, we can use larger hypothesis spaces to reduce approximation error while keeping estimation error manageable.</p> <hr/> <h3 id="conclusion"><strong>Conclusion</strong></h3> <p>Empirical Risk Minimization (ERM) provides a foundational framework for supervised learning by optimizing a model’s performance on training data. However, achieving a balance between approximation, estimation, and optimization errors is key to building effective models. This naturally raises the question: <strong>How do we efficiently minimize empirical risk in practice, especially for complex models and large datasets?</strong></p> <p>In the next blog, we’ll dive into <strong>Gradient Descent</strong>, one of the most powerful and widely used optimization algorithms for minimizing risk, and explore how it enables us to tackle the challenges of ERM. Stay tuned and see you! 👋</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.]]></summary></entry><entry><title type="html">Understanding the Supervised Learning Setup</title><link href="https://monishver11.github.io/blog/2024/supervised-learning/" rel="alternate" type="text/html" title="Understanding the Supervised Learning Setup"/><published>2024-12-24T15:35:00+00:00</published><updated>2024-12-24T15:35:00+00:00</updated><id>https://monishver11.github.io/blog/2024/supervised-learning</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/supervised-learning/"><![CDATA[<p>Supervised learning is a cornerstone of machine learning, enabling systems to learn from labeled data to make predictions or decisions. In this post, we will explore the various components and formalizations of supervised learning to build a solid foundation.</p> <h4 id="goals-in-supervised-learning"><strong>Goals in Supervised Learning</strong></h4> <p>In supervised learning problems, we typically aim to:</p> <ul> <li><strong>Make a decision:</strong> For instance, determining whether to move an email to a spam folder.</li> <li><strong>Take an action:</strong> As in self-driving cars, deciding when to make a right turn.</li> <li><strong>Reject a hypothesis:</strong> Such as testing the hypothesis that \(\theta = 0\) in classical statistics.</li> <li><strong>Produce some output:</strong> Examples include identifying whose face is in an image, translating a Japanese sentence into Hindi, or predicting the location of a storm an hour into the future.</li> </ul> <p>Each of these goals involves predicting or generating some form of output based on given inputs.</p> <h5 id="labels-the-key-to-supervised-learning"><strong>Labels: The Key to Supervised Learning</strong></h5> <p>Supervised learning involves pairing inputs with <strong>labels</strong>, which serve as the ground truth. Examples of labels include:</p> <ul> <li>Whether or not a picture contains an animal.</li> <li>The storm’s location one hour after a query.</li> <li>Which, if any, of the suggested URLs were selected.</li> </ul> <p>These labels allow us to evaluate the performance of our predictions systematically.</p> <hr/> <h4 id="evaluation-criterion"><strong>Evaluation Criterion</strong></h4> <p>The next step in supervised learning is finding <strong>optimal outputs</strong> under various definitions of optimality. Some examples of evaluation criteria include:</p> <ul> <li><strong>Classification Accuracy:</strong> Is the predicted class correct?</li> <li><strong>Exact Match:</strong> Does the transcription exactly match the spoken words?</li> <li><strong>Partial Credit:</strong> How do we account for partially correct answers (e.g., getting some words right)?</li> <li><strong>Prediction Distance:</strong> How far is the storm’s actual location from the predicted one?</li> <li><strong>Density Prediction:</strong> How likely is the storm’s actual location under the predicted distribution?</li> </ul> <p>These criteria ensure that we can quantitatively measure the performance of our models.</p> <hr/> <h4 id="typical-sequence-of-events"><strong>Typical Sequence of Events</strong></h4> <p>Supervised learning problems can often be formalized through the following sequence:</p> <ol> <li><strong>Observe Input (\(x\)):</strong> Receive an input data point.</li> <li><strong>Predict Output (\(\hat{y}\)):</strong> Use a prediction function to generate an output.</li> <li><strong>Observe Label (\(y\)):</strong> Compare the predicted output with the true label.</li> <li><strong>Evaluate Output:</strong> Assess the prediction’s quality based on the label.</li> </ol> <p>This sequence is at the heart of most supervised learning frameworks.</p> <hr/> <h4 id="formalizing-supervised-learning"><strong>Formalizing Supervised Learning</strong></h4> <p>A <strong>prediction function</strong> is a mathematical function \(f: X \to Y\) that takes an input \(x \in X\) and produces an output \(\hat{y} \in Y\).</p> <p>A <strong>loss function</strong> evaluates the discrepancy between the predicted output \(\hat{y}\) and the true outcome \(y\). It quantifies the “cost” of making incorrect predictions.</p> <h6 id="the-goal-optimal-prediction"><strong>The Goal: Optimal Prediction</strong></h6> <p>The primary goal is to find the <strong>optimal prediction function</strong>. The intuition is simple: If we can evaluate how good a prediction function is, we can turn this into an optimization problem.</p> <ul> <li>The loss function \(\ell\) evaluates a single output.</li> <li>To evaluate the prediction function as a whole, we need to formalize the concept of “average performance.”</li> </ul> <h6 id="data-generating-distribution"><strong>Data Generating Distribution</strong></h6> <p>Assume there exists a data-generating distribution \(P_{X \times Y}\). All input-output pairs \((x, y)\) are generated independently and identically distributed (i.i.d.) from this distribution.</p> <p>A common objective is to have a prediction function \(f(x)\) that performs well <strong>on average</strong>:</p> \[\ell(f(x), y)\] <p>is small, in some sense.</p> <h6 id="risk-definition"><strong>Risk Definition</strong></h6> <p>The <strong>risk</strong> of a prediction function \(f: X \to Y\) is defined as:</p> \[R(f) = \mathbb{E}_{(x, y) \sim P_{X \times Y}} [\ell(f(x), y)].\] <p>In words, this is the expected loss of \(f\) over the data-generating distribution \(P_{X \times Y}\). However, since we do not know \(P_{X \times Y}\), we cannot compute this expectation directly. Instead, we estimate it.</p> <hr/> <h4 id="the-bayes-prediction-function"><strong>The Bayes Prediction Function</strong></h4> <p><strong>Definition</strong></p> <p>The <strong>Bayes prediction function</strong> \(f^*: X \to Y\) achieves the minimal risk among all possible functions:</p> \[f^* \in \underset{f}{\text{argmin}} \ R(f),\] <p>where the minimum is taken over all functions from \(X\) to \(Y\).</p> <p><strong>Bayes Risk</strong></p> <p>The risk associated with the Bayes prediction function is called the <strong>Bayes risk</strong>. This function is often referred to as the “target function” because it represents the best possible predictor.</p> <hr/> <h4 id="example-multiclass-classification"><strong>Example: Multiclass Classification</strong></h4> <p>In multiclass classification, the output space is:</p> \[Y = \{1, 2, \dots, k\}.\] <p>The <strong>0-1 loss</strong> function is defined as:</p> \[\ell(\hat{y}, y) = \mathbb{1}[\hat{y} \neq y] := \begin{cases} 1 &amp; \text{if } \hat{y} \neq y, \\ 0 &amp; \text{otherwise.} \end{cases}\] <ul> <li>Here, \(\mathbb{1}[\hat{y} \neq y]\) is an <strong>indicator function</strong>. It returns a value of 1 when the condition \(\hat{y} \neq y\) is true (i.e., the prediction is incorrect) and 0 otherwise. This signifies whether the prediction is correct or incorrect and is commonly used to measure classification errors.</li> </ul> <p>The risk \(R(f)\) under the 0-1 loss can be expanded as follows:</p> \[R(f) = \mathbb{E}[\mathbb{1}[f(x) \neq y]] = \mathbb{P}(f(x) \neq y),\] <p>where:</p> <ul> <li>\(\mathbb{E}[\mathbb{1}[f(x) \neq y]]\) represents the expected value of the indicator function, which counts the proportion of incorrect predictions.</li> <li>\(\mathbb{P}(f(x) \neq y)\) is the probability of the prediction \(f(x)\) being different from the true label \(y\).</li> </ul> <p>Further, this can be rewritten using the decomposition of probabilities:</p> \[R(f) = 0 \cdot \mathbb{P}(f(x) = y) + 1 \cdot \mathbb{P}(f(x) \neq y),\] <p>which simplifies back to:</p> \[R(f) = \mathbb{P}(f(x) \neq y).\] <p><strong>Explanation</strong></p> <ul> <li>The term \(0 \cdot \mathbb{P}(f(x) = y)\) accounts for the cases where the prediction is correct (loss is 0).</li> <li>The term \(1 \cdot \mathbb{P}(f(x) \neq y)\) accounts for the cases where the prediction is incorrect (loss is 1).</li> </ul> <p>Thus, \(R(f)\) directly measures the <strong>misclassification error rate</strong>, which is the probability of the model making an incorrect prediction.</p> <p><strong>Bayes Prediction Function</strong></p> <p>The Bayes prediction function returns the most likely class:</p> \[f^*(x) \in \underset{1 \leq c \leq k}{\text{argmax}} \ P(y = c \mid x).\] <hr/> <h4 id="estimating-risk"><strong>Estimating Risk</strong></h4> <p>We cannot compute the true risk \(R(f) = \mathbb{E}[\ell(f(x), y)]\) because the true distribution \(P_{X \times Y}\) is unknown. However, we can estimate it.</p> <p>Assume we have sample data:</p> \[D_n = \{(x_1, y_1), \dots, (x_n, y_n)\},\] <p>where the samples are i.i.d. from \(P_{X \times Y}\). By the strong law of large numbers, the empirical average of losses converges to the expected value. If \(z_1, . . . , z_n\) are i.i.d. with expected value \(\mathbb{E}[z]\), then</p> \[\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n z_i = \mathbb{E}[z],\] <p>with probability 1.</p> <p>This leads us to the concept of <strong>empirical risk</strong> and its minimization, which we will explore in the next post.</p> <hr/> <p>In the next blog, we will dive into <strong>empirical risk minimization</strong> and how it helps solve supervised learning problems effectively.</p> <p><strong>Stay tuned!</strong></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.]]></summary></entry><entry><title type="html">Timeline of Machine Learning History</title><link href="https://monishver11.github.io/blog/2024/ml-history/" rel="alternate" type="text/html" title="Timeline of Machine Learning History"/><published>2024-12-23T21:59:00+00:00</published><updated>2024-12-23T21:59:00+00:00</updated><id>https://monishver11.github.io/blog/2024/ml-history</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/ml-history/"><![CDATA[<p>Machine learning has come a long way, evolving through decades of research and innovation. This timeline highlights the pivotal moments that have defined the field.</p> <h4 id="1910s1940s-early-computational-foundations"><strong>1910s–1940s: Early Computational Foundations</strong></h4> <ul> <li> <p><strong>1913:</strong> <code class="language-plaintext highlighter-rouge">Markov Chains</code> <br/> Andrey Markov introduces techniques later known as Markov chains, fundamental to many machine learning algorithms.</p> </li> <li> <p><strong>1936:</strong> <code class="language-plaintext highlighter-rouge">Turing's Theory of Computation</code><br/> Alan Turing proposes the theory of computation, forming the foundation for modern computing and machine learning.</p> </li> <li> <p><strong>1940:</strong> <code class="language-plaintext highlighter-rouge">ENIAC</code><br/> The first electronic general-purpose computer is created, paving the way for future computational advancements.</p> </li> <li> <p><strong>1943:</strong> <code class="language-plaintext highlighter-rouge">McCulloch-Pitts Model</code><br/> Walter Pitts and Warren McCulloch publish the first mathematical model of a neural network, laying the foundation for neural networks.</p> </li> <li> <p><strong>1949:</strong> <code class="language-plaintext highlighter-rouge">Hebbian Learning</code><br/> Donald Hebb publishes “The Organization of Behavior,” introducing concepts crucial to neural network development.</p> </li> </ul> <hr/> <h4 id="1950s1960s-foundations-of-artificial-intelligence"><strong>1950s–1960s: Foundations of Artificial Intelligence</strong></h4> <ul> <li> <p><strong>1950:</strong> <code class="language-plaintext highlighter-rouge">Turing Test</code><br/> Alan Turing proposes the Turing Test, a benchmark for machine intelligence.</p> </li> <li> <p><strong>1951:</strong> <code class="language-plaintext highlighter-rouge">SNARC</code><br/> Marvin Minsky and Dean Edmonds build SNARC, the first artificial neural network machine.</p> </li> <li> <p><strong>1952:</strong> <code class="language-plaintext highlighter-rouge">First Learning Program</code><br/> Arthur Samuel writes the first computer program capable of learning, a checkers-playing program.</p> </li> <li> <p><strong>1956:</strong> <code class="language-plaintext highlighter-rouge">Dartmouth Conference</code><br/> The term “Artificial Intelligence” is coined, marking the birth of AI as a field.</p> </li> <li> <p><strong>1957:</strong> <code class="language-plaintext highlighter-rouge">Perceptron</code><br/> Frank Rosenblatt invents the perceptron, an early type of neural network capable of binary classification.</p> </li> <li> <p><strong>1963:</strong> <code class="language-plaintext highlighter-rouge">Machine Learning in Games</code><br/> Donald Michie creates a machine that uses reinforcement learning to play Tic-tac-toe.</p> </li> <li> <p><strong>1967:</strong> <code class="language-plaintext highlighter-rouge">Nearest Neighbor Algorithm</code><br/> The Nearest Neighbor algorithm is developed, marking the birth of pattern recognition in computers.</p> </li> <li> <p><strong>1969:</strong> <code class="language-plaintext highlighter-rouge">Limitations of Neural Networks</code><br/> Marvin Minsky and Seymour Papert publish “Perceptrons,” highlighting limitations of early neural networks.</p> </li> </ul> <hr/> <h4 id="1970s1980s-growth-and-challenges"><strong>1970s–1980s: Growth and Challenges</strong></h4> <ul> <li> <p><strong>1970s:</strong> <code class="language-plaintext highlighter-rouge">First AI Winter</code><br/> Funding and interest in AI declined due to unmet expectations and computational limitations.</p> </li> <li> <p><strong>1979:</strong> <code class="language-plaintext highlighter-rouge">Stanford Cart</code><br/> Stanford University invents the “Stanford Cart,” an early autonomous mobile robot.</p> </li> <li> <p><strong>1981:</strong> <code class="language-plaintext highlighter-rouge">Explanation-Based Learning</code><br/> Gerald Dejong introduces the concept of explanation-based learning.</p> </li> <li> <p><strong>1985:</strong> <code class="language-plaintext highlighter-rouge">NetTalk</code><br/> Terry Sejnowski invents NetTalk, demonstrating machine learning of pronunciation.</p> </li> <li> <p><strong>1988:</strong> <code class="language-plaintext highlighter-rouge">Universal Approximation Theorem</code><br/> Kurt Hornik proves the universal approximation theorem for neural networks.</p> </li> <li> <p><strong>1989:</strong> <code class="language-plaintext highlighter-rouge">CNN for Handwriting Recognition</code><br/> Yann LeCun, Yoshua Bengio, and Patrick Haffner demonstrate CNNs for handwriting recognition.</p> </li> <li> <p><strong>1989:</strong> <code class="language-plaintext highlighter-rouge">Q-learning</code><br/> Christopher Watkins develops Q-learning, advancing reinforcement learning.</p> </li> </ul> <hr/> <h4 id="1990s-statistical-learning-and-commercial-ai"><strong>1990s: Statistical Learning and Commercial AI</strong></h4> <ul> <li> <p><strong>1992:</strong> <code class="language-plaintext highlighter-rouge">TD-Gammon</code><br/> Gerald Tesauro invents TD-Gammon, a backgammon program using neural networks.</p> </li> <li> <p><strong>1997:</strong> <code class="language-plaintext highlighter-rouge">Deep Blue Defeats Chess Champion</code><br/> IBM’s Deep Blue defeats world chess champion Garry Kasparov, demonstrating AI in games.</p> </li> <li> <p><strong>1997:</strong> <code class="language-plaintext highlighter-rouge">LSTMs Introduced</code><br/> Sepp Hochreiter and Jürgen Schmidhuber invent Long Short-Term Memory (LSTM) networks.</p> </li> <li> <p><strong>1998:</strong> <code class="language-plaintext highlighter-rouge">MNIST Database Released</code> Yann LeCun releases the MNIST database, a benchmark for handwriting recognition.</p> </li> <li> <p><strong>1998:</strong> <code class="language-plaintext highlighter-rouge">Furby Released</code><br/> Tiger Electronics releases Furby, introducing simple AI to the mass market.</p> </li> <li> <p><strong>1999:</strong> <code class="language-plaintext highlighter-rouge">AIBO Robot Dog</code><br/> Sony launches AIBO, showcasing AI in consumer robotics.</p> </li> </ul> <hr/> <h4 id="2000s-big-data-and-ml-techniques"><strong>2000s: Big Data and ML Techniques</strong></h4> <ul> <li> <p><strong>2000:</strong> <code class="language-plaintext highlighter-rouge">Nomad Robot</code><br/> The Nomad robot explores Antarctica, becoming the first robot to discover a meteorite.</p> </li> <li> <p><strong>2002:</strong> <code class="language-plaintext highlighter-rouge">Torch Library Released</code><br/> The Torch machine learning library is first released, enabling research in ML.</p> </li> <li> <p><strong>2009:</strong> <code class="language-plaintext highlighter-rouge">Netflix Prize</code><br/> Netflix awards $1 million for improving its recommendation system.</p> </li> </ul> <hr/> <h4 id="2010s-the-deep-learning-revolution"><strong>2010s: The Deep Learning Revolution</strong></h4> <ul> <li> <p><strong>2010:</strong> <code class="language-plaintext highlighter-rouge">Kaggle Launch</code><br/> Kaggle, a platform for machine learning competitions, is launched.</p> </li> <li> <p><strong>2010:</strong> <code class="language-plaintext highlighter-rouge">Kinect for Xbox</code><br/> Microsoft releases Kinect, showcasing advanced computer vision capabilities.</p> </li> <li> <p><strong>2011:</strong> <code class="language-plaintext highlighter-rouge">IBM Watson Wins Jeopardy!</code><br/> IBM Watson defeats human champions, showcasing NLP and ML capabilities.</p> </li> <li> <p><strong>2012:</strong> <code class="language-plaintext highlighter-rouge">AlexNet Wins ImageNet</code><br/> Deep CNNs significantly outperformed traditional approaches, heralding the deep learning era.</p> </li> <li> <p><strong>2013:</strong> <code class="language-plaintext highlighter-rouge">Deep Reinforcement Learning</code><br/> DeepMind introduces deep reinforcement learning, advancing RL applications.</p> </li> <li> <p><strong>2013:</strong> <code class="language-plaintext highlighter-rouge">Word2Vec</code><br/> Google introduces Word2Vec, a tool for vectorizing natural language.</p> </li> <li> <p><strong>2018:</strong> <code class="language-plaintext highlighter-rouge">Alibaba's AI</code><br/> Alibaba’s AI outscores humans on Stanford University’s reading comprehension test.</p> </li> </ul> <hr/> <h4 id="2020s-large-scale-ai-and-generative-models"><strong>2020s: Large-Scale AI and Generative Models</strong></h4> <ul> <li> <p><strong>2020:</strong> <code class="language-plaintext highlighter-rouge">GPT-3 Released</code><br/> OpenAI’s large-scale language model demonstrated the power of generative pre-trained transformers.</p> </li> <li> <p><strong>2020:</strong> <code class="language-plaintext highlighter-rouge">Turing NLG</code> Microsoft introduces Turing Natural Language Generation.</p> </li> <li> <p><strong>2022:</strong> <code class="language-plaintext highlighter-rouge">AlphaFold Breakthrough</code><br/> DeepMind solved the protein folding problem, revolutionizing biology with ML.</p> </li> <li> <p><strong>2023:</strong> <code class="language-plaintext highlighter-rouge">Generative AI Adoption</code><br/> Widespread use of diffusion models and ChatGPT showcased the practical impact of generative AI.</p> </li> </ul> <hr/> <h4 id="2024s-cutting-edge-ai-innovations"><strong>2024s: Cutting-Edge AI Innovations</strong></h4> <ul> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">OpenAI's O1 Model</code><br/> Advanced reasoning capabilities in mathematics and coding, enhancing AI’s problem-solving skills.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Google DeepMind's GenCast</code><br/> Improved weather predictions to optimize agriculture and disaster preparedness.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Microsoft's Copilot Vision</code><br/> AI integration with digital environments to boost productivity.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">AI Video Creation Tools</code><br/> Transformation of content creation with tools like Google’s Veo and OpenAI’s Sora.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Anthropic's Claude Chatbot</code><br/> Enhanced AI safety and reliability for critical applications like disaster response.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Multimodal AI Advancements</code><br/> Integration of text, audio, and visual inputs in AI models like ChatGPT-4.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Small Language Models (SLMs) Rise</code><br/> Increased popularity of efficient AI models that require fewer computing resources.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Customizable Generative AI</code><br/> Development of tailored AI systems for niche markets and specific user needs.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Geo-Llama</code><br/> Advanced AI technique for generating realistic simulated data on human movement in urban settings.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">GPT-4 Enhancements</code><br/> Improved emotional recognition capabilities from a third-person perspective.</p> </li> </ul> <p>The list of discoveries/events mentioned is extensive i guess, and apologies if I’ve missed any significant developments. The field of AI is advancing at a rapid pace, and we are eagerly awaiting the first steps toward AGI. As my focus remains on machine learning, I aim to contribute to this vibrant community, and I hope you’re as excited about the future of AI as I am. That’s likely why you’re reading this now. I wish you all the best and invite you to dive deeper into the realm of supervised learning in my next blog.</p> <p><strong>Stay tuned, and I’ll see you there!</strong></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[A concise timeline of machine learning's history, showcasing key milestones and breakthroughs that shaped the field.]]></summary></entry><entry><title type="html">Advanced Probability Concepts for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/probability-2/" rel="alternate" type="text/html" title="Advanced Probability Concepts for Machine Learning"/><published>2024-12-22T19:21:00+00:00</published><updated>2024-12-22T19:21:00+00:00</updated><id>https://monishver11.github.io/blog/2024/probability-2</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/probability-2/"><![CDATA[<h3 id="bayes-rule-and-associated-properties-a-key-concept"><strong>Bayes’ Rule and Associated Properties: A Key Concept</strong></h3> <p>Bayes’ Rule is a foundational concept in probability theory that plays a critical role in machine learning, especially in tasks involving classification, decision-making, and model inference. It provides a mathematical framework to update our beliefs about a hypothesis based on new evidence.</p> <h4 id="what-is-bayes-rule"><strong>What is Bayes’ Rule?</strong></h4> <p>Bayes’ Rule describes the relationship between conditional probabilities. It allows us to reverse conditional probabilities, which can be very useful in machine learning when we need to compute the probability of a certain hypothesis given observed data.</p> <p>Mathematically, Bayes’ Rule is expressed as:</p> \[P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}\] <p>Where:</p> <ul> <li>\(P(A \mid B)\) is the <strong>posterior probability</strong>: the probability of the hypothesis \(A\) being true given the evidence \(B\).</li> <li>\(P(B \mid A)\) is the <strong>likelihood</strong>: the probability of observing the evidence \(B\) given that the hypothesis \(A\) is true.</li> <li>\(P(A)\) is the <strong>prior probability</strong>: the initial probability of the hypothesis \(A\) before seeing any evidence.</li> <li>\(P(B)\) is the <strong>evidence</strong> or <strong>normalizing constant</strong>: the total probability of observing the evidence across all possible hypotheses.</li> </ul> <p><strong>To follow along, consider this analogy:</strong></p> <p>Imagine you’re trying to diagnose whether someone has a certain disease. You have:</p> <ul> <li>The prior probability of having the disease (\(P(A)\)), which could be based on general statistics about the disease.</li> <li>The likelihood (\(P(B \mid A)\)), which is the probability that a person with the disease would test positive on a medical test.</li> <li>The evidence (\(P(B)\)), which is the total probability that anyone, sick or healthy, would test positive.</li> </ul> <p>Bayes’ Rule helps us combine this information to update our belief about the probability of the disease (hypothesis \(A\)) given the test result (evidence \(B\)).</p> <h4 id="why-is-bayes-rule-crucial-in-machine-learning"><strong>Why is Bayes’ Rule Crucial in Machine Learning?</strong></h4> <p>Bayes’ Rule is central to a variety of machine learning models, particularly in probabilistic and Bayesian approaches. Some key applications include:</p> <ol> <li> <p><strong>Naive Bayes’ Classifier</strong>: In supervised learning, the Naive Bayes’ classifier uses Bayes’ Rule to classify data based on conditional probabilities. It assumes independence between features, simplifying the computation of probabilities.</p> </li> <li> <p><strong>Model Inference and Parameter Estimation</strong>: Bayesian methods in machine learning, like Bayesian neural networks, use Bayes’ Rule to update the distribution of model parameters as new data is observed, instead of relying on point estimates.</p> </li> <li> <p><strong>Decision Theory</strong>: Bayes’ Rule helps in decision-making processes by quantifying the uncertainty associated with different outcomes, especially when there is a probabilistic component to the environment or model.</p> </li> </ol> <h4 id="associated-properties-of-bayes-rule"><strong>Associated Properties of Bayes’ Rule</strong></h4> <ol> <li> <p><strong>Bayes’ Theorem for Multiple Events</strong>: Bayes’ Rule can be extended to more complex situations, such as when dealing with multiple hypotheses or events. This is useful when making predictions over many possible outcomes or when dealing with complex models in machine learning.</p> </li> <li> <p><strong>Conjugacy</strong>: In some models, certain prior distributions are chosen because they lead to mathematical simplicity when combined with Bayes’ Rule. These priors are called <strong>conjugate priors</strong>. For example, in Gaussian processes, using a conjugate prior for the likelihood of Gaussian data results in a simpler update process.</p> </li> <li> <p><strong>The Law of Total Probability</strong>: Bayes’ Rule is closely related to the Law of Total Probability, which decomposes the total probability of an event into a sum of conditional probabilities. This can be useful when considering multiple sources of evidence or when performing integration in complex models.</p> </li> </ol> <p>Bayes’ Rule is an indispensable tool in machine learning for reasoning about uncertainty, updating beliefs with new evidence, and making decisions in the face of incomplete information.</p> <hr/> <h3 id="joint-probability-and-independence"><strong>Joint Probability and Independence:</strong></h3> <p>In machine learning, understanding how different events relate to each other is critical. Two key concepts that help us analyze these relationships are <strong>joint probability</strong> and <strong>independence</strong>.</p> <h4 id="what-is-joint-probability"><strong>What is Joint Probability?</strong></h4> <p>Joint probability refers to the probability of two or more events occurring simultaneously. In other words, it is the likelihood that multiple events happen at the same time, and is often represented as \(P(A \cap B)\) for two events \(A\) and \(B\).</p> <p>Mathematically, the joint probability of events \(A\) and \(B\) is defined as:</p> \[P(A \cap B) = P(A \mid B) \cdot P(B) = P(B \mid A) \cdot P(A)\] <p>This equation shows that the joint probability of two events \(A\) and \(B\) can be computed by multiplying the conditional probability of one event given the other by the probability of the second event. The reverse relationship also holds true.</p> <p><strong>Now, consider this analogy:</strong></p> <p>Imagine you’re rolling two dice and are interested in the probability that the first die shows a 4 and the second die shows a 6. The joint probability, \(P(\text{Die 1 = 4 and Die 2 = 6})\), is the probability of both events happening at once. Since the dice rolls are independent, we can compute this as the product of the individual probabilities:</p> \[P(\text{Die 1 = 4}) \cdot P(\text{Die 2 = 6})\] <p>This product gives the likelihood that both dice will show these values simultaneously.</p> <h4 id="why-is-joint-probability-important-in-machine-learning"><strong>Why is Joint Probability Important in Machine Learning?</strong></h4> <p>Joint probability is used to model relationships between different features or variables in a dataset. Some applications include:</p> <ol> <li> <p><strong>Multivariate Probability Models</strong>: In many machine learning problems, we are dealing with multiple features simultaneously. Joint probability helps model the dependencies between these features, which is essential for tasks like classification or clustering. For example, in a classification task, joint probabilities allow us to compute the likelihood of a particular outcome given multiple features.</p> </li> <li> <p><strong>Markov Chains</strong>: In sequence-based tasks like time series forecasting, the joint probability of a sequence of events (e.g., states in a Markov Chain) is crucial in determining the probability distribution over future states based on previous ones.</p> </li> </ol> <h4 id="what-is-independence"><strong>What is Independence?</strong></h4> <p>Two events \(A\) and \(B\) are said to be <strong>independent</strong> if the occurrence of one event does not affect the probability of the other event occurring. Mathematically, two events are independent if:</p> \[P(A \cap B) = P(A) \cdot P(B)\] <p>This property is a key assumption in many machine learning algorithms, especially those based on probabilistic reasoning.</p> <p><strong>For Intuition, think it this way:</strong></p> <p>Think of tossing a coin and rolling a die. The outcome of the coin toss does not affect the outcome of the die roll. These two events are independent, meaning the joint probability can be computed as the product of their individual probabilities. So, the probability of getting heads on the coin toss and a 6 on the die roll is:</p> \[P(\text{Heads}) \cdot P(\text{Die = 6})\] <h4 id="why-is-independence-important-in-machine-learning"><strong>Why is Independence Important in Machine Learning?</strong></h4> <p>Independence is a simplifying assumption in many machine learning models and can significantly reduce the complexity of computations:</p> <ol> <li> <p><strong>Naive Bayes Classifier</strong>: The Naive Bayes classifier makes a strong independence assumption—that the features are conditionally independent given the class. This simplifies the computation of joint probabilities for multiple features and makes the model efficient even with high-dimensional data.</p> </li> <li> <p><strong>Factorization</strong>: In probabilistic models, assuming independence allows for factorizing the joint probability distribution into simpler, more manageable parts. This can be particularly useful in situations like generative models or in deep learning when modeling complex dependencies in large datasets.</p> </li> <li> <p><strong>Feature Independence</strong>: In feature engineering, assuming that features are independent can help simplify model design and speed up training. It’s often used as a heuristic, particularly when exploring models like Gaussian Mixture Models (GMM) or Hidden Markov Models (HMM).</p> </li> </ol> <h4 id="conditional-independence"><strong>Conditional Independence</strong></h4> <p>While not the same as plain independence, <strong>conditional independence</strong> is another important concept. Two events \(A\) and \(B\) are conditionally independent given a third event \(C\) if:</p> \[P(A \cap B \mid C) = P(A \mid C) \cdot P(B \mid C)\] <p>This property is widely used in Bayesian networks and machine learning models to break down complex dependencies into simpler conditional ones.</p> <hr/> <h3 id="conditional-probability-and-conditional-distributions-building-blocks-for-predictive-models"><strong>Conditional Probability and Conditional Distributions: Building Blocks for Predictive Models</strong></h3> <p>Conditional probability and conditional distributions concepts help us refine predictions based on additional information and allow us to build more accurate, data-driven models by considering how the likelihood of one event changes when we know about the occurrence of another.</p> <h4 id="what-is-conditional-probability"><strong>What is Conditional Probability?</strong></h4> <p>Conditional probability is the probability of an event occurring given that another event has already occurred. In other words, it quantifies the likelihood of an event, assuming that certain information is known. It is expressed as:</p> \[P(A \mid B) = \frac{P(A \cap B)}{P(B)}\] <p>Where:</p> <ul> <li>\(P(A \mid B)\) is the <strong>conditional probability</strong> of event \(A\) given event \(B\).</li> <li>\(P(A \cap B)\) is the <strong>joint probability</strong> of both \(A\) and \(B\) occurring.</li> <li>\(P(B)\) is the <strong>probability</strong> of event \(B\).</li> </ul> <p>This formula helps us understand how the occurrence of one event (\(B\)) affects the likelihood of another event (\(A\)).</p> <p><strong>Intuition and Analogy for Conditional Probability</strong></p> <p>Imagine you’re at a concert and you’re interested in the probability that a person will be wearing a red T-shirt, given that they are in the front row. Without any information, the probability of someone wearing a red T-shirt might be 30%. But if you know that the person is in the front row (which could imply a certain type of concertgoer), this could affect the probability—perhaps fans who are in the front row are more likely to wear red.</p> <p>This situation is an example of <strong>conditional probability</strong>, where the event \(A\) (person wearing a red T-shirt) is conditioned on the event \(B\) (person being in the front row). Conditional probability helps refine your predictions based on new information.</p> <h4 id="why-is-conditional-probability-important-in-machine-learning"><strong>Why is Conditional Probability Important in Machine Learning?</strong></h4> <p>Conditional probability is essential in many machine learning models for predicting outcomes based on known data. Key applications include:</p> <ol> <li> <p><strong>Classification and Regression</strong>: In supervised learning, we use conditional probability to predict the class label (in classification) or continuous values (in regression) based on observed features. For instance, in logistic regression, we compute the conditional probability of a binary outcome given certain feature values.</p> </li> <li> <p><strong>Naive Bayes Classifier</strong>: The Naive Bayes algorithm, which assumes conditional independence of features, uses conditional probabilities to predict class labels. It calculates the probability of the class label given the observed features using Bayes’ Theorem.</p> </li> <li> <p><strong>Bayesian Inference</strong>: In Bayesian methods, we continuously update the probability of a hypothesis based on new data. This is done using conditional probabilities and allows for probabilistic reasoning in models such as Bayesian networks.</p> </li> </ol> <h4 id="what-are-conditional-distributions"><strong>What are Conditional Distributions?</strong></h4> <p>A <strong>conditional distribution</strong> is a probability distribution of a subset of variables given the values of other variables. It generalizes conditional probability to the case of multiple random variables and helps us understand how the distribution of one variable changes when the values of others are known.</p> <p>For example, if you have two variables \(X\) and \(Y\), the conditional distribution of \(X\) given \(Y = y\) is the probability distribution of \(X\) when you know that \(Y\) takes the specific value \(y\).</p> <p>Mathematically, a conditional distribution is denoted as:</p> \[P(X \mid Y = y)\] <p>Where:</p> <ul> <li>\(P(X \mid Y = y)\) is the conditional distribution of \(X\) given that \(Y = y\).</li> <li>This describes how the distribution of \(X\) changes when \(Y\) is fixed at a particular value.</li> </ul> <p><strong>Intuition and Analogy for Conditional Distributions</strong></p> <p>Consider a scenario where you’re trying to predict a person’s income (\(X\)) based on their level of education (\(Y\)). If you know that someone has a college degree, the distribution of their income (the possible range of incomes they could have) will be different than if you only know their high school education level.</p> <p>In this case, \(P(X \mid Y)\) would describe the distribution of income (\(X\)) conditional on a specific level of education (\(Y\)). The distribution will shift depending on the value of \(Y\), helping refine your predictions of income based on known education levels.</p> <h4 id="why-are-conditional-distributions-important-in-machine-learning"><strong>Why are Conditional Distributions Important in Machine Learning?</strong></h4> <p>Conditional distributions are vital in machine learning for understanding relationships between features and predicting outcomes. Some key uses include:</p> <ol> <li> <p><strong>Generative Models</strong>: In models like Gaussian Mixture Models (GMM) or Hidden Markov Models (HMM), conditional distributions are used to model how data points (such as observations or states) are generated given certain parameters.</p> </li> <li> <p><strong>Bayesian Networks</strong>: In Bayesian networks, the conditional distributions represent the probabilistic dependencies between variables. Each node (representing a random variable) has a conditional distribution based on its parent nodes, and the overall network structure allows us to compute the joint distribution of all variables.</p> </li> <li> <p><strong>Expectation-Maximization (EM) Algorithm</strong>: The EM algorithm, used for unsupervised learning and model fitting, relies on conditional distributions to estimate parameters in models with missing or incomplete data. The E-step computes the conditional distributions of hidden variables given the observed data.</p> </li> </ol> <p>While conditional probability allows us to adjust our expectations based on new information, conditional distributions give us a broader view of how data is distributed when specific conditions are known.</p> <hr/> <h3 id="law-of-total-probability-a-fundamental-tool-for-dealing-with-uncertainty"><strong>Law of Total Probability: A Fundamental Tool for Dealing with Uncertainty</strong></h3> <p>The Law of Total Probability is a key principle that allows us to compute the probability of an event by considering all possible ways that event could occur, based on different conditions or scenarios. This law is often used in machine learning when dealing with complex models where outcomes depend on multiple factors, or when some information is missing or unknown.</p> <h4 id="what-is-the-law-of-total-probability"><strong>What is the Law of Total Probability?</strong></h4> <p>It helps us calculate the probability of an event by partitioning the sample space into different mutually exclusive events and then summing up the probabilities of the event occurring in each of these partitions.</p> <p>Mathematically, the law is expressed as:</p> \[P(A) = \sum_{i} P(A \mid B_i) P(B_i)\] <p>Where:</p> <ul> <li>\(P(A)\) is the total probability of event \(A\).</li> <li>\(B_1, B_2, \dots, B_n\) are a partition of the sample space, meaning these events are mutually exclusive and exhaustive (they cover all possible outcomes).</li> <li>\(P(A \mid B_i)\) is the <strong>conditional probability</strong> of \(A\) given \(B_i\), i.e., the probability of \(A\) occurring under the condition that \(B_i\) occurs.</li> <li>\(P(B_i)\) is the probability of event \(B_i\).</li> </ul> <p>The law essentially breaks down the probability of \(A\) into cases based on different conditions \(B_1, B_2, \dots, B_n\), and then combines them weighted by the likelihood of each condition.</p> <p><strong>So, how to internalize this idea:</strong></p> <p>Imagine you are trying to determine the probability that a customer will purchase a product \(A\), but you have different types of customers \(B_1, B_2, \dots, B_n\) (e.g., based on their age group, spending history, etc.). The probability that a customer purchases the product will vary depending on their group. The Law of Total Probability tells you to:</p> <ol> <li>Calculate the probability of purchasing given each customer type (e.g., \(P(A \mid B_1)\), \(P(A \mid B_2)\), etc.).</li> <li>Multiply these by the probability of each customer type occurring (e.g., \(P(B_1)\), \(P(B_2)\)).</li> <li>Sum these products to find the total probability of purchasing across all customer types.</li> </ol> <p>In this way, the law allows you to compute the overall probability by considering all relevant scenarios (customer types) and weighing them accordingly.</p> <h4 id="why-is-the-law-of-total-probability-important-in-machine-learning"><strong>Why is the Law of Total Probability Important in Machine Learning?</strong></h4> <p>In machine learning, the Law of Total Probability is widely used for various tasks, especially in probabilistic modeling, classification, and predictive analytics. Some key applications include:</p> <ol> <li> <p><strong>Bayesian Inference</strong>: When updating beliefs about a hypothesis (or class) based on new data, the total probability is calculated over all possible hypotheses or classes. This helps refine predictions and is foundational in models such as <strong>Naive Bayes</strong>.</p> </li> <li> <p><strong>Handling Missing Data</strong>: In models dealing with missing data, the law helps to marginalize over the unknown values by considering all possible ways the data could be missing. For example, in the <strong>Expectation-Maximization (EM)</strong> algorithm, the law is used to estimate the missing values based on the observed data.</p> </li> <li> <p><strong>Class Conditional Probability in Classification</strong>: In classification problems, especially when working with multiple classes, the law allows the decomposition of class probabilities into conditional probabilities based on different features, facilitating the calculation of total class probabilities.</p> </li> </ol> <p><strong>Example: Applying the Law of Total Probability</strong></p> <p>Let’s consider an example in a classification task. Suppose you are trying to predict whether a customer will buy a product \(A\) (event \(A\)), and you have two features that classify the customer: whether they are a <strong>new customer</strong> (\(B_1\)) or a <strong>returning customer</strong> (\(B_2\)).</p> <p>The Law of Total Probability helps you compute the total probability of purchasing the product, considering both new and returning customers:</p> \[P(\text{Buy}) = P(\text{Buy} \mid \text{New Customer}) \cdot P(\text{New Customer}) + P(\text{Buy} \mid \text{Returning Customer}) \cdot P(\text{Returning Customer})\] <p>Here:</p> <ul> <li>\(P(\text{Buy} \mid \text{New Customer})\) is the probability that a new customer buys the product.</li> <li>\(P(\text{New Customer})\) is the probability that the customer is new.</li> <li>Similarly, \(P(\text{Buy} \mid \text{Returning Customer})\) and \(P(\text{Returning Customer})\) are for the returning customers.</li> </ul> <p>This allows you to compute the total probability of a customer buying the product, considering both customer types.</p> <h4 id="connection-with-conditional-probability"><strong>Connection with Conditional Probability</strong></h4> <p>The Law of Total Probability is built on conditional probability. It helps us to marginalize over unknown or unobserved conditions, ensuring we account for all possible scenarios that could influence the event of interest.</p> <p>For example, in a machine learning model that makes predictions based on different feature values, the law allows us to break down the total probability of an outcome by conditioning on the feature values and summing over all possible feature combinations.</p> <p>Whether you are building a Bayesian model, dealing with missing data, or predicting outcomes in complex scenarios, the Law of Total Probability provides a systematic way to combine multiple probabilities and refine your model’s predictions.</p> <hr/> <h3 id="expectation-and-variance-essential-measures"><strong>Expectation and Variance: Essential Measures</strong></h3> <p>They provide valuable insights into the behavior of data and are widely used in machine learning to understand the characteristics of models, assess uncertainty, and make predictions. Here’s a breakdown of each concept and its relevance to machine learning.</p> <h4 id="what-is-expectation"><strong>What is Expectation?</strong></h4> <p>The <strong>expectation</strong> (or <strong>mean</strong>) of a random variable represents its <strong>average</strong> or <strong>central tendency</strong>. It is the weighted average of all possible values that the variable can take, where the weights are given by the probabilities of these values.</p> <p>For a discrete random variable \(X\), the expectation \(E(X)\) is defined as:</p> \[E(X) = \sum_{i} x_i P(x_i)\] <p>Where:</p> <ul> <li>\(x_i\) are the possible values that \(X\) can take.</li> <li>\(P(x_i)\) is the probability of \(X\) taking the value \(x_i\).</li> </ul> <p>For a continuous random variable with probability density function \(f(x)\), the expectation is:</p> \[E(X) = \int_{-\infty}^{\infty} x f(x) \, dx\] <p><strong>Intuition for Expectation:</strong></p> <p>Think of expectation as the “balance point” of a distribution. For example, if you were to imagine a physical rod with different weights placed at various points, the <strong>center of mass</strong> of the rod would represent the expectation.</p> <p>In machine learning, the expectation helps us understand the <strong>average behavior</strong> of the data. For instance, in regression tasks, the expectation of the target variable provides a baseline prediction.</p> <h4 id="what-is-variance"><strong>What is Variance?</strong></h4> <p>The <strong>variance</strong> of a random variable quantifies the spread or dispersion of the variable around its expectation. A high variance indicates that the values are widely spread out, while a low variance indicates that the values are clustered around the mean.</p> <p>For a discrete random variable \(X\), the variance \(\text{Var}(X)\) is defined as:</p> \[\text{Var}(X) = E[(X - E(X))^2] = \sum_{i} (x_i - E(X))^2 P(x_i)\] <p>For a continuous random variable:</p> \[\text{Var}(X) = \int_{-\infty}^{\infty} (x - E(X))^2 f(x) \, dx\] <p>Alternatively, variance can also be computed as:</p> \[\text{Var}(X) = E(X^2) - (E(X))^2\] <p>Where \(E(X^2)\) is the expectation of \(X^2\), i.e., the expected value of the square of \(X\).</p> <p><strong>Intuition for Variance:</strong></p> <p>Variance tells us about the <strong>spread</strong> of the data. Imagine measuring the height of a group of people:</p> <ul> <li>If everyone has a similar height, the variance will be low.</li> <li>If the group includes both very short and very tall individuals, the variance will be high.</li> </ul> <p>In machine learning, variance provides insights into <strong>model uncertainty</strong>. High variance in a model’s predictions indicates overfitting, while low variance suggests underfitting.</p> <h4 id="why-are-expectation-and-variance-important-in-machine-learning"><strong>Why Are Expectation and Variance Important in Machine Learning?</strong></h4> <ol> <li><strong>Expectation</strong>: <ul> <li><strong>Model Evaluation</strong>: Used as a baseline for evaluating model predictions (e.g., in regression tasks).</li> <li><strong>Loss Functions</strong>: Central to defining loss functions like Mean Squared Error (MSE).</li> <li><strong>Feature Engineering</strong>: Understanding the average behavior of features aids in creating or selecting the most informative ones.</li> </ul> </li> <li><strong>Variance</strong>: <ul> <li><strong>Bias-Variance Tradeoff</strong>: Balancing model complexity to avoid overfitting (high variance) or underfitting (low variance).</li> <li><strong>Model Complexity</strong>: Guides the choice of model complexity (e.g., simpler models like linear regression have lower variance).</li> <li><strong>Uncertainty Estimation</strong>: Quantifies confidence in probabilistic models like Gaussian Processes.</li> <li><strong>Performance Metrics</strong>: Used in cross-validation to measure consistency across datasets.</li> </ul> </li> </ol> <hr/> <h3 id="covariance-and-correlation-measuring-relationships-between-variables"><strong>Covariance and Correlation: Measuring Relationships Between Variables</strong></h3> <p>Covariance and correlation are statistical tools used to understand the relationships between two random variables. In machine learning, these concepts are essential for identifying feature interactions, reducing dimensionality, and improving model performance.</p> <h4 id="what-is-covariance"><strong>What is Covariance?</strong></h4> <p>Covariance measures the <strong>direction</strong> of the linear relationship between two variables, indicating whether they increase or decrease together.</p> <p>For two random variables \(X\) and \(Y\), the covariance is defined as:</p> \[\text{Cov}(X, Y) = E\left[(X - E(X))(Y - E(Y))\right]\] <p>Where:</p> <ul> <li>\(E(X)\) and \(E(Y)\) are the expectations of \(X\) and \(Y\).</li> <li>\((X - E(X))\) and \((Y - E(Y))\) represent deviations from their means.</li> </ul> <p><strong>Interpretation</strong>:</p> <ul> <li>\(\text{Cov}(X, Y) &gt; 0\): Positive relationship (as \(X\) increases, \(Y\) tends to increase).</li> <li>\(\text{Cov}(X, Y) &lt; 0\): Negative relationship (as \(X\) increases, \(Y\) tends to decrease).</li> <li>\(\text{Cov}(X, Y) = 0\): No linear relationship.</li> </ul> <h4 id="what-is-correlation"><strong>What is Correlation?</strong></h4> <p>Correlation is a <strong>scaled version of covariance</strong> that provides the strength and direction of the relationship on a fixed scale \([-1, 1]\).</p> <p>The <strong>Pearson correlation coefficient</strong> is defined as:</p> \[\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\] <p>Where:</p> <ul> <li>\(\sigma_X\) and \(\sigma_Y\) are the standard deviations of \(X\) and \(Y\).</li> </ul> <p><strong>Interpretation</strong>:</p> <ul> <li>\(\rho(X, Y) = 1\): Perfect positive linear relationship.</li> <li>\(\rho(X, Y) = -1\): Perfect negative linear relationship.</li> <li>\(\rho(X, Y) = 0\): No linear relationship.</li> </ul> <h4 id="key-differences"><strong>Key Differences</strong></h4> <table> <thead> <tr> <th><strong>Aspect</strong></th> <th><strong>Covariance</strong></th> <th><strong>Correlation</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Scale</strong></td> <td>Depends on the units of variables</td> <td>Unitless, standardized</td> </tr> <tr> <td><strong>Range</strong></td> <td>\((-\infty, \infty)\)</td> <td>\([-1, 1]\)</td> </tr> <tr> <td><strong>Use Case</strong></td> <td>Direction of relationship</td> <td>Strength and direction combined</td> </tr> </tbody> </table> <p>//</p> <h4 id="applications-in-machine-learning"><strong>Applications in Machine Learning</strong></h4> <ol> <li><strong>Feature Relationships</strong>: <ul> <li>Covariance highlights how features interact.</li> <li>Correlation quantifies redundancy or relevance.</li> </ul> </li> <li><strong>Feature Selection</strong>: <ul> <li>Retain features with high correlation to the target.</li> <li>Remove features with high inter-correlation to reduce multicollinearity.</li> </ul> </li> <li><strong>Dimensionality Reduction</strong>: <ul> <li><strong>Principal Component Analysis (PCA)</strong> uses covariance or correlation matrices to identify directions of maximum variance.</li> </ul> </li> </ol> <hr/> <h3 id="central-limit-theorem-the-foundation-of-statistical-inference"><strong>Central Limit Theorem: The Foundation of Statistical Inference</strong></h3> <p>The <strong>Central Limit Theorem (CLT)</strong> explains why normal distributions appear so frequently in practice and is key for making inferences about data.</p> <h4 id="what-is-the-central-limit-theorem"><strong>What is the Central Limit Theorem?</strong></h4> <p>The <strong>Central Limit Theorem</strong> states that for a population with a finite mean \(\mu\) and variance \(\sigma^2\), the distribution of the <strong>sample mean</strong> from sufficiently large random samples will approximate a <strong>normal distribution</strong>, regardless of the original distribution of the population.</p> <p>Mathematically, if \(X_1, X_2, \dots, X_n\) are i.i.d. random variables drawn from a population with mean \(\mu\) and variance \(\sigma^2\), the sample mean \(\bar{X}\) is defined as:</p> \[\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i\] <p>As the sample size \(n\) increases, the sample mean has the following properties:</p> <ul> <li>The <strong>mean</strong> of \(\bar{X}\) is \(\mu\) (the population mean).</li> <li>The <strong>variance</strong> of \(\bar{X}\) is \(\frac{\sigma^2}{n}\), meaning the variance decreases as \(n\) increases.</li> <li>The distribution of \(\bar{X}\) approaches a <strong>normal distribution</strong> with mean \(\mu\) and variance \(\frac{\sigma^2}{n}\), and the <strong>standard deviation</strong> becomes \(\frac{\sigma}{\sqrt{n}}\), called the <strong>standard error</strong>.</li> </ul> <p><strong>How do we remember this?</strong></p> <p>Imagine you are sampling from a non-normal distribution, such as the distribution of ages in a city. A small sample might produce a skewed or non-normal distribution. However, as you increase the sample size, the distribution of the sample mean will become increasingly normal, regardless of the original distribution shape.</p> <p>This phenomenon is like averaging noisy measurements in engineering. A single measurement might be noisy, but averaging multiple measurements reduces the noise, making the result more predictable and normally distributed.</p> <h4 id="why-is-the-central-limit-theorem-important-in-machine-learning"><strong>Why is the Central Limit Theorem Important in Machine Learning?</strong></h4> <p>The Central Limit Theorem is foundational in statistics and machine learning for the following reasons:</p> <ol> <li><strong>Foundation for Inference</strong>: <ul> <li>The CLT enables statistical inference techniques like hypothesis testing and confidence intervals. When drawing random samples, the sample mean will follow a normal distribution, allowing for probabilistic statements about population parameters.</li> </ul> </li> <li><strong>Simplifying Assumptions</strong>: <ul> <li>Many machine learning algorithms assume normality (e.g., linear regression). The CLT allows us to assume that, for sufficiently large datasets, estimators of model parameters will follow a normal distribution, making them easier to analyze.</li> </ul> </li> <li><strong>Sample Size Considerations</strong>: <ul> <li>The CLT shows that, for large datasets, we can assume normality even if the underlying data is non-normal. As the sample size increases, algorithms become more stable and their performance becomes more predictable.</li> </ul> </li> </ol> <p><strong>Example of the Central Limit Theorem in Practice</strong></p> <p>Imagine you are analyzing <strong>house prices</strong> in a city, and the distribution of house prices is highly skewed due to a few luxury homes. You want to estimate the average house price.</p> <ul> <li> <p><strong>Without CLT</strong>: The highly skewed data would result in a mean that doesn’t reflect the typical house price.</p> </li> <li> <p><strong>With CLT</strong>: By taking random samples, computing the mean for each sample, and repeating the process many times, the distribution of sample means will become normal, even though the underlying distribution of house prices is skewed. The sample mean will be a more reliable estimator of the population mean, allowing for more accurate confidence intervals.</p> </li> </ul> <h4 id="central-limit-theorem-in-machine-learning"><strong>Central Limit Theorem in Machine Learning</strong></h4> <p>The CLT is useful in several machine learning contexts:</p> <ol> <li><strong>Regression Models</strong>: <ul> <li>In linear regression, the CLT implies that, for large sample sizes, the distribution of estimated coefficients will be approximately normal. This enables the use of confidence intervals to assess uncertainty in the coefficients.</li> </ul> </li> <li><strong>Bootstrap Methods</strong>: <ul> <li>The CLT is essential for bootstrap resampling methods, which estimate the variability of a statistic (like the mean) by repeatedly sampling from the data. Due to the CLT, the distribution of these sample statistics will be approximately normal.</li> </ul> </li> <li><strong>Confidence Intervals and Hypothesis Testing</strong>: <ul> <li>Many machine learning techniques rely on the CLT to estimate confidence intervals and perform hypothesis testing. For example, in regression, the standard error of the coefficients is derived from the CLT.</li> </ul> </li> </ol> <h4 id="conditions-for-the-central-limit-theorem"><strong>Conditions for the Central Limit Theorem</strong></h4> <p>For the CLT to hold, the following conditions are necessary:</p> <ol> <li><strong>Independence</strong>: The samples must be independent.</li> <li><strong>Sample Size</strong>: The sample size should be large enough. Typically, a sample size of 30 or more is considered sufficient for the CLT to apply.</li> <li><strong>Finite Variance</strong>: The population must have a finite variance.</li> </ol> <p>By leveraging the CLT, you can make reliable estimates, perform hypothesis testing, and create models that work well, even when the underlying data distribution is non-normal.</p> <hr/> <p>Finally, we’ve explored key concepts in probability theory that are important to machine learning. From understanding the basics of probability distributions and Bayes’ Theorem to the relationships between variables through covariance and correlation, these concepts provide the mathematical layer for building robust models. Finally, the Central Limit Theorem ties everything together, offering insight into statistical inference and ensuring that predictions and model estimates are reliable.</p> <p>In the next post, we’ll continue diving deeper into the brief history of machine learning and what are we upto currently.</p> <p><strong>See you there!</strong></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog explores key probability theory concepts, from distributions and Bayes' Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.]]></summary></entry><entry><title type="html">Understanding the Basics of Probability Theory for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/probability-1/" rel="alternate" type="text/html" title="Understanding the Basics of Probability Theory for Machine Learning"/><published>2024-12-22T11:55:00+00:00</published><updated>2024-12-22T11:55:00+00:00</updated><id>https://monishver11.github.io/blog/2024/probability-1</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/probability-1/"><![CDATA[<p>Probability theory forms the foundation of machine learning by enabling models to quantify uncertainty and make evidence-based predictions. This article delves into core probability concepts, providing explanations, examples, and analogies designed for clarity and practical sense.</p> <hr/> <h3 id="probability-definition-interpretation-and-basic-axioms"><strong>Probability: Definition, Interpretation, and Basic Axioms</strong></h3> <p>In simple terms, <strong>probability</strong> is a measure of the likelihood of an event occurring. It quantifies uncertainty by assigning a number between 0 and 1, where:</p> <ul> <li>A probability of <strong>0</strong> means the event is impossible.</li> <li>A probability of <strong>1</strong> means the event is certain.</li> <li>Values between 0 and 1 represent the likelihood of an event, with higher values indicating greater likelihood.</li> </ul> <p>Mathematically, probability is defined as a function:\(P: \mathcal{F} \to [0, 1]\) where \(\mathcal{F}\) is a collection of events, and \(P\) assigns a value to each event representing its likelihood.</p> <h4 id="interpretation-of-probability"><strong>Interpretation of Probability</strong></h4> <ol> <li><strong>Frequentist Interpretation</strong>: Probability is the long-run relative frequency of an event occurring after repeated trials. Example: In flipping a fair coin many times, the probability of getting heads is \(0.5\), as heads appear in roughly 50% of the flips.</li> <li><strong>Bayesian Interpretation</strong>: Probability reflects a degree of belief or confidence in an event occurring, updated as new evidence becomes available. Example: If the chance of rain tomorrow is initially assigned as \(0.7\), this reflects 70% confidence based on current information.</li> </ol> <h4 id="basic-axioms-of-probability"><strong>Basic Axioms of Probability</strong></h4> <p>The three fundamental axioms govern how probabilities are assigned:</p> <ol> <li><strong>Non-negativity</strong>: \(P(E) \geq 0 \quad \text{for all events } E\). Probabilities cannot be negative.</li> <li><strong>Normalization</strong>: \(P(S) = 1\). Here, \(S\) is the <strong>sample space</strong> (all possible outcomes). The probability of \(S\) is 1, as one outcome must occur. Example: For a die roll, \(S = \{1, 2, 3, 4, 5, 6\}\), and \(P(S) = 1\).</li> <li> <p><strong>Additivity</strong>:</p> \[P(E_1 \cup E_2) = P(E_1) + P(E_2) \quad \text{if } E_1 \text{ and } E_2 \text{ are mutually exclusive}\] <p>For mutually exclusive events \(E_1\) and \(E_2\), the probability of either occurring is the sum of their individual probabilities. <strong>Example</strong>: For a die roll, let \(E_1 = \{1\}\) and \(E_2 = \{2\}\): \(P(E_1 \cup E_2) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3}\)</p> </li> </ol> <h4 id="consequences-of-the-axioms"><strong>Consequences of the Axioms</strong></h4> <ol> <li><strong>Complementary Rule:</strong> \(P(E^c) = 1 - P(E)\). The probability of the complement of \(E\) (event not occurring) equals \(1\) minus \(P(E)\). Example: If \(P(\text{rain}) = 0.8\), then \(P(\text{no rain}) = 1 - 0.8 = 0.2\).</li> <li> <p><strong>Addition Rule for Non-Mutually Exclusive Events:</strong></p> \[P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)\] <p>For events that are not mutually exclusive, subtract the overlap probability. Example: Overlapping probabilities in surveys or data categorization.</p> </li> <li> <p><strong>Multiplication Rule for Independent Events:</strong></p> \[P(E_1 \cap E_2) = P(E_1) \cdot P(E_2) \quad \text{:if } E_1 \text{ and } E_2 \text{ are independent}\] <p>Example: Probability of flipping two heads with two coins:\(P(\text{heads on coin 1} \cap \text{heads on coin 2}) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\)</p> </li> <li> <p><strong>Conditional Probability:</strong></p> \[P(E_1 \mid E_2) = \frac{P(E_1 \cap E_2)}{P(E_2)} \quad \text{if } P(E_2) &gt; 0\] <p>Conditional probability calculates \(E_1\)’s likelihood given \(E_2\) has occurred. Example: Used in models like <strong>Naive Bayes</strong> for predictions under given conditions.</p> </li> </ol> <h4 id="why-is-probability-important-in-machine-learning"><strong>Why is Probability Important in Machine Learning?</strong></h4> <p>Probability theory plays a critical role in machine learning by enabling:</p> <ol> <li><strong>Modeling Uncertainty</strong>: Essential in probabilistic models like Bayesian networks and Hidden Markov Models.</li> <li><strong>Decision Making</strong>: Used in reinforcement learning for action selection under uncertainty.</li> <li><strong>Risk Assessment and Confidence</strong>: Provides confidence intervals and helps quantify prediction risks.</li> <li><strong>Bayesian Inference</strong>: Updates beliefs about parameters or predictions using new data.</li> </ol> <hr/> <h3 id="random-variables-discrete-and-continuous"><strong>Random Variables: Discrete and Continuous</strong></h3> <p>Building upon the foundational axioms of probability, understanding <strong>random variables</strong> is the next critical step. Random variables bridge the gap between abstract probabilistic events and numerical representation, enabling a deeper connection between data and mathematical models.</p> <p>A <strong>random variable</strong> is a variable whose possible values are outcomes of a random process or experiment. It maps outcomes from a probabilistic event to real numbers, playing a central role in probability theory and machine learning. Random variables are typically categorized into two types: <strong>discrete</strong> and <strong>continuous</strong>.</p> <p><strong>Intuition for Random Variables</strong></p> <p>Think of a random variable as a “number generator” that transforms outcomes of a random process into numbers. For instance:</p> <ul> <li>In a dice roll, the outcome (e.g., rolling a 4) is translated to the random variable \(X = 4\).</li> <li>In measuring rainfall, the amount (e.g., 12.5 mm) is assigned to the random variable \(X = 12.5\). This abstraction helps in applying mathematical operations and deriving distributions.</li> </ul> <h4 id="discrete-random-variables"><strong>Discrete Random Variables</strong></h4> <p>A <strong>discrete random variable</strong> takes on a countable number of distinct values. These values are often integers or counts, representing outcomes that are distinct and separate from one another. For example, the number of heads obtained when flipping a coin multiple times is a discrete random variable.</p> <p><strong>Characteristics</strong>:</p> <ul> <li><strong>Countable Outcomes</strong>: The possible values can be listed, even if the list is infinite (e.g., the number of calls received by a call center).</li> <li><strong>Probability Mass Function (PMF)</strong>: The probability distribution of a discrete random variable is described by a probability mass function (PMF), which assigns probabilities to each possible value. The PMF satisfies:</li> </ul> \[P(X = x) \geq 0 \quad \text{for all } x\] \[\sum_{x} P(X = x) = 1\] <p>Here, the sum is over all possible values \(x\) that the random variable can take.</p> <p><strong>Examples of Discrete Random Variables</strong>:</p> <ol> <li> <p><strong>Number of heads in coin flips</strong>: Let \(X\) be the number of heads in 3 flips of a fair coin. The possible values of \(X\) are \(0, 1, 2,\) and \(3\). The probabilities for each outcome can be computed using the binomial distribution.</p> </li> <li> <p><strong>Number of customers arriving at a store</strong>: Let \(X\) represent the number of customers who arrive at a store during a 1-hour period. If customers arrive independently with an average rate of 3 per hour, \(X\) could follow a <strong>Poisson distribution</strong>.</p> </li> </ol> <h4 id="continuous-random-variables"><strong>Continuous Random Variables</strong></h4> <p>A <strong>continuous random variable</strong> can take on an infinite number of possible values within a given range. These values are not countable but form a continuum. For example, the height of a person is a continuous random variable because it can take any value within a range, such as \(5.6\) feet, \(5.65\) feet, \(5.654\) feet, and so on.</p> <p><strong>Characteristics</strong>:</p> <ul> <li><strong>Uncountably Infinite Outcomes</strong>: The possible values form a continuum, often represented by intervals on the real number line.</li> <li><strong>Probability Density Function (PDF)</strong>: The probability distribution of a continuous random variable is described by a probability density function (PDF). Unlike a PMF, the PDF does not give the probability of any specific outcome but rather the probability of the random variable falling within a certain range. The total area under the PDF curve is equal to 1, and the probability of the variable falling in an interval \([a, b]\) is given by:</li> </ul> \[P(a \leq X \leq b) = \int_{a}^{b} f_X(x) \, dx\] <p>where \(f_X(x)\) is the PDF of \(X\).</p> <p><strong>Examples of Continuous Random Variables</strong>:</p> <ol> <li><strong>Height of a person</strong>: The height \(X\) of a person can take any value within a realistic range (e.g., between 4 and 7 feet). The exact value is not countable, and it is typically modeled by a normal distribution.</li> <li><strong>Time taken to complete a task</strong>: If you measure the time \(X\) taken by someone to complete a task, this can take any value (e.g., 2.5 minutes, 2.55 minutes, etc.). The distribution could be modeled using an exponential or normal distribution, depending on the scenario.</li> </ol> <h4 id="why-are-random-variables-important-in-machine-learning"><strong>Why are Random Variables Important in Machine Learning?</strong></h4> <ol> <li><strong>Modeling Uncertainty</strong>: In machine learning, random variables allow us to model uncertainty in data and predictions. For instance, in regression models, the target variable is often modeled as a random variable with some uncertainty, usually represented as a continuous distribution.</li> <li><strong>Bayesian Inference</strong>: In Bayesian models, parameters are treated as random variables, and their distributions are updated with new data. This allows for probabilistic reasoning and uncertainty quantification in predictions.</li> <li><strong>Stochastic Processes</strong>: Many machine learning algorithms, such as those in reinforcement learning or Monte Carlo simulations, involve <strong>stochastic processes</strong>, where future states or outcomes are modeled as random variables with given distributions.</li> </ol> <hr/> <h3 id="more-on-probability-distribution-and-types"><strong>More on Probability Distribution and Types</strong></h3> <p>A probability distribution describes how probabilities are assigned to different possible outcomes of a random variable. It provides a mathematical function that represents the likelihood of each possible value the random variable can take. In simpler terms, it tells us how likely each outcome of an experiment or process is.</p> <p>Formally, A <strong>probability distribution</strong> of a random variable \(X\) is a function that provides the probabilities of occurrence of different possible outcomes for the random variable. Depending on the nature of the random variable, the probability distribution can take different forms:</p> <ol> <li><strong>Discrete Probability Distribution</strong>: This applies when the random variable can only take on a finite or countably infinite number of values (e.g., the number of heads in a coin flip). The distribution is described by a <strong>probability mass function (PMF)</strong>.</li> <li><strong>Continuous Probability Distribution</strong>: This applies when the random variable can take on an infinite number of values within a range (e.g., the height of a person). The distribution is described by a <strong>probability density function (PDF)</strong>.</li> </ol> <p>For both types, the total probability across all possible outcomes must sum (or integrate) to 1:</p> <ul> <li> <p>For discrete distributions:</p> \[\sum_{x} P(X = x) = 1\] </li> <li> <p>For continuous distributions:</p> \[\int_{-\infty}^{\infty} f_X(x) \, dx = 1\] <p>where \(f_X(x)\) is the probability density function.</p> </li> </ul> <h5 id="1-discrete-probability-distributions"><strong>1. Discrete Probability Distributions</strong></h5> <p>Discrete distributions are used when the random variable can take a countable number of distinct values. Some common discrete probability distributions are:</p> <ul> <li><strong>Bernoulli Distribution</strong>: Models a binary outcome (success/failure, \(1/0\)) of a single trial. The probability of success is \(p\), and the probability of failure is \(1 - p\). The PMF is: \(P(X = 1) = p, \quad P(X = 0) = 1 - p\) <ul> <li><strong>Example</strong>: The outcome of a coin flip (Heads = 1, Tails = 0).</li> </ul> </li> <li> <p><strong>Binomial Distribution</strong>: Describes the number of successes in a fixed number of independent Bernoulli trials. The random variable \(X\) counts the number of successes. The PMF is:</p> \[P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}\] <p>where \(n\) is the number of trials, \(p\) is the probability of success, and \(k\) is the number of successes.</p> <ul> <li><strong>Example</strong>: The number of heads in 10 coin flips.</li> </ul> </li> <li> <p><strong>Poisson Distribution</strong>: Models the number of events occurring in a fixed interval of time or space, given a constant average rate of occurrence. The PMF is:</p> \[P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}\] <p>where \(\lambda\) is the average rate of occurrence (mean), and \(k\) is the number of events.</p> <ul> <li><strong>Example</strong>: The number of phone calls received by a call center in an hour.</li> </ul> </li> </ul> <h5 id="2-continuous-probability-distributions">2. <strong>Continuous Probability Distributions</strong></h5> <p>Continuous distributions are used when the random variable can take any value within a given range or interval. These distributions are described by probability density functions (PDFs), where the probability of any single point is zero, and probabilities are calculated over intervals. Some common continuous probability distributions include:</p> <ul> <li> <p><strong>Uniform Distribution</strong>: A continuous distribution where all values within a given interval are equally likely. The PDF is:</p> \[f_X(x) = \frac{1}{b-a} \quad \text{for} \ a \leq x \leq b\] <ul> <li><strong>Example</strong>: The time it takes for a bus to arrive, uniformly distributed between 5 and 15 minutes.</li> </ul> </li> <li> <p><strong>Normal (Gaussian) Distribution</strong>: A continuous distribution that is symmetric and bell-shaped. It is fully described by its mean \(\mu\) and standard deviation \(\sigma\). The PDF is:</p> \[f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\] <ul> <li><strong>Example</strong>: The distribution of heights in a population.</li> </ul> </li> <li><strong>Exponential Distribution</strong>: A continuous distribution often used to model the time between events in a Poisson process (events happening at a constant rate). The PDF is: \(f_X(x) = \lambda e^{-\lambda x} \quad \text{for} \ x \geq 0\) <ul> <li><strong>Example</strong>: The time between arrivals of customers at a service station.</li> </ul> </li> <li> <p><strong>Gamma Distribution</strong>: A generalization of the exponential distribution, used for modeling the sum of multiple exponentially distributed random variables. Its PDF is:</p> \[f_X(x) = \frac{x^{k-1} e^{-x/\theta}}{\Gamma(k) \theta^k} \quad \text{for} \ x \geq 0\] <ul> <li><strong>Example</strong>: The waiting time until a certain number of events occur in a Poisson process.</li> </ul> </li> <li> <p><strong>Beta Distribution</strong>: A continuous distribution on the interval \([0, 1]\), often used to model probabilities and proportions. Its PDF is:</p> \[f_X(x) = \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha, \beta)}\] <ul> <li><strong>Example</strong>: Modeling the proportion of customers who prefer a certain product in a market research study.</li> </ul> </li> </ul> <h4 id="why-are-probability-distributions-important-in-machine-learning"><strong>Why are Probability Distributions Important in Machine Learning?</strong></h4> <ol> <li><strong>Modeling Uncertainty</strong>: Many machine learning models assume that the data follows a certain probability distribution (e.g., Gaussian distribution in linear regression).</li> <li><strong>Inference and Prediction</strong>: In probabilistic models, such as Bayesian inference or Hidden Markov Models, understanding the probability distributions of variables allows for reasoning about uncertainty and making predictions based on observed data.</li> <li><strong>Risk Analysis</strong>: Distributions help quantify the risk or uncertainty in machine learning predictions. For example, a model’s output might be a probability distribution over potential outcomes, providing insights into the confidence of predictions.</li> </ol> <h3 id="cumulative-distribution-function-cdf"><strong>Cumulative Distribution Function (CDF)</strong></h3> <p>CDF is closely related to the <strong>Probability Density Function (PDF)</strong> in the case of continuous random variables and the <strong>Probability Mass Function (PMF)</strong> for discrete variables. The CDF gives the cumulative probability that a random variable takes a value less than or equal to a particular point.</p> <p>For a random variable \(X\), the <strong>Cumulative Distribution Function (CDF)</strong>, denoted by \(F_X(x)\), is defined as the probability that the random variable \(X\) takes a value less than or equal to \(x\). Formally:</p> \[F_X(x) = P(X \leq x)\] <p>The CDF is a function that provides the cumulative probability up to a point \(x\) and is computed by integrating (for continuous variables) or summing (for discrete variables) the corresponding probability distributions.</p> <h4 id="properties-of-the-cdf"><strong>Properties of the CDF</strong></h4> <ol> <li> <p><strong>Non-decreasing</strong>: The CDF is a non-decreasing function, meaning that the probability increases as \(x\) increases:</p> \[F_X(x_1) \leq F_X(x_2) \quad \text{if} \ x_1 \leq x_2\] </li> <li> <p><strong>Range</strong>: The CDF always lies within the range \([0, 1]\) : \(0 \leq F_X(x) \leq 1 \quad \text{for all} \ x\)</p> </li> <li><strong>Limits</strong>: <ul> <li>As \(x \to -\infty\), the CDF tends to 0: \(\lim_{x \to -\infty} F_X(x) = 0\)</li> <li>As \(x \to \infty\), the CDF tends to 1: \(\lim_{x \to \infty} F_X(x) = 1\)</li> </ul> </li> <li><strong>Continuity</strong>: <ul> <li>For <strong>continuous random variables</strong>, the CDF is continuous and smooth.</li> <li>For <strong>discrete random variables</strong>, the CDF is a step function, with jumps corresponding to the probabilities at specific points.</li> </ul> </li> </ol> <h4 id="cdf-for-discrete-random-variables"><strong>CDF for Discrete Random Variables</strong></h4> <p>For a <strong>discrete random variable</strong> \(X\), the CDF is computed by summing the probabilities given by the PMF. If the possible values of \(X\) are \(x_1, x_2, \dots\), the CDF is:</p> \[F_X(x) = P(X \leq x) = \sum_{x_i \leq x} P(X = x_i)\] <p><strong>Example</strong> Consider a discrete random variable \(X\) that represents the outcome of a fair 6-sided die. The possible values for \(X\) are \(1, 2, 3, 4, 5, 6\), each with a probability of \(\frac{1}{6}\). The CDF for \(X\) is:</p> \[F_X(x) = \begin{cases} 0 &amp; \text{for} \ x &lt; 1 \\ \frac{1}{6} &amp; \text{for} \ 1 \leq x &lt; 2 \\ \frac{2}{6} &amp; \text{for} \ 2 \leq x &lt; 3 \\ \frac{3}{6} &amp; \text{for} \ 3 \leq x &lt; 4 \\ \frac{4}{6} &amp; \text{for} \ 4 \leq x &lt; 5 \\ \frac{5}{6} &amp; \text{for} \ 5 \leq x &lt; 6 \\ 1 &amp; \text{for} \ x \geq 6 \end{cases}\] <h4 id="cdf-for-continuous-random-variables"><strong>CDF for Continuous Random Variables</strong></h4> <p>For a <strong>continuous random variable</strong> \(X\), the CDF is obtained by integrating the PDF:</p> \[F_X(x) = P(X \leq x) = \int_{-\infty}^{x} f_X(t) \, d t\] <p><strong>Example</strong> For a continuous random variable \(X\) that follows a <strong>uniform distribution</strong> on the interval \([0, 1]\), the PDF is:</p> \[f_X(x) = \begin{cases} 1 &amp; \text{for} \ 0 \leq x \leq 1 \\ 0 &amp; \text{otherwise} \end{cases}\] <p>The CDF is:</p> \[F_X(x) = \begin{cases} 0 &amp; \text{for} \ x &lt; 0 \\ x &amp; \text{for} \ 0 \leq x \leq 1 \\ 1 &amp; \text{for} \ x &gt; 1 \end{cases}\] <p>This shows that for values of \(x\) between 0 and 1, the probability increases linearly from 0 to 1.</p> <h4 id="relationship-between-pdf-and-cdf"><strong>Relationship Between PDF and CDF</strong></h4> <p>For a continuous random variable \(X\), the PDF is the derivative of the CDF:</p> \[f_X(x) = \frac{d}{dx} F_X(x)\] <p>Conversely, the CDF can be obtained by integrating the PDF:</p> \[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\] <h4 id="why-is-the-cdf-important-in-machine-learning"><strong>Why is the CDF Important in Machine Learning?</strong></h4> <ol> <li><strong>Data Interpretation</strong>: The CDF provides a clear interpretation of the distribution of data and is useful for understanding the likelihood of a random variable being less than or equal to a specific value.</li> <li><strong>Probabilistic Decision Making</strong>: The CDF helps integrate outcomes for decision-making in models like <strong>Naive Bayes</strong> or <strong>Bayesian networks</strong>.</li> </ol> <h4 id="how-pmf-pdf-and-cdf-interrelate"><strong>How PMF, PDF, and CDF Interrelate</strong></h4> <ul> <li><strong>Discrete Variables</strong>: <ul> <li>PMF: \(P(X = x_i)\)</li> <li>CDF:</li> </ul> \[F_X(x) = \sum_{x_i \leq x} P(X = x_i)\] </li> <li><strong>Continuous Variables</strong>: <ul> <li>PDF: \(f_X(x)\)</li> <li>CDF:</li> </ul> \[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\] <ul> <li>PDF from CDF:</li> </ul> \[f_X(x) = \frac{d}{dx} F_X(x)\] </li> </ul> <h3 id="choosing-the-right-distribution-in-ml"><strong>Choosing the Right Distribution in ML</strong></h3> <p>Choosing the appropriate probability distribution for a given machine learning (ML) problem is crucial to making accurate predictions. Each probability distribution captures a unique set of characteristics regarding the randomness and uncertainty in the data. A proper understanding of these distributions can directly influence the performance and efficiency of your model. Here’s a more detailed look at the various distributions commonly used in ML:</p> <ul> <li> <p><strong>Bernoulli/Binomial Distributions:</strong> These are useful for binary or count outcomes. The <strong>Bernoulli distribution</strong> models a single trial with two possible outcomes, often labeled as success (1) or failure (0). The <strong>Binomial distribution</strong> generalizes this by modeling the number of successes in a fixed number of independent Bernoulli trials. These distributions are especially useful in classification tasks, such as predicting whether an email is spam or not.</p> </li> <li> <p><strong>Gaussian (Normal) Distribution:</strong> The Gaussian or <strong>normal distribution</strong> is one of the most widely used distributions in statistics and machine learning, especially when the data exhibits natural variability. It is characterized by its symmetric bell-shaped curve, defined by its mean and standard deviation. It’s particularly useful when you have continuous data that tends to cluster around a central value, such as the distribution of heights in a population or the error terms in regression models. Many machine learning algorithms, such as <strong>linear regression</strong> and <strong>k-nearest neighbors (KNN)</strong>, assume that the underlying data follows a Gaussian distribution.</p> </li> <li> <p><strong>Poisson/Exponential Distributions:</strong> These distributions model events that occur over time or space, where the events happen at a constant average rate. The <strong>Poisson distribution</strong> models the number of events happening in a fixed interval of time or space (e.g., the number of customer arrivals at a service station). The <strong>Exponential distribution</strong> is often used to model the time between events in a Poisson process. Both distributions are important in scenarios involving queues or event-based systems, like predicting the time between customer purchases or server failures in network systems.</p> </li> </ul> <h4 id="why-does-choosing-the-right-distribution-matter">Why Does Choosing the Right Distribution Matter?</h4> <ol> <li><strong>Tailoring Models to Data:</strong> Understanding the underlying distribution of the data helps in selecting the right model for your problem. For example, if the data is normally distributed, using models like <strong>linear regression</strong> (which assumes normality of residuals) can result in more accurate predictions. On the other hand, when data is binary (e.g., yes/no outcomes), a <strong>logistic regression</strong> or <strong>Bernoulli distribution</strong> approach would be more appropriate.</li> <li><strong>Improving Model Efficiency:</strong> When we align the assumptions of a machine learning algorithm with the real-world distribution of the data, models tend to be more efficient and require less computation. For instance, algorithms that work with Gaussian-distributed data can be optimized to take advantage of the symmetry of the distribution, leading to faster convergence in training.</li> <li><strong>Quantifying Uncertainty:</strong> Different distributions provide unique ways to handle uncertainty in predictions. For example, when working with <strong>Poisson distributions</strong>, we can predict the expected number of events in a fixed period with a known variance. In contrast, the <strong>Exponential distribution</strong> models waiting times, making it suitable for applications like survival analysis or reliability engineering.</li> </ol> <h4 id="further-exploration"><strong>Further Exploration</strong></h4> <p>While this overview has touched on the most commonly used distributions, the world of probability distributions in machine learning is vast. As you dive deeper into various ML topics, you’ll encounter additional distributions tailored to specific data types and problems. Understanding how these distributions behave allows you to refine your models for more accurate, effective predictions.</p> <p>If you’re eager to explore further, we will be diving deeper into these distributions as we continue our series on probability theory. The next section will introduce even more important concepts, so stay tuned for that!</p> <p><strong>See you in the next post!</strong></p> <h3 id="references"><strong>References:</strong></h3> <ul> <li>Add links for common distributions for making it visually imaginable and relatable!</li> <li>Go through it one more last time for corrections.</li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog explores essential probability concepts and their significance in machine learning.]]></summary></entry><entry><title type="html">Multivariate Calculus - Prerequisites for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/multivariate-calculus/" rel="alternate" type="text/html" title="Multivariate Calculus - Prerequisites for Machine Learning"/><published>2024-12-20T00:10:00+00:00</published><updated>2024-12-20T00:10:00+00:00</updated><id>https://monishver11.github.io/blog/2024/multivariate-calculus</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/multivariate-calculus/"><![CDATA[<p>Before we dive into the math-heavy world of machine learning, it’s crucial to build a solid foundation in multivariate calculus. This blog post covers the essentials you’ll need to understand and work with the mathematical concepts underpinning ML.</p> <p>If you’re already familiar with undergraduate-level topics like functions and vectors, this should feel like a natural progression. If not, I recommend revisiting those fundamentals first—they’ll make this much easier to grasp. While this list isn’t exhaustive, it’s sufficient to get started and serves as a reference as we build on these ideas.</p> <p>Let’s dive in!</p> <hr/> <h3 id="what-is-calculus"><strong>What is Calculus?</strong></h3> <p>Calculus is the study of continuous change, centered on two main concepts:</p> <ol> <li><strong>Differentiation</strong>: The process of finding rates of change or slopes of curves. It helps analyze how quantities change over time or in response to other variables.</li> <li><strong>Integration</strong>: The reverse of differentiation, used to calculate areas under curves and accumulate quantities.</li> </ol> <p>These tools are applied in fields like physics, engineering, economics, and computer science to solve real-world problems involving continuous change.</p> <hr/> <h3 id="what-is-multivariate-calculus"><strong>What is Multivariate Calculus?</strong></h3> <p>Multivariate calculus, or multivariable calculus, extends single-variable calculus to functions with multiple variables. Key components include:</p> <ul> <li><strong>Partial Derivatives</strong>: Measure how a function changes with respect to one variable while keeping others constant.</li> <li><strong>Multiple Integration</strong>: Used to calculate volumes and higher-dimensional quantities for functions of multiple variables.</li> <li><strong>Vector Calculus</strong>: Explores vector fields, gradients, and theorems like Green’s and Stokes’ theorems.</li> </ul> <h4 id="why-multivariate-calculus-matters"><strong>Why Multivariate Calculus Matters</strong></h4> <p>Multivariate calculus is indispensable for:</p> <ul> <li>Analyzing systems with multiple inputs.</li> <li>Solving optimization problems in higher dimensions.</li> <li>Modeling physical phenomena in 3D or more dimensions.</li> <li>Understanding advanced concepts in physics, engineering, and data science.</li> </ul> <p>By extending the tools of calculus to multidimensional problems, multivariate calculus equips us to tackle complex systems and optimize machine learning models.</p> <hr/> <h3 id="derivatives-a-fundamental-concept"><strong>Derivatives: A Fundamental Concept</strong></h3> <p>Derivatives are the backbone of calculus, measuring how a function’s output changes in response to its input. They play a critical role in understanding rates of change and optimizing functions.</p> <p>Here are some analogies to grasp the concept of derivatives:</p> <ol> <li><strong>The Speedometer Analogy</strong>:<br/> Imagine driving a car—the speedometer shows your instantaneous speed at any given moment. Similarly, a derivative tells you how fast a function is changing at a specific point.</li> <li><strong>Slope of a Tangent Line</strong>:<br/> Visually, the derivative represents the slope of the tangent line at a point on a graph. Think of it as placing a ruler on a curve to measure its tilt.</li> <li><strong>Sensitivity Measure</strong>:<br/> A derivative reveals how sensitive a function’s output is to small changes in its input. For example, it’s like determining how much the water level in a bathtub rises when you add a cup of water.</li> </ol> <h3 id="partial-derivatives-and-their-role-in-ml"><strong>Partial Derivatives and Their Role in ML</strong></h3> <p>Expanding on the concept of derivatives, <strong>partial derivatives</strong> allow us to analyze how functions of multiple variables change with respect to a single variable while keeping others constant. This is a cornerstone of multivariate calculus and a fundamental tool in machine learning.</p> <h4 id="what-are-partial-derivatives"><strong>What are Partial Derivatives?</strong></h4> <p>A partial derivative extends the idea of a derivative to multivariable functions. It isolates the effect of one variable while treating the others as fixed.</p> <p>Here are a couple of analogies to help visualize partial derivatives:</p> <ol> <li><strong>Mountain Climbing</strong>:<br/> Imagine standing on a mountain. A partial derivative measures the steepness in one specific direction (e.g., north-south or east-west), while you stay fixed in the same spot.</li> <li><strong>Baking a Cake</strong>:<br/> Consider a recipe where the outcome (taste) depends on multiple ingredients (variables). A partial derivative tells you how the taste changes when you adjust one ingredient (e.g., sugar) while keeping all others constant.</li> </ol> <h4 id="applications-in-machine-learning"><strong>Applications in Machine Learning</strong></h4> <p>Partial derivatives are indispensable in machine learning, especially when working with models that involve multiple parameters.</p> <ol> <li><strong>Gradient Descent</strong>: <ul> <li>The <strong>gradient vector</strong>, composed of partial derivatives, points in the direction of the steepest increase of a function.</li> <li>Gradient descent uses this information to move in the opposite direction, minimizing the loss function and optimizing model parameters.</li> </ul> </li> <li><strong>Back-propagation in Neural Networks</strong>: <ul> <li>Partial derivatives calculate how each weight in a neural network contributes to the overall error, enabling efficient training through backpropagation.</li> </ul> </li> <li><strong>Optimization of Complex Loss Functions</strong>: <ul> <li>In high-dimensional parameter spaces, partial derivatives help navigate the loss landscape to find optimal solutions.</li> </ul> </li> <li><strong>Sensitivity Analysis</strong>: <ul> <li>They provide insights into how sensitive a model’s output is to changes in individual input variables, aiding interpretability and robustness.</li> </ul> </li> <li><strong>Second-Order Optimization</strong>: <ul> <li>Techniques like Newton’s method use the <strong>Hessian matrix</strong> (second-order partial derivatives) to achieve faster convergence.</li> </ul> </li> </ol> <p>By employing partial derivatives, machine learning algorithms can effectively optimize performance across multiple parameters and gain insights into intricate relationships within the data.</p> <hr/> <h3 id="gradient-vectors-a-multivariable-power-tool"><strong>Gradient Vectors: A Multivariable Power Tool</strong></h3> <p>The <strong>gradient vector</strong> combines partial derivatives to form a powerful tool for analyzing multivariable functions. It indicates both the direction and magnitude of the steepest ascent at any given point.</p> <h4 id="what-is-a-gradient-vector"><strong>What is a Gradient Vector?</strong></h4> <p>The gradient vector generalizes the derivative to functions with multiple variables, providing a way to “sense” the terrain of the function.</p> <p>Here are some analogies to conceptualize gradient vectors:</p> <ol> <li><strong>Mountain Climber’s Compass</strong>:<br/> Imagine you’re on a mountain, and you have a compass that always points uphill in the steepest direction. That’s the gradient vector—it guides you to the quickest ascent.</li> <li><strong>Water Flow on a Surface</strong>:<br/> Think of water droplets on a curved surface. The gradient vector at any point shows the direction water would flow—the steepest descent.</li> <li><strong>Heat-Seeking Missile</strong>:<br/> The gradient works like a heat-seeking missile’s guidance system, constantly recalibrating to move toward the function’s maximum.</li> </ol> <h4 id="applications-in-machine-learning-1"><strong>Applications in Machine Learning</strong></h4> <p>Gradient vectors are pivotal in optimization and training algorithms:</p> <ol> <li><strong>Gradient Descent</strong>: <ul> <li>The gradient vector points toward the steepest ascent, so moving in the opposite direction minimizes the loss function.</li> </ul> </li> <li><strong>Adaptive Learning Rate Methods</strong>: <ul> <li>Advanced algorithms like AdaGrad and Adam utilize the gradient vector to adjust learning rates dynamically for each parameter.</li> </ul> </li> <li><strong>Local Linear Approximation</strong>: <ul> <li>The gradient provides a local linear estimate of the function, helping algorithms make informed adjustments to parameters.</li> </ul> </li> </ol> <p>By leveraging the gradient vector, machine learning algorithms efficiently navigate complex parameter spaces, optimize models, and adapt to data characteristics. The gradient not only informs about the loss landscape but also helps shape strategies to improve performance.</p> <p>You might be wondering why the gradient points in the direction of steepest ascent—see the references below for more details.</p> <hr/> <h3 id="hessian-and-jacobian-higher-order-tools"><strong>Hessian and Jacobian: Higher-Order Tools</strong></h3> <p>Building on partial derivatives and gradient vectors, <strong>Hessian</strong> and <strong>Jacobian matrices</strong> offer even deeper insights into multivariable functions. These higher-order constructs are essential for advanced optimization techniques and will be explored in detail next down.</p> <h4 id="jacobian-matrix"><strong>Jacobian Matrix</strong></h4> <p>The Jacobian matrix generalizes the gradient vector for vector-valued functions, capturing how changes in multiple inputs affect multiple outputs.</p> <h5 id="definition"><strong>Definition</strong></h5> \[\text{For a function } \mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m, \text{ the Jacobian matrix } \mathbf{J} \text{ is an } m \times n \text{ matrix defined as:}\] \[\mathbf{J} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n} \end{bmatrix}\] \[\text{The entries of the Jacobian matrix are partial derivatives of the component functions } f_i\] \[\text{ with respect to the input variables } x_j.\] <h4 id="hessian-matrix"><strong>Hessian Matrix</strong></h4> <p>The Hessian matrix contains all second-order partial derivatives of a scalar-valued function. It provides information about the curvature of the function, making it essential for understanding the landscape of optimization problems.</p> <h5 id="definition-1"><strong>Definition</strong></h5> \[\text{For a function } f: \mathbb{R}^n \to \mathbb{R}, \text{ the Hessian matrix } \mathbf{H} \text{ is an } n \times n \text{ symmetric matrix defined as:}\] \[\mathbf{H} = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}\] \[\text{The Hessian provides information about the curvature of the function in all directions.}\] <p>Both the Jacobian and Hessian matrices extend the concept of partial derivatives to offer deeper insights into multivariable functions. In machine learning, these tools enhance the ability to analyze and optimize models by providing detailed information about variable interdependencies and the curvature of the function’s landscape. The Jacobian is particularly useful for understanding transformations and sensitivities in vector-valued functions, while the Hessian aids in second-order optimization by characterizing curvature and guiding efficient convergence. Together, they enable sophisticated analysis and optimization techniques, making it possible to tackle the complexities of high-dimensional spaces typical in machine learning tasks.</p> <hr/> <h3 id="taylor-series"><strong>Taylor Series</strong></h3> <p>The Taylor series is a powerful tool that approximates complex functions using simpler polynomial expressions. This approximation is widely used in optimization and machine learning.</p> <h4 id="definition-2"><strong>Definition</strong></h4> \[\text{For a function } f(x) \text{ that is infinitely differentiable at a point } a,\] \[\text{ the Taylor series is given by:}\] \[f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \cdots\] \[\text{And more generally the Taylor series is given by:}\] \[f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x - a)^n\] <p>Quick analogies to idealize it:</p> <ol> <li> <p><strong>Function Microscope</strong>:<br/> The Taylor series “zooms in” on a specific point of a function, describing its local behavior with increasing precision.</p> </li> <li> <p><strong>Prediction Machine</strong>:<br/> Think of it as a step-by-step refinement of a guess about the function’s behavior. It starts with a constant, then linear, quadratic, cubic, and so on, improving accuracy at each step.</p> </li> </ol> <h4 id="applications-in-machine-learning-2"><strong>Applications in Machine Learning</strong></h4> <ol> <li><strong>Function Approximation</strong>:<br/> Simplifies complex functions into polynomials that are computationally easier to work with.</li> <li><strong>Optimization Algorithms</strong>:<br/> Techniques like Newton’s method use Taylor approximations to estimate minima or maxima.</li> <li><strong>Gradient Estimation</strong>:<br/> When direct computation of gradients is challenging, Taylor series can approximate them.</li> </ol> <p>The Taylor series provides a detailed local understanding of functions, facilitating optimization, gradient estimation, and model interpretation. This makes it a valuable tool for bridging the gap between continuous mathematical phenomena and practical computational algorithms, playing a crucial role in solving complex problems and enhancing the efficiency of machine learning workflows.</p> <p>This foundational knowledge from multivariate calculus sets the stage for deeper exploration. If you’ve made it this far, congratulations! You’ve taken an important step in understanding and internalizing key concepts in machine learning. Take a moment to reflect, and when you’re ready, let’s move forward to the next topic. See you there!</p> <h3 id="references"><strong>References</strong></h3> <ul> <li><a href="https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent" target="_blank">Why is gradient the direction of steepest ascent?</a></li> <li><a href="https://betterexplained.com/articles/calculus-building-intuition-for-the-derivative/" target="_blank">Calculus: Building Intuition for the Derivative – BetterExplained</a></li> <li><a href="https://photomath.com/articles/what-is-calculus-definition-applications-and-concepts/" target="_blank">What is Calculus? Definition, Applications, and Concepts – Photomath</a></li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.]]></summary></entry><entry><title type="html">Linear Algebra - Prerequisites for Machine Learning</title><link href="https://monishver11.github.io/blog/2024/linear-algebra/" rel="alternate" type="text/html" title="Linear Algebra - Prerequisites for Machine Learning"/><published>2024-12-20T00:10:00+00:00</published><updated>2024-12-20T00:10:00+00:00</updated><id>https://monishver11.github.io/blog/2024/linear-algebra</id><content type="html" xml:base="https://monishver11.github.io/blog/2024/linear-algebra/"><![CDATA[<p>Linear algebra forms the backbone of modern machine learning. As a branch of mathematics, it deals with vector spaces and the linear transformations between them. This area of study allows for the manipulation and efficient computation of datasets, making it fundamental to various machine learning algorithms. Whether you’re working with deep learning, regression models, or optimization techniques, a solid understanding of linear algebra will be crucial to mastering machine learning.</p> <hr/> <h2 id="core-components-of-linear-algebra"><strong>Core Components of Linear Algebra</strong></h2> <h3 id="vectors-and-matrices"><strong>Vectors and Matrices</strong></h3> <h4 id="1-vectors"><strong>1. Vectors</strong></h4> <p>A <strong>vector</strong> is a fundamental concept in linear algebra and is essentially a one-dimensional array of numbers. In machine learning, vectors can represent different elements, including features, weights, or data points.</p> <ul> <li><strong>Definition:</strong> A vector is a set of numbers arranged in a specific order, and it can be represented either as a <strong>row vector</strong> or a <strong>column vector</strong>. <ul> <li>Row vector: \(\mathbf{v} = [v_1, v_2, \dots, v_n]\)</li> <li>Column vector: \(\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n\end{bmatrix}\)</li> </ul> </li> <li> <p><strong>Properties:</strong></p> <ul> <li><strong>Magnitude (Norm):</strong> The magnitude of a vector, often referred to as its norm, measures the vector’s length. The most common norms used are the L2 norm (Euclidean norm) and L1 norm.</li> </ul> \[\|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2} \ \ ; \quad \|\mathbf{v}\|_1 = \sum_{i=1}^{n} |v_i|\] <ul> <li><strong>Dot Product:</strong> The dot product of two vectors measures their similarity. The dot product between two vectors \(\mathbf{v}_1​\) and \(\mathbf{v}_2\)​ is computed as:</li> </ul> \[\mathbf{v}_1 \cdot \mathbf{v}_2 = \sum_{i=1}^{n} v_{1i} v_{2i}\] <ul> <li><strong>Distance:</strong> The Euclidean distance is a common way to measure the difference between two vectors:</li> </ul> \[d(\mathbf{v}_1, \mathbf{v}_2) = \sqrt{\sum_{i=1}^{n} (v_{1i} - v_{2i})^2}\] </li> <li><strong>Operations on Vectors:</strong> <ul> <li><strong>Addition:</strong> Vectors of the same size can be added element-wise.</li> <li><strong>Scalar Multiplication:</strong> Multiplying each element of a vector by a scalar.</li> <li><strong>Dot Product:</strong> A fundamental operation for determining the similarity between two vectors.</li> </ul> </li> </ul> <h4 id="2-matrices"><strong>2. Matrices</strong></h4> <p>A <strong>matrix</strong> is a two-dimensional array of numbers, and it is widely used in machine learning for data storage, transformations, and solving systems of equations.</p> <ul> <li> <p><strong>Definition:</strong> A matrix consists of rows and columns and is denoted as \(A\), where \(A_{ij}\)​ represents the element in the i-th row and j-th column.</p> \[A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn} \end{bmatrix}\] </li> <li><strong>Properties of Matrices:</strong> <ul> <li><strong>Rank:</strong> The rank of a matrix is the maximum number of linearly independent rows or columns, indicating the number of independent dimensions in the matrix.</li> <li><strong>Trace:</strong> The trace is the sum of the diagonal elements of a square matrix. It is often involved in optimization problems.</li> <li><strong>Determinant:</strong> The determinant helps in determining whether a matrix is invertible. A non-zero determinant implies that the matrix is invertible.</li> <li><strong>Invertibility:</strong> A matrix \(A\) is invertible if it has full rank and a non-zero determinant. The inverse of a matrix \(A\) is denoted by \(A^{-1}\), and it satisfies the equation: \(A A^{-1} = I\) where \(I\) is the identity matrix.</li> </ul> </li> <li><strong>Operations on Matrices:</strong> <ul> <li><strong>Matrix Addition/Subtraction:</strong> Matrices of the same dimension can be added or subtracted element-wise.</li> <li><strong>Matrix Multiplication:</strong> Matrix multiplication is the dot product of rows and columns between two matrices. This operation is central to machine learning algorithms.</li> <li><strong>Transpose:</strong> The transpose of a matrix \(A\) is denoted as \(A^T\) and involves flipping its rows and columns.</li> <li><strong>Inverse:</strong> If a matrix is invertible, its inverse can be used to solve systems of linear equations.</li> </ul> </li> </ul> <h3 id="vectors-and-matrices-in-ml"><strong>Vectors and Matrices in ML</strong></h3> <p>Vectors and matrices play a pivotal role in representing both data and models in machine learning.</p> <p><strong>1. Data Representation</strong></p> <ul> <li>In supervised learning, each data point is typically represented as a feature vector. For example, if a dataset has \(m\) samples and \(n\) features, it can be represented as an \(m \times n\) matrix, where each row corresponds to a feature vector for a data point. The corresponding labels or target values are often stored in a vector.</li> </ul> <p><strong>2. Model Representation</strong></p> <ul> <li>In models like linear regression and neural networks, the weights that transform input data are stored in vectors or matrices. For example, in linear regression, the model is defined as: \(\hat{y} = Xw + b\) where \(X\) is the data matrix, \(w\) is the weight vector, and \(b\) is the bias term.</li> </ul> <p><strong>3. Operations in Machine Learning Algorithms</strong></p> <ul> <li><strong>Linear Regression:</strong> In linear regression, matrix operations are used to solve for the optimal weights. The normal equation for linear regression is:</li> </ul> \[w=(X^TX)^{−1}X^Ty\] <p>where \(X\) is the matrix of input features and \(y\) is the vector of target values.</p> <ul> <li><strong>Neural Networks:</strong> Each layer of a neural network applies a linear transformation to its input, which is represented by matrix multiplication:</li> </ul> \[y=XW+b\] <p>where \(X\) is the input matrix, \(W\) is the weight matrix, and \(b\) is the bias vector.</p> <ul> <li><strong>Gradient Descent:</strong> The gradient descent optimization algorithm frequently uses vector and matrix operations to update model parameters iteratively. In deep learning, the gradient of the loss function with respect to the weights and biases is calculated using matrix operations during back-propagation.</li> </ul> <p><strong>4. Dimensionality Reduction</strong></p> <ul> <li>Principal Component Analysis (PCA) is a popular technique for dimensionality reduction. It involves finding the eigenvectors and eigenvalues of the covariance matrix of the data. Eigenvalues and Eigenvectors are explained down below.</li> </ul> <hr/> <h3 id="eigenvalues-and-eigenvectors"><strong>Eigenvalues and Eigenvectors</strong></h3> <h5 id="definition"><strong>Definition:</strong></h5> <ul> <li> <p><strong>Eigenvector:</strong><br/> An eigenvector of a square matrix \(\mathbf{A}\) is a non-zero vector \(\mathbf{v}\) that, when the matrix \(\mathbf{A}\) is applied to it, only scales the vector without changing its direction:</p> \[\mathbf{A} \mathbf{v} = \lambda \mathbf{v}\] <p>where:</p> <ul> <li>\(\mathbf{v}\) is the eigenvector,</li> <li>\(\lambda\) is the eigenvalue, the scalar that represents how much the eigenvector is scaled by the transformation.</li> </ul> </li> <li> <p><strong>Eigenvalue:</strong><br/> The eigenvalue \(\lambda\) is the factor by which the eigenvector is scaled when the matrix \(\mathbf{A}\) acts on it.</p> </li> </ul> <p><strong>To build intuition, consider this analogy:</strong></p> <p>Imagine a squishy sheet of rubber (the matrix) and a point in space (the vector). If you apply a transformation (like stretching, rotating, or shearing) to the point using the rubber sheet, most points move to new locations. However, some special points, called <strong>eigenvectors</strong>, only get <strong>stretched</strong> or <strong>compressed</strong> but <strong>stay in the same direction</strong>. The amount of stretching or compression is determined by the <strong>eigenvalue</strong>.</p> <p><strong>Mathematical Properties of Eigenvalues and Eigenvectors</strong></p> <ul> <li>Eigenvectors must be <strong>non-zero vectors</strong>.</li> <li>Eigenvalues can be <strong>real</strong> or <strong>complex</strong> (but are often real in machine learning applications).</li> <li>A matrix can have multiple eigenvectors corresponding to the <strong>same eigenvalue</strong> (if it is <strong>degenerate</strong>) or distinct eigenvalues corresponding to distinct eigenvectors.</li> </ul> <h4 id="why-are-eigenvalues-and-eigenvectors-important-in-machine-learning"><strong>Why Are Eigenvalues and Eigenvectors Important in Machine Learning?</strong></h4> <p><strong>PCA</strong> is a widely used technique for <strong>dimensionality reduction</strong> in machine learning. It reduces the number of features while retaining the most important information in the dataset.</p> <ul> <li><strong>Covariance Matrix:</strong> PCA begins by computing the covariance matrix to capture relationships between features.</li> <li><strong>Eigenvectors of Covariance Matrix:</strong> The eigenvectors represent the directions of maximum variance in the data—these are the <strong>principal components</strong>.</li> <li><strong>Eigenvalues:</strong> The corresponding eigenvalues indicate the magnitude of variance in each direction.</li> </ul> <p>Key steps in PCA:</p> <ul> <li>Sort eigenvectors in decreasing order of their eigenvalues.</li> <li>Select the top <strong>k</strong> eigenvectors to reduce dimensionality while preserving most of the variance.</li> </ul> <h4 id="key-takeaways-of-eigenvalues-and-eigenvectors-in-ml"><strong>Key takeaways of Eigenvalues and Eigenvectors in ML</strong></h4> <ul> <li><strong>Diagonalizability:</strong><br/> A matrix is diagonalizable if it has enough eigenvectors to form a full basis. This property is essential in PCA and <strong>Singular Value Decomposition (SVD)</strong>, enabling efficient computation and interpretation.</li> <li><strong>Magnitude of Eigenvalues:</strong><br/> The magnitude of eigenvalues corresponds to the <strong>variance captured</strong> by the associated eigenvectors (principal components). Larger eigenvalues imply more variance explained.</li> <li><strong>Orthogonality of Eigenvectors (Symmetric Matrices):</strong><br/> For <strong>symmetric matrices</strong>, eigenvectors are <strong>orthogonal</strong>. This is critical in PCA, where the principal components are orthogonal, ensuring that reduced dimensions remain <strong>uncorrelated</strong>.</li> </ul> <hr/> <h3 id="a-few-more-key-matrices-types-relevant-to-ml"><strong>A Few More Key Matrices Types Relevant to ML</strong></h3> <h4 id="symmetric-matrix"><strong>Symmetric Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix \(A\) is <strong>symmetric</strong> if \(A = A^T\), meaning it is equal to its transpose.</li> <li><strong>Properties:</strong> <ul> <li>A symmetric matrix always has real eigenvalues and orthogonal eigenvectors.</li> <li>If \(A\) is symmetric, it is always diagonalizable.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Covariance Matrices:</strong> Covariance matrices are always symmetric because the covariance between two features is the same regardless of the order.</li> <li><strong>Optimization Problems:</strong> Many optimization problems in machine learning involve symmetric matrices (e.g., in second-order optimization methods like Newton’s method or in regularization).</li> </ul> </li> </ul> <h4 id="orthogonal-matrix"><strong>Orthogonal Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix \(A\) is <strong>orthogonal</strong> if \(A^T A = I\), where \(I\) is the identity matrix.</li> <li><strong>Properties:</strong> <ul> <li>The rows and columns of an orthogonal matrix are orthonormal (i.e., they are both orthogonal and of unit length).</li> <li>The inverse of an orthogonal matrix is equal to its transpose \((A^{-1} = A^T)\).</li> <li>The determinant of an orthogonal matrix is either +1 or -1.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Rotation and Transformation:</strong> Orthogonal matrices are used in certain machine learning algorithms for transformations that preserve distances and angles. For example, in PCA, orthogonal transformation is used to create new orthogonal basis vectors.</li> </ul> </li> </ul> <h4 id="positive-definite-matrix-pd"><strong>Positive Definite Matrix (PD):</strong></h4> <ul> <li><strong>Definition:</strong> A square matrix \(A\) is <strong>positive definite</strong> if for any non-zero vector \(\mathbf{v}\), \(\mathbf{v}^T A \mathbf{v} &gt; 0\). In simpler terms, it means that the matrix has strictly positive eigenvalues.</li> <li><strong>Properties:</strong> <ul> <li>All eigenvalues are positive.</li> <li>The matrix is invertible (non-singular).</li> <li>It implies that the quadratic form is always positive.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Optimization Problems:</strong> In convex optimization, the Hessian matrix of a convex function is often positive definite. This ensures that a function has a unique local minimum, making optimization well-posed.</li> <li><strong>Covariance Matrices:</strong> The covariance matrix of any dataset with multiple features is positive semi-definite. In special cases (e.g., full rank), it can be positive definite.</li> </ul> </li> </ul> <h4 id="positive-semi-definite-matrix-psd"><strong>Positive Semi-Definite Matrix (PSD):</strong></h4> <ul> <li><strong>Definition:</strong> A matrix \(A\) is <strong>positive semi-definite</strong> if for any vector \(\mathbf{v}\), \(\mathbf{v}^T A \mathbf{v} \geq 0\). In other words, all eigenvalues are non-negative (i.e., zero or positive).</li> <li><strong>Properties:</strong> <ul> <li>Eigenvalues are non-negative \((\lambda_i \geq 0)\).</li> <li>The matrix may not be invertible if it has zero eigenvalues.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Covariance Matrices:</strong> As mentioned, the covariance matrix of a dataset is positive semi-definite. This is essential because covariance cannot be negative and the matrix represents the relationship between features.</li> <li><strong>Kernel Matrices (in SVMs, Gaussian Processes, etc.):</strong> The kernel matrix in algorithms like SVM and kernel PCA is always positive semi-definite. It measures similarity between data points in a transformed feature space.</li> </ul> </li> </ul> <h4 id="covariance-matrix"><strong>Covariance Matrix:</strong></h4> <ul> <li> <p><strong>Definition:</strong> A <strong>covariance matrix</strong> is a square matrix that contains the covariances between pairs of features in a dataset. If a dataset has \(n\) features, the covariance matrix will be an \(n \times n\) matrix, where each entry represents the covariance between two features.</p> </li> <li> <p><strong>Covariance of two variables X and Y:</strong></p> \[\text{Cov}(X, Y) = \frac{1}{m} \sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})\] <p>where \(m\) is the number of data points, and \(\bar{x}\) and \(\bar{y}\)​ are the means of the features \(X\) and \(Y\), respectively.</p> </li> <li> <p><strong>Covariance Matrix Definition:</strong> For a dataset with \(n\) features, the covariance matrix \(\Sigma\) is an \(n \times n\) matrix where each entry is:</p> \[\Sigma_{ij} = \text{Cov}(X_i, X_j)\] <p>where \(X_i\)​ and \(X_j\)​ are the \(i\)-th and \(j\)-th features, respectively.</p> </li> <li><strong>Properties:</strong> <ul> <li><strong>Symmetry:</strong> The covariance matrix is always symmetric, i.e., \(\Sigma_{ij} = \Sigma_{ji}\)</li> <li><strong>Positive Semi-Definiteness (PSD):</strong> The covariance matrix is always positive semi-definite, meaning for any vector \(\mathbf{v}\), \(\mathbf{v}^T \Sigma \mathbf{v} \geq 0\).</li> <li><strong>Diagonal Entries (Variance):</strong> The diagonal entries represent the variance of individual features.</li> <li><strong>Off-Diagonal Entries (Covariance):</strong> The off-diagonal entries represent the covariance between different features. Positive covariance indicates that the features increase or decrease together, while negative covariance suggests they move inversely.</li> <li><strong>Eigenvalues and Eigenvectors:</strong> The eigenvectors of the covariance matrix represent the directions of maximum variance, while the eigenvalues represent the magnitude of variance along these directions.</li> <li><strong>Rank:</strong> The rank of the covariance matrix corresponds to the number of <strong>independent</strong> features. If the matrix is rank-deficient, it indicates linearly dependent features.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>PCA:</strong> In PCA, the covariance matrix is used to identify the directions (eigenvectors) of maximum variance in the dataset. Eigenvalues indicate how much variance is explained by each principal component. This helps in dimensionality reduction by selecting the most important components.</li> <li><strong>Multivariate Gaussian Distribution:</strong> In probabilistic models like <strong>Gaussian Mixture Models (GMM)</strong>, the covariance matrix defines the shape of the data distribution. It is used to model the distribution of features in a multi-dimensional space.</li> <li><strong>Feature Selection:</strong> Covariance matrices help identify correlated features. Features that show high covariance (i.e., strong correlation) can be dropped or combined to improve model performance and reduce dimensionality.</li> </ul> </li> </ul> <h4 id="full-rank-matrix"><strong>Full Rank Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix is <strong>full rank</strong> if its rank is equal to the smallest of its number of rows or columns. In other words, all rows (or columns) are linearly independent.</li> <li><strong>Properties:</strong> <ul> <li>A matrix \(A\) with full rank has no redundant or dependent rows or columns.</li> <li>If \(A\) is an \(m \times n\) matrix, and \(\text{rank}(A) = \min(m, n)\), the matrix is full rank.</li> <li>A full rank matrix is <strong>invertible</strong> if it is square (i.e., if \(m = n\)).</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Linear Regression:</strong> In linear regression, the design matrix \(X\) must be full rank to ensure a unique solution. If \(X\) is not full rank, the matrix \(X^T X\) is singular and cannot be inverted.</li> </ul> </li> </ul> <h4 id="singular-matrix"><strong>Singular Matrix:</strong></h4> <ul> <li><strong>Definition:</strong> A matrix is <strong>singular</strong> if it is not invertible, meaning its determinant is zero. A singular matrix has linearly dependent rows or columns.</li> <li><strong>Properties:</strong> <ul> <li>The determinant of a singular matrix is 0.</li> <li>The matrix has at least one eigenvalue equal to 0.</li> </ul> </li> <li><strong>Relevance in Machine Learning:</strong> <ul> <li><strong>Linear Dependence:</strong> If the feature matrix \(X\) in a linear model is singular, some features are perfectly correlated, and this leads to instability in training and difficulties in solving for the model parameters.</li> </ul> </li> </ul> <hr/> <p>That wraps up the key linear algebra concepts for machine learning. This post is designed as a quick reference rather than an exhaustive guide. Don’t stress about memorizing everything—focus instead on understanding the concepts and knowing when to revisit them if needed.</p> <p>Math is a language, and like any language, it’s more about learning to use it than memorizing rules. Treat this as a foundation to build on, and come back to refresh your knowledge whenever necessary.</p> <p>Up next, we’ll explore the prerequisites of <strong>Probability Theory</strong> for machine learning. Since probability can often feel trickier, we’ll focus more on “what,” “why,” and “how” questions to make the concepts intuitive and approachable.</p> <p>See you in the next one!</p> <h3 id="references"><strong>References:</strong></h3> ]]></content><author><name></name></author><category term="sample-posts"/><category term="ML"/><summary type="html"><![CDATA[This blog post covers the key linear algebra concepts and their applications in machine learning.]]></summary></entry></feed>