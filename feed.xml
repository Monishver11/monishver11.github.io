<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://monishver11.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://monishver11.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-07T22:07:20+00:00</updated><id>https://monishver11.github.io/feed.xml</id><title type="html">blank</title><subtitle>A space for journaling my learnings, career, thoughts, and experiences in this amazing ride of life. </subtitle><entry><title type="html">Apache Flink</title><link href="https://monishver11.github.io/blog/2025/big-data-11-flink/" rel="alternate" type="text/html" title="Apache Flink"/><published>2025-12-22T23:54:00+00:00</published><updated>2025-12-22T23:54:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-11-flink</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-11-flink/"><![CDATA[<p><strong>Slide 17</strong></p> <p>Stateful stream processing refers to the design of applications that handle continuous, unbounded streams of events (like user clicks, orders, server logs, or sensor readings) while maintaining state—intermediate data that summarizes, aggregates, or tracks information across multiple events. Unlike stateless processing, where each event is handled independently, stateful processing allows the application to remember past events and use that information to compute results for new events.</p> <p>For example, a stateful stream processing application could:</p> <ul> <li>Track the running count of clicks per user on a website.</li> <li>Maintain a windowed sum of sales per product over the last hour.</li> <li>Store sensor readings to compute rolling averages or detect anomalies.</li> </ul> <p>In practice, the application reacts to each incoming event by reading from and/or writing to its state, performing computations that can depend on both the new event and historical information stored in the state. This enables complex operations like aggregations, joins, or pattern detection on real-time streams of data.</p> <p>State here is durable and fault-tolerant (managed by the stream processing framework, e.g., Flink), so even if the application crashes or is restarted, the state can be recovered and processing can continue correctly.</p> <p><strong>Slide 19 &amp; 20</strong></p> <p>Stateful stream processing applications often consume events from an event log such as Kafka. An event log is a durable, append-only store of events, which guarantees that events are written in order and that this order cannot be changed. This design allows multiple consumers to read the same stream independently and ensures that all consumers see events in the exact same order.</p> <p>When a stateful application like Flink connects to an event log, the log provides durable storage and deterministic replay of events. Each incoming event can be processed while updating the application’s state. If the application crashes or fails, Flink can restore the last consistent snapshot of its state from a checkpoint and reset its read position on the event log. It then replays events from that point onward until it catches up to the latest event in the stream.</p> <p>This mechanism ensures fault tolerance and exactly-once semantics: the application can recover from failures without losing data or processing events out of order. It also allows the same input stream to be reprocessed multiple times if needed—for example, to recompute results after a logic update—because the event log persists all events in order.</p> <p>In essence, the combination of stateful processing in Flink and the durable, ordered nature of event logs allows applications to maintain correct state, recover from failures, and guarantee deterministic processing over unbounded streams.</p> <p><strong>Slide 69-72</strong></p> <p>In stream processing, “one minute” can mean different things depending on the time semantics you choose, and this choice has a big impact on correctness and predictability.</p> <p>With processing time, “one minute” refers to one minute on the wall clock of the machine running the operator. A processing-time window simply groups together all events that arrive at the operator within that one-minute interval, regardless of when those events actually occurred in the real world. This makes processing-time windows fast and simple, but also sensitive to system behavior: network delays, backpressure, retries, or slower machines can shift events into different windows, meaning the same input stream may produce different results at different times or under different loads.</p> <p>With event time, “one minute” refers to one minute in the timeline of the events themselves, based on timestamps attached to each event (for example, when a user clicked a button or a sensor recorded a measurement). An event-time window groups events by when they happened, not when they arrived. Because the timestamps are part of the data, event time decouples processing speed from results: whether the stream is processed fast or slow, or events arrive late or out of order, the window computation is based on the same logical time boundaries. As a result, event-time operations are predictable and deterministic, yielding the same output as long as the input events and their timestamps are the same.</p> <p><strong>Slide 73-76</strong></p> <p>Watermarks are how a stream processor like Flink decides when an event-time window is complete and ready to be evaluated. Since events can arrive late or out of order, the system cannot rely on wall-clock time. Instead, a watermark acts as a logical event-time clock that advances as the system gains confidence about how far it has progressed in event time.</p> <p>Conceptually, a watermark with timestamp T means: “I believe that all events with event-time ≤ T have already arrived.” When an operator receives this watermark, it can safely close event-time windows that end at or before T, trigger their computation, and emit results. This makes watermarks essential for event-time windows and for correctly handling out-of-order events, because they tell operators when to stop waiting for older timestamps.</p> <p>Watermarks introduce a tradeoff between latency and correctness (confidence). If watermarks advance aggressively (eager watermarks), results are produced quickly, but there is a higher risk that late events will show up after a window has already been computed. If watermarks advance conservatively (relaxed watermarks), the system waits longer before triggering windows, increasing confidence that all relevant events have arrived—but at the cost of higher end-to-end latency.</p> <p>Because late events can still occur even after a watermark, stream processing systems must define how to handle them. Common strategies include dropping late events, logging or redirecting them for monitoring, or using them to update or correct previously emitted results (for example, via retractions or updates). This is why watermarks alone are not enough—the system’s late-event handling policy is just as important for correctness.</p> <p>In Flink, watermarks are defined by the developer and generated by the source operator of the stream. When a Flink job reads events from a source such as Kafka, the developer configures a watermark strategy that tells Flink how to extract event-time timestamps from each event and how much out-of-orderness (lateness) is allowed. Based on this logic, the source periodically emits watermarks that signal the system’s progress in event time, indicating that no more events with timestamps earlier than a certain time are expected. These watermarks then flow through the entire dataflow alongside the events, and downstream operators such as windows and joins use them to decide when it is safe to trigger computations. Operators themselves do not create watermarks; they only act upon the watermarks produced by the source.</p> <p><strong>Slide 77</strong></p> <p>Even though event time gives correct and deterministic results, processing time is still useful because it optimizes for speed and simplicity rather than correctness under disorder. Processing-time windows trigger based on the machine’s wall clock, so results are produced immediately as data arrives, giving the lowest possible latency with no need to wait for late events or watermarks. This makes them ideal for real-time monitoring, dashboards, and alerting, where users care more about seeing what is happening now than about perfectly accurate historical results. In addition, processing time reflects the actual arrival pattern of the stream, including delays and bursts, which can be valuable when analyzing system behavior or load rather than the underlying business events.</p> <p><strong>Slide 116-119</strong></p> <p>In Flink, state is always tied to a specific operator, and operators must register their state so that the runtime can track and manage it. There are two main types of state: operator state and keyed state. Operator state is scoped to a single operator task; all records processed by that task share the same state, but it cannot be accessed by other tasks of the same or different operators. Keyed state, on the other hand, is partitioned by a key defined in the records of the input stream. Each key has its own state instance, and all records with the same key are routed to the operator task that maintains that key’s state.</p> <p>To enable efficient state access with low latency, Flink keeps the state locally within each parallel task. How the state is stored, accessed, and maintained is handled by a state backend, such as RocksDB. The state backend is responsible for managing the local state for fast access during processing and for checkpointing the state to a remote location to enable fault tolerance and recovery. This design allows Flink to maintain consistent, high-performance state even in large-scale, distributed streaming applications.</p> <p><strong>Slide 121</strong></p> <p>Flink’s recovery mechanism relies on consistent checkpoints of the application’s state. A checkpoint is a snapshot of the state of all tasks in the streaming application at a specific point in time. For the checkpoint to be consistent, it must capture the state such that all tasks have processed exactly the same input events up to that point. This ensures that, in case of a failure, the application can be restored to a previous checkpoint and resume processing without losing data or processing events out of order.</p> <p><strong>Slide 123</strong></p> <p>Flink can provide exactly-once state consistency through its checkpointing and recovery mechanism, but only if certain conditions are met. First, all operators must correctly checkpoint and restore their state, so that after a failure, the internal state of the application reflects a precise moment in time. Second, all input streams must be reset to the exact position they were at when the checkpoint was taken, so the same events can be replayed deterministically. Whether this is possible depends on the data source: event logs like Kafka support resetting to a previous offset, allowing Flink to reprocess events reliably, whereas sources like sockets cannot be reset, since consumed data is lost. When both state restoration and input replay are possible, Flink can guarantee that each event affects the application state exactly once, even in the presence of failures.</p> <p><strong>Slide 124</strong></p> <p>Flink’s checkpointing and recovery mechanism guarantees exactly-once consistency only for the application’s internal state, not automatically for the external systems where results are written. When a failure occurs, Flink restores operator state from the latest checkpoint and replays input events, which can cause some output records to be emitted again. If the sink does not support transactions or idempotent writes (for example, a plain filesystem, database, or event log), these repeated emissions may lead to duplicate results in downstream systems. To achieve end-to-end exactly-once semantics, the sink itself must be able to handle duplicates safely or participate in Flink’s transactional mechanisms.</p> <p><strong>Slide 125</strong></p> <p>End-to-end exactly-once consistency is achieved when both Flink’s internal state and the external sink observe each event exactly once. Flink provides special exactly-once sink implementations for some storage systems (such as Kafka or transactional filesystems) that buffer output and only commit the emitted records when a checkpoint successfully completes, ensuring that partial results from failed executions are never made visible. For storage systems that do not support transactions, idempotent updates are commonly used: the sink is designed so that writing the same record multiple times produces the same final result (for example, using upserts or overwriting by a unique key). With transactional sinks or idempotent writes, replayed events during recovery do not cause incorrect or duplicated outcomes, enabling true end-to-end exactly-once behavior.</p> <p><strong>Slide 126 - 136</strong></p> <p>Flink’s checkpointing algorithm is based on the Chandy–Lamport distributed snapshot algorithm, which allows the system to take a consistent snapshot of a running distributed application without stopping the entire dataflow. Instead of pausing all processing, Flink decouples checkpointing from normal execution so that tasks can keep processing data while checkpoints are being coordinated and state is being persisted. This is crucial for low-latency stream processing.</p> <p>The core mechanism behind this is the checkpoint barrier. A checkpoint barrier is a special marker record that Flink’s source operators inject into the data stream when a checkpoint is triggered by the JobManager. Each barrier carries a checkpoint ID and flows through the same channels as normal records, but it cannot be overtaken by other records. Conceptually, the barrier splits the stream: all state updates caused by records before the barrier belong to the current checkpoint, and updates caused by records after the barrier belong to a future checkpoint.</p> <p>When a checkpoint starts, the JobManager instructs all source tasks to begin a new checkpoint. Each source temporarily pauses emitting records, snapshots its local state to the configured state backend, and then broadcasts checkpoint barriers on all its outgoing streams. Once the state snapshot is complete, the source acknowledges the checkpoint to the JobManager and resumes normal data emission.</p> <p>When an intermediate or downstream task receives a checkpoint barrier, it must ensure consistency across all its inputs. The task performs barrier alignment: it waits until it has received the same checkpoint barrier from all its input partitions. While waiting, it continues processing records from inputs that have not yet sent a barrier, but buffers records from inputs that already forwarded a barrier. This guarantees that no record belonging to the “future” checkpoint is processed too early.</p> <p>As soon as the task has received barriers from all inputs, it snapshots its own state to the state backend, forwards the checkpoint barrier to its downstream tasks, and then resumes processing by draining the buffered records. This process continues through the entire dataflow graph. Finally, when the JobManager has received checkpoint acknowledgements from all tasks, it marks the checkpoint as complete. At that point, Flink has a globally consistent snapshot of the application state, which can be used for recovery and provides the foundation for exactly-once state consistency.</p>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Personal Notes 10]]></summary></entry><entry><title type="html">Apache Kafka</title><link href="https://monishver11.github.io/blog/2025/big-data-10-kafka/" rel="alternate" type="text/html" title="Apache Kafka"/><published>2025-12-17T19:02:00+00:00</published><updated>2025-12-17T19:02:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-10-kafka</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-10-kafka/"><![CDATA[<p><strong>Data at rest versus data in motion</strong></p> <ul> <li>So far in this semester, we have been focusing on data at rest. <ul> <li>The data is stored in a storage system.</li> <li>Examples: HDFS, Hive, HBase, …</li> </ul> </li> <li>Today, we will focus on data in motion.</li> <li>The data is in route between source and destination in the network.</li> <li>We need to handle the continuous flow of data.</li> </ul> <p><strong>Why is it important?</strong></p> <ul> <li>We need to get the data from where it is created to where it can be analyzed. <ul> <li>The faster we can do this, the more agile and responsive our business can be.</li> <li>The less eﬀort we spend on moving data around, the more we can focus on the core business at hand.</li> </ul> </li> <li>This is why the pipeline is a critical component in the data-driven business.</li> <li>How we move the data becomes nearly as important as the data itself.</li> </ul> <p><strong>A critical component of data-driven applications</strong></p> <ul> <li>Publish/subscribe (pub/sub) messaging is a pattern that is characterized by the sender (publisher) of a piece of data (message) not specifically directing it to a receiver.</li> <li>Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages.</li> <li>Publish/subscribe systems often have a broker, a central point where messages are published, to facilitate this pattern.</li> </ul> <p><strong>How it starts</strong></p> <ul> <li>Many use cases for publish/subscribe start out the same way: with a simple message queue or interprocess communication channel.</li> <li>Example: Suppose you create an application that needs to send monitoring information somewhere… <ul> <li>You open a direct connection from your application to an application that displays your metrics on a dashboard.</li> <li>Then, you push metrics over that connection.</li> </ul> </li> <li>As you have more applications that are using those servers to get individual metrics and use them for various purposes, your architecture soon becomes…</li> <li>To clean up that mess, you set up a single application that receives metrics from all the applications out there, and provide a server to query those metrics for any system that needs them.</li> <li>Maybe you need to process more than metrics data…</li> <li>There is still a lot of duplication in the system!</li> <li>What you would like to have is a single centralized system that allows for publishing generic types of data, which will grow as your business grows. Kafka to the rescue!</li> </ul> <div class="row justify-content-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-1-480.webp 480w,/assets/img/kafka-1-800.webp 800w,/assets/img/kafka-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-2-480.webp 480w,/assets/img/kafka-2-800.webp 800w,/assets/img/kafka-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-3-480.webp 480w,/assets/img/kafka-3-800.webp 800w,/assets/img/kafka-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="kafka-basics"><strong>Kafka basics</strong></h4> <p><strong>Overview</strong></p> <ul> <li>Kafka was developed as a publish/subscribe messaging system designed to solve this problem.</li> <li>It is often described as a distributing streaming platform.</li> <li>Data within Kafka is stored durably, in order, and can be read deterministically.</li> <li>In addition, the data can be distributed within the system to provide additional protections against failures, as well as significant opportunities for scaling performance.</li> <li>Q: What does “distributed streaming platform” mean in Kafka?</li> <li>A: A distributed streaming platform is a system that can collect, store, process, and deliver continuous streams of data across multiple machines. In Kafka’s case, “streaming” means data is produced and consumed continuously (not as one-time batches), and “distributed” means the data and workload are spread across many brokers for scalability and fault tolerance. Kafka keeps streams durably on disk, ordered within partitions, and replicated so that failures of individual machines do not cause data loss, while allowing many producers and consumers to operate in parallel.</li> </ul> <p><strong>Messages</strong></p> <ul> <li>The unit of data within Kafka is called a message. It’s similar to a row or a record in a database.</li> <li>A message is simply an array of bytes. The data contained within it does not have a specific format or meaning to Kafka.</li> <li>A message can have an optional piece of metadata, referred to as a key. <ul> <li>The key is also a byte array and also has no specific meaning to Kafka.</li> <li>Keys are used when messages are to be written to partitions in a more controlled manner.</li> </ul> </li> <li>Q: What does it mean that a message is an “array of bytes”?</li> <li>A: When Kafka says a message (and its key) is an array of bytes, it means Kafka treats message data as raw binary data (byte[]) without interpreting its structure or semantics. Kafka does not care whether those bytes represent a string, JSON, Avro, Protobuf, an image, or anything else. It simply stores and transfers bytes efficiently. It is the responsibility of producers to serialize objects into bytes and consumers to deserialize those bytes back into meaningful data using an agreed-upon format.</li> </ul> <p><strong>Batches</strong></p> <ul> <li>For eﬃciency, messages are written into Kafka in batches.</li> <li>A batch is just a collection of messages, all of which are being produced to the same topic and partition.</li> <li>This is a trade-oﬀ between latency and throughput. The larger the batches, the more messages that can be handled per unit of time, but the longer it takes an individual message to propagate.</li> <li>Batches are typically compressed, providing more eﬃcient data transfer and storage at the cost of some processing power.</li> </ul> <p><strong>Schemas</strong></p> <ul> <li>Although Kafka itself treats messages as opaque byte arrays, in practice applications need a schema to describe the structure and meaning of the data inside those bytes. A schema defines fields, data types, and sometimes rules, making messages easier to interpret, validate, and evolve over time. Without schemas, producers and consumers must rely on implicit agreements, which easily break as systems change.</li> <li>Formats like JSON or XML are popular because they are simple and human readable, but they lack strong typing and make it hard to safely evolve data formats without breaking consumers. Apache Avro, on the other hand, provides compact binary serialization, strict data types, and built-in support for schema evolution. Avro keeps schemas separate from the message payload and does not require regenerating code when schemas change, which fits well with Kafka’s long-lived data streams.</li> <li>A consistent schema allows producers and consumers to evolve independently. If schemas are versioned and stored in a shared repository (such as a Schema Registry), producers can start writing data with a new schema version while consumers continue to read older versions or gradually adapt. This avoids tight coordination where all producers and consumers must be updated at the same time, enabling safer upgrades and more flexible, scalable Kafka-based systems.</li> <li>Conceptual Flow: In Kafka, the message itself is just a byte array (both the value and the optional key). Kafka does not understand or enforce any schema. The schema is used by the producer and consumer, not by Kafka, to serialize data into bytes when writing and deserialize bytes back into structured data when reading. Typically, the producer serializes an object according to a schema (for example, Avro) into a byte array and sends it to Kafka, and the consumer uses the same (or a compatible) schema to interpret those bytes. When a schema registry is used, the schema information is stored separately and referenced by the message, enabling versioning and compatibility without embedding the full schema in every message.</li> </ul> <p><strong>Topics and partitions</strong></p> <ul> <li>Messages in Kafka are categorized into topics. They’re similar to tables in a database or folders in a filesystem.</li> <li>Topics are additionally broken down into a number of partitions.</li> <li>Example: Suppose we use Kafka to store the commit log. <ul> <li>Then, a partition would be a single log.</li> <li>Messages are written to it in an append-only fashion and are read in order from beginning to end.</li> </ul> </li> <li>A topic typically has multiple partitions.</li> <li>Kafka guarantees message ordering within each partition, but there is no guarantee of message ordering across the entire topic.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-4-480.webp 480w,/assets/img/kafka-4-800.webp 800w,/assets/img/kafka-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Partitions are the mechanism Kafka uses to achieve scalability and fault tolerance. Because each partition of a topic can be placed on a different broker (server), Kafka can spread the load of reads and writes across many machines, allowing a single topic to scale horizontally beyond the limits of one server. To handle failures, partitions can also be replicated, meaning the same partition is stored on multiple brokers; if one broker fails, another replica can take over without data loss.</li> <li>The term stream is a logical concept used when talking about how data flows through Kafka-based systems. A stream refers to all the data in a topic as a whole, even though that topic may be physically split across many partitions. From the perspective of producers and consumers, this represents a continuous flow of records over time.</li> <li>This terminology becomes especially important in stream processing, where frameworks (such as Kafka Streams, Flink, or Spark Streaming) process data as it arrives, rather than in batches. These frameworks treat a topic as a single stream of events, abstracting away the underlying partitions while still leveraging them internally for parallelism and scalability.</li> <li>More clearly: a Kafka topic is logically one stream of messages, but physically it is split into partitions. When replication is not used, each partition holds different messages, and together all partitions make up the complete data for the topic. No two partitions contain the same records. Messages are assigned to partitions (based on key or round-robin), and once written, they exist only in that one partition. Replication, when enabled, simply creates copies of a partition’s data on other brokers for fault tolerance; it does not change the fact that partitions themselves divide the topic’s data uniquely.</li> </ul> <p><strong>Producers and consumers</strong></p> <ul> <li>In Kafka, clients are applications that interact with the Kafka cluster, and they come in two fundamental types: producers and consumers. A producer publishes (writes) messages to Kafka topics, while a consumer subscribes to topics and reads messages from them.</li> <li>On top of these basic clients, Kafka provides advanced client APIs. Kafka Connect is used for data integration, allowing Kafka to reliably move data between Kafka and external systems like databases, filesystems, or cloud services with minimal custom code. Kafka Streams is a stream-processing library that lets applications read from Kafka topics, process the data in real time (such as filtering, aggregating, or joining streams), and write the results back to Kafka.</li> <li>Internally, both Kafka Connect and Kafka Streams are built using producers and consumers, but they expose higher-level abstractions so developers can focus on integration or processing logic rather than low-level messaging details.</li> </ul> <p><strong>Producers</strong></p> <ul> <li>Producers (a.k.a., publishers or writers) create new messages.</li> <li>A message will be produced to a specific topic.</li> <li>By default, the producer will balance messages over all partitions of a topic.</li> <li>In some cases, the producer will direct messages to specific partitions. <ul> <li>This is typically done using the message key and a partitioner that will generate a hash of the key and map it to a specific partition. This ensures that all messages produced with the same key will get written to the same partition.</li> <li>The producer could also use a custom partitioner that follows other business rules for mapping messages to partitions.</li> </ul> </li> </ul> <p><strong>Consumers</strong></p> <ul> <li>Consumers (a.k.a., subscribers or readers) read messages.</li> <li>The consumer subscribes to one or more topics and reads the messages in the order in which they were produced to each partition.</li> <li>The consumer keeps track of which messages it has already consumed by keeping track of the oﬀset of messages.</li> </ul> <p><strong>Oﬀsets</strong></p> <ul> <li>Kafka tracks a consumer’s progress using offsets, which are monotonically increasing integer values that Kafka assigns to messages as they are appended to a partition.</li> <li>Each message in a partition has a unique offset, and later messages always have larger offsets (though some numbers may be skipped).</li> <li>A consumer records the next offset to read for each partition it consumes from; this offset is stored persistently (earlier in ZooKeeper, now typically in Kafka itself). Because offsets are stored outside the consumer process, a consumer can stop, crash, or restart and then resume reading from exactly where it left off, without losing or reordering data.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-5-480.webp 480w,/assets/img/kafka-5-800.webp 800w,/assets/img/kafka-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Consumer group</strong></p> <ul> <li>Consumers work as part of a consumer group, which is one or more consumers that work together to consume a topic.</li> <li>The group ensures that each partition is only consumed by one member.</li> <li>The mapping of a consumer to a partition is often called ownership of the partition by the consumer.</li> <li>In this way, consumers can horizontally scale to consume topics with a large number of messages.</li> <li>If a single consumer fails, the remaining members of the group will reassign the partitions being consumed to take over for the missing member.</li> <li>Note: <ul> <li>A consumer can consume from multiple partitions if there are fewer consumers than partitions.</li> <li>If there are more consumers than partitions, some consumers will be idle.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-6-480.webp 480w,/assets/img/kafka-6-800.webp 800w,/assets/img/kafka-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Brokers</strong></p> <ul> <li>A single Kafka server is called a broker.</li> <li>The broker receives messages from producers, assigns oﬀsets to them, and writes the messages to storage on disk.</li> <li>It also services consumers, responding to fetch requests for partitions and responding with the messages that have been published.</li> <li>A single broker can usually handle thousands of partitions and millions of messages per second.</li> </ul> <p><strong>Clusters</strong></p> <ul> <li>Kafka brokers are designed to operate as part of a cluster.</li> <li>Within a cluster of brokers, one broker will also function as the cluster controller (automatically elected from the live members of the cluster).</li> <li>The controller is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures.</li> </ul> <p><strong>Leader and followers</strong></p> <ul> <li>A partition is owned by a single broker in the cluster. That broker is called the leader of the partition.</li> <li>A replicated partition is assigned to additional brokers. They are called followers of the partition.</li> <li>Replication provides redundancy of messages in the partition. If there is a broker failure, one of the followers can take over leadership.</li> <li>All producers must connect to the leader in order to publish messages, but consumers may fetch from either the leader or one of the followers.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-7-480.webp 480w,/assets/img/kafka-7-800.webp 800w,/assets/img/kafka-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Retention</strong></p> <ul> <li>Retention is the durable storage of messages for some period of time.</li> <li>Kafka brokers are configured with a default retention setting for topics. <ul> <li>Time-based retention: retaining messages for some period of time (e.g., 7 days)</li> <li>Space-based retention: retaining until the partition reaches a certain size in bytes (e.g., 1 GB).</li> </ul> </li> <li>Once these limits are reached, messages are expired and deleted.</li> <li>Individual topics can also be configured with their own retention settings so that messages are stored for only as long as they are useful.</li> <li>Example <ul> <li>A tracking topic might be retained for several days.</li> <li>Application metrics might be retained for only a few hours.</li> </ul> </li> <li>Topics can also be configured as log compacted. <ul> <li>Kafka will retain only the last message produced with a specific key.</li> <li>Useful for changelog data, where only the last update is interesting.</li> </ul> </li> </ul> <p><strong>Multiple clusters</strong></p> <ul> <li>As Kafka deployments grow, it also supports multiple clusters.</li> <li>Why is it useful? <ul> <li>Segregation of types of data.</li> <li>Isolation for security requirements.</li> <li>Multiple data centers (disaster recovery).</li> </ul> </li> <li>When working with multiple data centers in particular, it is often required that messages be copied between them. However, the replication mechanisms within the Kafka clusters are designed only to work within a single cluster, not between multiple clusters.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-8-480.webp 480w,/assets/img/kafka-8-800.webp 800w,/assets/img/kafka-8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-8" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>MirrorMaker is a tool for replicating data to other clusters.</li> <li>At its core, MirrorMaker is simply a Kafka consumer and producer, linked together with a queue.</li> <li>Messages are consumed from one Kafka cluster and produced to another.</li> </ul> <p><strong>What makes Kafka a good choice?</strong></p> <ul> <li>Multiple producers: Ideal for aggregating data from many frontend systems and making it consistent.</li> <li>Multiple consumers: Multiple consumers can read any single stream of messages without interfering with other clients.</li> <li>Disk-based retention: If a consumer falls behind or temporarily becomes oﬄine, there is no danger of losing data.</li> <li>Scalable: Expansions can be performed while the cluster is online, without impacting whole system availability.</li> <li>High performance: Kafka provides sub-second message latency from producing a message to availability to consumers.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-9-480.webp 480w,/assets/img/kafka-9-800.webp 800w,/assets/img/kafka-9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-9" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Note:</strong></p> <ul> <li>If a topic has 2 partitions and 4 consumers in the same consumer group, Kafka assigns at most one consumer per partition, so only 2 consumers actively read data (one per partition) while the remaining 2 stay idle; adding more consumers does not increase throughput because parallelism is bounded by the number of partitions. If one of the active consumers fails, Kafka automatically triggers a rebalance and assigns the affected partition to one of the idle consumers, which then continues consuming from the last committed offset. If the 4 consumers belong to different consumer groups, each group independently consumes all messages from both partitions, meaning all 4 consumers receive the full data stream.</li> <li>Follow-up: What happens when partitions are replicated?</li> <li>When partitions are replicated, each partition has one leader replica and one or more follower replicas on different brokers. Consumers always read from the leader replica. If the broker hosting the leader fails, Kafka automatically elects one of the in-sync follower replicas as the new leader, and consumers continue reading transparently without data loss (assuming the replica was in sync). Replication therefore provides fault tolerance and high availability, but it does not increase consumer parallelism—only the number of partitions does.</li> </ul> <p><strong>Use cases</strong></p> <ul> <li>Activity tracking <ul> <li>The original use case for Kafka at LinkedIn is user activity tracking.</li> <li>A website’s users interact with frontend applications, which generate messages regarding actions the user is taking. <ul> <li>Passive information: page views, click tracking, …</li> <li>More complex information: a user adds to their profile, …</li> </ul> </li> <li>The messages are published to one or more topics, which are then consumed by applications on the backend. These applications generate reports, feed machine learning systems, update search results, or perform other operations to provide a rich user experience.</li> </ul> </li> <li>Messaging <ul> <li>Kafka is also used for messaging, where applications need to send notifications (e.g., emails) to users.</li> <li>Those applications can produce messages without needing to be concerned about formatting or how the messages will actually be sent.</li> <li>A single application can then read all the messages to be sent and handle them consistently, including: <ul> <li>Formatting the messages (a.k.a., decorating) using a common look and feel.</li> <li>Collecting multiple messages into a single notification to be sent.</li> <li>Applying a user’s preferences for how they want to receive messages.</li> </ul> </li> </ul> </li> <li>Metrics and logging <ul> <li>Kafka is also ideal for collecting application and system metrics and logs.</li> <li>Applications publish metrics on a regular basis to a Kafka topic, and those metrics can be consumed by systems for monitoring and alerting.</li> <li>They can also be used in an oﬄine system (e.g., Hadoop) to perform longer-term analysis (e.g., growth projections), or routed to dedicated log search systems (e.g., Elasticsearch, security analysis applications).</li> <li>When the destination system needs to change (e.g., it’s time to update the log storage system), there is no need to alter the frontend applications or the means of aggregation.</li> </ul> </li> <li>Commit log <ul> <li>Database changes can be published to Kafka, and applications can easily monitor this stream to receive live updates as they happen.</li> <li>This changelog stream can also be used for replicating database updates to a remote system, or for consolidating changes from multiple applications into a single database view.</li> <li>Durable retention is useful here for providing a buﬀer for the changelog, so it can be replayed in the event of a failure of the consuming applications.</li> <li>Alternately, log-compacted topics can be used to provide longer retention by only retaining a single change per key.</li> </ul> </li> <li>Stream processing <ul> <li>Stream processing refers to applications that provide similar functionality to map/reduce processing in Hadoop.</li> <li>Hadoop usually relies on aggregation of data over a long time frame, either hours or days. On the other hand, stream processing operates on data in real time, as quickly as messages are produced.</li> <li>Stream frameworks allow users to write small applications to operate on Kafka messages, performing tasks such as counting metrics, partitioning messages for eﬃcient processing by other applications, or transforming messages using data from multiple sources.</li> </ul> </li> </ul> <h4 id="kafka-internals"><strong>Kafka internals</strong></h4> <p><strong>Cluster membership</strong></p> <ul> <li>Kafka uses ZooKeeper to maintain the list of brokers that are currently members of a cluster. Recall the group membership example from ZooKeeper</li> <li>Every broker has a unique identifier. Every time a broker process starts, it registers itself with its ID in ZooKeeper by creating an ephemeral znode under the path “/brokers/ids”.</li> <li>Kafka brokers, the controller, and some of the ecosystem tools place a watch on that path so that they get notified when brokers are added or removed.</li> <li>When a broker loses connectivity to ZooKeeper, the ephemeral znode that the broker created when starting will be automatically removed from ZooKeeper.</li> <li>Kafka components that are watching the list of brokers will be notified that the broker is gone.</li> <li>Although the znode representing the broker is gone when the broker is stopped, the broker ID still exists in Kafka’s internal data structures.</li> <li>This way, if you completely lose a broker and start a brand-new broker with the ID of the old one, it will immediately join the cluster in place of the missing broker with the same partitions and topics assigned to it.</li> </ul> <p><strong>Controller</strong></p> <ul> <li>The controller is one of the Kafka brokers that, in addition to the usual broker functionality, is responsible for electing partition leaders. <ul> <li>The first broker that starts in the cluster becomes the controller by creating an ephemeral znode in ZooKeeper called “/controller”.</li> <li>When other brokers start, they also try to create this node but receive a “node already exists” exception, which causes them to “realize” that the controller node already exists and that the cluster already has a controller.</li> <li>The brokers create a ZooKeeper watch on the controller znode so they get notified of changes to this znode.</li> </ul> </li> <li>This way, we guarantee that the cluster will only have one controller at a time.</li> <li>When the controller broker is stopped or loses connectivity to ZooKeeper, the ephemeral znode will disappear.</li> <li>Other brokers in the cluster will be notified through the ZooKeeper watch that the controller is gone and will attempt to create the controller znode in ZooKeeper themselves.</li> <li>The first node to create the new controller in ZooKeeper becomes the next controller, while the other nodes will receive a “node already exists” exception and re-create the watch on the new controller znode.</li> <li>When the controller first comes up, it has to read the latest replica state map from ZooKeeper before it can start managing the cluster metadata and performing leader elections.</li> <li>When the controller notices that a broker left the cluster, it knows that all the partitions that had a leader on that broker will need a new leader.</li> <li>It goes over all the partitions that need a new leader and determines who the new leader should be (simply the next replica in the replica list of that partition).</li> <li>Then it persists the new state to ZooKeeper and sends information about the leadership change to all the brokers that contain replicas for those partitions.</li> <li>Example flow: <ul> <li>Suppose a Kafka cluster has three brokers: B1, B2, and B3. When the cluster starts, B1 starts first and creates the ephemeral znode /controller in ZooKeeper, becoming the controller. When B2 and B3 start, they try to create the same znode but get a “node already exists” exception, so they know B1 is already the controller and set a watch on /controller. Now, imagine B1 crashes or loses connectivity. Its ephemeral znode disappears, and B2 and B3 are notified via their watches. Both try to create /controller; whichever succeeds first becomes the new controller (say B2), while the other (B3) gets an exception and resets its watch on /controller.</li> <li>After B2 becomes the new controller, it first reads the latest replica state from ZooKeeper to understand the current mapping of partitions and their leaders. Suppose the cluster has a topic orders with 3 partitions: <ul> <li>Partition 0 → replicas [B1, B2, B3], leader was B1</li> <li>Partition 1 → replicas [B1, B3, B2], leader was B1</li> <li>Partition 2 → replicas [B2, B1, B3], leader was B2</li> </ul> </li> <li>Since B1 has failed, partitions 0 and 1 need new leaders. B2 examines the replica lists: <ul> <li>For Partition 0, the next replica in the list after B1 is B2, so B2 becomes the new leader.</li> <li>For Partition 1, the next replica after B1 is B3, so B3 becomes the new leader.</li> <li>Partition 2 is unaffected because its leader, B2, is still active.</li> </ul> </li> <li>The controller then updates ZooKeeper with the new leadership information for partitions 0 and 1. It also notifies all brokers that host replicas of these partitions (B2 and B3 for Partition 0, B2 and B3 for Partition 1) about the changes.</li> <li>This ensures that producers and consumers can continue sending and receiving messages without disruption. By following this process, Kafka guarantees automatic failover, consistent leadership, and uninterrupted availability of all partitions in the cluster.</li> </ul> </li> </ul> <p><strong>A Quick Review:</strong></p> <ul> <li>Broker as leader: <ul> <li>Each partition in a topic has exactly one leader at a time.</li> <li>A broker can be the leader for multiple partitions, even across multiple topics. So if a broker handles multiple topics, it can simultaneously be the leader for several partitions of different topics.</li> <li>Followers replicate the leader’s data for fault tolerance, but only the leader handles reads/writes for that partition.</li> </ul> </li> <li>Consumers and partitions: <ul> <li>A consumer in a consumer group can read from one or more partitions, but each partition can be read by only one consumer in the same group at a time.</li> <li>For a single topic with multiple partitions, multiple consumers in the same group allow parallel consumption, but each partition is still assigned to only one consumer.</li> <li>If a consumer subscribes to multiple topics, it can consume from one or more partitions from each topic, depending on how partitions are assigned to it.</li> </ul> </li> <li>Example: <ul> <li>Topic A has 3 partitions: P0, P1, P2. Topic B has 2 partitions: Q0, Q1.</li> <li>Broker B1 might be leader for P0 and Q1; B2 leader for P1 and Q0; B3 leader for P2.</li> <li>Consumer C1 in a consumer group could be assigned P0 (topic A) and Q0 (topic B), while C2 gets P1 and Q1.</li> </ul> </li> <li>So, brokers can be leaders for multiple partitions across topics, and consumers can read from multiple partitions across topics, but within a consumer group, each partition is served by only one consumer at a time.</li> </ul> <p><strong>Replication</strong></p> <ul> <li>Replication is critical because it is the way Kafka guarantees availability and durability when individual nodes inevitably fail.</li> <li>As we’ve already discussed, data in Kafka is organized by topics. <ul> <li>Each topic is partitioned.</li> <li>Each partition can have multiple replicas.</li> </ul> </li> <li>Those replicas are stored on brokers, and each broker typically stores hundreds or even thousands of replicas belonging to diﬀerent topics and partitions.</li> <li>There are two types of replicas.</li> <li>Leader replica <ul> <li>Each partition has a single replica designated as the leader.</li> <li>All produce requests go through the leader to guarantee consistency.</li> <li>Clients can consume from either the leader replica or its followers.</li> </ul> </li> <li>Follower replica <ul> <li>All replicas for a partition that are not leaders are called followers.</li> <li>If a leader replica for a partition fails, one of the follower replicas will be promoted to become the new leader for the partition.</li> </ul> </li> <li>Kafka provides the following reliability guarantees: <ul> <li>Kafka provides order guarantee of messages in a partition.</li> <li>Produced messages are considered “committed” when they were written to the partition on all its in-sync replicas (but not necessarily flushed to disk). In-sync replicas are the leader and followers that lags within 10 seconds (configurable).</li> <li>Messages that are committed will not be lost as long as at least one replica remains alive.</li> <li>Consumers can only read messages that are committed.</li> </ul> </li> <li>Q: Can Kafka consumers read from any replica of a partition, or only from the leader?</li> <li>A: In Kafka, each partition has a single leader replica and zero or more follower replicas. By default, consumers read from the leader replica, which guarantees that they see all committed messages in order and avoids inconsistencies. Followers replicate the leader’s data and can serve as read sources only if the system is explicitly configured to allow reading from followers, for example, to balance read load. However, followers that are out-of-sync may not have all committed messages or may lag behind the leader, so reading from them can result in missing or stale data. Therefore, reading from the leader is the default and safest approach, ensuring correctness, ordering, and durability, while followers primarily provide redundancy and high availability in case of leader failure.</li> </ul> <p><strong>Request processing</strong></p> <ul> <li>A Kafka broker processes requests sent to the partition leaders from clients, partition replicas, and the controller.</li> <li>Clients always initiate connections and send requests, and the broker processes the requests and responds to them.</li> <li>All requests sent to the broker from a specific client will be processed in the order in which they were received This guarantee allows Kafka to behave as a message queue and provide ordering guarantees on the messages it stores.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-10-480.webp 480w,/assets/img/kafka-10-800.webp 800w,/assets/img/kafka-10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-10" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>How do clients know where to send the requests?</li> <li>Clients know where to send requests because each partition has a designated leader, and brokers maintain metadata about which broker is the leader for which partition. When a client wants to produce or consume messages, it first fetches the cluster metadata (from ZooKeeper in older versions or from the Kafka bootstrap servers). This metadata includes the list of brokers, topics, partitions, and the leader for each partition. Using this information, the client can send requests directly to the broker that is the leader for the partition it wants to read from or write to.</li> <li>This ensures that requests always reach the correct partition leader, maintaining message ordering and consistency guarantees, and reduces unnecessary network hops through other brokers.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-11-480.webp 480w,/assets/img/kafka-11-800.webp 800w,/assets/img/kafka-11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-11" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Clients can set an upper/lower bound on the amount of data the broker can return, as well as a timeout.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-12-480.webp 480w,/assets/img/kafka-12-800.webp 800w,/assets/img/kafka-12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-12" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Consumers only see messages that were replicated to in-sync replicas.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-13-480.webp 480w,/assets/img/kafka-13-800.webp 800w,/assets/img/kafka-13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-13" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Physical storage</strong></p> <ul> <li>The basic storage unit of Kafka is a partition replica.</li> <li>Partitions cannot be split between multiple brokers, and not even between multiple disks on the same broker.</li> <li>Therefore, the size of a partition is limited by the space available on a single mount point.</li> <li>Kafka 3.9 (November 2024) supports “tiered storage”. (see KIP-405) <ul> <li>The local tier: store the log segments on the local disks. (same as current)</li> <li>The remote tier: store the completed log segments on HDFS, S3, … (new)</li> </ul> </li> <li>When you create a topic, Kafka first decides how to allocate the partitions between brokers. <ul> <li>The default replication factor is 3.</li> <li>Users can add or remove replicas even after a topic exists.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-14-480.webp 480w,/assets/img/kafka-14-800.webp 800w,/assets/img/kafka-14-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-14" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Kafka does not keep data forever. It doesn’t even wait for all consumers to read a message before deleting it.</li> <li>Instead, the Kafka administrator configures a retention period for each topic. <ul> <li>Time-based: how long to store messages before deleting them.</li> <li>Space-based: how much data to store before older messages are purged.</li> </ul> </li> <li>Each partition is split into segments. By default, each segment contains either 1 GB of data or a week of data, whichever is smaller.</li> <li>As a Kafka broker is writing to a partition, if the segment limit is reached, it closes the file and starts a new one.</li> <li>Each segment is stored in a single data file, which stores Kafka messages and their oﬀsets.</li> <li>The format of the data on the disk is identical to the format of the messages that we send from the producer to the broker and later from the broker to the consumers (i.e., the wire protocol). <ul> <li>This allows Kafka to use a zero-copy method to send messages to clients. Kafka sends messages from the file (or more likely, the Linux filesystem cache) directly to the network channel without any intermediate buﬀers.</li> <li>It also avoids decompressing and recompressing messages that the producer already compressed.</li> </ul> </li> <li> <p>Note: Zero-copy is an optimization in which Kafka sends data from disk to the network without multiple memory copies between user space and kernel space. Normally, sending data involves copying from disk to kernel buffers, then to user-space buffers, and back to kernel network buffers before transmission. With zero-copy (using OS features like Linux sendfile), Kafka can send messages directly from the on-disk segment files or the Linux page cache to the network socket. This reduces CPU and memory usage, speeds up message delivery, and works efficiently because Kafka’s on-disk segment format matches the wire protocol, so no data transformation is needed. The wire protocol is the format and rules that define how data is encoded and transmitted over the network between clients and servers.</p> </li> <li>Kafka provides flexible message retrieval and retention mechanisms. Consumers can start reading messages from any offset, and to make this efficient, each partition maintains an offset-to-segment index, which maps offsets to their exact position within segment files.</li> <li>Additionally, Kafka maintains a timestamp-to-offset index to quickly locate messages by time, which is useful for Kafka Streams and certain failover scenarios.</li> <li>For retention, topics can be configured in three ways: delete, which removes messages older than a configured time; compact, which keeps only the latest value for each key; or delete and compact, combining compaction with time-based deletion.</li> <li>Note: Both compaction and deletion operate on the messages stored on the brokers for a given topic.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-15-480.webp 480w,/assets/img/kafka-15-800.webp 800w,/assets/img/kafka-15-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-15" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="kafka-producer-flow"><strong>Kafka Producer Flow</strong></h4> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-16-480.webp 480w,/assets/img/kafka-16-800.webp 800w,/assets/img/kafka-16-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-16" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>When a producer wants to send messages to a Kafka topic, it first connects to one of the bootstrap brokers configured in its setup. It then requests metadata from the broker, which includes the list of partitions for the topic, the leader broker for each partition, and the replicas for each partition. The producer caches this metadata so that it can send records directly to the correct leader broker for the target partition.</li> <li>If the leader changes due to a failover or if the cached metadata expires, the producer automatically refreshes the metadata by querying the brokers again. This ensures that the producer always knows which broker to contact for each partition and guarantees that messages are sent to the appropriate leader.</li> </ul> <h4 id="kafka-consumer---commits-and-oﬀsets"><strong>Kafka Consumer - Commits and oﬀsets</strong></h4> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-17-480.webp 480w,/assets/img/kafka-17-800.webp 800w,/assets/img/kafka-17-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kafka-17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-17" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>In Kafka, consumers track which messages they have read by maintaining offsets, which represent their current position in each partition. Whenever a consumer calls poll(), it retrieves all messages from the last committed offset up to the latest.</li> <li>Unlike traditional message queues, Kafka does not track individual acknowledgments for each message. Instead, a consumer commits offsets, which updates a special internal topic called __consumer_offsets. This commit records the last successfully processed message in a partition, implicitly marking all prior messages as processed. By default, Kafka consumers commit offsets automatically every few seconds, but this can be disabled to allow explicit commits, which helps prevent message loss or duplication during rebalancing events.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 9]]></summary></entry><entry><title type="html">Apache ZooKeeper</title><link href="https://monishver11.github.io/blog/2025/big-data-9-zookeeper/" rel="alternate" type="text/html" title="Apache ZooKeeper"/><published>2025-12-17T02:58:00+00:00</published><updated>2025-12-17T02:58:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-9-zookeeper</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-9-zookeeper/"><![CDATA[<h4 id="introduction"><strong>Introduction</strong></h4> <ul> <li>So far in this semester, we have been studying large-scale distributed data processing systems.</li> <li>Today is diﬀerent. We will study how to build general distributed applications using Hadoop’s distributed coordination service, ZooKeeper.</li> </ul> <h4 id="distributed-coordination"><strong>Distributed coordination</strong></h4> <ul> <li>What should an autonomic service provide? <ul> <li>Self-configuring: All configuration should happen with little or no intervention by the user.</li> <li>Self-healing: When a process, service, or node stops working, it should repair itself.</li> <li>Self-protecting: The service should continually assess its own health. Redundant, cooperating health checkers can health-check each other.</li> <li>Self-optimizing: The service should continually assess its own performance. Perform load-balancing operations or request resources to maintain a desired level of performance.</li> </ul> </li> <li>Why is building distributed applications hard? <ul> <li>The main reason is partial failure.</li> <li>When a message is sent across the network between two nodes and the network fails, the sender does not know whether the receiver got the message. <ul> <li>It may have gotten through before the network failed.</li> <li>Or it may not have gotten through at all.</li> <li>Or perhaps the network is just too slow, and the message is just delayed?</li> </ul> </li> <li>The only way that the sender can find out what happened is to reconnect to the receiver and ask it.</li> <li>This is partial failure: when we don’t even know if an operation failed.</li> <li>Distributed programming is fundamentally diﬀerent from local programming.</li> <li>Without fresh, accurate, and timely information, it’s very diﬃcult to identify the root cause of the failure in realtime. <ul> <li>Network failure.</li> <li>Process failure.</li> <li>Shared resource failure (e.g., data store).</li> <li>Physical machine failure.</li> <li>Virtual machine failure.</li> <li>Configuration errors.</li> </ul> </li> </ul> </li> <li>How can ZooKeeper help? <ul> <li>ZooKeeper can’t make partial failures go away: Partial failures are intrinsic to distributed systems.</li> <li>ZooKeeper does not hide partial failures, either: What ZooKeeper does is give you a set of tools to build distributed applications that can safely handle partial failures.</li> </ul> </li> </ul> <h4 id="zookeeper"><strong>ZooKeeper</strong></h4> <p><strong>Characteristics</strong></p> <ul> <li>ZooKeeper is simple: At its core, ZooKeeper is a stripped-down filesystem that exposes a few simple operations.</li> <li>ZooKeeper is expressive: The ZooKeeper primitives are a rich set of building blocks that can be used to build many systems.</li> <li>ZooKeeper is highly available: ZooKeeper runs on a collection of machines with no single point of failure.</li> <li>ZooKeeper facilitates loosely coupled interactions: Participants do not need to know about one another or exist simultaneously.</li> <li>ZooKeeper is a library: ZooKeeper has a vibrant open-source community and rich documentation.</li> </ul> <p><strong>Data model</strong></p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-1-480.webp 480w,/assets/img/zk-1-800.webp 800w,/assets/img/zk-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Znodes <ul> <li>ZooKeeper maintains a hierarchical tree of nodes called znodes.</li> <li>A znode stores data and has an associated access-control list and version number.</li> <li>The default size limit of the stored data is 1MB. This is because ZooKeeper is designed for coordination (which typically uses small datafiles), not high-volume data storage.</li> <li>Accessing the data associated with a znode is atomic. <ul> <li>A client reading the data will either receive the entire data or fail.</li> <li>A client writing the data will replace all the data with a znode or fail.</li> </ul> </li> <li>There is no such thing as a partial read/write.</li> <li>ZooKeeper does not support an append operation. These characteristics contrast with HDFS. HDFS is designed for high-volume data storage with streaming data access and provides an append operation.</li> <li>In ZooKeeper, the statement “no append operation” means that you cannot add data incrementally to a znode. When writing to a znode, the entire content must be replaced in a single operation; partial writes or incremental appends are not supported.</li> <li>Znodes are referenced by paths.</li> <li>A path is a slash-delimited Unicode string, similar to a filesystem path.</li> <li>Paths must be absolute, i.e., begin with a slash.</li> <li>All paths are canonical, i.e., each path has a single representation. Paths do not undergo resolution. You cannot use “.” or “..” as in a filesystem path.</li> <li>The path “/zookeeper” is reserved to store management information.</li> <li>There are two types of znodes. Ephemeral znode: It will be deleted by the ZooKeeper service when the client that created it disconnects, either explicitly or because the client terminates for whatever reason. And Persistent znode: It will not be deleted when the client disconnects. The type of a znode is set at creation time and may not be changed later.</li> <li>Ephemeral znodes <ul> <li>An ephemeral znode is automatically deleted by ZooKeeper when the creating client’s session ends.</li> <li>An ephemeral znode may not have children, not even ephemeral ones. If ephemeral nodes were allowed to have children, it would create complications when the parent is deleted, what happens to the children? To avoid this problem, ZooKeeper enforces that ephemeral znodes are always leaf nodes.</li> <li>Even though ephemeral nodes are tied to a client session, they are visible to all clients (subject to their access-control list policies).</li> <li>Ephemeral znodes are ideal for building applications that need to know when certain distributed resources are available.</li> </ul> </li> <li>Persistent znodes <ul> <li>A persistent znode is not tied to the client’s session.</li> <li>It is deleted only when explicitly deleted by a client. Not necessarily by the client that created it.</li> <li>A persistent node can have children, similar to a directory in a filesystem.</li> </ul> </li> </ul> </li> <li>Sequence numbers <ul> <li>A sequential znode is given a sequence number by ZooKeeper as a part of its name.</li> <li>If a znode is created with the sequential flag set, then the value of a monotonically increasing counter is appended to its name. The counter is maintained by its parent znode.</li> <li>Sequence numbers can be used to impose a global ordering on events in a distributed system and may be used by the client to infer the ordering.</li> <li>Example: if the client asks to create a sequential znode named “/a/b-”, the service may create a znode named “/a/b-3” and later another znode “/a/b-5”.</li> </ul> </li> <li>Watches <ul> <li>Watches allow clients to get notifications when a znode changes in some way.</li> <li>The client can set a watch with read operations on the ZooKeeper service. Watches are triggered by write operations on the ZooKeeper service.</li> <li>Watches are triggered only once. To receive multiple notifications, a client needs to reregister the watch.</li> <li>For example, a client can call exists on a znode with a watch; if the znode does not exist, the call returns false, but later, when another client creates that znode, the watch fires and the first client is notified.</li> <li>Watches are set on specific znodes, not on the entire ZooKeeper service. <ul> <li>You attach a watch to a znode when performing a read operation (exists, getData, or getChildren).</li> <li>The watch is only triggered by changes to that particular znode (or its children, in the case of getChildren).</li> </ul> </li> <li>A client can have multiple watches simultaneously, each on different znodes or different aspects of the same znode. <ul> <li>Each watch is one-time: once triggered, it must be reregistered if the client wants future notifications.</li> <li>So at any given time, a client can be watching many znodes, receiving multiple notifications as changes occur—but each individual watch only fires once.</li> </ul> </li> </ul> </li> </ul> <p><strong>Operations</strong></p> <ul> <li>Overview <ul> <li><code class="language-plaintext highlighter-rouge">create</code>: Create a znode (the parent znode must already exist).</li> <li><code class="language-plaintext highlighter-rouge">delete</code>: Delete a znode (the znode must not have any children).</li> <li><code class="language-plaintext highlighter-rouge">exists</code>: Tests whether a znode exists and retrieves its metadata.</li> <li><code class="language-plaintext highlighter-rouge">getACL, setACL</code>: Get/set the access-control list of a znode.</li> <li><code class="language-plaintext highlighter-rouge">getChildren</code>: Get a list of the children of a znode.</li> <li><code class="language-plaintext highlighter-rouge">getData, setData</code>: Get/set the data associated with a znode.</li> <li><code class="language-plaintext highlighter-rouge">sync</code>: Synchronize a client’s view of a znode with ZooKeeper. <ul> <li>It ensures that any read operation after sync reflects the most up-to-date data for that znode, even in a distributed system with multiple clients and servers.</li> <li>Essentially, it forces the client to catch up with all recent updates that may have been applied by other clients or nodes, providing a consistent and current view of the znode’s data.</li> <li><code class="language-plaintext highlighter-rouge">sync</code> is session-wide, not per-znode, but it’s usually used to ensure freshness before reading a specific znode.</li> </ul> </li> </ul> </li> <li>Version number <ul> <li>The exists operation returns the version number in the znode’s metadata.</li> <li>Update operations (delete, setData) in ZooKeeper are conditional and nonblocking. <ul> <li>The client has to specify the version number of the znode that is being updated.</li> <li>If the version number does not match, the update will fail. It means that another process updated the znode in the meantime.</li> <li>This is an optimistic concurrency mechanism that allows clients to detect conflicts over znode modification without locking.</li> <li>The version check can be bypassed by using a version number of -1.</li> </ul> </li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-2-480.webp 480w,/assets/img/zk-2-800.webp 800w,/assets/img/zk-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Diﬀerence from filesystems <ul> <li>Although ZooKeeper can be viewed as a filesystem, it does away with some filesystem primitives for simplicity.</li> <li>In ZooKeeper, files are small and are written and read in their entirety. <ul> <li>There is no need to provide open, close, or seek operations.</li> <li>Therefore, ZooKeeper does not use handles to access znodes.</li> <li>Each operation includes the full path of the znode being operated on.</li> </ul> </li> <li>The sync operation is also diﬀerent from fsync in a filesystem.</li> <li>Writes in ZooKeeper are atomic and does not need to be synced.</li> <li>The sync operation is there to allow a client to catch up with the latest state.</li> <li>Q: What does it mean when ZooKeeper says “writes are atomic and do not need to be synced,” even though it runs on many machines?</li> <li>A: In ZooKeeper, a write operation (such as create, delete, or setData) is atomic, meaning it either fully succeeds or fully fails—there is no partial update that clients can observe. Although ZooKeeper is a distributed system running on multiple machines, it uses a consensus protocol (ZAB) to ensure that every write is agreed upon, replicated to a majority of servers, and committed in the same order before the client receives a success response. Because durability and ordering are already guaranteed internally by this protocol, the client does not need to call a filesystem-style fsync to force data to disk. ZooKeeper’s sync operation is therefore not about making writes durable; instead, it allows a client to catch up and ensure its read view reflects the latest committed state.</li> </ul> </li> <li>Multiupdate <ul> <li>ZooKeeper also provides the multi operation that batches together multiple primitive operations into a single unit that either succeeds or fails in its entirety.</li> <li>Multiupdate is useful for building structures that maintain some global invariant.</li> <li>Example: an undirected graph. <ul> <li>Each vertex in the graph is represented as a znode.</li> <li>We need to update two znodes to add or remove an edge.</li> <li>Batching the updates on the two znodes into one multi operation ensures that the update is atomic.</li> </ul> </li> </ul> </li> <li>Synchronous and asynchronous API <ul> <li>ZooKeeper provides both synchronous and asynchronous APIs for all operations to suit different programming needs. In the synchronous API, a call (such as exists) blocks until the operation completes and directly returns a result, typically a Stat object or null, or throws an exception on failure. In contrast, the asynchronous API is non-blocking: the client issues the request and continues execution, and ZooKeeper later delivers the result via a callback method (processResult), which includes the return code, path, context, and result data. This allows applications to handle ZooKeeper operations efficiently without blocking threads, which is especially useful in high-concurrency or event-driven systems.</li> <li>A callback is a function (or method) that you provide to an API so it can be invoked later when an asynchronous operation completes. Instead of blocking and waiting for a result, the program continues running, and when the operation finishes, the system “calls back” your function with the result (success, failure, or data). In ZooKeeper, callbacks are used in asynchronous APIs to deliver operation results—such as status codes and metadata—once the server has processed the request.</li> </ul> </li> <li>Watch triggers <ul> <li>Read operations (exists, getChildren, getData) may have watches set on them.</li> <li>The watches are triggered by write operations (create, delete, setData).</li> <li>Operations on the access-control list (getACL, setACL) do not participate in watches.</li> <li>When a watch is triggered, a watch event is generated, which includes the path of the znode that wa. s involved in the event. <ul> <li>The watch event does not provide the changed data itself.</li> <li>To discover the data, the client needs to reissue a read operation.</li> </ul> </li> <li>The watch event’s type depends both on the watch and the operation that triggered it. <ul> <li>A watch set on an exists operation will be triggered when the znode being watched is created, deleted, or has its data updated.</li> <li>A watch set on a getData operation will be triggered when the znode being watched is deleted or has its data updated.</li> <li>A watch set on a getChildren operation will be triggered when a child of the znode being watched is created or deleted, or when the znode itself is deleted.</li> </ul> </li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-3-480.webp 480w,/assets/img/zk-3-800.webp 800w,/assets/img/zk-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Access-control lists <ul> <li>A znode can be created with an access-control list, which determines who can perform certain operations on it.</li> <li>The client identifies itself to ZooKeeper using an authentication scheme.</li> <li>Employing authentication and access-control lists is optional.</li> </ul> </li> </ul> <p><strong>Implementation</strong></p> <ul> <li>Ensemble <ul> <li>ZooKeeper runs on a cluster of machines called an ensemble.</li> <li>ZooKeeper achieves high availability through replication, and can provide a service as long as a majority of the machines in the ensemble are live.</li> <li>Example: a 5-node ensemble can tolerate at most 2 node failures. It is usual to have an odd number of machines in an ensemble.</li> <li>Note: it is critical that ZooKeeper can perform its functions in a timely manner. Therefore, ZooKeeper should run on machines that are dedicated to ZooKeeper alone. Having other applications contend for resources can cause ZooKeeper’s performance to degrade significantly.</li> <li>Conceptually, ZooKeeper is very simple.</li> <li>All it has to do is ensure that every modification to the tree of znodes is replicated to a majority of the ensemble. <ul> <li>If a minority of the machines fail, then at least one machine will survive with the latest state.</li> <li>The other remaining replicas will eventually catch up with this state.</li> </ul> </li> </ul> </li> <li>Zab <ul> <li>ZooKeeper uses a protocol called Zab that runs in two phases, which may be repeated indefinitely.</li> <li>Phase 1: Leader election: <ul> <li>The machines in an ensemble go through a process of electing a distinguished member called the leader.</li> <li>The other machines are called followers.</li> <li>This phase is finished once a majority of followers (called “quorum”) have synchronized their state with the leader.</li> </ul> </li> <li>Phase 2: Atomic broadcast: <ul> <li>All write requests are forwarded to the leader.</li> <li>The leader then broadcasts the update to the followers.</li> <li>When a majority have persisted the change, the leader commits the update, and the client gets a response saying the update succeeded.</li> <li>The protocol for achieving consensus is designed to be atomic, so a change either succeeds or fails. It resembles a two-phase commit protocol (2PC).</li> </ul> </li> </ul> </li> <li>What if the leader fails? <ul> <li>If the leader fails, the remaining machines hold another leader election and continue as before with the new leader.</li> <li>If the old leader later recovers, it then starts as a follower.</li> <li>Leader election is very fast (~200ms), so performance does not noticeably degrade during an election.</li> </ul> </li> <li>Replicated database <ul> <li>Each zookeeper replica maintains an in-memory database containing the entire znode tree.</li> <li>Writes: All machines in the ensemble first write updates to disk, and then update their in-memory copies of the znode tree.</li> <li>Reads: Read requests may be serviced from any machine. Read requests are very fast because they involve only a lookup from memory.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-4-480.webp 480w,/assets/img/zk-4-800.webp 800w,/assets/img/zk-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Consistency</strong></p> <ul> <li>Understanding consistency <ul> <li>In ZooKeeper, a follower may lag the leader by a number of updates.</li> <li>This is because only a majority and not all members of the ensemble need to have persisted a change before it is committed.</li> <li>A client has no control whether it is connected to a leader or a follower, and cannot even know this.</li> </ul> </li> <li>ZooKeeper transaction ID <ul> <li>Every update made to the znode tree is given a globally unique identifier, called a zxid (i.e., ZooKeeper transaction ID).</li> <li>In ZooKeeper, updates are totally ordered. If zxid is less than , then happened before according to ZooKeeper.</li> </ul> </li> <li>Consistency guarantees <ul> <li>Guarantee 1: Sequential consistency <ul> <li>Updates from any particular client are applied in the order that they are sent. If a client updates the znode <code class="language-plaintext highlighter-rouge">z</code> to the value <code class="language-plaintext highlighter-rouge">a</code>, and in a later operation, it <code class="language-plaintext highlighter-rouge">z</code> updates to the value <code class="language-plaintext highlighter-rouge">b</code>, then no client will ever see <code class="language-plaintext highlighter-rouge">z</code> with value <code class="language-plaintext highlighter-rouge">a</code> after it has seen it with value b (if no other updates are made to <code class="language-plaintext highlighter-rouge">z</code>).</li> </ul> </li> <li>Guarantee 2: Atomicity <ul> <li>Updates either succeed or fail. If an update fails, no client will ever see it.</li> </ul> </li> <li>Guarantee 3: Single system image <ul> <li>A client will see the same view of the system, regardless of the server it connects to.</li> <li>If a client connects to a new server during the same session, it will not see an older state of the system than the one it saw with the previous server.</li> <li>When a server fails and a client tries to connect to another in the ensemble, a server that is behind the one that failed will not accept connections from the client until it has caught up with the failed server.</li> </ul> </li> <li>Guarantee 4: Durability <ul> <li>Once an update has succeeded, it will persist and will not be undone.</li> <li>Update will survive server failures.</li> </ul> </li> <li>Guarantee 5: Timeliness <ul> <li>The lag in any client’s view of the system is bounded, so it will not be out of date by more than some multiple of tens of seconds.</li> <li>Rather than allow a client to see data that is very stale, a server will shut down, forcing the client to switch to a more up-to-date server.</li> </ul> </li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-5-480.webp 480w,/assets/img/zk-5-800.webp 800w,/assets/img/zk-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>(In-)consistency across clients <ul> <li>For performance reasons, reads are satisfied from a ZooKeeper server’s memory and do not participate in the global ordering of writes.</li> <li>ZooKeeper does not provide simultaneously consistent cross-client views. <ul> <li>It is possible for two clients to observe updates at diﬀerent times.</li> <li>If two clients communicate outside ZooKeeper (“hidden channel”), the diﬀerence becomes apparent.</li> </ul> </li> <li>If a client need to catch up with the latest state, the sync operation forces the ZooKeeper server to which the client is connected to catch up with the leader.</li> <li>Gist: This means that ZooKeeper optimizes reads by serving them locally, without coordinating them through the global consensus protocol that orders writes. Concretely, when a client issues a read, the ZooKeeper server it is connected to simply returns the value from its in-memory state, instead of synchronizing with the leader or other servers. Because of this, reads are not totally ordered with respect to writes across the entire ensemble. As a result, two clients connected to different servers may temporarily see different versions of the same znode, even though all writes themselves are globally ordered and atomic. This design trades strict read consistency for low latency and high throughput, and clients that need the most up-to-date view can explicitly call sync to force their server to catch up with the leader before reading.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-6-480.webp 480w,/assets/img/zk-6-800.webp 800w,/assets/img/zk-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Sessions</strong></p> <ul> <li>Client startup <ul> <li>A ZooKeeper client is configured with the list of servers in the ensemble.</li> <li>On startup, it tries to connect to one of the servers in the list.</li> <li>If the connection fails, it tries another server in the list, and so on, until it either successfully connects to one of them or fails because all ZooKeeper servers are unavailable.</li> <li>Once a connection has been made with a ZooKeeper server, the server creates a new session for the client.</li> <li>Q: Can a ZooKeeper server handle multiple client sessions simultaneously?</li> <li>A: Yes, a ZooKeeper server can manage multiple client sessions at the same time. Each connected client is assigned a unique session that tracks ephemeral znodes, watches, and session timeouts. The server maintains the session state for all clients in memory, allowing it to serve many clients concurrently. If a client disconnects temporarily, its session can remain active for the configured timeout period, enabling the client to reconnect without losing ephemeral nodes or watches. Thus, a single ZooKeeper server is designed to handle multiple client sessions efficiently, while the ensemble as a whole provides scalability and fault tolerance.</li> </ul> </li> <li>Session timeout <ul> <li>In ZooKeeper, the client specifies the session timeout when connecting to the ensemble by sending a requested timeout value during the handshake. The server may adjust this value based on its configuration and load, but generally tries to honor the client’s request within allowed limits. This timeout determines how long the server will keep the session active without receiving any requests or heartbeats from the client. Once the timeout expires, the session is terminated, it may not be reopened and any ephemeral znodes created under it are automatically deleted.</li> </ul> </li> <li>Heartbeats <ul> <li>In ZooKeeper, each client session has a timeout. To prevent the server from thinking the client is dead, the client periodically sends heartbeats—small messages that indicate it is still alive—even when no other requests are being made. The interval between heartbeats is set short enough so that: <ul> <li>If a server fails, the client detects it quickly (through a read timeout).</li> <li>The client can reconnect to another server in the ensemble before the session timeout expires, keeping the session active.</li> </ul> </li> <li>Without heartbeats, the server might expire the session, deleting any ephemeral znodes tied to it. So heartbeats are essential to maintain session liveness during idle periods.</li> <li>And although the session is tracked by the server, ZooKeeper’s design allows the session state to be transferred to another server: when the client connects to a new server, the ensemble ensures that the new server resumes the same session with all associated ephemeral znodes and watches intact. This mechanism allows the client to maintain a consistent session across server failures or reboots, keeping ephemeral nodes alive as long as the session timeout is not exceeded.</li> </ul> </li> <li>Failover <ul> <li>Failover to another ZooKeeper server is handled automatically by the ZooKeeper client.</li> <li>Sessions (and associated ephemeral znodes) are still valid after another server takes over from the failed one.</li> <li>During failover, the application will receive notifications of disconnections and connections to the service. <ul> <li>Watch notifications will not be delivered while the client is disconnected, but they will be delivered when the client successfully reconnects.</li> <li>If the application tries to perform an operation while the client is reconnecting to another server, the operation will fail.</li> </ul> </li> </ul> </li> <li>Time <ul> <li>There are several time parameters in ZooKeeper.</li> <li>The tick time is the fundamental period of time in ZooKeeper and is used by servers in the ensemble to define the schedule on which their interactions run. Other settings are defined in terms of tick time or constrained by it.</li> <li>A common tick time setting is 2 seconds.</li> <li>Example: the session timeout can only be configured between 2~20 ticks (i.e., 4~40 seconds).</li> <li>The tick time in ZooKeeper is configurable, not fixed. Administrators can tune tick time based on the deployment’s latency and performance requirements.</li> </ul> </li> <li>States <ul> <li>In ZooKeeper, a client’s ZooKeeper object goes through different states during its lifecycle, such as connecting, connected, disconnected, or expired. You can query the current state at any time using the getState() method.</li> <li>Additionally, clients can register a Watcher to receive notifications whenever the state changes, allowing the application to respond to events like connection loss, reconnection, or session expiration.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/zk-7-480.webp 480w,/assets/img/zk-7-800.webp 800w,/assets/img/zk-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/zk-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zk-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="building-applications-with-zookeeper"><strong>Building applications with ZooKeeper</strong></h4> <p><strong>Group membership</strong></p> <ul> <li>ZooKeeper can be used to implement group membership for distributed services, allowing clients to locate active servers reliably. To maintain the membership list without a single point of failure, ZooKeeper uses znodes: a parent znode represents the group, and child znodes represent individual members. To create a group, a znode (e.g., /zoo) is created for the group itself. Servers join the group by creating ephemeral child znodes, which automatically disappear if the server fails or exits, ensuring the membership list reflects the current active members. Clients can list all members using the getChildren() method on the parent znode. Deleting a group requires first removing all child znodes, since ZooKeeper does not support recursive deletion; the delete() method is used, and specifying -1 for the version bypasses version checks. This approach provides an active, fault-tolerant membership list that dynamically updates as servers join or leave.</li> </ul> <p><strong>Configuration service</strong></p> <ul> <li>ZooKeeper can serve as a highly available configuration service for distributed applications, enabling machines in a cluster to share common configuration data. At a basic level, it stores configuration as key-value pairs, where the keys are represented by znode paths and the values are strings. Clients can retrieve or update this data, and using watches, interested clients can be notified automatically when configuration changes occur, creating an active configuration service. This model often assumes a single client performs updates at a time—for example, a master node like the HDFS NameNode updating information that worker nodes need—ensuring consistent and up-to-date configuration across the cluster.</li> </ul> <p><strong>Lock service</strong></p> <ul> <li>ZooKeeper provides a robust distributed lock service, which is essential for coordinating access to shared resources in a distributed system. A distributed lock ensures mutual exclusion, meaning that at any given time, only one client can hold the lock. To implement this, a lock znode is designated, for example /leader, representing the lock on a resource. Clients that wish to acquire the lock create ephemeral sequential child znodes under this parent znode. ZooKeeper assigns a unique, monotonically increasing sequence number to each child znode, providing a total ordering among clients. The client whose znode has the lowest sequence number is considered the current lock holder. Ephemeral znodes ensure automatic cleanup in case a client crashes, releasing the lock for the next contender without manual intervention.</li> <li>When multiple clients contend for the lock, efficient notification is crucial to avoid overloading the ZooKeeper ensemble. In a naïve approach, all clients watch the lock znode for changes in its children. However, this leads to the herd effect, where thousands of clients are notified of changes even though only one can acquire the lock at a time, creating traffic spikes and performance bottlenecks. ZooKeeper avoids this by having each waiting client set a watch only on the znode immediately preceding its own in sequence (i.e., the znode with the previous sequence number). For example, if the lock znodes are /leader/lock-1, /leader/lock-2, and /leader/lock-3, the client for lock-3 watches only lock-2. When lock-2 is deleted, the client for lock-3 is notified and can acquire the lock. This selective notification ensures that only the next contender is alerted, reducing unnecessary traffic and preventing the herd effect while maintaining correct lock ordering.</li> <li>The lock acquisition process works as follows: a client creates its ephemeral sequential znode, retrieves the list of children under the lock znode, and checks if its znode has the lowest sequence number. If it does, the lock is acquired. If not, the client sets a watch on the znode immediately preceding its own and waits. When that znode is deleted, the client is notified, repeats the check, and acquires the lock if it now has the lowest sequence number. This process guarantees deterministic ordering among clients while maintaining fairness in lock acquisition.</li> <li>Lock release and failover are automatic and reliable due to ephemeral znodes. When the lock-holding client deletes its znode—or if the client crashes, causing ZooKeeper to remove the ephemeral znode—the next client in sequence is notified and acquires the lock. This mechanism ensures automatic failover without manual intervention. The combination of sequential numbering and ephemeral znodes allows the system to handle client crashes gracefully while maintaining strict mutual exclusion.</li> <li>Overall, ZooKeeper’s distributed lock service leverages ephemeral sequential znodes, watches on immediate predecessors, and deterministic ordering to provide a scalable, fault-tolerant, and efficient locking mechanism. It avoids unnecessary notifications, supports automatic failover, and guarantees that locks are acquired fairly among contending clients. This design makes it ideal for building higher-level coordination services, such as leader election, configuration management, and other distributed synchronization tasks in large-scale systems.</li> </ul> <hr/> <p><strong>Doubts:</strong></p> <ul> <li>Q: How is a leader elected in ZooKeeper?</li> <li> <p>A: ZooKeeper elects a leader to coordinate write operations and maintain consistency across the ensemble. When servers start, each enters the LOOKING state and proposes a leader candidate, typically based on the highest transaction ID (zxid) or server ID. Servers exchange votes, and a candidate becomes leader once it receives a majority of votes. The elected leader transitions to the LEADING state, while others become FOLLOWERS. If the leader fails, the remaining servers automatically re-enter the election process. This election uses the Zab (ZooKeeper Atomic Broadcast) protocol to ensure the chosen leader is consistent and preserves all committed transactions.</p> </li> <li>Q: Why is there no full consensus needed in ZooKeeper’s ephemeral sequential znode leader election?</li> <li>A: In ephemeral sequential znode leader election, a full consensus protocol is not needed because ZooKeeper itself guarantees atomic and ordered creation of sequential znodes. Each server creates a znode with a unique increasing sequence number, and the leader is deterministically chosen as the server with the smallest sequence number. Since all servers can independently see the same sequence order, there are no conflicting proposals and no ambiguity in leader selection. Consensus protocols like Zab are still required for replicating write transactions, but leader election leverages ZooKeeper’s built-in ordering guarantees, making the process coordination-free and deterministic.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 8]]></summary></entry><entry><title type="html">Apache HBase</title><link href="https://monishver11.github.io/blog/2025/big-data-8-hbase/" rel="alternate" type="text/html" title="Apache HBase"/><published>2025-12-16T20:36:00+00:00</published><updated>2025-12-16T20:36:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-8-hbase</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-8-hbase/"><![CDATA[<h4 id="oltp-online-transactional-processing"><strong>OLTP (online transactional processing)</strong></h4> <ul> <li>OLTP enables the real-time execution of large numbers of database transactions by large numbers of people, typically over the internet.</li> <li>A database transaction is a change, insertion, deletion, or query of data in a database.</li> <li>In OLTP, the common, defining characteristic of any database transaction is its atomicity—a transaction either succeeds as a whole or fails (or is canceled). It cannot remain in a pending or intermediate state.</li> <li>Characteristics of OLTP systems <ul> <li>Process a large number of relatively simple transactions.</li> <li>Enable multi-user access to the same data, while ensuring data integrity.</li> <li>Emphasize very rapid processing, with response times measured in milliseconds.</li> <li>Provide indexed datasets.</li> <li>Are available 24/7/365.</li> </ul> </li> <li>OLTP versus OLAP</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbase-1-480.webp 480w,/assets/img/hbase-1-800.webp 800w,/assets/img/hbase-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hbase-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hbase-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <h4 id="nosql-not-only-sql-databases"><strong>NoSQL (Not only SQL) databases</strong></h4> <ul> <li>NoSQL refers to Not only SQL databases.</li> <li>They are non-relational databases for storing huge datasets eﬀectively.</li> <li>They are good for: <ul> <li>Indexing huge amount of documents.</li> <li>Serving pages on high-traﬃc websites.</li> <li>Delivering streaming media.</li> </ul> </li> <li>Consistency is less important. ACID properties are traded for performance.</li> <li>NoSQL refers to a range of databases that are not relational databases. <ul> <li>They do not support SQL.</li> <li>They often do not guarantee ACID properties.</li> </ul> </li> <li>Many NoSQL databases are descendants of Google’s Bigtable and Amazon’s Dynamo. <ul> <li>They are designed to be distributed across many nodes.</li> <li>They usually provide eventual consistency.</li> <li>They have very flexible schema.</li> </ul> </li> <li>NoSQL databases do not use SQL for data manipulation. <ul> <li>The database is optimized for retrieval and append operations.</li> <li>They oﬀer a key-value store. Some are built on GFS/HDFS.</li> </ul> </li> <li>NoSQL databases are designed for scalability and performance. <ul> <li>They are useful for big data in applications where a relational model is not needed.</li> <li>They can easily scale up by adding inexpensive commodity servers. Much easier than with relational databases.</li> </ul> </li> <li>NoSQL database systems are developed to manage large volumes of data that do not necessarily follow a fixed schema. <ul> <li>Data is sharded and stored across many servers.</li> <li>The architecture is distributed and fault-tolerant.</li> <li>They are useful for managing large amounts of data where satisfying realtime constraints is the priority. The goal is near-realtime or soft realtime, i.e., fast enough for a web service.</li> </ul> </li> <li>Q: Do NoSQL databases relax consistency but keep atomicity?</li> <li>A: In most NoSQL databases, atomicity is preserved but only at a limited scope, such as a single row, document, or key. Consistency across multiple records is often relaxed, and isolation guarantees are weaker, while durability is usually maintained. This trade-off is intentional, allowing NoSQL systems to achieve high scalability and performance in distributed environments.</li> <li>Google’s solution for NoSQL <ul> <li>Bigtable: a distributed storage system for structured data</li> <li>Best Paper award at OSDI’06 (one of the two most prestigious systems conferences held once every two years).</li> <li>As a side note, Google also published Chubby in the same conference.</li> <li>Today, Bigtable is still one of the most widely-used NoSQL databases. Google oﬀers Cloud Bigtable as part of its cloud computing services.</li> </ul> </li> <li>Hadoop’s solution for NoSQL <ul> <li>HBase is modeled after Google’s Bigtable.</li> <li>Bigtable is closed-source; HBase is open-source.</li> <li>It is suitable for extremely large databases. Billions of rows, millions of columns.</li> <li>It is distributed across thousands of nodes.</li> <li>Facebook used HBase for its messaging system from 2010 to 2018.</li> </ul> </li> </ul> <h4 id="hbase"><strong>HBase</strong></h4> <ul> <li><strong>Use cases</strong> <ul> <li>Facebook messages: At the high end: over one million HBase cluster operations per second. Processing thousands of records per second per node.</li> <li>Large amount of stored data: Queries only require small amount of rows in response.</li> <li>Use HBase for random reading or writing, or both: When your data is in the TB range.</li> </ul> </li> <li><strong>NoJOIN?</strong> <ul> <li>Traditional JOIN operations are not supported in NoSQL.</li> <li>Implementing JOIN is impractical. <ul> <li>In HBase, data is sharded across many servers.</li> <li>HBase wants to provide fast response.</li> </ul> </li> <li>But in HBase, the capability exists for very, very large rows (millions of columns). <ul> <li>JOINs are not needed.</li> <li>The recommendation is that data should be de-normalized.</li> </ul> </li> <li>Conceptual Flow: <ul> <li>HBase does not support traditional SQL JOIN operations because implementing joins in a distributed NoSQL system is impractical and expensive. In HBase, data is sharded across many servers, so performing a join would require large amounts of network communication and coordination, which would significantly increase latency. Since HBase is designed for fast, low-latency access, joins would violate its performance goals. Instead, HBase supports very wide rows with potentially millions of columns, allowing related data to be stored together in a single row. Because of this design, joins are usually unnecessary, and the recommended approach is to denormalize the data, storing all frequently accessed related data together to enable fast reads.</li> <li>In HBase, data is sharded (partitioned) by row key, not by columns. Each row can be extremely wide (even millions of columns), and all columns for a given row are stored together. HBase uses the row key to determine which region (and therefore which server/node) holds that row.</li> <li>So when a request comes in, HBase routes it directly to the node responsible for that row key, allowing fast reads and writes without needing joins. This row-based sharding, combined with wide rows and denormalized data, is what enables HBase to scale horizontally while maintaining low-latency access.</li> </ul> </li> </ul> </li> <li><strong>Sparse rows</strong> <ul> <li>HBase is a wide-column store.</li> <li>HBase can handle sparse records. <ul> <li>Sparse records means that some columns are not filled in.</li> </ul> </li> <li>In HBase, there is no penalty for sparse data because no space is allocated. This is in contrast to relational databases in which an unpopulated field is also allocated space.</li> </ul> </li> <li><strong>Designing tables</strong> <ul> <li>The RDBMS approach to design is relationship-centric.</li> <li>However, HBase requires an access-centric approach to design.</li> <li>We cannot take an RDBMS and model it directly in HBase. <ul> <li>RDBMS will have normalized data. However, HBase will perform better with de-normalized data.</li> <li>HBase columns must be grouped into column families based on expected access patterns.</li> <li>The columns of a column family are stored close together on disk for fast access.</li> </ul> </li> <li>We’ll see more on this in HBase’s Data Model (see below).</li> </ul> </li> <li><strong>NoACID?</strong> <ul> <li>HBase is not a fully ACID-compliant database. <ul> <li>HBase does not provide strong consistency.</li> <li>It does not provide atomicity across rows.</li> <li>However, it does provide atomic operations at the row level.</li> </ul> </li> </ul> </li> <li><strong>Storage platform</strong> <ul> <li>HBase uses HDFS underneath for storage.</li> <li>HBase does not convert commands to MapReduce jobs.</li> <li>HBase supports random reads and writes. <ul> <li>Writes are implemented through versioning of cells.</li> <li>The user can define the max number of versions to maintain.</li> <li>The user can also define the time-to-live. After that time, the row is automatically marked for deletion by HBase.</li> </ul> </li> </ul> </li> </ul> <h4 id="hbase-architecture"><strong>HBase Architecture</strong></h4> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbase-2-480.webp 480w,/assets/img/hbase-2-800.webp 800w,/assets/img/hbase-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hbase-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hbase-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>An HBase cluster is comprised of master and worker nodes.</li> <li>Similar master-worker architecture as ween with… <ul> <li>HDFS: NameNode and DataNodes.</li> <li>YARN: ResourceManager and NodeManagers.</li> <li>MapReduce: ApplicationMaster and tasks.</li> <li>Trino: Coordinator and workers.</li> </ul> </li> <li><strong>Master node</strong> <ul> <li>The master node manages a cluster of Regionservers.</li> <li>It bootstraps the initial install.</li> <li>It assigns regions to registered Regionservers.</li> <li>It recovers Regionserver failures.</li> </ul> </li> <li>The master node is lightly loaded. <ul> <li>Client data does not move through the master node.</li> <li>Clients do not rely on the master node for region location information.</li> </ul> </li> <li><strong>Regionservers</strong> <ul> <li>Each Regionserver carries zero or more regions. <ul> <li>A region is a subset of a table’s rows.</li> <li>Row updates are atomic.</li> </ul> </li> <li>They handle read/write requests.</li> <li>They perform region splits, which they communicate to the master node. <ul> <li>Writes cause regions to grow. Eventually, they must be split.</li> </ul> </li> </ul> </li> <li><strong>ZooKeeper cluster</strong> <ul> <li>ZooKeeper is a distributed coordination service (e.g., configuration, synchronization, naming registry).</li> <li>HBase uses ZooKeeper to host vitals such as: <ul> <li>The location of the hbase:meta catalog table.</li> <li>The address of the current cluster master.</li> </ul> </li> <li>HBase also uses ZooKeeper to host the transaction state of region assignments. This is to support fast recovery.</li> <li>More clearly: In HBase, the transaction state of region assignments stored in ZooKeeper represents the current and in-progress states of assigning regions to RegionServers, such as unassigned, assigning, or assigned. ZooKeeper maintains this shared state so the HBase master and RegionServers have a consistent view of region ownership. If a master or RegionServer fails during an assignment, the new master can read this state from ZooKeeper and safely resume or recover the operation, ensuring that each region is assigned to exactly one RegionServer at a time and enabling fast, reliable cluster recovery.</li> <li>Fresh clients connect to the ZooKeeper to learn the location of hbase:meta. The result is cached at the clients until there is a fault.</li> </ul> </li> <li><strong>The hbase:meta catalog table</strong> <ul> <li>It maintains the current list, state, and locations of all user-space regions afloat on the cluster.</li> <li>Entries in hbase:meta are keyed by region name, which is made up of: <ul> <li>The table name.</li> <li>The start row key.</li> <li>The creation time.</li> <li>A checksum.</li> </ul> </li> <li>Conceptual Flow: <ul> <li>The hbase:meta catalog table is a special system table in HBase that acts as the central directory for all user-space regions in the cluster. It keeps track of the current list of regions, their states (e.g., online, offline, splitting), and the RegionServers that host them. Each entry in hbase:meta is keyed by a region name, which is a combination of the table name, the region’s start row key, the creation timestamp, and a checksum to ensure uniqueness.</li> <li>This catalog table is essential for HBase’s operation: when a client wants to read or write data, it queries hbase:meta (directly or via ZooKeeper) to determine which RegionServer holds the desired row. Similarly, the HBase master uses it to manage region assignments, splits, and recovery. By maintaining an up-to-date map of the cluster’s regions, hbase:meta enables fast lookups, load balancing, and fault-tolerant access to data in a distributed environment.</li> </ul> </li> </ul> </li> <li><strong>Cluster expansion</strong> <ul> <li>Expanding an HBase cluster is much easier than scaling a traditional relational database.</li> <li>In RDBMS systems, expansion is often complex, error-prone, and difficult to maintain, because horizontal scaling was not part of their original design. Features like joins, complex queries, and strict ACID guarantees make distributing data across multiple nodes challenging.</li> <li>In contrast, HBase is designed for horizontal scalability, allowing nodes to be added with minimal disruption. Its row-based sharding, denormalized data models, and simplified consistency guarantees make it straightforward to expand the cluster and handle growing datasets efficiently.</li> </ul> </li> </ul> <h4 id="hbase-data-model"><strong>HBase Data model</strong></h4> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbase-3-480.webp 480w,/assets/img/hbase-3-800.webp 800w,/assets/img/hbase-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hbase-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hbase-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li><strong>Column family</strong> <ul> <li>HBase is a distributed column-family-oriented database built on top of HDFS.</li> <li>A column family is a grouping of columns. <ul> <li>Columns that belong to a given column family are physically stored together.</li> <li>A column is referenced as columnFamilyName:columnName. The columnName is sometimes referred to as the qualifier.</li> <li>In HBase, tuning is performed on a column family bases.</li> </ul> </li> <li>HBase column families are defined at table definition time, but columns can be defined dynamically.</li> <li>Conceptual Flow: <ul> <li>In HBase, a column family is a grouping of columns whose data is physically stored together in HDFS files called HFiles, allowing efficient access to all columns in the family.</li> <li>Columns are referenced as columnFamily:columnName, where columnName is also called the qualifier. Column families are defined at table creation, but individual columns within a family can be added dynamically.</li> <li>All regions of a table share the same column family structure, even though many rows may be sparse and lack values for some columns; empty cells are simply not stored.</li> <li>HFiles are stored on HDFS and subject to the HDFS block size (typically 128 MB); if a column family’s data exceeds a block, it is automatically split across multiple blocks distributed across different data nodes. HBase manages reading and writing from these HFiles, including handling multiple blocks, region splits, and region-to-node assignments, using metadata and indexing to ensure fast, atomic access at the row level.</li> <li>This design supports very wide tables, sparse and denormalized data, and scalable, high-throughput access while keeping related columns physically close for efficient reads.</li> </ul> </li> </ul> </li> <li><strong>Cells</strong> <ul> <li>Values are stored in HBase cells.</li> <li>A cell lies at the intersection of a row and a column, at a particular version.</li> <li>By default, a cell version is a timestamp. The timestamp is automatically assigned by HBase.</li> <li>To reference a value in a cell, use: row key + columnFamily:columnName + timestamp.</li> <li>A cell’s content is an uninterpreted array of bytes.</li> </ul> </li> <li>Regions <ul> <li>A region (“tablet” in Google’s jargon) is a subset of an HBase table’s rows.</li> <li>It is defined by a start row key (inclusive) and an end row key (exclusive). Every table’s rows belong to some HBase region.</li> <li>Initially, by default, a table is comprised of just one region. As the size of the region grows, it splits.</li> <li>A region splits at a row boundary into two new regions of about equal size. The threshold at which a region will be split is configurable.</li> <li>In HBase, when a region splits at a row boundary, it means the split happens between two rows, not in the middle of a row’s data. Each row is the atomic unit for storage and updates, so HBase ensures that a row’s contents are never divided across two regions.</li> <li>Regions are the units that get distributed throughout an HBase cluster. This is how a table can grow very large without the constraints that hamper RDBMS table grows.</li> <li>A table’s total content is the full set of the regions that hold its rows.</li> </ul> </li> <li>Row keys <ul> <li>In HBase, an operation on a given row is atomic. So, the locking model is simple.</li> <li>HBase provides just one index: the row key.</li> <li>Row keys are stored in sorted order. It is fast and easy to locate a particular row via lookup. Each row has a row key. The row key is also an array of bytes.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hbase-4-480.webp 480w,/assets/img/hbase-4-800.webp 800w,/assets/img/hbase-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hbase-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hbase-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li><strong>Write operation</strong> <ul> <li>On the Regionserver, writes are assisted by the Write Ahead Log (WAL).</li> <li>All writes are first written to the WAL on HDFS, and then to the memstore (Google calls it the memtable) for quick lookup.</li> <li>When the amount of data in the memstore reaches a threshold (configurable), the memstore is flushed to HDFS.</li> <li>Each time data is flushed from the memstore, it is stored on disk in an HFile (Google uses SSTable).</li> </ul> </li> <li>Read operation <ul> <li>The region’s memstore is first consulted. If more versions are needed, flush files are consulted from newest to oldest.</li> <li>Reads are assisted by the block cache: The block cache is in the Regionserver. It uses the Least Recently Used (LRU) algorithm.</li> <li>The cache is configurable: It can model multi-level cache. It can choose where data is cached.</li> <li>So while the cache is local to each RegionServer, you can tune it independently on different servers to optimize read performance based on workload and memory availability.</li> </ul> </li> <li>Delete operation <ul> <li>When you delete a version, it means to delete all cells where the version is less than or equal to this version.</li> <li>Since HBase never modifies data in place, it will not immediately delete (or mark as deleted) the entries in the HFiles.</li> <li>Instead, it writes a tombstone marker, which will mask the deleted values.</li> </ul> </li> <li><strong>Compactions</strong> <ul> <li>Minor compaction is a process that compacts a (configurable) number of adjacent small HFiles into a large HFile. It does not drop deletes or expired versions.</li> <li>Major compaction is a heavyweight process that: Rewrites all files within a column family for a region into a single new file. Removes tombstone markers any dead entries. Deletes any expired data (Based on TTL).</li> </ul> </li> <li><strong>Note:</strong> <ul> <li>Q: What is the relationship between HFiles and HDFS blocks in HBase?</li> <li>A: HFiles are logical files managed by HBase that store the data for a column family. They are physically stored on HDFS, which splits files into blocks (typically 128 MB). If an HFile is smaller than the HDFS block size, it may share a block with other small files, but each HFile is usually treated as a separate file. When an HFile grows larger than the block size, HDFS splits it across multiple blocks stored on different data nodes. HBase manages these HFiles independently, using indexing and metadata for efficient reads, while HDFS handles their block-level storage and distribution.</li> <li>So, HFiles are a logical abstraction created by HBase to manage data storage on HDFS. They enable HBase to efficiently store column-family data, maintain indexes, metadata, and Bloom filters for fast reads, and handle memstore flushes, compactions, and region splits. Physically, HFiles are just normal HDFS files, but the abstraction allows HBase to manage wide rows, sparse data, multiple versions, and efficient retrieval without exposing the underlying HDFS complexities to users.</li> <li>Conceptual flow of how HBase routes read and write requests to the correct RegionServer: When a client wants to read or write a row, it first contacts ZooKeeper to find the location of the hbase:meta catalog table. The client then queries hbase:meta to determine which RegionServer hosts the region containing the target row key. With this information, the client communicates directly with the appropriate RegionServer, which manages the region’s memstore and HFiles to handle the read or write. The client caches the region-to-RegionServer mapping for future requests to avoid repeated lookups. If a region moves due to splits or server failures, the cache is updated using ZooKeeper and hbase:meta. Throughout this process, the HBase master is not in the path of reads or writes; it only coordinates region assignments, splits, and recovery, while RegionServers perform the actual data storage and access.</li> </ul> </li> </ul> <h4 id="hbase-usage"><strong>HBase Usage</strong></h4> <ul> <li><strong>Programming</strong> <ul> <li>Aside from the HBase shell, you can use compiled languages with the HBase API.</li> <li>Java is natively supported.</li> <li>Python is also supported with the Thrift server. Python will be slower than Java due to the Thrift server overhead (additional interface).</li> <li>Multiple Thrift servers may be needed for improved performance when datasets are large.</li> </ul> </li> <li><strong>MapReduce</strong> <ul> <li>HBase can be used as a source and/or sink in MapReduce jobs.</li> <li>The TableInputFormat class makes splits on region boundaries so maps are handed a single region to work on.</li> <li>The TableOutputFormat class will write the result of the reduce into HBase.</li> </ul> </li> </ul> <h4 id="summary"><strong>Summary</strong></h4> <ul> <li>HBase is a distributed column-oriented database built on top of HDFS.</li> <li>No real indexes: rows and columns are stored sequentially.</li> <li>Automatic partitioning: Regions are split and distributed across the cluster.</li> <li>Scale linearly with new nodes: Regions automatically rebalance.</li> <li>Commodity hardware: much less I/O hungry than RDBMSs.</li> <li>Fault tolerance: no need to worry about individual node downtime.</li> <li>Batch processing: support distributed MapReduce jobs with locality awareness.</li> </ul> <hr/> <p><strong>Doubts:</strong></p> <ul> <li>In Webtable example(Slide 31), What is the reason for having the row-key as “com:cnn.www”? Understand it clearly.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 7]]></summary></entry><entry><title type="html">Hive &amp;amp; Trino</title><link href="https://monishver11.github.io/blog/2025/big-data-7-hive/" rel="alternate" type="text/html" title="Hive &amp;amp; Trino"/><published>2025-12-16T03:40:00+00:00</published><updated>2025-12-16T03:40:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-7-hive</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-7-hive/"><![CDATA[<h4 id="online-analytical-processing-olap"><strong>Online Analytical Processing (OLAP)</strong></h4> <ul> <li>OLAP is software for performing multidimensional analysis at high speeds on large volumes of data from a data warehouse, data mart, or some other unified, centralized data store.</li> <li>Most business data have multiple dimensions—multiple categories into which the data are broken down for presentation, tracking, or analysis.</li> <li>But in a data warehouse, data sets are stored in tables, each of which can organize data into just two of these dimensions at a time. OLAP extracts data from multiple relational data sets and reorganizes it into a multidimensional format that enables very fast processing and very insightful analysis.</li> <li>Example: Sales figures might have several dimensions related to Location (region, country, state/province, store), Time (year, month, week, day), Product (clothing, men/women/children, brand, type);</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hive-1-480.webp 480w,/assets/img/hive-1-800.webp 800w,/assets/img/hive-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hive-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hive-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="hive"><strong>Hive</strong></h4> <ul> <li>Hive is a data warehousing framework developed by Facebook.</li> <li>Hive is built on top of HDFS (for storage) and MapReduce (for processing).</li> <li>Hive supports HiveQL: <ul> <li>HiveQL is a dialect of SQL.</li> <li>HiveQL is heavily influenced by MySQL.</li> </ul> </li> <li> <p>User data are organized into tables, which are stored as files on HDFS.</p> </li> <li><strong>Metastore</strong> <ul> <li>It’s the central repository of Hive metadata such as table schemas.</li> <li>It’s a relational database automatically created by Hive.</li> </ul> </li> <li>Q: What is stored in the Hive Metastore?</li> <li>A: The Hive Metastore stores metadata about Hive objects, not the actual data. This includes database and table definitions, column names and data types, partition information and locations, storage formats and SerDe details, table properties, statistics used by the query optimizer, and access privileges. The actual table data itself is stored separately in HDFS.</li> <li>Q: Why does Hive use an RDBMS for the Metastore?</li> <li> <p>A: Hive uses an RDBMS for the Metastore because metadata is highly structured and relational, requiring fast, indexed lookups and strong consistency. An RDBMS provides ACID guarantees and supports concurrent access from multiple clients, ensuring metadata remains consistent and durable. It is also optimized for frequent small reads and writes, which is inefficient on HDFS, making an RDBMS the right choice for storing Hive metadata.</p> </li> <li>Hive converts your query into a set of MapReduce jobs. Therefore: <ul> <li>It’s batch-oriented.</li> <li>Its response time is relatively long (tens of seconds to minutes).</li> </ul> </li> <li>Hive provides a shell for interactively issuing commands.</li> <li>There are a number of Hive services in addition to the Hive shell.</li> <li>For example, the hiveserver2 service exposes a Thrift service <ul> <li>It enables access by clients written in diﬀerent languages.</li> <li>It supports applications using Thrift, JDBC, ODBC connectors.</li> </ul> </li> <li>RPC (Remote Procedure Call) is a communication mechanism that allows a program to execute a function or procedure on another machine or process as if it were a local function call. The client sends a request with the function name and arguments over the network, the server executes the function, and the result is sent back to the client. RPC abstracts away network details like sockets and message passing, making distributed systems easier to build.</li> <li>Thrift lets a service provide a set of functions (APIs) that can be called remotely, and programs written in different programming languages can use those functions without worrying about the details of how the network communication works. For example, HiveServer2 can run in Java, but a Python or C++ client can still send queries to it using Thrift. Essentially, Thrift handles the translation between languages and the network communication for you.</li> <li> <p>HiveServer2 also supports standard connectivity through JDBC (Java Database Connectivity) for Java applications and ODBC (Open Database Connectivity) for applications in various other languages, enabling external tools and BI applications to interact with Hive without using the Hive shell directly.</p> </li> <li><strong>Architecture</strong></li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hive-2-480.webp 480w,/assets/img/hive-2-800.webp 800w,/assets/img/hive-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hive-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hive-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>Schema on write versus schema on read</strong> <ul> <li>Schema on write <ul> <li>Traditional databases uses “schema on write”.</li> <li>Used by relational databases like MySQL.</li> <li>Schema is enforced at load time.</li> <li>If the data being loaded does not conform to the schema, the load fails.</li> <li>Queries are faster because columns can be indexed and data are compressed at load time. However, loading takes a long time.</li> </ul> </li> <li>Schema on read <ul> <li>Hive uses “schema on read”.</li> <li>Table’s schema is not enforced until a query is issued.</li> <li>Makes for much faster loading.</li> <li>Schema on read is more flexible.</li> <li>Multiple schemas can be supported simultaneously.</li> <li>A Hive table is essentially an HDFS directory containing one or more files that comprises the table.</li> <li>Users may define multiple Hive table schemas for a given Hive table.</li> </ul> </li> </ul> </li> <li><strong>Features</strong> <ul> <li>Older versions of Hive did not support updates due to limitations of HDFS.</li> <li>Throughout the years, Hive has been supporting more and more usages.</li> <li>Now, Hive supports INSERT INTO for adding rows to existing tables.</li> <li>Hive also supports: <ul> <li>Indexes (as of version 0.7.0).</li> <li>Primitive types as found in Java.</li> <li>Java complex types ARRAY and MAP.</li> <li>STRUCT type, which is a record type.</li> </ul> </li> </ul> </li> <li><strong>Tables</strong> <ul> <li>A Hive table is logically made up of the data being stored and the associated metadata describing the layout of the data in the table.</li> <li>The data typically resides in HDFS. The metadata is stored in the metastore, which is a relational database.</li> <li>Hive supports managed tables and external tables.</li> <li>Managed tables <ul> <li>Hive moves the data into its warehouse directory.</li> <li>When you DROP a managed table, its metadata and its data are deleted.</li> </ul> </li> <li>External tables <ul> <li>You control the creation and deletion of the data.</li> <li>Hive refers to the data at an existing location outside the warehouse directory.</li> <li>When you DROP an external table, only the metadata is deleted, and the data is untouched.</li> </ul> </li> <li>In Hive, the warehouse directory is the default location on HDFS where Hive stores the data files for managed tables. By default, this is usually something like /user/hive/warehouse/. When you create a managed table, Hive moves or stores the table’s data in this directory, and it manages the lifecycle of both the metadata and the data. For external tables, the data stays outside this directory, and Hive only keeps metadata pointing to its location.</li> </ul> </li> <li><strong>Partitions</strong> <ul> <li>Hive allows partitions to be defined.</li> <li>It divides a table based on the value of a partition column (e.g., date).</li> <li>Using partitions can make it faster to do queries on slices of the data.</li> <li>Partitions are nested subdirectories of the table directory on HDFS.</li> <li>If a table has a nested directory structure in HDFS but no partition column is defined in Hive, Hive will not treat those directories as partitions. The table will be considered unpartitioned, and all the data files under the table directory (including subdirectories) will be read as a single logical table. Queries won’t automatically skip directories, they’ll scan all data files, which can make queries slower.</li> <li>While Creating Hive partitions, note that while the partitions are referenced in queries just as if they were fields, no data has been added to the table contents.</li> <li>Ex: hive&gt; CREATE TABLE logs (timestamp BIGINT, line STRING) PARTITIONED BY (theDate STRING, campus STRING);</li> <li>The LOAD DATA command is used to populate a table with data. The source of the data is specified after the INPATH keyword.</li> <li>If multiple files are found in the same directory, they are all part of the table.</li> <li>You can issue SQL queries on the Hive table.</li> <li>In the query, the partition column is treated just like a column internal to the table, even though values are not contained in the data files.</li> <li>Gist: A Hive table is a logical abstraction over data stored in HDFS and can be either managed, where Hive owns and stores the data in its warehouse directory, or external, where the data lives outside Hive and only metadata is managed. A partition in a Hive table means a logical subdivision of the table’s rows based on the values of one or more partition columns (for example, date or campus). Physically, each partition maps to a subdirectory in HDFS, named using the partition column values (e.g., date=2025-12-15/campus=NYC/). All files within a partition directory belong to that partition, and multiple files are treated as one logical chunk of the table. Partition column values are not stored inside the data files; Hive infers them from the directory structure. Data is loaded into tables and partitions using LOAD DATA or external tools, and Hive automatically associates files with partitions based on their HDFS paths. When querying, partition columns behave like normal table columns, but Hive can use them for partition pruning, reading only the relevant directories instead of scanning the entire table, which significantly improves query performance.</li> </ul> </li> <li><strong>Joins</strong> <ul> <li>Hive supports conventional SQL joins: Inner joins, Left/right/full outer joins, and Left/right semi joins.</li> <li>Remember replicated joins (map-side joins or broadcast join)? They are automatic in Hive.</li> </ul> </li> <li><strong>Extensibility</strong> <ul> <li>Hive is extensible via: <ul> <li>UDFs: user-defined functions -&gt; Input one row, output one row.</li> <li>UDAFs: user-defined aggregate functions -&gt; Input multiple rows, output one row.</li> <li>UDTFs: user-defined table-generating functions -&gt; Input one row, output multiple rows — a table.</li> </ul> </li> </ul> </li> <li><strong>Presto &amp; Trino</strong> <ul> <li>Since 2008, Hive became widely used within Facebook for running analytics against data in HDFS on its very large Hadoop cluster.</li> <li>Hive was not suitable for interactive queries at Facebook’s Scale.</li> <li>By 2012, Facebook’s Hive data warehouse was 250 PB in size and needed to handle</li> <li>hundreds of users issuing 10k+ queries each day.</li> <li>Hive could not query other data sources. This means Hive was largely limited to querying data stored in HDFS (and Hive-managed tables) and was not designed to easily query multiple, heterogeneous data sources in a single query.</li> <li>Hive either could not access these sources directly or required complex ingestion pipelines to first copy the data into HDFS. This made interactive analytics slow and inflexible.</li> <li>In 2012, Facebook started to develop Presto from scratch to address the performance, scalability, and extensibility needs for analytics at Facebook.</li> <li>In 2018, its creators left Facebook with their project and later renamed it Trino.</li> </ul> </li> </ul> <h4 id="trino"><strong>Trino</strong></h4> <p>TODO: Add Notes for Trino;</p>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 6]]></summary></entry><entry><title type="html">MapReduce Design Patterns</title><link href="https://monishver11.github.io/blog/2025/big-data-5-mr-dp/" rel="alternate" type="text/html" title="MapReduce Design Patterns"/><published>2025-12-15T22:40:00+00:00</published><updated>2025-12-15T22:40:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-5-mr-dp</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-5-mr-dp/"><![CDATA[<h4 id="serialization"><strong>Serialization</strong></h4> <ul> <li>Serialization is the process of turning structured objects into a byte stream for transmission over a network or for writing to persistent storage.</li> <li>Deserialization is the reverse process of turning a byte stream back into a series of structured objects.</li> <li>A good serialization format should be compact, fast, extensible and interoperable.</li> <li>Hadoop uses its own serialization format: Writable.</li> <li>Hadoop comes with a large selection of Writable classes, which are available in the org.apache.hadoop.io package.</li> <li>There are Writable wrappers for all the Java primitive types except char (which can be stored in an IntWritable).</li> <li>All Writable wrappers have a get() and set() method for retrieving and storing the wrapped value. Example: IntWritable count = new IntWritable(42).</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mr-dp1-480.webp 480w,/assets/img/mr-dp1-800.webp 800w,/assets/img/mr-dp1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mr-dp1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mr-dp-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Text is the Writable wrapper for mutable UTF-8 strings. Ex: Text word = new Text(“Hadoop”);</li> <li>BytesWritable is the Writable wrapper for byte[].</li> <li>NullWritable is a special type of Writable, which has zero-length serialization. No bytes are written to or read from the stream. It is used as a placeholder.</li> <li>For example, in MapReduce, a key or a value can be declared as a NullWritable when you don’t need to use that position, eﬀectively storing a constant empty value. It is an immutable singleton, and the instance can be retrieved by calling NullWritable.get(). Ex: NullWritable nullKey = NullWritable.get();</li> </ul> <h4 id="counters"><strong>Counters</strong></h4> <ul> <li>Counters are a useful channel for gathering statistics about the job: For quality control (Example: what’s the percentage of records that are invalid?), For application-level statistics (Example: how many users in the dataset are between the ages of 18—64?)</li> <li>MapReduce allows user code to define a set of counters, which are then incremented as desired in the mapper or reducer.</li> <li>Counters are defined by a Java enum, which serves to group related counters.</li> <li>A job may define any number of enums, each with any number of fields. The name of the enum is the group name. The enum’s fields are the counter names.</li> <li>Counters are global: the MapReduce framework aggregates them across all mappers and reducers to produce a grand total at the end of the job.</li> <li>Ex: enum Temperature { MISSING, MALFORMED}</li> <li>context.getCounter(Temperature.MALFORMED).increment(1);</li> <li>context.getCounter(Temperature.MISSING).increment(1);</li> <li>context.getCounter(“TemperatureQuality”, parser.getQuality()).increment(1); //dynamic counter, here “TemperatureQuality” is a manual group name (not an enum).</li> <li>Note: In Hadoop, counters are defined and incremented by the Mapper or Reducer, tracked locally by the NodeManager, aggregated by the Application Master, and finally reported to the client by the Resource Manager at job completion.</li> <li>Note: Hadoop also provides built-in counter groups such as FileSystemCounters (bytes read/written), TaskCounters (records processed, spilled data), and JobCounters (launched or failed tasks).</li> <li>Ex: Counter hdfsRead = context.getCounter(“FileSystemCounters”, “HDFS_BYTES_READ”); //access built-in FileSystem counter</li> </ul> <h4 id="mapreduce-design-patterns"><strong>MapReduce design patterns</strong></h4> <ul> <li>Summarization patterns <ul> <li>Numerical summarizations</li> <li>Inverted index summarizations</li> <li>Counting with counters</li> </ul> </li> <li>Filtering patterns <ul> <li>Filtering</li> <li>Bloom filtering</li> <li>Top ten</li> <li>Distinct</li> </ul> </li> <li>Data organization patterns <ul> <li>Structured to hierarchical</li> <li>Partitioning</li> <li>Binning</li> <li>Total order sorting</li> <li>Shuﬄing</li> </ul> </li> <li>Join patterns <ul> <li>Reduce-side join</li> <li>Replicated join</li> <li>Cartesian product</li> </ul> </li> <li>Metapatterns <ul> <li>Job chaining</li> <li>Chain folding</li> <li>Job merging</li> </ul> </li> <li>Input and output patterns</li> </ul> <p><strong>TODO:</strong> Add details, explanation and images to MapReduce design patterns;</p>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 5]]></summary></entry><entry><title type="html">Big Data Processing Concepts &amp;amp; MapReduce</title><link href="https://monishver11.github.io/blog/2025/big-data-4-mapreduce/" rel="alternate" type="text/html" title="Big Data Processing Concepts &amp;amp; MapReduce"/><published>2025-12-15T22:39:00+00:00</published><updated>2025-12-15T22:39:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-4-mapreduce</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-4-mapreduce/"><![CDATA[<h4 id="big-data-processing-concepts"><strong>Big data processing concepts</strong></h4> <ul> <li>Parallel data processing <ul> <li>Parallel data processing reduces the execution time by dividing a single large job into multiple smaller tasks that run concurrently. Ex: a single machine with multiple processors or cores.</li> </ul> </li> <li>Distributed data processing <ul> <li>Distributed data processing is achieved through physically separate machines that are networked together as a cluster.</li> </ul> </li> <li>Hadoop <ul> <li>Hadoop is an open-source framework for large-scale data storage and data processing that is compatible with commodity hardware.</li> <li>It can be used as an analytics engine for processing large amounts of structured, semi-structured and unstructured data.</li> <li>It implements the MapReduce processing framework.</li> </ul> </li> <li>Processing workloads <ul> <li>A processing workload in Big Data is defined as the amount and nature of data that is processed within a certain amount of time.</li> <li>Workloads are usually divided into two types: Batch processing and transactional processing.</li> <li>Batch processing: <ul> <li>(A.k.a. offline processing) involves processing data in batches and usually imposes delays, which results in high-latency responses.</li> <li>Batch workloads typically involve large quantities of data with sequential read/writes and comprise of groups of read or write queries.</li> <li>Queries can be complex and involve multiple joins.</li> <li>OLAP (online analytical processing) systems commonly process workloads in batches.</li> <li>Gist: In batch processing, data is read and written in large, continuous chunks on disk (sequentially), which is faster than random access. Instead of many small operations, the system groups multiple read/write queries—such as aggregations, joins, or filters—into a single large job (like MapReduce), processing them together for high throughput but with higher latency.</li> </ul> </li> <li>Transactional processing: <ul> <li>(A.k.a online processing) follows an approach whereby data is processed interactively without delay, resulting in low-latency responses.</li> <li>Transaction workloads involve small amounts of data with random reads and writes and fewer joins.</li> <li>OLTP (online transaction processing) system fall within this category.</li> </ul> </li> </ul> </li> </ul> <h4 id="mapreduce"><strong>MapReduce</strong></h4> <ul> <li>MapReduce is a widely used implementation of a batch processing framework.</li> <li>It is highly scalable and reliable and is based on the principle of divide-and-conquer, which provides built-in fault tolerance and redundancy.</li> <li>MapReduce does not require that the input data conform to any particular data model. Therefore, it can be used to process schema-less datasets.</li> <li>A dataset is broken down into multiple smaller parts, and operations are performed on each part independently and in parallel.</li> <li>The results from all operations are then summarized to arrive at the answer.</li> <li>Traditionally, data processing requires moving data from the storage node to the processing node that runs the data processing algorithm. This approach works fine for smaller datasets. However, with large datasets, moving data can incur more overhead than the actual processing of the data.</li> <li>With MapReduce, the data processing algorithm is instead moved to the nodes that store the data. The data processing algorithm executes in parallel on these nodes, thereby eliminating the need to move the data first. It saves network bandwidth and reduces processing time for large datasets.</li> <li>Terminology: <ul> <li>A MapReduce job is a unit of work that the client wants to be performed.</li> <li>Hadoop runs the job by dividing it into tasks: map tasks and reduce tasks.</li> <li>Hadoop divides the input to a MapReduce job into fixed-size splits.</li> <li>Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-1-480.webp 480w,/assets/img/mapreduce-1-800.webp 800w,/assets/img/mapreduce-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Map: <ul> <li>The dataset is divided into multiple smaller splits. Each split contains multiple key-value pairs.</li> <li>The map function (“mapper”) executes user-defined logic on each split: (K1, V1) -&gt; list(K2, V2)</li> <li>A mapper may generate zero(filtering) or more than one(demultiplexing) key-value pairs.</li> </ul> </li> <li>Combine: <ul> <li>With larger datasets, moving data between map and reduce stages is more expensive than the actual processing.</li> <li>The optional combiner function summarizes a mapper’s output before it gets processed by the reducer: (K2, list(V2)) -&gt; list(K2, V2)</li> </ul> </li> <li>Partition: <ul> <li>If more than one reducer is involved, a partitioning function (“partitioner”) divides the output from the mapper or combiner (if used) into partitions between reducer instances.</li> <li>Although each partition contains multiple key-value pairs, all records for a particular key are assigned to the same partition.</li> </ul> </li> <li>Shuffle &amp; Sort: <ul> <li>Output from all partitioners is copied across the network to the nodes running the reduce task.</li> <li>The key-value pairs are grouped and sorted according to the keys, list(K2, V2) -&gt; (K2, list(V2))</li> </ul> </li> <li>Reduce: <ul> <li>The reduce function (“reducer”) further summarizes its input: (K2, list(V2)) -&gt; list(K3, V3).</li> <li>The output of the reducer is then written as a separate file. One file per reducer.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-2-480.webp 480w,/assets/img/mapreduce-2-800.webp 800w,/assets/img/mapreduce-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>MapReduce in action (Ex: NCDC weather dataset) <ul> <li>Note: In Hadoop’s TextInputFormat, each line of a file is given a key representing its byte offset from the start of the entire file, not just the split. Offsets are used instead of line numbers because files are split and processed in parallel across multiple mappers, and byte offsets allow each mapper to locate its data efficiently without reading previous lines, ensuring scalability and consistency.</li> <li>Note: Line numbers can’t be used in Hadoop because the input files are split and processed in parallel by multiple mappers. Each mapper starts reading from a different byte position in the file, so it has no way to know how many lines came before its split without scanning the entire file sequentially. Byte offsets, on the other hand, can be determined directly from the file’s position on disk, making them independent, efficient, and uniquely identifiable across splits — perfect for distributed processing.</li> <li>The map function extracts the year and the air temperature, and emits them as its output.</li> <li>The output from the map function is processed by the MapReduce framework before being sent to the reduce function. This processing sorts and groups the key-value pairs by key.</li> <li>For each input, the reduce function iterates through the list and picks up the maximum reading ￼</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-3-480.webp 480w,/assets/img/mapreduce-3-800.webp 800w,/assets/img/mapreduce-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-4-480.webp 480w,/assets/img/mapreduce-4-800.webp 800w,/assets/img/mapreduce-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ ￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-5-480.webp 480w,/assets/img/mapreduce-5-800.webp 800w,/assets/img/mapreduce-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-6-480.webp 480w,/assets/img/mapreduce-6-800.webp 800w,/assets/img/mapreduce-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ ￼</p> <ul> <li>Data flow <ul> <li>Where to run the map task?</li> <li>Data locality: run the map task on a node where the input data resides in HDFS, because it doesn’t use valuable cluster bandwidth.</li> <li>If not possible, the job scheduler will look for a free map slot on a node in the same rack as one of the blocks, and the required data block is transferred over the rack’s local network.</li> <li>If still not possible, an oﬀ-rack node is used, which results in an inter-rack network transfer.</li> <li>So, what’s transferred is the actual HDFS block data required by the Map task to perform its computation. ￼</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-7-480.webp 480w,/assets/img/mapreduce-7-800.webp 800w,/assets/img/mapreduce-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ </p> <ul> <li>How large is an input split? <ul> <li>Map tasks process input splits in parallel. So the processing is better load balanced when the splits are small.</li> <li>However, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time.</li> <li>For most jobs, a good split size tends to be the size of an HDFS block, which is 128 MB by default. It is the largest size of input that can be guaranteed to be stored on a single node.</li> </ul> </li> <li>Where to store the map output? <ul> <li>Map tasks write their output to the local disk, not to HDFS. Why? Map output is intermediate output. It’s processed by reduce tasks to produce the final output. Once the job is complete, the map output can be thrown away.</li> <li>Storing it in HDFS with replication would be overkill.</li> <li>What if the node running the map task fails before the map output has been consumed by the reduce task? If a node fails before the reduce phase reads its map output, Hadoop simply re-runs the failed map task on another node. Since the map output is intermediate and deterministic, it can be regenerated from the original input data stored safely in HDFS.</li> </ul> </li> <li>Why doesn’t Hadoop move the map task to another node that holds the same HDFS block instead of transferring the data? Why move data instead of computation? <ul> <li>A: Hadoop’s scheduler tries first to move computation (the map task) to where the data already resides — this is called data locality, and it’s the preferred option. However, if all nodes holding that block are busy (no free map slots), the scheduler can’t wait indefinitely because it would delay the job. In that case, it assigns the task to another available node and transfers the required data block over the network. So, Hadoop only moves data as a fallback when moving computation isn’t immediately possible, balancing performance and cluster utilization.</li> </ul> </li> <li>Do reduce tasks enjoy data locality?</li> <li>The input to each reduce task is normally the output from all mappers. ￼</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-8-480.webp 480w,/assets/img/mapreduce-8-800.webp 800w,/assets/img/mapreduce-8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-8" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Minimizing the data transferred between map and reduce tasks <ul> <li>The combiner function is an optimization that runs on the map output to reduce the amount of data transferred to the reducers by performing local aggregation. Hadoop does not provide a guarantee of how many times it will call it for a particular map output record. Calling the combiner function zero, one, or many times should produce the same output from the reducer.</li> <li>It works best for associative and commutative operations, where partial results can be safely combined.</li> <li>Which of the following data processing can benefit from a combiner function? <ul> <li>Count the number of occurrences. YES</li> <li>Find the maximum value. YES</li> <li>Find the average value. NO, since the combiner may process subsets differently, leading to incorrect results unless additional logic (like tracking sums and counts separately) is used.</li> <li>Filter values based on a predicate. YES</li> </ul> </li> </ul> </li> </ul> <h4 id="architecture"><strong>Architecture</strong></h4> <p><strong>YARN (Yet Another Resource Negotiator)</strong></p> <ul> <li>YARN is Hadoop’s cluster resource management system.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-9-480.webp 480w,/assets/img/mapreduce-9-800.webp 800w,/assets/img/mapreduce-9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-9" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼ ￼</p> <ul> <li>YARN provides its core services via two types of long-running daemons.</li> <li>Resource manager (only one in the cluster): manage the use of resources across the cluster.</li> <li>Node managers (on each node): launch and monitor containers. A container executes an application-specific process with a constrained set of resources (memory, CPU, …). ￼</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-10-480.webp 480w,/assets/img/mapreduce-10-800.webp 800w,/assets/img/mapreduce-10-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-10" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Running an application: A client contacts the resource manager and asks it to run an application master process. The resource manager finds a node manager that can launch the application master in a container. The application master may request more containers from the resource manager. The application master use them to run a distributed computation (e.g., MapReduce).</li> <li>Types of YARN scheduler: (FIFO, Capacity, FAIR) scheduler.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-11-480.webp 480w,/assets/img/mapreduce-11-800.webp 800w,/assets/img/mapreduce-11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-11" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <p><strong>Hadoop</strong></p> <ul> <li>Running a MapReduce job:</li> <li>Client: submit the MapReduce job.</li> <li>YARN resource manager: coordinate the allocation of compute resources in cluster.</li> <li>YARN node managers: launch and monitor the containers on machines in the cluster.</li> <li>MapReduce application master: coordinate the tasks running the MR job. The application master and the MapReduce tasks run in containers scheduled by the resource manager and managed by the node managers.</li> <li>HDFS: share job files between the other entities.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-12-480.webp 480w,/assets/img/mapreduce-12-800.webp 800w,/assets/img/mapreduce-12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-12" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <ul> <li>Progress &amp; status updates:</li> <li>A job and each of its tasks have a status.</li> <li>The state of the job or task (e.g., running, successfully completed, failed).</li> <li>The progress of maps and reduces.</li> <li>The values of the job’s counters.</li> <li>A status message or description.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-13-480.webp 480w,/assets/img/mapreduce-13-800.webp 800w,/assets/img/mapreduce-13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-13" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <ul> <li>Q: Does each node hold HDFS blocks and containers for task execution, managed by the NodeManager?</li> <li>A: Yes. Each node stores blocks as part of HDFS and can also run containers, which execute tasks (Map or Reduce) scheduled by YARN. The NodeManager on each node handles launching, monitoring, and reporting on these containers, while the ResourceManager coordinates cluster-wide resource allocation.</li> <li>Q: What does the YARN scheduler do? Does it schedule Map and Reduce tasks, and how is memory/CPU utilization handled?</li> <li>A: The YARN scheduler (FIFO, Capacity, or FAIR) manages the allocation of resources—CPU, memory, and containers—across the cluster. It decides which nodes get containers for tasks. While it schedules containers for Map and Reduce tasks indirectly via the ApplicationMaster, the actual memory and CPU usage is constrained per container on individual nodes as specified by the scheduler.</li> <li>Q: Does the ResourceManager use HDFS block information to improve data locality and distributed efficiency?</li> <li>A: Yes. The ResourceManager, through the ApplicationMaster, tries to schedule tasks on nodes that already hold the input HDFS blocks to exploit data locality, reducing network transfer and improving performance. If local nodes aren’t available, it may schedule tasks on the same rack or another node as a fallback.</li> <li>Q: What are job counters in Hadoop, and what do they mean?</li> <li>A: Counters are metrics collected during job execution. They track things like the number of bytes read/written, number of records processed, map/reduce task attempts, and custom user-defined counters. Counters provide insight into job progress, efficiency, and can help debug or optimize jobs.</li> <li>Q: What is a status message, and how is it used?</li> <li>A: A status message is a short description of the current state of a job or task (e.g., “Reading input”, “Merging outputs”, “Task failed”). It helps users and administrators monitor progress, understand failures, and debug issues during job execution.</li> <li>Gist: In Hadoop with YARN, each node stores HDFS blocks and also runs containers for executing tasks, with the NodeManager handling container lifecycle and reporting. The ResourceManager coordinates cluster-wide resource allocation, while the YARN scheduler (FIFO, Capacity, or FAIR) decides which nodes get containers, controlling CPU and memory usage per container. To maximize efficiency, tasks are ideally scheduled on nodes holding the relevant HDFS blocks, leveraging data locality; if unavailable, tasks may run on the same rack or another node. During execution, job counters track metrics like bytes read/written, records processed, and task attempts, providing insight for monitoring and optimization. Status messages report the current state of jobs or tasks, helping users and administrators monitor progress and debug issues.</li> </ul> <p><strong>Resilience</strong></p> <ul> <li>Where can a failure happen? It can happen in 4 places: A MapReduce task, the MapReduce application master, YARN node manager or the YARN resource manager.</li> <li>Task failure: <ul> <li>Possible causes: Mapper/reducer bug, JVM bug, Hanging tasks: timeout (default: 10 min) exceeded without a progress update.</li> <li>The application master will reschedule the task on another node manager.</li> <li>If a task fails too many times (default: 4), it will not be retried again, and the whole job will fail.</li> </ul> </li> <li>Application master failure: <ul> <li>An application master sends periodic heartbeats to the resource manager.</li> <li>In the event of application master failure, the resource manager will detect the failure and start a new instance of the master running in a new container (managed by a node manager).</li> <li>In the case of the MapReduce application master, it’ll use the job history to recover the state of the tasks that were already run by the (failed) application, so they don’t have to be rerun.</li> <li>If a MapReduce application master fails too many times (default: 2), it will not be retried again, and the whole job will fail.</li> <li>Q: Where is the MapReduce job history stored and retrieved if the application master fails?</li> <li>A: The job history is persisted in HDFS, not in the application master’s memory. When a new application master is started after a failure, it reads the job history from HDFS to recover the state of already completed or partially completed tasks, so they don’t have to be rerun.</li> </ul> </li> <li>Node manager failure: <ul> <li>The resource manager will notice a node manager that has stopped sending heartbeats (default: 10 min) and remove it from its pool of nodes to schedule containers on.</li> <li>Q: If a NodeManager fails, do we need to rerun completed Map tasks?</li> <li>Yes, because the intermediate results are only stored in the node’s disk and not in hdfs. So, we’ve to rerun, so as to collect all the intermediate results from all mappers before the reducer part.</li> <li>Q: If a NodeManager fails, do we need to rerun completed Reduce tasks?</li> <li>A: No. Completed Reduce tasks write their output to HDFS, which is replicated and durable. Only in-progress Reduce tasks on the failed node need to be rescheduled on another node.</li> </ul> </li> <li>Resource manager failure: <ul> <li>Failure of the resource manager is serious, because without it, neither jobs nor task containers can be launched.</li> <li>In the default configuration, the resource manager is a single point of failure, since in the (unlikely) event of machine failure, all running jobs fails, and can’t be recovered.</li> <li>For high availability (HA), we need to configure a standby resource manager.</li> <li>Q: In ResourceManager failure, do we need to store information about all running applications?A: Yes. To recover after a ResourceManager failure, information about all running applications—including job metadata and application master states—needs to be persisted(not in HDFS). In HA setups, a standby ResourceManager keeps this information in ZooKeeper or shared storage(NFS) to resume operations without losing job information.</li> <li>Q: In ResourceManager failure, do we need to store NodeManager information?A: No. It can be reconstructed back again from the NodeManager’s heartbeats.</li> <li>Q: In ResourceManager failure, what about task information?A: Task information (status, progress, container allocation) is primarily tracked by the ApplicationMaster. The ResourceManager coordinates resources but doesn’t store detailed task outputs. In HA setups, the standby ResourceManager works with running ApplicationMasters to continue scheduling containers and managing tasks without losing track of progress.</li> </ul> </li> <li>Shuffle and sort (Most important part) <ul> <li>MapReduce guarantees that the input to every reducer is sorted by key.</li> <li>Flow: In MapReduce, each map task processes an input split, which contains multiple records. The map function runs on each record and generates intermediate key-value pairs, which are initially buffered in memory rather than written to disk immediately. This in-memory buffering allows the system to perform local partitioning, which determines which reducer each key belongs to, as well as optional combining, which reduces the number of intermediate records by aggregating data locally before transfer. Additionally, the intermediate data is sorted in memory by key to facilitate efficient merging later. Once the buffer reaches a configured threshold (default 100 MB), it is spilled to disk as a temporary file. Large map outputs may generate multiple spill files, which are later merged using a merge sort into a single sorted file per partition. During the shuffle phase, reducers fetch the intermediate data from all mappers, ensuring that each reducer receives all data corresponding to its assigned partition. Because the map output is already sorted locally, reducers perform a multi-way merge rather than a full sort, which improves efficiency. The final merged data is then streamed directly to the reduce function without writing it back to disk, minimizing I/O. If the intermediate data is too large to fit in memory, MapReduce applies external sorting, reading chunks of data from disk, merging them in memory, and writing back to disk. After merging, the reducer processes the records using the reduce function and writes the final output to HDFS. Throughout this process, combiners may optionally reduce the volume of data transferred, merges are optimized with multi-way strategies, and copying of map outputs can begin even before all map tasks finish, though the reduce function only runs after all intermediate data for that partition has been fetched.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-14-480.webp 480w,/assets/img/mapreduce-14-800.webp 800w,/assets/img/mapreduce-14-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mapreduce-14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-14" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼￼</p> <p><strong>Speculative execution</strong></p> <ul> <li>The job execution time is sensitive to slow-running tasks (“stragglers”).</li> <li>Hadoop doesn’t try to diagnose and fix slow-running tasks; instead, it tries to detect when a task is running slower than expected and launches another equivalent task as a backup.</li> <li>The scheduler tracks the progress of all tasks of the same type (map and reduce) in a job, and only launches speculative duplicates for the small portion that are running significantly slower than the average.</li> <li>When a task completes successfully, any duplicate tasks that are running are killed since they are no longer needed.</li> <li>Q: Which part of Hadoop handles speculative execution?</li> <li>A: Speculative execution is managed by the Hadoop MapReduce framework itself, specifically by the ApplicationMaster in YARN (Hadoop 2+).</li> <li>Q: Is the YARN scheduler part of the ApplicationMaster, and how do they interact?</li> <li>A: No, the YARN scheduler is part of the ResourceManager and operates cluster-wide, deciding which nodes get containers based on available resources and scheduling policies (FIFO, Capacity, FAIR). It does not manage task-level details like progress or stragglers. The ApplicationMaster, on the other hand, is job-specific. It requests containers from the ResourceManager (which allocates them via the scheduler), monitors task progress, handles failures, and manages speculative execution. Essentially, the ApplicationMaster depends on the scheduler for container allocation, and once allocated, it schedules and manages tasks within those containers, coordinating retries, merges, and data flow. The scheduler handles resource allocation, while the ApplicationMaster handles job and task management.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4]]></summary></entry><entry><title type="html">Hadoop Distributed File System (HDFS)</title><link href="https://monishver11.github.io/blog/2025/big-data-3-hdfs/" rel="alternate" type="text/html" title="Hadoop Distributed File System (HDFS)"/><published>2025-12-15T22:35:00+00:00</published><updated>2025-12-15T22:35:00+00:00</updated><id>https://monishver11.github.io/blog/2025/big-data-3-hdfs</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/big-data-3-hdfs/"><![CDATA[<ul> <li>The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware.</li> <li>It provides an interface similar to a POSIX file system (files and directories), but with relaxed requirements.</li> <li>It scales up to 100+ PB of storage and thousands of servers, supporting close to a billion files and blocks.</li> <li>Small gist about POSIX: POSIX (Portable Operating System Interface) is a standard that defines how file systems and operating systems should behave, ensuring consistency across Unix-like systems such as Linux and macOS. In file systems, POSIX specifies that every write must be immediately visible to all readers, files can be modified at any position (supporting random writes), and operations like open, close, read, write, delete, and rename must appear atomic and consistent. It also enforces strict locking and consistency rules for concurrent access, ensuring multiple processes can safely read and write files simultaneously without data corruption.</li> </ul> <h4 id="assumptions-and-goals"><strong>Assumptions and goals:</strong></h4> <ul> <li>Commodity hardware: Hardware failure is the norm rather than the exception. An HDFS instance may consist of thousands of servers. Each component has a non-trivial probability of failure. As a result, some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS.</li> <li>Streaming data access: HDFS is designed more for batch processing (e.g., MapReduce) rather than interactive use by users. The emphasis is on high throughput of data access rather than low latency of data access. Therefore, some POSIX semantics has been relaxed to increase data throughput rates.</li> <li>Large datasets: A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. It should provide high aggregate data bandwidth. It should scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.</li> <li>Simple coherency model: HDFS applications need a write-once-read-many (WORM) access model for files. Files cannot be modified except for appends and truncates. This design avoids complex consistency and synchronization problems that arise from random writes or concurrent updates. By restricting how data can change, HDFS simplifies data coherency management across replicas and achieves higher throughput for large-scale, sequential data access. Ex: a MapReduce application or a web crawler application fits perfectly with this model.</li> <li>Moving computation is cheaper than moving data: A computation requested by an application is much more efficient if it is executed near the data it operates on, especially when the size of the dataset is huge. This minimizes network congestion and increases the overall throughput of the system. So, it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. HDFS provides interfaces for applications to move themselves closer to where the data is located.</li> <li> <p>Portability across heterogeneous hardware &amp; software platforms: HDFS has been designed to be easily portable from on platform to another. It is implemented as a user-level filesystem in Java. Because it’s written in Java, it runs anywhere a Java Virtual Machine (JVM) is available, such as Linux, Windows, or macOS. The JVM acts as a layer between the Java program and the underlying operating system, translating Java instructions into native instructions the OS can execute. And unlike traditional file systems built directly into the operating system kernel, HDFS is a user-level file system, meaning it operates as a regular application process instead of requiring kernel-level changes. This makes it easier to install, update, and move between environments. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.</p> </li> <li>A gist on how Java program is executed from code to instructions in device: When a Java program like HDFS is executed, the process starts with writing source code in .java files, which is then compiled by the Java compiler (javac) into bytecode stored in .class files. These class files, along with necessary resources and configuration, are often bundled into a JAR (Java ARchive) file for easy distribution and deployment. On the target machine, the Java Virtual Machine (JVM) loads the bytecode from the class or JAR files and interprets or Just-In-Time (JIT) compiles it into native machine instructions that the operating system and hardware can execute. This abstraction provided by the JVM allows the same Java program or JAR file to run on any device or OS with a JVM installed, making Java programs, including HDFS, portable across heterogeneous platforms.</li> </ul> <h4 id="architecture"><strong>Architecture</strong></h4> <ul> <li>HDFS has a master-workers architecture.</li> <li>One NameNode: Manages the file system metadata.</li> <li>Many DataNodes: Stores the actual data blocks</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-1-480.webp 480w,/assets/img/hdfs-1-800.webp 800w,/assets/img/hdfs-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The HDFS namespace is a hierarchy of files and dirs. It supports user quotas and access permissions.</li> <li>The file content is split into large blocks (typically 128MB). Large blocks can minimize seek time. A file smaller than a block does not occupy a full block’s worth of storage.</li> <li>Clarification: Disk seek time is the delay caused by moving the read/write head to the data location. By using large blocks, HDFS allows more data to be read sequentially once the head is positioned, reducing the number of seeks needed. Fewer, larger blocks overall improve throughput for reading large files, which is especially beneficial for batch processing like MapReduce.</li> <li>Benefits of blocks as primitive unit: <ul> <li>Support very large files, even larger than any single disk in the network.</li> <li>Simplify storage management and decouple metadata from blocks.</li> <li>Blocks can be replicated for fault tolerance and availability.</li> </ul> </li> <li>NameNode: <ul> <li>The NameNode manages the HDFS namespace.</li> <li>It maintains the file system tree and the metadata for all the files and directories. This information is persisted on disk. The NameNode loads the entire namespace image into memory at startup.</li> <li>The NameNode also knows where every block is located. This information is in memory only, not persisted on disk. It can be reconstructed from DataNodes when the system starts (via heartbeats).</li> <li>Without the NameNode, the file system cannot be used.</li> </ul> </li> <li>DataNode: <ul> <li>DataNodes are the workhorses of HDFS.</li> <li>Each block is independently replicated at multiple DataNodes. An application can specify the number of replicas of a file that should be maintained by HDFS. It is called the replication factor of that file (typically 3).</li> <li>DataNodes store and retrieve blocks when asked by clients or the NameNode.</li> <li>DataNodes sends heartbeats to the NameNode (typically every 3 seconds).</li> <li>DataNodes also report to the NameNode periodically with the lists of blocks they are storing (at startup and every hour).</li> </ul> </li> <li>HDFS client: <ul> <li>The HDFS client is a library that exports the HDFS file system interface.</li> <li>Applications access the file system using the HDFS client.</li> <li>The user application does not need to know that the filesystem metadata and storage are on different servers, and that blocks have multiple replicas.</li> <li>However, the block locations are exposed to the client, so that applications like MapReduce can schedule tasks to where the data are located.</li> </ul> </li> <li>Reading a file <ul> <li>First, the HDFS clients asks the NameNode for the list of DataNodes that host replicas of the blocks of the file.</li> <li>The list is sorted by the network topology distance from the client.</li> <li>Then the client contacts a DataNode directly and requests the transfer of the desired block.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-2-480.webp 480w,/assets/img/hdfs-2-800.webp 800w,/assets/img/hdfs-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Network topology <ul> <li>The distance between two nodes is the sum of their distances to their closest common ancestor.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-3-480.webp 480w,/assets/img/hdfs-3-800.webp 800w,/assets/img/hdfs-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>Writing a file <ul> <li>First, the HDFS client asks the NameNode to choose DataNodes to host replicas of the first block of the file.</li> <li>The client organizes a pipeline from node to node and sends the data.</li> <li>When the first block is filled, the client requests new DataNodes to be chosen to host replicas of the next block. A new pipeline is organized, and the client sends the further bytes of the file.</li> <li>Choice of DataNodes for each block is likely to be different.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-4-480.webp 480w,/assets/img/hdfs-4-800.webp 800w,/assets/img/hdfs-4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Block placement <ul> <li>Trade-oﬀ between minimizing the write cost, and maximizing data reliability, availability and aggregate read bandwidth. <ul> <li>#1: same node as the client.</li> <li>#2: diﬀerent rack from #1.</li> <li>#3: same rack as #2, but diﬀerent node.</li> <li>This default strategy gives a good balance among: <ul> <li>Reliability: blocks are stored on two racks.</li> <li>Write bandwidth: writes only have to traverse a single network switch.</li> <li>Read performance: choice of two racks to read from.</li> <li>Block distribution across the cluster: clients only write a single block on the local rack.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-5-480.webp 480w,/assets/img/hdfs-5-800.webp 800w,/assets/img/hdfs-5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>The single-writer, multiple-reader model <ul> <li>The HDFS client that opens a file for writing is granted a lease (lock) for the file; no other client can write to the file. The writer’s lease doesn’t prevent other clients from reading the file; a file may have many concurrent readers.</li> <li>The writing client periodically renews the lease by sending a heartbeat to the NameNode.</li> <li>When the file is closed, the lease is revoked.</li> </ul> </li> <li>Coherency model <ul> <li>A coherency model for a filesystem describes the data visibility of reads and writes for a file.</li> <li>HDFS trades off some POSIX semantics for performance.</li> <li>After creating a file, it is visible in the filesystem namespace. However, the last block’s content may not be visible until the file is closed.</li> <li>If the client needs the visibility guarantee, it can call hflush() explicitly. It guarantees that the data written up to that point in the file has reached all the DataNodes in the write pipeline and is visible to all new readers. However, the data may be in the DataNodes’ memory only.</li> <li>To guarantee that the DataNodes have written the data to disk, call hsync().</li> <li>So, in short: HDFS’s coherency model is tied to its block structure - a file is visible immediately, but the last block may be partially written. Visibility and durability of that block are controlled explicitly via hflush() (memory) and hsync() (disk).</li> </ul> </li> </ul> <h4 id="resilience"><strong>Resilience</strong></h4> <ul> <li>NameNode <ul> <li>NameNode persists checkpoint + journal on disk.</li> <li>Checkpoint: the file system tree and metadata at the specific point in time.</li> <li>Journal (“edit log”): All changes to HDFS since the last checkpoint.</li> <li>CheckpointNode periodically combines the existing checkpoint and journal into a new checkpoint and sends it back to NameNode, which truncates the journal.</li> <li>BackupNode maintains a read-only, synchronized namespace state of the NameNode without block locations.</li> </ul> </li> <li>DataNode <ul> <li>When writing a file, the client computes the checksum for each data block. DataNodes store the checksums locally in a metadata file.</li> <li>When reading a file, the client verifies the checksum. If a block is corrupted, the client notifies the NameNode and fetches another replica of the block.</li> <li>If a DataNode fails or a block is corrupted: <ul> <li>Data can be retrieved from another DataNode storing the block replica.</li> <li>The NameNode marks the replica as unavailable/corrupt.</li> <li>The NameNode schedules creation of new replicas on other DataNodes.</li> </ul> </li> </ul> </li> <li>A small gist about checksum: A checksum is a small value computed from a block of data to verify its integrity. It is generated by applying a hash function (e.g., CRC32) to a block of data. The resulting number changes if even a single bit of the data changes. In HDFS, when data is written, a checksum is generated and stored alongside it. During reads, the checksum is recalculated and compared with the stored value to detect any corruption. If they don’t match, HDFS identifies the block as damaged and retrieves a healthy replica from another DataNode. This ensures reliable and consistent data storage across the system.</li> </ul> <h4 id="optimizations"><strong>Optimizations</strong></h4> <ul> <li>Block caching <ul> <li>Frequently accessed blocks may be explicitly cached in DataNode’s memory.</li> <li>By default, a block is cached in only “one” DataNode’s memory. This # of DataNode’s is configurable on a per-file basis.</li> <li>Users or applications tell the NameNode which files to cache, and for how long.</li> <li>Applications (e.g., MapReduce) can schedule tasks on the DataNode where a block is cached, for increased read performance.</li> <li>The NameNode maintains which DataNodes holds the cache of a specific block, along with other bookkeeping information.</li> </ul> </li> <li>HDFS federation <ul> <li>On a very large clusters with many files, the NameNode’s memory becomes the limiting factor for scaling. This is because the NameNode keeps a reference to every file and block in the filesystem in memory.</li> <li>HDFS federation allows a cluster to scale by adding NameNodes. Each NameNode manages a portion of the filesystem namespace(its own part of the directory tree).</li> <li>Under federation, each NameNode manages a namespace volume, which contains the metadata for the namespace.</li> <li>Namespace volumes are independent of each other. NameNodes do not communicate with one another. If one NameNode fails, the availability of the namespaces managed by other NameNodes will not be affected.</li> <li>Under federation, each NameNode also manages a block pool, which contains all the blocks for the files in the namespace.</li> <li>Block pool storage is not partitioned among DataNodes. DataNodes register with each NameNode in the cluster and store blocks from multiple block pools.</li> <li>The conceptual flow: In very large HDFS clusters, the single NameNode becomes a scalability bottleneck because it must keep metadata for every file and block in memory. HDFS Federation solves this by allowing multiple independent NameNodes, each managing its own namespace volume—a separate portion of the filesystem’s directory structure. Along with the namespace, each NameNode also manages a corresponding block pool, which contains all the physical data blocks belonging to files in that namespace. The DataNodes in the cluster are shared among all NameNodes and register with each of them, storing blocks from multiple block pools simultaneously. Note that the DataNode storage is shared physically, but logically partitioned into multiple block pools - one per NameNode. This design separates metadata management from physical storage, allowing the system to scale horizontally, isolate faults (a failure in one NameNode doesn’t affect others), and support multiple namespaces while still using the same underlying DataNodes for efficient shared storage.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-6-480.webp 480w,/assets/img/hdfs-6-800.webp 800w,/assets/img/hdfs-6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-7-480.webp 480w,/assets/img/hdfs-7-800.webp 800w,/assets/img/hdfs-7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/hdfs-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>HDFS High Availability (HA) <ul> <li>Although checkpoints of the NameNode protect against data loss, they don’t provide high availability of the filesystem.</li> <li>The NameNode is still a single point of failure (SPOF).</li> <li>If the NameNode fails or performs routine maintenance, the entire Hadoop ecosystem becomes out of service until a new NameNode is brought online. Manual intervention is required. On large clusters with many files and blocks, starting a new NameNode can take 30 minutes or more.</li> <li>With HDFS high availability (HA), there are a pair of NameNodes in an active-standby configuration.</li> <li>If the Active NameNode fails, the Standby NameNode takes over its duties to continue servicing client requests without a significant interruption (~1 minute).</li> <li>Even if the Standby NameNode is down when the Active NameNode fails, the sysadmin can still start the Standby NameNode from cold (same as non-HA).</li> <li>HDFS High Availability (HA) requires a few architectural changes: <ul> <li>The NameNodes must use highly available shared storage (e.g., NFS or the Quorum Journal Manager) to share the journal.</li> <li>DataNodes must send block reports to both NameNodes because the block mappings are stored in a NameNode’s memory, not on disk.</li> <li>Clients must be configured to handle NameNode failover transparently.</li> <li>Checkpoint/BackupNode’s role is subsumed by the Standby NameNode, which takes periodic checkpoints of the Active NameNode’s namespace.</li> </ul> </li> <li>The transition from the Active NameNode to the Standby NameNode is managed by a failover controller. By default, it uses ZooKeeper to ensure that only one NameNode is active.</li> <li>Each NameNode runs a lightweight failover controller process, which monitors its NameNode for failures (using a simple heartbeating mechanism) and triggers a failover should a NameNode fail.</li> <li>Failover may also be initiated manually (e.g., for routine maintenance). This is known as a graceful failover, since the failover controller arranges an orderly transition for both NameNodes to switch roles.</li> <li>However, in the case of an ungraceful failover, it’s impossible to be sure that the failed NameNode has stopped running.</li> <li>For example, a slow network or a network partition can trigger a failover transition, even though the previous Active NameNode is still running and thinks it’s still the Active NameNode.</li> <li>The HA implementation employs fencing methods to ensure that the previous Active NameNode is prevented from doing any damage or causing corruption.</li> <li>Have you heard of STONITH (shoot the other node in the head)?</li> <li>Q: How does each NameNode monitor failures in HDFS High Availability? A: Each NameNode runs a lightweight Failover Controller (ZKFC) that monitors the health of its own NameNode using a heartbeat mechanism. Both ZKFCs communicate with ZooKeeper, which coordinates and ensures that only one NameNode is Active at any time. If the Active NameNode fails, ZooKeeper triggers the Standby’s ZKFC to take over and become Active, ensuring continuous service.</li> <li>Q: What are fencing methods and why are they needed? A: Fencing is a safety mechanism used during failover to prevent the old Active NameNode from making changes after a Standby takes over. This is crucial because the old Active might still be running but disconnected (e.g., due to a network partition). Fencing isolates or disables the old node, commonly by killing its process, revoking access to shared storage, or even powering off the machine, ensuring that only one NameNode can write to the filesystem metadata, preventing corruption.</li> <li>Q: What is STONITH and how is it related? A: STONITH (Shoot The Other Node In The Head) is a type of fencing where the old node is forcibly shut down before the Standby takes over. In HDFS HA, it guarantees that the previous Active NameNode cannot interfere with the new Active, providing a strong safeguard against simultaneous writes and ensuring data consistency.</li> </ul> </li> <li>Balancer <ul> <li>Over time, the distribution of blocks across DataNodes can become unbalanced. An unbalanced cluster can aﬀect locality for applications (e.g., MapReduce), and it puts a greater strain on the highly utilized DataNodes.</li> <li>The balancer program is a daemon… <ul> <li>It redistributes blocks by moving them from overutilized DataNodes to underutilized DataNodes.</li> <li>It adheres to the block replica placement policy that makes data loss unlikely by placing block replicas on diﬀerent racks.</li> <li>It minimizes inter-rack data copying in the balancing process.</li> </ul> </li> <li>Q: What is the HDFS Balancer daemon and where does it run? A: The Balancer is a separate user-level daemon that runs on the Hadoop cluster, typically launched from a client node or NameNode host. It is not part of the NameNode or DataNode processes.</li> <li>Q: How does the Balancer work? A: It communicates with the NameNode to get cluster metadata, identifies overutilized and underutilized DataNodes, and moves blocks accordingly. It follows replica placement rules to avoid data loss and minimize inter-rack transfers.</li> <li>Q: What happens if there are failovers or crashes? A: If the Active NameNode fails, the Balancer reconnects to the new Active NameNode. If a DataNode fails, block moves to that node are paused or rescheduled. If the Balancer itself crashes, it can be restarted safely, resuming block moves without affecting cluster integrity.</li> <li>Q: If the Balancer runs outside HDFS, how does it access block information? A: The Balancer accesses block information through the HDFS client interface. It communicates with the NameNode using HDFS RPC APIs to retrieve metadata about all blocks, including which DataNodes store them and their disk utilization.</li> <li>Q: How does the Balancer move blocks between DataNodes? A: After obtaining metadata from the NameNode, the Balancer schedules block transfers between DataNodes. The actual data movement happens directly between DataNodes, while the Balancer only coordinates the process based on the metadata it received.</li> <li>Q: Why can the Balancer manage blocks even though it’s outside HDFS processes? A: Because it uses the official HDFS client APIs, the Balancer can read cluster metadata, monitor utilization, and orchestrate block redistribution without being part of the NameNode or DataNode processes.</li> </ul> </li> <li>Block scanner <ul> <li>Every DataNode runs a block scanner, which periodically verifies all the blocks stored on the DataNode.</li> <li>This allows bad blocks to be detected and fixed before they are read by clients.</li> <li>The scanner maintains a list of blocks to verify and scans them one by one for checksum errors.</li> <li>It also employs a throttling mechanism to preserve disk bandwidth on the DataNode. This throttling mechanism limits the speed or resource usage of the block scanner so it does not overwhelm the DataNode’s disk or network.</li> </ul> </li> </ul> <h4 id="usage"><strong>Usage</strong></h4> <ul> <li>The Hadoop FS shell provides commands that directly interact with HDFS.</li> <li>Ex: hadoop fs -command <args></args></li> </ul> <p><strong>Doubt: Is HDFS C+P or A+P under CAP theorem?</strong></p> <ul> <li>From the sources, I feel it falls under C+P. One of the answer/argument that makes sense is below.</li> <li>The cluster is consistent as long as the primary namenode is available. when a namenode becomes unavailable, the secondary namenode gets queried, however writes can become delayed / rejected. so despite not being an explicit “single point of failure”, it will still affect the “perceived availability and/or consistency of data” to external interfaces. At the end of the day, the primary namenode handles data consistency, and is synced to the secondary namenode (i.e. it does not require consensus). This is only “partially available” in practice.</li> </ul>]]></content><author><name></name></author><category term="RBDA-NYU"/><summary type="html"><![CDATA[Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3]]></summary></entry><entry><title type="html">Reading Notes from Aleksa Gordic’s GPU BlogPost</title><link href="https://monishver11.github.io/blog/2025/aleksagordic-gpu-blog-notes/" rel="alternate" type="text/html" title="Reading Notes from Aleksa Gordic’s GPU BlogPost"/><published>2025-12-15T13:28:00+00:00</published><updated>2025-12-15T13:28:00+00:00</updated><id>https://monishver11.github.io/blog/2025/aleksagordic-gpu-blog-notes</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/aleksagordic-gpu-blog-notes/"><![CDATA[<p><a href="https://www.aleksagordic.com/blog/matmul">https://www.aleksagordic.com/blog/matmul</a></p> <p><strong>Fundamentals of NVIDIA GPU architecture(on H100):</strong></p> <ul> <li>Tensor cores: wgmma instrutions needed to fully exploit it.</li> <li>CUDA cores: arithmetic instructions usually have a latency of ~4-15 cycles.</li> <li>(Tensor cores, CUDA cores, warp scheduler, LD/ST and register file(16k 32 bit)) x4 of these per SM, aka quadrants.</li> <li>TMA(Tensor memory accelerator): For small requests, TMA loads have higher latency than regular async copies (due to address generation overhead). It handles load/store transfers between GMEM and SMEM (with swizzling).</li> <li>1KiB of SMEM(Shared memory) goes for system use per block, so effectively we have: 228 - numblocks * 1kB kBs.</li> <li>Shared mem is faster than L1, as there is no need to tag stage comparisons for hit/miss.</li> <li>L1 cache line = 128B (=threads in warp fetching 4B floats)</li> <li>L1 <em>can</em> be used for register spill-over when register pressure is high</li> <li>Distributed shared memory(DSMEM): pooled shared memories (SMEM) of a physically close group of SMs (a GPC). Worse bandwidth/latency compared to shared mem, but better than L2.</li> <li>No. of SM’s: N=144(on the die), N=132(SXM5) and N=114(PCIe)</li> <li>L2: <ul> <li>We can set the granularity of the data fetch size to 32, 64 or 128B using cudaDeviceSetLimit.</li> <li>It is physically partitioned into two parts; each SM connects directly to only one partition and indirectly to the other through the crossbar.</li> <li>Residency control: we can set a part of L2 cache for persistent data accesses and map it to a chunk of GMEM</li> <li>It’s possible to redirect power from L2 to SMs (demonstrated in MLPerf 2024)</li> <li>L2 cache line = 128B, 4 sectors (sector==32B), same as L1</li> <li>Contains data compression circuitry and does global atomics</li> <li>60 MiB on the die (50MiB for SXM/PCIe)</li> <li>real read BW = 12-14 TB/s (near), far is significantly slower. Latency ~200 cycles.</li> </ul> </li> <li>GPC: Graphics processing clusters. Each GPC contains 18 SMs, so there are 8 GPCs on the GPU. Four GPCs connect directly to one L2 partition and the other four to the second partition.</li> <li>VRAM/device memory (80 GB, common form factor): <ul> <li>GMEM - 32, 64, 128B mem transactions granularity</li> <li>constant memory - very small ~64KiB</li> <li>local memory - 512 KiB/thread (register spill space)</li> </ul> </li> <li>To connect to other GPUs - nvlink v4. Bi-BW = 900 GB/s, Uni-BW = 450 GB/s (18 links with 25GB/s)</li> <li>To connect to x86 CPU, DPUs etc - PCIe Gen 5. Bi-BW = 128 GB/s and Uni-BW = 64 GB/s</li> <li> <p>Note: There are a few other smaller caches for instructions.</p> </li> <li>The memory system in a GPU is highly hierarchical, much like in CPU architectures.</li> <li>This hierarchy is dictated by physics and circuit design: SRAM cells are faster but larger (the control circuitry that enables their speed also increases their area), while DRAM cells are smaller/denser but slower. The result is that faster memory is lower capacity and expensive, while slower memory can be provided in much larger quantities.</li> <li>This trade-off between capacity and latency is exactly why cache hierarchies exist.</li> <li>Moving from device memory down to registers (levels 1-5), you see a clear trend: bandwidth increases by orders of magnitude, while both latency and capacity decrease by similar orders of magnitude.</li> <li>A few immediate implications follow: <ul> <li>Keep the most frequently accessed data as close as possible to the compute units.</li> <li>Minimize accesses to the lower levels of the hierarchy, especially device memory (GMEM).</li> </ul> </li> <li>One additional component worth noting is the Tensor Memory Accelerator (TMA), introduced with Hopper. TMA enables asynchronous data transfers between global memory and shared memory, as well as across shared memories within a cluster. It also supports swizzling to reduce bank conflicts.</li> </ul> <p>Compute:</p> <ul> <li>The fundamental unit is the streaming multiprocessor (SM). Hopper H100 (SXM5) integrates 132 SMs in total.</li> <li>SMs are grouped into graphics processing clusters (GPCs): each GPC contains 18 SMs, and there are 8 GPCs on the GPU. Four GPCs connect directly to one L2 partition, and the other four to the second partition.</li> <li>Tensor Cores: Specialized units that execute matrix multiplications on small tiles (e.g., 64x16 @ 16x256) at high throughput. Large matrix multiplications are decomposed into many such tile operations, so leveraging them effectively is critical for reaching peak performance.</li> <li>CUDA cores and SFUs: The so-called “CUDA cores” (marketing speech) execute standard floating-point operations such as FMA (fused multiply-add: c = a * b + c). Special Function Units (SFUs) handle transcendental functions such as sin, cos, exp, log, but also algebraic functions such as sqrt, rsqrt, etc.</li> <li>Load/Store (LD/ST) units: Circuits that service load and store instructions, complementary to the TMA engine.</li> <li>Warp schedulers: Each SM contains schedulers that issue instructions for groups of 32 threads (called warps in CUDA). A warp scheduler can issue one warp instruction per cycle.</li> <li>Each SM is physically divided into four quadrants, each housing a subset of the compute units described above.</li> <li>Parallelism vs Concurrency (Imp.) <ul> <li>An SM can issue instructions from at most four warps simultaneously (i.e., 128 threads in true parallel execution at a given cycle).</li> <li>However, an SM can host up to 2048 concurrent threads (64 warps). These warps are resident and scheduled in and out over time, allowing the hardware to hide memory/pipeline latency.</li> <li>In other words, instruction parallelism (how many threads start executing an instruction on a given cycle) is limited to 128 threads per SM at once (4 32-wide warp instructions), while concurrency (how many threads are tracked in the scheduler and eligible to run) extends to 2048 threads.</li> </ul> </li> <li>What is the ceiling—the maximum compute throughput of a GPU? This is often referred to as the “speed of light” (SoL) performance: the upper bound dictated by the physical characteristics of the chip.</li> <li>There are multiple ceilings depending on the data type. In LLM training workloads, bfloat16 (bf16) has been the dominant format in recent years, though fp8 and 4-bit formats are becoming increasingly important (for inference fp8 is fairly standard).</li> <li>The peak throughput is calculated as: <code class="language-plaintext highlighter-rouge">perf = freq_clk_max * num_tc * flop_per_tc_per_clk</code> or in words: maximum clock frequency × number of tensor cores × FLOPs per tensor core per cycle.</li> <li>The “speed of light” is not actually constant.</li> <li>In practice, the peak throughput depends on the actual clock frequency, which can vary under power or thermal throttling. If the GPU clock drops, so does the effective speed of light.</li> <li>Normally on H100 SXM the max clock freq is 1830 MHz =&gt; clock cycle takes ~0.55 ns</li> <li>But GPU might experience power throttling causing it to automatically drop the clock freq in order to reduce the transistor switching power.</li> </ul> <p>Doubts:</p> <ul> <li>In cache what is k-way set associative cache?</li> <li>What is transistor switching power and how its related to clock freq and power throttling?</li> </ul> <p>Further reading: Horace He went into this phenomenon in more depth in <a href="https://www.thonking.ai/p/strangely-matrix-multiplications">his blog post (3)</a>.</p> <p><strong>CUDA programming model</strong></p> <ul> <li>Thread has private registers synchronization: SMEM</li> </ul> <p>GPU assembly languages: PTX and SASS</p> <ul> <li>(D) In fig 18, is the blockIdx.x and blockIdx.y mentioned pictorially right? In fig 19, its mentioned as “warp 1 from block (blockIdx.x, blockIdx.y) = (1,0)”, with that as the case, then x goes from top to bottom right, but in the fig 18, x row of block dims span accross left to right.</li> <li>PTX code is generated for a thread block right? and will the PTX code shown executed per thread in the thread block in parallel?</li> <li>What is exposing ILP (in PTX code explanation)? - Instruction-Level Parallelism</li> <li> <p>How loop unrolling exposes Instruction-Level Parallelism? So, does these instructions, run in parallel, as the warp takes and executes them?</p> </li> <li>SASS was a bit harder to understand, may be it could be due to the fact that its not explained in detail. For now, just got the gist and proceeding.</li> </ul> <p>Designing near-SOTA synchronous matmul kernel</p> <ul> <li>(D) Loading A → As. This step is trickier because As is transposed. The reason for the transpose is that it enables vectorized loads (LDS.128) later during the compute phase.</li> <li>(D) The trade-off is that the stores cannot be vectorized: the 4 floats fetched from a row of A must now be scattered into a column of As, which maps into the same memory bank. That’s acceptable because we prioritize fast loads — each element of As will be accessed multiple times during computation, while the stores happen only once.</li> </ul>]]></content><author><name></name></author><category term="GPU"/><summary type="html"><![CDATA[Reading notes for my reference from Aleksa Gordic's GPU BlogPost]]></summary></entry><entry><title type="html">Ta Experience</title><link href="https://monishver11.github.io/blog/2025/ta-experience/" rel="alternate" type="text/html" title="Ta Experience"/><published>2025-12-15T00:00:00+00:00</published><updated>2025-12-15T00:00:00+00:00</updated><id>https://monishver11.github.io/blog/2025/ta-experience</id><content type="html" xml:base="https://monishver11.github.io/blog/2025/ta-experience/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry></feed>