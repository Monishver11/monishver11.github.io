<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SmallGraphGCN - Accelerating GNN Training on Batched Small Graphs | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Discover how fused-edge-centric CUDA kernels dramatically accelerate Graph Neural Network training on molecular datasets. By rethinking parallelism strategies for batched small graphs, our approach achieves up to 3.1× faster forward execution and 1.3× end-to-end training speedup over PyTorch Geometric."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/projects/4_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">SmallGraphGCN - Accelerating GNN Training on Batched Small Graphs</h1> <p class="post-description">Discover how fused-edge-centric CUDA kernels dramatically accelerate Graph Neural Network training on molecular datasets. By rethinking parallelism strategies for batched small graphs, our approach achieves up to 3.1× faster forward execution and 1.3× end-to-end training speedup over PyTorch Geometric.</p> </header> <article> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_4/gcn-1-480.webp 480w,/assets/img/project_4/gcn-1-800.webp 800w,/assets/img/project_4/gcn-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_4/gcn-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="smallgraphgcn-intro" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Graph Neural Networks (GNNs) are transforming molecular property prediction and drug discovery. But existing frameworks like PyTorch Geometric struggle with a critical workload: training on millions of small molecular graphs. We present SmallGraphGCN, a custom CUDA implementation that achieves up to 3.1× faster forward execution and 1.3× end-to-end training speedup by fundamentally rethinking how we parallelize GNN computations for small graphs.</p> <p>This project was developed as part of the <strong>GPU Architecture and Programming</strong> course during the fall 2025 semester in the <strong>MSCS program at NYU Courant</strong>. Our team — <strong>Muhammed Abdur Rahman</strong> and myself.</p> <hr> <h5 id="introduction-and-motivation"><strong>Introduction and Motivation</strong></h5> <p>Graph Neural Networks have become the state-of-the-art approach for learning on molecular data, with applications ranging from predicting drug toxicity to discovering new materials. However, molecular datasets present a unique computational challenge that existing GNN frameworks aren’t optimized for: processing hundreds of thousands of small graphs, each with only 10-50 nodes.</p> <p>Unlike social networks or knowledge graphs that contain millions of nodes in a single massive graph, molecular datasets like QM9 (134,000 molecules) and ZINC (250,000 molecules) consist of many tiny graphs processed in large batches. This workload exposes a fundamental mismatch between how GNN frameworks are designed and how molecular machine learning actually works.</p> <h5 id="the-kernel-launch-bottleneck"><strong>The Kernel Launch Bottleneck</strong></h5> <p>The problem becomes clear when we profile PyTorch Geometric (PyG) on a typical molecular training run. For an 8-layer GCN processing a batch of 4,096 molecules, PyG launches over 32,000 CUDA kernels. Many of these kernels execute for less than a millisecond—but each launch incurs fixed overhead for parameter setup, memory transfers, and GPU scheduling.</p> <p>For large graphs with millions of nodes, this overhead is negligible compared to the actual computation time. But for molecular graphs with 20 nodes, the kernel launch overhead becomes the dominant cost. The GPU spends more time preparing to compute than actually computing.</p> <p>This isn’t just a theoretical concern. In pharmaceutical screening pipelines, researchers train hundreds of GNN models across different molecular datasets, architectures, and hyperparameters. A 2-3× training speedup translates directly to weeks of saved computational time and faster iteration cycles in drug discovery.</p> <h5 id="our-approach-edge-centric-parallelism"><strong>Our Approach: Edge-Centric Parallelism</strong></h5> <p>Our key insight is that small molecular graphs don’t need the full generality of sparse tensor operations. Instead, they benefit from a simpler execution model: <strong>directly parallelize over edges and features</strong>.</p> <p>Rather than following PyG’s message-passing abstraction with separate scatter, gather, and aggregate operations, we fuse all graph operations into just two CUDA kernels per GCN layer:</p> <ol> <li> <strong>Fused Aggregation Kernel</strong>: Combines message construction, edge weighting, and neighborhood aggregation in a single pass</li> <li> <strong>Fused Linear Kernel</strong>: Performs dense transformation, bias addition, and ReLU activation together</li> </ol> <p>Each CUDA thread processes a single <code class="language-plaintext highlighter-rouge">(edge, feature)</code> pair, creating massive uniform parallelism that keeps GPUs fully occupied even on tiny graphs.</p> <h5 id="impact-and-applications"><strong>Impact and Applications</strong></h5> <p>This optimization strategy has broad implications beyond molecular property prediction:</p> <ul> <li> <strong>Drug Discovery</strong>: Faster training accelerates screening of millions of candidate molecules</li> <li> <strong>Materials Science</strong>: Enables rapid iteration on crystal structure prediction models</li> <li> <strong>Protein Engineering</strong>: Speeds up training on protein fragment graphs</li> <li> <strong>Chemical Reaction Prediction</strong>: Improves throughput for reaction outcome models</li> </ul> <p>The fundamental principle—that workload-specific kernel design can dramatically outperform general-purpose frameworks—applies anywhere small graphs appear in large batches.</p> <div class="row justify-content-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_4/gcn-2-480.webp 480w,/assets/img/project_4/gcn-2-800.webp 800w,/assets/img/project_4/gcn-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_4/gcn-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kernel-launch-comparison" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Comparison of kernel launches: PyG vs SmallGraphGCN for an 8-layer GCN. Our fused approach reduces kernel launches by 68%. </div> <hr> <h4 id="background-gcns-and-the-small-graph-problem"><strong>Background: GCNs and the Small Graph Problem</strong></h4> <h5 id="graph-convolutional-networks"><strong>Graph Convolutional Networks</strong></h5> <p>Graph Convolutional Networks (Kipf &amp; Welling, 2016) learn node representations by iteratively aggregating features from neighboring nodes. The core GCN layer computes:</p> \[H^{(l+1)} = \sigma\left(D^{-\frac{1}{2}}\hat{A}D^{-\frac{1}{2}}H^{(l)}W^{(l)}\right)\] <p>where:</p> <ul> <li>\(\hat{A} = A + I\) adds self-loops to the adjacency matrix</li> <li>\(D\) is the degree matrix</li> <li>\(W^{(l)}\) is a learnable weight matrix</li> <li>\(\sigma\) is a nonlinearity (typically ReLU)</li> </ul> <p>The symmetric normalization \(D^{-\frac{1}{2}}\hat{A}D^{-\frac{1}{2}}\) ensures that aggregated features have similar magnitudes regardless of node degree.</p> <h5 id="pytorch-geometrics-design-philosophy"><strong>PyTorch Geometric’s Design Philosophy</strong></h5> <p>PyTorch Geometric implements GCNs through a flexible message-passing interface that works across diverse graph types:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MessagePassing</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">):</span>
        <span class="c1"># Step 1: Create messages
</span>        <span class="n">messages</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">message</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">x</span><span class="p">[</span><span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        
        <span class="c1"># Step 2: Aggregate messages
</span>        <span class="n">aggregated</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">aggregate</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="c1"># Step 3: Update node features
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">aggregated</span><span class="p">)</span>
</code></pre></div></div> <p>This abstraction is powerful for rapid prototyping and supports heterogeneous graphs, dynamic batching, and various aggregation schemes. However, each step typically launches separate CUDA kernels:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">message()</code> creates edge features</li> <li> <code class="language-plaintext highlighter-rouge">aggregate()</code> uses scatter operations to sum messages</li> <li> <code class="language-plaintext highlighter-rouge">update()</code> applies transformations</li> </ul> <p>For molecular graphs, this creates excessive overhead.</p> <h5 id="profiling-the-bottleneck"><strong>Profiling the Bottleneck</strong></h5> <p>We instrumented PyG with NVIDIA Nsight Systems to quantify the overhead. For a representative configuration (QM9 dataset, batch size 4,096, 8 layers, 32 hidden dimensions), we found:</p> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>PyG</strong></th> <th><strong>Impact</strong></th> </tr> </thead> <tbody> <tr> <td>Total kernels launched</td> <td>32,670</td> <td>Baseline</td> </tr> <tr> <td>Kernel execution time</td> <td>203.85 ms</td> <td>Actual computation</td> </tr> <tr> <td>Memory transfer time</td> <td>1.04 ms</td> <td>CPU-GPU transfers</td> </tr> <tr> <td>Average kernel duration</td> <td>6.2 μs</td> <td>Launch overhead dominates</td> </tr> </tbody> </table> <p><strong>Key observation</strong>: The average kernel executes for only 6.2 microseconds, but kernel launch overhead is typically 5-10 microseconds. For small graphs, we’re spending nearly equal time launching kernels as executing them.</p> <p>Additionally, PyG’s scatter operations require auxiliary kernels for:</p> <ul> <li>Index manipulation and sorting</li> <li>Parallel prefix sums (CUB’s <code class="language-plaintext highlighter-rouge">DeviceSelectSweepKernel</code>)</li> <li>Buffer initialization (<code class="language-plaintext highlighter-rouge">DeviceCompactInitKernel</code>)</li> </ul> <p>These support operations, necessary for handling arbitrary sparse graphs, add overhead that’s unnecessary for our fixed small-graph workload.</p> <h5 id="the-opportunity"><strong>The Opportunity</strong></h5> <p>Our profiling revealed three optimization opportunities:</p> <ol> <li> <strong>Kernel Fusion</strong>: Combine message-passing steps into single kernels</li> <li> <strong>Direct Edge Processing</strong>: Eliminate scatter-gather overhead by iterating edges directly</li> <li> <strong>Static Memory Layout</strong>: Pre-allocate buffers instead of dynamic allocation</li> </ol> <p>These principles guided our SmallGraphGCN design.</p> <hr> <h4 id="system-design-edge-centric-execution"><strong>System Design: Edge-Centric Execution</strong></h4> <h5 id="design-principles"><strong>Design Principles</strong></h5> <p>SmallGraphGCN is built on three core principles that directly address the bottlenecks in PyG:</p> <p><strong>Principle 1: Edge-Feature Parallelism</strong></p> <p>We parallelize over <code class="language-plaintext highlighter-rouge">(edge, feature)</code> pairs rather than nodes or edges alone. Each CUDA thread handles one scalar computation—one feature dimension for one edge. This creates a uniform, easily schedulable workload that achieves high GPU occupancy even with tiny graphs.</p> <p><strong>Principle 2: Kernel Fusion with Minimal Launches</strong></p> <p>Each GCN layer uses exactly two kernels:</p> <ol> <li>Aggregation kernel (fuses message construction, normalization, and accumulation)</li> <li>Linear transformation kernel (fuses matrix multiplication, bias, and ReLU)</li> </ol> <p>This replaces PyG’s 5-7 kernel launches per layer with just 2, drastically reducing launch overhead.</p> <p><strong>Principle 3: Static-Stride Ping-Pong Buffers</strong></p> <p>All feature tensors use a fixed stride determined by <code class="language-plaintext highlighter-rouge">MAX_FEATURES = 128</code>. Layers with smaller dimensions reuse this capacity rather than triggering new allocations. We alternate between two pre-allocated buffers across layers, eliminating in-place update hazards and memory management overhead.</p> <h5 id="mathematical-formulation"><strong>Mathematical Formulation</strong></h5> <p>Rather than forming the normalized adjacency matrix explicitly, we operate directly on the edge list \(E = \{(s_e, d_e)\}_{e=1}^{E}\) with precomputed symmetric normalization weights \(\{w_e\}_{e=1}^{E}\):</p> \[(\hat{A}H^{(l)})_{d,f} = \sum_{e: d_e = d} w_e H^{(l)}_{s_e, f}\] <p>where:</p> <ul> <li>\(s_e\) is the source node of edge \(e\)</li> <li>\(d_e\) is the destination node</li> <li>\(w_e = \frac{1}{\sqrt{\deg(s_e) \cdot \deg(d_e)}}\) is the normalization weight</li> </ul> <p>This formulation is highly parallel across edges and features, enabling efficient GPU execution.</p> <h5 id="kernel-implementations"><strong>Kernel Implementations</strong></h5> <h5 id="1-edge-centric-aggregation-kernel"><strong>1. Edge-Centric Aggregation Kernel</strong></h5> <p>The aggregation kernel (<code class="language-plaintext highlighter-rouge">gcn_aggregate_edge_centric</code>) fuses message passing, edge weighting, and neighborhood accumulation:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">__global__</span> <span class="kt">void</span> <span class="nf">gcn_aggregate_edge_centric</span><span class="p">(</span>
    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">x_input</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">int64_t</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">edge_src</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">int64_t</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">edge_dst</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">edge_weight</span><span class="p">,</span>
    <span class="kt">float</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">aggregated</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">total_edges</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">num_features</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">input_stride</span>
<span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Total work = total number of edge-feature pairs</span>
    <span class="kt">int</span> <span class="n">total_work</span> <span class="o">=</span> <span class="n">total_edges</span> <span class="o">*</span> <span class="n">num_features</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
         <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">total_work</span><span class="p">;</span> 
         <span class="n">idx</span> <span class="o">+=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
        
        <span class="kt">int</span> <span class="n">edge_idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">/</span> <span class="n">num_features</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">feat_idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">%</span> <span class="n">num_features</span><span class="p">;</span>
        
        <span class="kt">int</span> <span class="n">src</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">edge_src</span><span class="p">[</span><span class="n">edge_idx</span><span class="p">]);</span>
        <span class="kt">int</span> <span class="n">dst</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">edge_dst</span><span class="p">[</span><span class="n">edge_idx</span><span class="p">]);</span>
        <span class="kt">float</span> <span class="n">ew</span> <span class="o">=</span> <span class="n">edge_weight</span><span class="p">[</span><span class="n">edge_idx</span><span class="p">];</span>
        
        <span class="c1">// Read source feature</span>
        <span class="kt">float</span> <span class="n">src_feat</span> <span class="o">=</span> <span class="n">x_input</span><span class="p">[</span><span class="n">src</span> <span class="o">*</span> <span class="n">input_stride</span> <span class="o">+</span> <span class="n">feat_idx</span><span class="p">];</span>
        <span class="kt">float</span> <span class="n">weighted_feat</span> <span class="o">=</span> <span class="n">ew</span> <span class="o">*</span> <span class="n">src_feat</span><span class="p">;</span>
        
        <span class="c1">// Atomic add to destination (handles multiple edges to same node)</span>
        <span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">aggregated</span><span class="p">[</span><span class="n">dst</span> <span class="o">*</span> <span class="n">num_features</span> <span class="o">+</span> <span class="n">feat_idx</span><span class="p">],</span> <span class="n">weighted_feat</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Key design decisions:</strong></p> <ul> <li> <strong>Thread Mapping</strong>: <code class="language-plaintext highlighter-rouge">blockIdx.x</code> indexes edges, <code class="language-plaintext highlighter-rouge">threadIdx.x</code> indexes features</li> <li> <strong>Memory Access</strong>: Coalesced reads from input features</li> <li> <strong>Atomic Operations</strong>: Handle multiple edges targeting the same node</li> <li> <strong>Normalization</strong>: Edge weights precomputed on CPU to reduce kernel complexity</li> </ul> <p><strong>Atomic Contention Analysis</strong>: For molecular graphs with average degree 2-3, atomic contention is minimal. Most destination nodes receive messages from only 2-3 edges, so the GPU can efficiently serialize these operations. Our profiling showed atomic operations account for &lt; 15% of kernel time.</p> <h5 id="2-fused-linear-transformation-kernel"><strong>2. Fused Linear Transformation Kernel</strong></h5> <p>The linear kernel (<code class="language-plaintext highlighter-rouge">gcn_linear_transform_warp</code>) performs dense matrix multiplication with ReLU activation:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">__global__</span> <span class="kt">void</span> <span class="nf">gcn_linear_transform_warp</span><span class="p">(</span>
    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">aggregated</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">W</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">b</span><span class="p">,</span>
    <span class="kt">float</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">output</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">total_nodes</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">in_features</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">out_features</span><span class="p">,</span>
    <span class="kt">bool</span> <span class="n">apply_relu</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">output_stride</span>
<span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">total_work</span> <span class="o">=</span> <span class="n">total_nodes</span> <span class="o">*</span> <span class="n">out_features</span><span class="p">;</span>
    
    <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">total_work</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
    
    <span class="kt">int</span> <span class="n">node_idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">/</span> <span class="n">out_features</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">out_f</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">%</span> <span class="n">out_features</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">out_f</span><span class="p">];</span>
    
    <span class="c1">// Dot product: aggregated[node_idx, :] · W[:, out_f]</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">in_f</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">in_f</span> <span class="o">&lt;</span> <span class="n">in_features</span><span class="p">;</span> <span class="n">in_f</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span> <span class="n">agg</span> <span class="o">=</span> <span class="n">aggregated</span><span class="p">[</span><span class="n">node_idx</span> <span class="o">*</span> <span class="n">in_features</span> <span class="o">+</span> <span class="n">in_f</span><span class="p">];</span>
        <span class="kt">float</span> <span class="n">w</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">in_f</span> <span class="o">*</span> <span class="n">out_features</span> <span class="o">+</span> <span class="n">out_f</span><span class="p">];</span>
        <span class="n">result</span> <span class="o">+=</span> <span class="n">agg</span> <span class="o">*</span> <span class="n">w</span><span class="p">;</span>
    <span class="p">}</span>
    
    <span class="c1">// Apply ReLU if not final layer</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">apply_relu</span> <span class="o">&amp;&amp;</span> <span class="n">result</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
    
    <span class="n">output</span><span class="p">[</span><span class="n">node_idx</span> <span class="o">*</span> <span class="n">output_stride</span> <span class="o">+</span> <span class="n">out_f</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Optimization considerations:</strong></p> <ul> <li> <strong>Warp-Level Parallelism</strong>: Threads in the same warp compute different output features for the same node</li> <li> <strong>Register Usage</strong>: The inner loop accumulates in registers before writing to global memory</li> <li> <strong>Activation Fusion</strong>: ReLU is fused with the matrix multiplication</li> <li> <strong>Memory Coalescing</strong>: Consecutive threads access consecutive elements in the weight matrix</li> </ul> <h5 id="3-backward-pass-reverse-aggregation"><strong>3. Backward Pass: Reverse Aggregation</strong></h5> <p>For backpropagation, we implement <code class="language-plaintext highlighter-rouge">gcn_reverse_aggregate_kernel</code> that computes gradients with respect to input features:</p> \[\nabla_X L = D^{-\frac{1}{2}}\hat{A}^TD^{-\frac{1}{2}}\nabla_{\hat{A}H}L\] <p>The symmetric normalization means edge weights remain unchanged—we simply swap source and destination:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">__global__</span> <span class="kt">void</span> <span class="nf">gcn_reverse_aggregate_kernel</span><span class="p">(</span>
    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">grad_aggregated</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">int64_t</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">edge_src</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">int64_t</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">edge_dst</span><span class="p">,</span>
    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">edge_weight</span><span class="p">,</span>
    <span class="kt">float</span><span class="o">*</span> <span class="k">__restrict__</span> <span class="n">grad_input</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">total_edges</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">num_features</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">input_stride</span><span class="p">,</span>
    <span class="kt">int</span> <span class="n">output_stride</span>
<span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">total_work</span> <span class="o">=</span> <span class="n">total_edges</span> <span class="o">*</span> <span class="n">num_features</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
         <span class="n">idx</span> <span class="o">&lt;</span> <span class="n">total_work</span><span class="p">;</span> 
         <span class="n">idx</span> <span class="o">+=</span> <span class="n">gridDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">edge_idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">/</span> <span class="n">num_features</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">feat_idx</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">%</span> <span class="n">num_features</span><span class="p">;</span>
        
        <span class="c1">// Reverse direction: dst becomes source, src becomes destination</span>
        <span class="kt">int</span> <span class="n">src</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">edge_dst</span><span class="p">[</span><span class="n">edge_idx</span><span class="p">]);</span>
        <span class="kt">int</span> <span class="n">dst</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">edge_src</span><span class="p">[</span><span class="n">edge_idx</span><span class="p">]);</span>
        <span class="kt">float</span> <span class="n">ew</span> <span class="o">=</span> <span class="n">edge_weight</span><span class="p">[</span><span class="n">edge_idx</span><span class="p">];</span>
        
        <span class="kt">float</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">grad_aggregated</span><span class="p">[</span><span class="n">src</span> <span class="o">*</span> <span class="n">input_stride</span> <span class="o">+</span> <span class="n">feat_idx</span><span class="p">];</span>
        <span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">grad_input</span><span class="p">[</span><span class="n">dst</span> <span class="o">*</span> <span class="n">output_stride</span> <span class="o">+</span> <span class="n">feat_idx</span><span class="p">],</span> <span class="n">ew</span> <span class="o">*</span> <span class="n">grad</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Current Limitation</strong>: This implementation uses a naive global-atomic strategy. All threads updating the same node’s gradient serialize their writes, creating contention. We discuss optimization opportunities in the Analysis section.</p> <h5 id="pytorch-integration"><strong>PyTorch Integration</strong></h5> <p>We integrate with PyTorch’s autograd system through a custom <code class="language-plaintext highlighter-rouge">torch.autograd.Function</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SmallGraphGCNFunction</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_weight</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">):</span>
        <span class="c1"># Launch CUDA kernels for all layers
</span>        <span class="n">output</span> <span class="o">=</span> <span class="nf">cuda_forward_pass</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_weight</span><span class="p">,</span> 
                                   <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
        
        <span class="c1"># Save tensors for backward pass
</span>        <span class="n">ctx</span><span class="p">.</span><span class="nf">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_weight</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="n">ctx</span><span class="p">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        
        <span class="k">return</span> <span class="n">output</span>
    
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_weight</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">ctx</span><span class="p">.</span><span class="n">saved_tensors</span>
        
        <span class="c1"># Compute gradients using reverse aggregation
</span>        <span class="n">grad_x</span><span class="p">,</span> <span class="n">grad_weights</span><span class="p">,</span> <span class="n">grad_biases</span> <span class="o">=</span> <span class="nf">cuda_backward_pass</span><span class="p">(</span>
            <span class="n">grad_output</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">edge_index</span><span class="p">,</span> <span class="n">edge_weight</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">ctx</span><span class="p">.</span><span class="n">num_layers</span>
        <span class="p">)</span>
        
        <span class="k">return</span> <span class="n">grad_x</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">grad_weights</span><span class="p">,</span> <span class="n">grad_biases</span><span class="p">,</span> <span class="bp">None</span>
</code></pre></div></div> <p>This provides seamless integration with PyTorch’s automatic differentiation while leveraging our custom kernels.</p> <h5 id="batched-data-layout"><strong>Batched Data Layout</strong></h5> <p>Multiple graphs are represented as a single disconnected graph, consistent with PyG’s batching strategy:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example: Batching 3 graphs
# Graph 1: nodes [0, 1], edges [(0,1)]
# Graph 2: nodes [2, 3, 4], edges [(2,3), (3,4)]  
# Graph 3: nodes [5, 6], edges [(5,6)]
</span>
<span class="n">batched_nodes</span> <span class="o">=</span> <span class="mi">7</span>  <span class="c1"># Total nodes
</span><span class="n">batched_edges</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">)]</span>
<span class="n">batch_assignment</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>  <span class="c1"># Which graph each node belongs to
</span></code></pre></div></div> <p>Our kernels operate on this flat representation without graph-specific branching, processing the entire batch in one pass. A separate <code class="language-plaintext highlighter-rouge">batch</code> tensor tracks graph membership for pooling operations.</p> <hr> <h4 id="experimental-setup"><strong>Experimental Setup</strong></h4> <h5 id="datasets"><strong>Datasets</strong></h5> <p>We evaluated SmallGraphGCN on two standard molecular property prediction benchmarks:</p> <p><strong>QM9 Dataset</strong></p> <ul> <li>134,000 organic molecules with up to 9 heavy atoms (C, N, O, F)</li> <li>11-dimensional node features encoding atom type, hybridization, aromaticity</li> <li>Average of 18 nodes per graph</li> <li>Used for predicting quantum mechanical properties</li> </ul> <p><strong>ZINC Dataset</strong></p> <ul> <li>250,000 drug-like molecules</li> <li>28-dimensional one-hot node features for atom types</li> <li>Average of 23 nodes per graph</li> <li>Commonly used for molecular generation benchmarks</li> </ul> <p>Both datasets represent the small-graph regime where our optimizations should be most effective.</p> <h5 id="model-architecture"><strong>Model Architecture</strong></h5> <p>We used a standard GCN architecture for molecular property prediction:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → GCN Layer 1 → ... → GCN Layer L → Global Mean Pool → Output
</code></pre></div></div> <p><strong>Configuration space</strong>:</p> <ul> <li>Number of layers: \(L \in \{4, 8, 12, 16, 20, 24, 28, 32\}\)</li> <li>Hidden dimension: \(H = 32\)</li> <li>Batch sizes: \(B \in \{4096, 8192, 16384\}\)</li> </ul> <p>The large batch sizes ensure high GPU utilization for both PyG and SmallGraphGCN, making the comparison fair. We verified that GPU memory utilization exceeded 75% for all configurations.</p> <h5 id="hardware-configuration"><strong>Hardware Configuration</strong></h5> <p><strong>Primary Testing</strong>:</p> <ul> <li>NYU CIMS cuda5 cluster nodes</li> <li>NVIDIA GeForce RTX 4070 GPU (12GB VRAM)</li> <li>CUDA 12.1, PyTorch 2.5.1</li> </ul> <p><strong>Verification Testing</strong>:</p> <ul> <li>Vast.ai dedicated instance</li> <li>NVIDIA GeFororce RTX 3060 GPU (12GB VRAM)</li> <li>Same software configuration</li> </ul> <p>We replicated all experiments on both platforms to ensure results weren’t artifacts of cluster contention or specific hardware quirks. The speedup factors remained consistent across both setups.</p> <h5 id="benchmarking-methodology"><strong>Benchmarking Methodology</strong></h5> <p>To ensure fair and reproducible comparisons:</p> <p><strong>Warmup Phase</strong>: 20 iterations to stabilize GPU clocks and populate caches</p> <p><strong>Timing Methodology</strong>:</p> <ul> <li>Forward pass time (including graph construction and data movement)</li> <li>Backward pass time (including gradient computation)</li> <li>Total training time (complete train loop with optimizer step)</li> </ul> <p><strong>Measurement Protocol</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Warmup
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="nf">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="c1"># Timed measurement  
</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
</code></pre></div></div> <p>Each configuration was measured 3 times, and we report the median to reduce variance from system noise.</p> <h5 id="evaluation-metrics"><strong>Evaluation Metrics</strong></h5> <p><strong>Primary Metrics</strong>:</p> <ol> <li> <strong>Forward Pass Speedup</strong>: \(\text{Time}_{\text{PyG}} / \text{Time}_{\text{SmallGraphGCN}}\)</li> <li> <strong>Backward Pass Speedup</strong>: Same ratio for gradient computation</li> <li> <strong>End-to-End Training Speedup</strong>: Total training time ratio</li> </ol> <p><strong>Profiling Metrics</strong> (via NVIDIA Nsight Systems):</p> <ul> <li>Number of kernel launches</li> <li>Total kernel execution time</li> <li>Memory transfer overhead</li> <li>Average kernel duration</li> </ul> <p>These metrics comprehensively evaluate both raw performance and the underlying system efficiency.</p> <hr> <h4 id="results-and-analysis"><strong>Results and Analysis</strong></h4> <h5 id="overall-performance"><strong>Overall Performance</strong></h5> <p>Our results demonstrate consistent and substantial improvements over PyTorch Geometric across both datasets and all configurations tested.</p> <div class="row justify-content-center"> <div class="col-sm-5 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_4/gcn-3-480.webp 480w,/assets/img/project_4/gcn-3-800.webp 800w,/assets/img/project_4/gcn-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_4/gcn-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="qm9-forward-speedup" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-5 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_4/gcn-4-480.webp 480w,/assets/img/project_4/gcn-4-800.webp 800w,/assets/img/project_4/gcn-4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_4/gcn-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="zinc-forward-speedup" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Forward pass speedup of SmallGraphGCN over PyG on QM9 (left) and ZINC (right) across all evaluated configurations. </div> <h5 id="qm9-dataset-results"><strong>QM9 Dataset Results</strong></h5> <table> <thead> <tr> <th>Batch</th> <th>Layers</th> <th>Hidden</th> <th>Forward</th> <th>Backward</th> <th>Total</th> </tr> </thead> <tbody> <tr> <td>4096</td> <td>12</td> <td>32</td> <td><strong>2.95×</strong></td> <td>0.69×</td> <td><strong>1.27×</strong></td> </tr> <tr> <td>4096</td> <td>20</td> <td>32</td> <td><strong>3.00×</strong></td> <td>0.70×</td> <td><strong>1.29×</strong></td> </tr> <tr> <td>4096</td> <td>28</td> <td>32</td> <td><strong>3.13×</strong></td> <td>0.69×</td> <td><strong>1.30×</strong></td> </tr> <tr> <td>8192</td> <td>12</td> <td>32</td> <td><strong>2.51×</strong></td> <td>0.71×</td> <td><strong>1.16×</strong></td> </tr> <tr> <td>8192</td> <td>20</td> <td>32</td> <td><strong>2.57×</strong></td> <td>0.68×</td> <td><strong>1.14×</strong></td> </tr> <tr> <td>8192</td> <td>28</td> <td>32</td> <td><strong>2.65×</strong></td> <td>0.66×</td> <td><strong>1.14×</strong></td> </tr> <tr> <td>16384</td> <td>12</td> <td>32</td> <td><strong>2.36×</strong></td> <td>0.68×</td> <td><strong>1.08×</strong></td> </tr> <tr> <td>16384</td> <td>20</td> <td>32</td> <td><strong>2.38×</strong></td> <td>0.67×</td> <td><strong>1.08×</strong></td> </tr> <tr> <td>16384</td> <td>28</td> <td>32</td> <td><strong>2.37×</strong></td> <td>0.65×</td> <td><strong>1.06×</strong></td> </tr> </tbody> </table> <p><strong>Key observations</strong>:</p> <ul> <li>Forward speedups range from <strong>2.36× to 3.13×</strong> </li> <li>Best performance at batch size 4,096 with 28 layers (<strong>3.13× forward</strong>, <strong>1.30× total</strong>)</li> <li>Speedup increases with depth (more layers = more kernel launches saved)</li> <li>Backward pass consistently slower (0.65-0.71×) due to unoptimized atomic reductions</li> </ul> <h5 id="zinc-dataset-results"><strong>ZINC Dataset Results</strong></h5> <table> <thead> <tr> <th>Batch</th> <th>Layers</th> <th>Hidden</th> <th>Forward</th> <th>Backward</th> <th>Total</th> </tr> </thead> <tbody> <tr> <td>4096</td> <td>12</td> <td>32</td> <td><strong>2.58×</strong></td> <td>0.68×</td> <td><strong>1.16×</strong></td> </tr> <tr> <td>4096</td> <td>20</td> <td>32</td> <td><strong>2.71×</strong></td> <td>0.67×</td> <td><strong>1.17×</strong></td> </tr> <tr> <td>4096</td> <td>28</td> <td>32</td> <td><strong>2.73×</strong></td> <td>0.67×</td> <td><strong>1.17×</strong></td> </tr> <tr> <td>8192</td> <td>12</td> <td>32</td> <td><strong>2.35×</strong></td> <td>0.66×</td> <td><strong>1.07×</strong></td> </tr> <tr> <td>8192</td> <td>20</td> <td>32</td> <td><strong>2.39×</strong></td> <td>0.66×</td> <td><strong>1.07×</strong></td> </tr> <tr> <td>8192</td> <td>28</td> <td>32</td> <td><strong>2.40×</strong></td> <td>0.66×</td> <td><strong>1.07×</strong></td> </tr> <tr> <td>16384</td> <td>12</td> <td>32</td> <td><strong>2.31×</strong></td> <td>0.65×</td> <td><strong>1.05×</strong></td> </tr> <tr> <td>16384</td> <td>20</td> <td>32</td> <td><strong>2.32×</strong></td> <td>0.66×</td> <td><strong>1.05×</strong></td> </tr> <tr> <td>16384</td> <td>28</td> <td>32</td> <td><strong>2.34×</strong></td> <td>0.65×</td> <td><strong>1.05×</strong></td> </tr> </tbody> </table> <p><strong>Trends</strong>:</p> <ul> <li>Forward speedups: <strong>2.31× to 2.73×</strong> (slightly lower than QM9 due to larger graphs)</li> <li>Best configuration: batch=4,096, layers=28 (<strong>2.73× forward</strong>, <strong>1.17× total</strong>)</li> <li>More consistent backward performance (0.65-0.68×)</li> <li>Positive total speedup across all 18 configurations tested</li> </ul> <h5 id="why-forward-pass-dominates"><strong>Why Forward Pass Dominates</strong></h5> <p>Our profiling with NVIDIA Nsight Systems reveals three factors driving the exceptional forward pass performance:</p> <p><strong>1. Kernel Launch Reduction</strong></p> <p><strong>PyG per-layer operations</strong> (example for batch=4,096, 8 layers):</p> <ul> <li>Message construction: 1 kernel</li> <li>Scatter for aggregation: 1 kernel</li> <li>Index preparation: 2-3 auxiliary kernels</li> <li>Normalization: 1 kernel</li> <li>Dense transformation: 1 kernel</li> <li>Activation: 1 kernel</li> </ul> <p><strong>Total</strong>: ~7 kernels per layer × 8 layers = <strong>56 kernels</strong></p> <p><strong>SmallGraphGCN</strong>:</p> <ul> <li>Aggregation: 1 kernel</li> <li>Linear + ReLU: 1 kernel</li> </ul> <p><strong>Total</strong>: 2 kernels per layer × 8 layers = <strong>16 kernels</strong></p> <p><strong>Result</strong>: <strong>71% reduction in kernel launches</strong> (56 → 16)</p> <p>Profiling confirms this:</p> <table> <thead> <tr> <th>Metric</th> <th>PyG</th> <th>SmallGraphGCN</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>Kernel Launches</td> <td>32,670</td> <td>19,470</td> <td><strong>1.68× fewer</strong></td> </tr> <tr> <td>Kernel Time (ms)</td> <td>203.85</td> <td>151.22</td> <td><strong>1.35× faster</strong></td> </tr> <tr> <td>Memcpy Time (ms)</td> <td>1.04</td> <td>0.21</td> <td><strong>4.90× less</strong></td> </tr> </tbody> </table> <p><strong>2. Memory Transfer Efficiency</strong></p> <p>SmallGraphGCN’s ping-pong buffer strategy and fused kernels dramatically reduce memory transfers:</p> <ul> <li> <strong>PyG</strong>: Each operation materializes intermediate tensors, triggering CPU-GPU transfers for gradient bookkeeping</li> <li> <strong>SmallGraphGCN</strong>: Feature buffers are pre-allocated and reused; intermediate results stay on-device</li> </ul> <p>The <strong>4.90× reduction in memcpy time</strong> (1.04 ms → 0.21 ms) reflects this efficiency.</p> <p><strong>3. Eliminated Indexing Overhead</strong></p> <p>PyG’s scatter operations require expensive index manipulation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PyG's scatter internally calls:
</span><span class="n">index</span> <span class="o">=</span> <span class="n">edge_index</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Destination nodes
</span><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">num_nodes</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>
<span class="n">out</span><span class="p">.</span><span class="nf">scatter_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">features</span><span class="p">),</span> <span class="n">messages</span><span class="p">)</span>
</code></pre></div></div> <p>This involves:</p> <ul> <li>Sorting or hashing indices for coalesced access</li> <li>Prefix sums to determine output locations (CUB kernels)</li> <li>Conditional execution based on index values</li> </ul> <p>SmallGraphGCN’s direct edge iteration:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Direct atomic accumulation</span>
<span class="kt">int</span> <span class="n">dst</span> <span class="o">=</span> <span class="n">edge_targets</span><span class="p">[</span><span class="n">edge_idx</span><span class="p">];</span>
<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">output_features</span><span class="p">[</span><span class="n">dst</span> <span class="o">*</span> <span class="n">feature_dim</span> <span class="o">+</span> <span class="n">feat_idx</span><span class="p">],</span> <span class="n">message</span><span class="p">);</span>
</code></pre></div></div> <p>No sorting, no indexing kernels—just direct writes with atomic synchronization.</p> <h5 id="why-backward-pass-lags"><strong>Why Backward Pass Lags</strong></h5> <p>The backward pass performance (0.65-0.71× vs PyG) stems from our current implementation’s naive atomic reduction strategy.</p> <p><strong>Atomic Contention Analysis</strong></p> <p>Our backward kernel parallelizes over <code class="language-plaintext highlighter-rouge">(edge, feature)</code> pairs, with every thread performing a direct atomic operation:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Simplified backward kernel</span>
<span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">grad_input</span><span class="p">[</span><span class="n">dst_node</span> <span class="o">*</span> <span class="n">feat_dim</span> <span class="o">+</span> <span class="n">feat_idx</span><span class="p">],</span> <span class="n">gradient_contribution</span><span class="p">);</span>
</code></pre></div></div> <p><strong>Contention scaling</strong>: For a node with degree \(d\), \(d\) threads attempt simultaneous writes to the same memory address. The GPU memory controller serializes these transactions, creating a bottleneck proportional to node degree.</p> <p><strong>Molecular graph characteristics</strong>:</p> <ul> <li>QM9 average degree: ~2.5</li> <li>ZINC average degree: ~2.8</li> <li>Maximum degree: ~6</li> </ul> <p>Even with low average degree, contention accumulates when processing large batches. For batch size 16,384, thousands of nodes are being updated simultaneously, exhausting atomic resources.</p> <p><strong>PyG’s Advantage</strong></p> <p>PyTorch Geometric (via <code class="language-plaintext highlighter-rouge">torch-scatter</code>) employs sophisticated multi-level reduction:</p> <ol> <li> <strong>Warp-level aggregation</strong>: Uses shuffle instructions to combine gradients within warps</li> <li> <strong>Shared memory buffering</strong>: Accumulates partial results in fast L1/shared memory</li> <li> <strong>Final global write</strong>: Single atomic operation per warp per node</li> </ol> <p>This hierarchical approach reduces global memory atomics by 32× (warp size), explaining PyG’s superior backward performance.</p> <p><strong>Opportunity for Improvement</strong></p> <p>Our forward kernel fusion proves the value of workload-specific optimization. The backward pass represents our next optimization target—implementing warp-level reductions should bring backward performance to parity or beyond PyG.</p> <h5 id="scaling-trends"><strong>Scaling Trends</strong></h5> <p><strong>Depth Scaling</strong></p> <p>Forward pass speedup improves with network depth:</p> <p><strong>QM9 (batch=4,096)</strong>:</p> <ul> <li>12 layers: 2.95× → 20 layers: 3.00× → 28 layers: 3.13×</li> </ul> <p><strong>Why</strong>: Deeper networks amplify PyG’s kernel launch overhead. Each additional layer adds 7 kernel launches for PyG but only 2 for SmallGraphGCN.</p> <p><strong>Batch Size Scaling</strong></p> <p>Larger batches show diminishing speedup:</p> <p><strong>QM9 (28 layers)</strong>:</p> <ul> <li>Batch 4,096: <strong>3.13×</strong> </li> <li>Batch 8,192: <strong>2.65×</strong> </li> <li>Batch 16,384: <strong>2.37×</strong> </li> </ul> <p><strong>Why</strong>: At very large batch sizes, PyG achieves better GPU occupancy on its individual kernels. The fixed launch overhead becomes relatively less significant when each kernel processes more data. SmallGraphGCN still wins due to total launch count, but the margin narrows.</p> <p><strong>Dataset Scaling (Graph Size)</strong></p> <p>ZINC (larger graphs) shows slightly lower speedups than QM9:</p> <p><strong>Best configurations</strong>:</p> <ul> <li>QM9 (18 avg nodes): <strong>3.13×</strong> forward</li> <li>ZINC (23 avg nodes): <strong>2.73×</strong> forward</li> </ul> <p><strong>Why</strong>: Larger graphs provide more arithmetic work per kernel launch, reducing the relative importance of launch overhead. SmallGraphGCN’s advantage is most pronounced in the smallest-graph regime.</p> <h5 id="numerical-validation"><strong>Numerical Validation</strong></h5> <p>We validated correctness against PyG across all configurations:</p> <p><strong>Forward Pass</strong>:</p> <ul> <li>Maximum relative error: &lt; 2% (attributable to different atomic accumulation orders)</li> <li>Mean relative error: &lt; 0.5%</li> </ul> <p><strong>Backward Pass</strong>:</p> <ul> <li>Gradient magnitudes within 1% of PyG</li> <li>Gradient direction cosine similarity &gt; 0.999</li> </ul> <p><strong>Training Convergence</strong>:</p> <ul> <li>Loss curves indistinguishable from PyG within numerical precision</li> <li>Final model performance identical (validated on held-out test set)</li> </ul> <p>The small forward pass discrepancies arise from non-deterministic atomic addition order but don’t affect training dynamics or final model quality.</p> <hr> <h4 id="limitations-and-future-work"><strong>Limitations and Future Work</strong></h4> <h5 id="current-limitations"><strong>Current Limitations</strong></h5> <p><strong>1. Unoptimized Backward Reduction</strong></p> <p>The primary limitation is the naive global-atomic strategy in our backward kernel. The 30-35% slowdown compared to PyG represents a clear optimization opportunity.</p> <p><strong>Planned solution</strong>: Implement hierarchical reduction:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Proposed two-phase backward kernel</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">backward_phase1_warp_reduce</span><span class="p">(...)</span>  <span class="p">{</span>
    <span class="c1">// Phase 1: Warp-level reduction using shuffle instructions</span>
    <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">warp_results</span><span class="p">[</span><span class="mi">32</span><span class="p">];</span>  <span class="c1">// One per warp</span>
    
    <span class="c1">// Accumulate within warp</span>
    <span class="kt">float</span> <span class="n">local_grad</span> <span class="o">=</span> <span class="n">compute_edge_gradient</span><span class="p">(...);</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="mi">16</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">local_grad</span> <span class="o">+=</span> <span class="n">__shfl_down_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">local_grad</span><span class="p">,</span> <span class="n">offset</span><span class="p">);</span>
    <span class="p">}</span>
    
    <span class="c1">// First thread in warp writes to shared memory</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">%</span> <span class="mi">32</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">warp_results</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="mi">32</span><span class="p">]</span> <span class="o">=</span> <span class="n">local_grad</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">__syncthreads</span><span class="p">();</span>
    
    <span class="c1">// Phase 2: Final atomic add from shared memory (one per block)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">grad_input</span><span class="p">[</span><span class="n">node_idx</span><span class="p">],</span> <span class="n">sum_of_warp_results</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>This approach should reduce global atomics by 32× and bring backward performance to parity with or beyond PyG.</p> <p><strong>2. Architecture Specificity</strong></p> <p>Current kernels are specialized for GCN-style spectral aggregation (\(D^{-1/2}\hat{A}D^{-1/2}\)) with scalar edge weights.</p> <p><strong>Not directly supported</strong>:</p> <ul> <li> <strong>Graph Attention Networks (GAT)</strong>: Require computing attention scores per edge, increasing register pressure</li> <li> <strong>Edge-conditioned convolutions</strong>: Need multi-dimensional edge features</li> <li> <strong>Heterogeneous graphs</strong>: Different node/edge types require conditional execution</li> </ul> <p><strong>Extension strategy</strong>: The edge-centric parallelism principle extends naturally, but implementation requires:</p> <ul> <li>Handling variable-length edge features</li> <li>Supporting multiple aggregation functions</li> <li>Managing increased kernel complexity</li> </ul> <p><strong>3. Fixed Feature Dimension</strong></p> <p>The <code class="language-plaintext highlighter-rouge">MAX_FEATURES = 128</code> constraint wastes memory for smaller models and limits larger models:</p> <p><strong>Current approach</strong>:</p> <ul> <li>32-dim model: 4× memory overhead</li> <li>256-dim model: Not supported</li> </ul> <p><strong>Solution</strong>: Template kernels or dynamic dispatch based on feature dimension at runtime.</p> <h5 id="future-optimizations"><strong>Future Optimizations</strong></h5> <p><strong>1. Multi-GPU Scaling</strong></p> <p>Molecular datasets with millions of graphs could benefit from data parallelism across GPUs:</p> <p><strong>Approach</strong>:</p> <ul> <li>NCCL-based gradient synchronization</li> <li>Overlapping computation with communication</li> <li>Specialized batching to balance graph sizes across GPUs</li> </ul> <p><strong>2. Mixed Precision Training</strong></p> <p>FP16 computation could provide additional speedup:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Convert to FP16 for computation, accumulate in FP32</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mixed_precision_aggregate</span><span class="p">(...)</span> <span class="p">{</span>
    <span class="n">half</span> <span class="n">input</span> <span class="o">=</span> <span class="n">__float2half</span><span class="p">(</span><span class="n">input_features</span><span class="p">[...]);</span>
    <span class="n">half</span> <span class="n">message</span> <span class="o">=</span> <span class="n">__hmul</span><span class="p">(</span><span class="n">edge_weight</span><span class="p">,</span> <span class="n">input</span><span class="p">);</span>
    <span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">output_features</span><span class="p">[...],</span> <span class="n">__half2float</span><span class="p">(</span><span class="n">message</span><span class="p">));</span>  <span class="c1">// Accumulate in FP32</span>
<span class="p">}</span>
</code></pre></div></div> <p>Tensor cores on modern GPUs could accelerate the linear transformation significantly.</p> <p><strong>3. Kernel Fusion with Loss Computation</strong></p> <p>For end-to-end training, fusing the final prediction with loss computation could eliminate one more kernel launch:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Combined: forward + loss computation</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">predict_and_compute_loss</span><span class="p">(...)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">forward_pass</span><span class="p">(...);</span>
    <span class="kt">float</span> <span class="n">loss_contribution</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">);</span>
    <span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">total_loss</span><span class="p">,</span> <span class="n">loss_contribution</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>4. CPU-GPU Overlapping</strong></p> <p>Asynchronous data loading while GPU computes could hide transfer latency:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Proposed async loading
</span><span class="n">stream1</span><span class="p">,</span> <span class="n">stream2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nc">Stream</span><span class="p">(),</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nc">Stream</span><span class="p">()</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">stream</span><span class="p">(</span><span class="n">stream1</span><span class="p">):</span>
    <span class="n">batch1</span> <span class="o">=</span> <span class="nf">load_to_gpu</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">stream</span><span class="p">(</span><span class="n">stream2</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">batch0</span><span class="p">)</span>  <span class="c1"># Overlaps with batch1 transfer
</span></code></pre></div></div> <h5 id="broader-applicability"><strong>Broader Applicability</strong></h5> <p>The edge-centric kernel design principles extend beyond molecular graphs:</p> <p><strong>Applicable domains</strong>:</p> <ul> <li> <strong>Social network analysis</strong>: Small community graphs</li> <li> <strong>Protein structure prediction</strong>: Amino acid fragment graphs</li> <li> <strong>3D point cloud processing</strong>: Local neighborhood graphs</li> <li> <strong>Recommendation systems</strong>: User-item interaction subgraphs</li> </ul> <p><strong>General principle</strong>: Whenever batches contain many small graphs (&lt; 1000 nodes), fused edge-centric kernels outperform general sparse primitives.</p> <hr> <h4 id="conclusion"><strong>Conclusion</strong></h4> <p>SmallGraphGCN demonstrates that workload-specific kernel design can dramatically outperform general-purpose GNN frameworks for batched small graph training. By replacing PyTorch Geometric’s flexible but overhead-heavy message-passing abstraction with fused edge-centric CUDA kernels, we achieve:</p> <ul> <li><strong>2.3-3.1× faster forward execution</strong></li> <li><strong>1.05-1.30× end-to-end training speedup</strong></li> <li><strong>68% reduction in kernel launches</strong></li> <li><strong>4.9× lower memory transfer overhead</strong></li> </ul> <p>These improvements translate directly to accelerated molecular discovery pipelines, where researchers train hundreds of models across different datasets and hyperparameters. A 1.3× training speedup across 200 experiments saves weeks of computational time.</p> <h5 id="key-takeaways"><strong>Key Takeaways</strong></h5> <p><strong>Insight 1</strong>: For small graphs, kernel launch overhead dominates computation time. Fusion is essential.</p> <p><strong>Insight 2</strong>: Edge-centric parallelism creates uniform, easily schedulable workloads that maximize GPU occupancy even on tiny graphs.</p> <p><strong>Insight 3</strong>: The backward pass represents the next frontier—hierarchical atomic reductions should close the performance gap.</p> <p><strong>Insight 4</strong>: Workload-specific optimization complements, rather than replaces, general frameworks. PyG remains ideal for rapid prototyping; SmallGraphGCN optimizes production training.</p> <h5 id="broader-impact"><strong>Broader Impact</strong></h5> <p>This work demonstrates a general principle: <strong>generic abstractions have inherent overhead that becomes the bottleneck for specialized workloads</strong>. As GNNs continue to grow in importance for scientific machine learning—from drug discovery to materials design to climate modeling—optimizing for the specific characteristics of scientific graph data will become increasingly critical.</p> <p>The edge-centric execution model we’ve developed represents one point in the design space of high-performance GNN systems. We hope it inspires further exploration of workload-specific kernel designs that push the boundaries of what’s possible with graph neural networks.</p> <h5 id="practical-recommendations"><strong>Practical Recommendations</strong></h5> <p><strong>When to use SmallGraphGCN</strong>:</p> <ul> <li>Training on molecular/protein datasets (graphs &lt; 100 nodes)</li> <li>Batch sizes &gt; 1000 graphs</li> <li>Deep models (&gt; 8 layers)</li> <li>Production training pipelines where performance matters</li> </ul> <p><strong>When to use PyG</strong>:</p> <ul> <li>Rapid prototyping and experimentation</li> <li>Large single graphs (&gt; 10,000 nodes)</li> <li>Heterogeneous or dynamic graphs</li> <li>Research requiring flexibility over performance</li> </ul> <p>The future of GNN acceleration lies in combining the flexibility of high-level frameworks with the performance of specialized kernels—using general frameworks for development and specialized implementations for production deployment.</p> <hr> <h4 id="resources"><strong>Resources</strong></h4> <ul> <li> <strong>Code Repository:</strong> <a href="https://github.com/Monishver11/smallgraphgcn" rel="external nofollow noopener" target="_blank">GitHub - SmallGraphGCN</a> </li> <li> <strong>Technical Report:</strong> <a href="https://drive.google.com/file/d/1Q7fAFt6w5dBB95fFnsv7ANrdn_A3Z0KW/view?usp=sharing" rel="external nofollow noopener" target="_blank">Full Paper (PDF)</a> </li> <li> <strong>Presentation:</strong> <a href="https://drive.google.com/file/d/1EFiZmogqffz4GiXlycKfb2wj6DG7dEuC/view?usp=sharing" rel="external nofollow noopener" target="_blank">Slides (PDF)</a> </li> </ul> <p><strong>References</strong>:</p> <ol> <li> <p>Kipf, T. N., &amp; Welling, M. (2016). Semi-supervised classification with graph convolutional networks. <em>arXiv preprint arXiv:1609.02907</em>.</p> </li> <li> <p>Fey, M., &amp; Lenssen, J. E. (2019). Fast graph representation learning with PyTorch Geometric. <em>ICLR Workshop on Representation Learning on Graphs and Manifolds</em>.</p> </li> <li> <p>Wang, M., et al. (2019). Deep graph library: A graph-centric, highly-performant package for graph neural networks. <em>arXiv preprint arXiv:1909.01315</em>.</p> </li> <li> <p>Wu, Z., et al. (2018). MoleculeNet: A benchmark for molecular machine learning. <em>Chemical Science</em>, 9(2), 513-530.</p> </li> <li> <p>Ramakrishnan, R., et al. (2014). Quantum chemistry structures and properties of 134 kilo molecules. <em>Scientific Data</em>, 1(1), 140022.</p> </li> </ol> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-apache-flink",title:"Apache Flink",description:"Realtime and Big Data Analytics Course at NYU Courant - Personal Notes 10",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-11-flink/"}},{id:"post-apache-kafka",title:"Apache Kafka",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 9",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-10-kafka/"}},{id:"post-apache-zookeeper",title:"Apache ZooKeeper",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 8",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-9-zookeeper/"}},{id:"post-apache-hbase",title:"Apache HBase",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 7",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-8-hbase/"}},{id:"post-hive-amp-trino",title:"Hive &amp; Trino",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 6",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-7-hive/"}},{id:"post-mapreduce-design-patterns",title:"MapReduce Design Patterns",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 5",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-5-mr-dp/"}},{id:"post-big-data-processing-concepts-amp-mapreduce",title:"Big Data Processing Concepts &amp; MapReduce",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-4-mapreduce/"}},{id:"post-hadoop-distributed-file-system-hdfs",title:"Hadoop Distributed File System (HDFS)",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-3-hdfs/"}},{id:"post-reading-notes-from-aleksa-gordic-39-s-gpu-blogpost",title:"Reading Notes from Aleksa Gordic&#39;s GPU BlogPost",description:"Reading notes for my reference from Aleksa Gordic&#39;s GPU BlogPost",section:"Posts",handler:()=>{window.location.href="/blog/2025/aleksagordic-gpu-blog-notes/"}},{id:"post-mlp-standard-derivatives-derivation",title:"MLP Standard Derivatives Derivation",description:"MLP Standard Derivatives Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-derivatives/"}},{id:"post-simple-mlp-forward-and-backward-pass-with-einsum-derivation",title:"Simple MLP - Forward and Backward Pass (With Einsum) Derivation",description:"Simple MLP - Forward and Backward Pass Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-fw-bwd/"}},{id:"post-gpu-notes",title:"GPU Notes",description:"GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes",section:"Posts",handler:()=>{window.location.href="/blog/2025/gpu-notes/"}},{id:"post-big-data-storage",title:"Big Data Storage",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 2",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-2-storage/"}},{id:"post-introduction-to-realtime-and-big-data-analytics",title:"Introduction to Realtime and Big Data Analytics",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-1-intro/"}},{id:"post-gpu-essentials-a-concise-technical-guide",title:"GPU Essentials - A Concise Technical Guide",description:"A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.",section:"Posts",handler:()=>{window.location.href="/blog/2025/GPU-Intro/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab",title:"Sharing personal reflections - Thoughts Tab",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-smallgraphgcn-accelerating-gnn-training-on-batched-small-graphs",title:"SmallGraphGCN - Accelerating GNN Training on Batched Small Graphs",description:"Discover how fused-edge-centric CUDA kernels dramatically accelerate Graph Neural Network training on molecular datasets. By rethinking parallelism strategies for batched small graphs, our approach achieves up to 3.1\xd7 faster forward execution and 1.3\xd7 end-to-end training speedup over PyTorch Geometric.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>