<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gaze-Guided Reinforcement Learning for Visual Search | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/projects/2_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gaze-Guided Reinforcement Learning for Visual Search</h1> <p class="post-description">Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.</p> </header> <article> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_2/project-2-intro-pic-480.webp 480w,/assets/img/project_2/project-2-intro-pic-800.webp 800w,/assets/img/project_2/project-2-intro-pic-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_2/project-2-intro-pic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="project-2-intro-pic" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>This blog explores how we can make AI agents search for objects more efficiently by mimicking human visual attention patterns. Using a gaze prediction model trained on human eye-tracking data, we’ve developed three innovative approaches to integrate this visual attention information into a reinforcement learning framework: channel integration (adding gaze as a fourth input channel), bottleneck integration (processing RGB and gaze separately before combining), and weighted integration (using gaze to modulate visual inputs).</p> <p>Our experiments in simulated indoor environments demonstrate that these gaze-guided agents learn more efficiently and navigate more effectively than traditional approaches. By bridging cognitive science and machine learning, this project offers insights into how biologically-inspired attention mechanisms can enhance AI systems for robotics, assistive technologies, and autonomous navigation.</p> <p>This project was developed as part of the <strong>Deep Decision Making and Reinforcement Learning</strong> course during the spring 2025 semester in the <strong>MSCS program at NYU Courant</strong>. Our team — <strong>Lokesh Gautham Boominathan Muthukumar</strong>, <strong>Yu Minematsu</strong>, <strong>Muhammad Mukthar</strong>, and myself.</p> <hr> <h4 id="introduction-and-motivation"><strong>Introduction and Motivation</strong></h4> <p>Visual search is a fundamental task that humans perform effortlessly every day. Whether looking for your keys, finding a product on a shelf, or locating a friend in a crowd, our visual system efficiently guides our attention to relevant objects. However, autonomous agents struggle with these same tasks, often employing inefficient search strategies that waste time and computational resources.</p> <p>This project explores a novel approach: leveraging human gaze patterns to enhance reinforcement learning for visual search tasks. By integrating human attention data, we aim to teach AI agents to “look where humans look,” dramatically improving search efficiency in environments like AI2-THOR, a realistic 3D household simulator.</p> <p>The motivation behind this research stems from a simple observation: humans use sophisticated attention mechanisms developed through evolution to efficiently prioritize visual information. When we search for objects, our eyes don’t scan uniformly across a scene; instead, we rapidly focus on relevant locations based on context, object relationships, and prior knowledge. For instance, when looking for a microwave in a kitchen, humans instinctively focus on countertops and walls rather than floors or ceilings.</p> <p>While gaze prediction has been explored in computer vision research, and various attention mechanisms have been applied to object detection, our work makes a novel contribution by specifically integrating human gaze patterns into reinforcement learning for embodied visual search in 3D environments. Previous studies have primarily focused on using gaze for image classification, intention prediction, or as auxiliary signals, but our approach represents one of the first comprehensive attempts to integrate gaze information at both perceptual and motivational levels for embodied agents performing active search tasks.</p> <p>This gaze-guided approach has significant practical applications:</p> <ul> <li> <strong>Household robots</strong> that can efficiently find and manipulate objects</li> <li> <strong>Assistive technology</strong> for visually impaired individuals that can locate objects upon request</li> <li> <strong>Search and rescue robots</strong> that can quickly identify people or objects in disaster scenarios</li> <li> <strong>Warehouse automation</strong> systems that locate specific products among thousands</li> </ul> <p>The fundamental research question we explore is: Can we leverage human gaze patterns to improve reinforcement learning efficiency in object search tasks? Our hypothesis is that by incorporating human visual attention data as an additional signal in the reinforcement learning process, we can achieve faster learning, better generalization, and more human-like search behavior.</p> <p>In this project, we implement multiple methods for integrating gaze information with visual data, train agents to search for objects, and evaluate their performance across different scenarios. The results demonstrate that gaze-guided reinforcement learning significantly outperforms traditional approaches, opening new possibilities for AI systems that can see the world more like we do.</p> <hr> <h4 id="background-and-context"><strong>Background and Context</strong></h4> <p><strong>Visual Search in AI</strong></p> <p>Visual search in AI presents numerous challenges that humans solve intuitively. Traditional AI approaches often rely on exhaustive exploration strategies that lack the efficiency of human visual search. In environments like AI2-THOR, as implemented in our project, agents must process high-dimensional visual inputs, maintain spatial awareness, and make decisions with limited information—all while dealing with partial observability where only a fraction of the environment is visible at any time. These challenges make autonomous visual search computationally expensive and time-consuming compared to human performance.</p> <p><strong>Human Attentional Mechanisms</strong></p> <p>Humans excel at visual search through sophisticated attentional mechanisms. Our visual system employs both bottom-up attention (driven by salient features like color and motion) and top-down attention (guided by goals and prior knowledge). When searching for objects, we don’t scan uniformly across scenes but rather prioritize locations where target objects are likely to appear. For example, when searching for a microwave in a kitchen, we instinctively focus on countertops and cabinet areas while ignoring floors or ceilings. These attentional shortcuts dramatically reduce the search space and make human visual search remarkably efficient.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_2/proj2-pic2-480.webp 480w,/assets/img/project_2/proj2-pic2-800.webp 800w,/assets/img/project_2/proj2-pic2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_2/proj2-pic2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="proj2-pic2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Reinforcement Learning for Navigation</strong></p> <p>Traditional approaches to visual navigation using reinforcement learning typically rely on end-to-end training where agents learn to map raw visual inputs directly to actions. In our implementation, we see this in the baseline <code class="language-plaintext highlighter-rouge">train_with_progress_logging.py</code> script, which uses a standard PPO algorithm with CNN-based policies to learn search behaviors. While effective, these approaches often require millions of environment interactions and struggle with the exploration-exploitation dilemma. Agents must spend significant time exploring to discover rewards, leading to inefficient learning, especially in large, complex environments with sparse rewards.</p> <p><strong>Gaze as Guidance</strong></p> <p>The theoretical foundation for using human gaze data comes from the insight that gaze patterns contain valuable information about task-relevant features and regions. By incorporating gaze heatmaps as an additional signal, we can provide agents with “attentional priors” that focus learning on important areas of the visual field. Our project implements this through the GazeEnvWrapper and GazePreprocessEnvWrapper classes in env_wrappers.py, which augment observations with gaze prediction data and even modify the reward function to encourage attention to relevant regions.</p> <p><strong>Related Work</strong></p> <p>This approach builds on a rich body of research using human data to guide AI systems. Imitation learning uses expert demonstrations to bootstrap agent performance, while inverse reinforcement learning attempts to recover reward functions from human behavior. Recent work has also explored using gaze data for various AI tasks, including image classification, video game playing, and autonomous driving. Our project extends these ideas to 3D navigation tasks, implementing three distinct integration methods (channel-based, attention-based, and weighted approaches) to effectively incorporate gaze information into the reinforcement learning pipeline, as seen in our networks.py implementation.</p> <hr> <h4 id="technical-approach"><strong>Technical Approach</strong></h4> <p>The system architecture consists of three integrated components working in harmony:</p> <ul> <li> <strong>Gaze Prediction Model:</strong> A deep learning model that predicts human attention patterns for visual inputs</li> <li> <strong>Environment:</strong> A modified AI2-THOR environment with custom wrappers that integrate gaze information</li> <li> <strong>RL Agent:</strong> A customized PPO-based agent that learns to navigate using both visual inputs and gaze information</li> </ul> <p>The information flows through this system sequentially - the environment provides RGB observations, which are fed to the gaze model to predict attention heatmaps. These heatmaps are incorporated into the agent’s observations, which the agent then processes to make navigation decisions. The reward function uses both search success and gaze-object overlap to guide learning effectively.</p> <div class="row justify-content-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_2/sys-arch-480.webp 480w,/assets/img/project_2/sys-arch-800.webp 800w,/assets/img/project_2/sys-arch-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_2/sys-arch.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="sys-arch" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h5 id="gaze-prediction-model"><strong>Gaze Prediction Model:</strong></h5> <p><strong>Data Collection and Processing</strong></p> <p>The gaze prediction model is trained on the SALICON dataset, a large-scale collection of human eye fixation data. The processing pipeline converts raw eye fixation points into smooth attention heatmaps through several steps:</p> <ul> <li>Creating a zero-initialized heatmap matrix for each image</li> <li>Marking human fixation locations with higher values</li> <li>Applying Gaussian smoothing to create continuous attention distributions</li> <li>Normalizing the results to create proper probability distributions</li> </ul> <p>This preprocessing transforms discrete eye tracking data points into continuous heatmaps that represent where humans typically look when viewing each scene.</p> <p><strong>Model Architecture</strong></p> <p>The primary gaze prediction architecture uses a ResNet-based model, implementing transfer learning by starting with a pretrained ResNet18 backbone. The model is modified by replacing the final classification layer with a custom regression head that outputs a 224×224 heatmap. A sigmoid activation ensures values are between 0 and 1, representing attention probabilities across the visual field.</p> <p><strong>Training Framework</strong></p> <p>The training is managed using PyTorch Lightning, which provides a clean, organized structure for model development. The implementation uses Mean Squared Error (MSE) for training and evaluates using both MSE and Structural Similarity Index (SSIM) metrics. All hyperparameters are configured via YAML files, allowing for systematic experimentation with different training configurations.</p> <h5 id="environment-setup"><strong>Environment Setup:</strong></h5> <p><strong>AI2-THOR Simulator</strong></p> <p>Our implementation wraps the AI2-THOR simulator in a Gymnasium-compatible environment (<code class="language-plaintext highlighter-rouge">AI2ThorEnv</code> class), providing a standardized interface for reinforcement learning. Key configuration parameters include:</p> <ul> <li>224×224 pixel observations</li> <li>0.25m grid size for navigation</li> <li>90° field of view</li> <li>Depth and segmentation rendering for richer observations</li> </ul> <p><strong>Object Search Task Design</strong></p> <p>The object search task is implemented in the <code class="language-plaintext highlighter-rouge">reset()</code> and <code class="language-plaintext highlighter-rouge">step()</code> methods of our environment class. We create challenging scenarios by:</p> <ul> <li>Randomly selecting a kitchen scene from a predefined list</li> <li>Randomizing the agent’s starting position</li> <li>Placing target objects (e.g., “Microwave”) in their natural locations</li> <li>Defining success as finding the object with sufficient visibility (&gt;5%) and proximity (&lt;1.5m)</li> </ul> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_2/env-1-480.webp 480w,/assets/img/project_2/env-1-800.webp 800w,/assets/img/project_2/env-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_2/env-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="env-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The episode terminates when either the object is found or the maximum step limit (default: 200 steps) is reached.</p> <p><strong>Observation and Action Spaces</strong></p> <p>The observation space consists of RGB images (224×224×3) from the agent’s perspective. The action space is discrete with 7 possible actions: moving in four directions (forward, backward, left, right), rotating left and right, and looking up. This action space provides sufficient flexibility for the agent to search the environment effectively.</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_2/env-2-480.webp 480w,/assets/img/project_2/env-2-800.webp 800w,/assets/img/project_2/env-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_2/env-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="env-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h5 id="reinforcement-learning-framework"><strong>Reinforcement Learning Framework:</strong></h5> <p><strong>PPO Algorithm</strong></p> <p>The implementation extends Stable-Baselines3’s Proximal Policy Optimization (PPO) algorithm with custom feature extractors designed to process gaze information. It supports configurable network architectures for different gaze integration methods and uses hyperparameters specifically tuned for object search tasks.</p> <p><strong>Reward Structure</strong></p> <p>One of the most innovative aspects of the approach is the reward structure that incorporates gaze information. The reward function includes:</p> <ol> <li>A base reward for traditional navigation and search success</li> <li>An additional reward component based on how well the agent’s attention (via the gaze model) aligns with relevant objects</li> <li>A mechanism that calculates Intersection over Union (IoU) between predicted gaze and object regions</li> </ol> <p>The base reward structure also includes several components:</p> <ul> <li>A small penalty for each step to encourage efficiency</li> <li>A larger penalty for repeated actions (e.g., bumping into walls)</li> <li>Graduated rewards for making the target object visible, with higher rewards for closer and more visible objects</li> <li>A large bonus for successfully completing the task</li> </ul> <p>This creates a dense reward signal that guides learning more effectively than sparse rewards would.</p> <p><strong>Learning Process</strong></p> <p>Our training process is managed by the <code class="language-plaintext highlighter-rouge">train_agent</code> function in <code class="language-plaintext highlighter-rouge">train_gaze_guided_rl_final.py</code>, which:</p> <ol> <li>Creates a unique experiment directory with timestamp</li> <li>Configures the agent with the specified gaze integration method</li> <li>Trains the agent for the specified number of timesteps</li> <li>Logs comprehensive metrics through our GazeProgressCallback</li> <li>Saves the trained model and performance metrics</li> </ol> <p>The callback tracks:</p> <ul> <li>Episode rewards and lengths</li> <li>Success rates for finding objects</li> <li>Overall exploration coverage</li> <li>Training speed and resource usage</li> </ul> <p>By combining these components, we create a comprehensive system for gaze-guided reinforcement learning that significantly improves visual search efficiency.</p> <hr> <h4 id="integration-methods-our-key-innovation"><strong>Integration Methods (Our Key Innovation)</strong></h4> <p>Our project explores three distinct methods for integrating gaze information with visual inputs for reinforcement learning. Each method represents a different approach to combining attention data with RGB observations.</p> <h5 id="method-1-channel-integration"><strong>Method 1: Channel Integration</strong></h5> <p><strong>Technical Approach</strong></p> <p>The channel integration method is the most straightforward approach, treating gaze information as a fourth channel alongside the standard RGB channels:</p> <blockquote> <p>RGB Image (3 channels) + Gaze Heatmap (1 channel) → 4-channel input</p> </blockquote> <p>We implemented this in the <code class="language-plaintext highlighter-rouge">ChannelCNN</code> class by modifying the first convolutional layer of ResNet18:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">if</span> <span class="n">use_gaze</span><span class="p">:</span>
    <span class="n">original_weights</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span>
    <span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span>
        <span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_weights</span>
        <span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_weights</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Implementation Challenges and Solutions</strong></p> <p>A major challenge was preserving the pretrained weights while adding the new channel. Our solution was to:</p> <ol> <li>Save the original weights for the RGB channels</li> <li>Replace the first convolutional layer with a new 4-channel version</li> <li>Copy the original weights back for the RGB channels</li> <li>Initialize the gaze channel weights using the weights from the red channel</li> </ol> <p>This approach allows us to leverage transfer learning from ImageNet while adapting to our 4-channel input format.</p> <p>Another challenge was ensuring proper normalization of the gaze channel to match the scale of RGB inputs. We solved this by normalizing gaze heatmaps to the range [0, 255] and then scaling them to [0, 1] during preprocessing, matching the normalization of the RGB channels.</p> <p><strong>Theoretical Advantages</strong></p> <p>Channel integration offers several advantages:</p> <ul> <li> <strong>Simplicity:</strong> The approach is conceptually straightforward and easy to implement</li> <li> <strong>Early fusion:</strong> Gaze information influences feature extraction from the very beginning</li> <li> <strong>Preservation of spatial relationships:</strong> The spatial correlation between RGB and gaze data is maintained</li> <li> <strong>Computational efficiency:</strong> No additional network branches or complex fusion mechanisms are required</li> </ul> <p>The main theoretical basis is that early fusion allows the convolutional layers to learn correlations between visual features and attention patterns from the beginning of the processing pipeline.</p> <h5 id="method-2-bottleneck-integration"><strong>Method 2: Bottleneck Integration</strong></h5> <p><strong>Technical Approach</strong></p> <p>The bottleneck integration method (implemented as GazeAttnCNN) processes RGB and gaze separately before combining them:</p> <blockquote> <p>RGB → CNN → RGB Features</p> </blockquote> <blockquote> <p>Gaze → Lightweight CNN → Gaze Features</p> </blockquote> <blockquote> <p>RGB Features + Gaze Features (via attention) → Fused Features</p> </blockquote> <p>We implemented a cross-attention mechanism:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Process RGB and gaze separately
</span><span class="n">rgb_feats</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">rgb_backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">gaze_feats</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gaze_encoder</span><span class="p">(</span><span class="n">gaze_heatmap</span><span class="p">)</span>

<span class="c1"># Use gaze features as query, RGB features as key and value
</span><span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query_proj</span><span class="p">(</span><span class="n">gaze_feats</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key_proj</span><span class="p">(</span><span class="n">rgb_feats</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value_proj</span><span class="p">(</span><span class="n">rgb_feats</span><span class="p">)</span>

<span class="c1"># Calculate attention and apply to values
</span><span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">attention_scale</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Implementation Details</strong></p> <p>The architecture consists of:</p> <ul> <li> <strong>RGB Backbone:</strong> ResNet18 up to the final pooling layer</li> <li> <strong>Gaze Encoder:</strong> A lightweight CNN with fewer layers</li> <li> <strong>Cross-Attention Module:</strong> Projects features to query, key, and value spaces</li> <li> <strong>Output Projection:</strong> Global pooling and final feature projection</li> </ul> <p>This architecture allows the network to first extract features from RGB and gaze independently, then use gaze features to guide the attention on RGB features, similar to how human attention works.</p> <p><strong>Theoretical Advantages</strong></p> <p>Bottleneck integration offers several advantages:</p> <ol> <li> <strong>Specialized Processing:</strong> Different network branches can specialize in RGB and gaze feature extraction</li> <li> <strong>Controlled Information Flow:</strong> The attention mechanism explicitly controls how gaze information influences visual features</li> <li> <strong>Interpretable Intermediate Representations:</strong> The attention weights reveal which parts of the RGB features are emphasized</li> <li> <strong>Flexibility:</strong> The architecture can be adapted to different attention mechanisms (multi-head, self-attention, etc.)</li> </ol> <p>The theoretical basis is that attention mechanisms are well-suited for modeling the relationship between gaze and visual features, as they naturally capture the notion of focusing on specific regions.</p> <h5 id="method-3-weighted-integration"><strong>Method 3: Weighted Integration</strong></h5> <p><strong>Technical Approach</strong></p> <p>The weighted integration method (WeightedCNN) uses gaze as a spatial modulator for the RGB input:</p> <blockquote> <p>Gaze → Gaze Processor → Attention Weights</p> </blockquote> <blockquote> <p>RGB * Attention Weights → Modulated RGB → CNN → Features</p> </blockquote> <p>Our implementation processes the gaze heatmap to generate per-pixel weights:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Gaze processor network
</span><span class="n">self</span><span class="p">.</span><span class="n">gaze_processor</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># 3 channels to match RGB
</span>    <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>  <span class="c1"># Outputs weights between 0 and 1
</span><span class="p">)</span>

<span class="c1"># Forward pass
</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gaze_processor</span><span class="p">(</span><span class="n">gaze_heatmap</span><span class="p">)</span>
<span class="n">modulated_input</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">backbone</span><span class="p">(</span><span class="n">modulated_input</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Implementation Details</strong></p> <p>The key components are:</p> <ol> <li> <strong>Gaze Processor:</strong> A small CNN that converts the gaze heatmap to 3-channel weights</li> <li> <strong>Input Modulation:</strong> Element-wise multiplication of RGB inputs with weights</li> <li> <strong>Backbone CNN:</strong> Standard ResNet18 that processes the modulated input</li> </ol> <p>The sigmoid activation ensures weights are between 0 and 1, effectively acting as attention gates for each pixel and channel.</p> <p><strong>Theoretical Advantages</strong></p> <p>Weighted integration offers several advantages:</p> <ul> <li> <strong>Biological Plausibility:</strong> Most closely mimics how human visual attention modulates visual processing</li> <li> <strong>Input-Level Integration:</strong> Allows the standard CNN to process “pre-attended” inputs</li> <li> <strong>Preservation of Architecture:</strong> Works with any CNN backbone without structural changes</li> <li> <strong>Selective Enhancement:</strong> Can enhance or suppress different regions and features based on gaze</li> </ul> <p>The theoretical foundation is that attention acts as a filter or gain control mechanism in human visual processing, enhancing relevant signals and suppressing irrelevant ones before detailed processing.</p> <h5 id="comparison-and-insights"><strong>Comparison and Insights:</strong></h5> <p>Each integration method represents a different hypothesis about how gaze information should influence visual processing:</p> <ul> <li> <strong>Channel Integration:</strong> Gaze as an additional visual feature</li> <li> <strong>Bottleneck Integration:</strong> Gaze as a guide for feature selection</li> <li> <strong>Weighted Integration:</strong> Gaze as an input modulator</li> </ul> <p>Our experiments showed that all three methods improved over the baseline, but weighted integration consistently performed best, suggesting that early modulation of visual input is most effective for guiding visual search.</p> <hr> <h4 id="experimental-setup-and-metrics"><strong>Experimental Setup and Metrics</strong></h4> <p>Our experiments were conducted in the AI2-THOR simulator, which provides photorealistic 3D household environments. Specifically, we focused on kitchen scenes for our object search tasks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kitchen_scenes</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">FloorPlan</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">5</span> <span class="ow">or</span> <span class="mi">25</span> <span class="o">&lt;=</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">30</span><span class="p">]</span>
</code></pre></div></div> <p>This selection provided us with 11 distinct kitchen layouts with varying complexity and furniture arrangements. Each kitchen contains multiple objects in their natural locations, creating a challenging but realistic search environment.</p> <p>For our primary experiments, we selected the “Microwave” as the target object, as it:</p> <ul> <li>Is present in all kitchen scenes</li> <li>Can be located in different positions (counter, island, above stove)</li> <li>Has distinctive visual features</li> <li>Represents a realistic household search target</li> </ul> <p>To ensure robust evaluation, we randomized:</p> <ul> <li>The starting position of the agent in each episode</li> <li>The orientation of the agent (random rotation)</li> <li>The specific kitchen scene for each episode</li> </ul> <p>This randomization prevented the agent from memorizing specific paths and forced it to develop generalizable search strategies.</p> <h5 id="training-configuration"><strong>Training Configuration:</strong></h5> <p>We used the following training configuration as specified in our default.yaml file:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Environment settings</span>
<span class="na">environment</span><span class="pi">:</span>
  <span class="na">scene_type</span><span class="pi">:</span> <span class="s2">"</span><span class="s">FloorPlan"</span>
  <span class="na">target_object</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Microwave"</span>  <span class="c1"># Default target</span>
  <span class="na">grid_size</span><span class="pi">:</span> <span class="m">0.25</span>             <span class="c1"># Navigation grid resolution</span>
  <span class="na">rotation_step</span><span class="pi">:</span> <span class="m">45</span>           <span class="c1"># Agent rotation angle per step</span>
  <span class="na">field_of_view</span><span class="pi">:</span> <span class="m">90</span>           <span class="c1"># Agent's camera FOV</span>
  <span class="na">max_steps</span><span class="pi">:</span> <span class="m">500</span>              <span class="c1"># Maximum episode length</span>
  <span class="na">visibility_distance</span><span class="pi">:</span> <span class="m">1.5</span>    <span class="c1"># Object detection threshold</span>
  <span class="na">done_on_target_found</span><span class="pi">:</span> <span class="s">True</span>  <span class="c1"># Episode terminates on success</span>
</code></pre></div></div> <p>For our PPO algorithm, we used these hyperparameters:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model settings</span>
<span class="na">model</span><span class="pi">:</span>
  <span class="c1"># PPO hyperparameters</span>
  <span class="na">lr</span><span class="pi">:</span> <span class="s">3.0e-4</span>                        <span class="c1"># Learning rate</span>
  <span class="na">n_steps</span><span class="pi">:</span> <span class="m">128</span>                      <span class="c1"># Steps per update</span>
  <span class="na">batch_size</span><span class="pi">:</span> <span class="m">64</span>                    <span class="c1"># Mini-batch size</span>
  <span class="na">n_epochs</span><span class="pi">:</span> <span class="m">4</span>                       <span class="c1"># Epochs per update</span>
  <span class="na">gamma</span><span class="pi">:</span> <span class="m">0.99</span>                       <span class="c1"># Discount factor</span>
  <span class="na">gae_lambda</span><span class="pi">:</span> <span class="m">0.95</span>                  <span class="c1"># GAE parameter</span>
  <span class="na">clip_range</span><span class="pi">:</span> <span class="m">0.2</span>                   <span class="c1"># PPO clip range</span>
  <span class="na">ent_coef</span><span class="pi">:</span> <span class="m">0.01</span>                    <span class="c1"># Entropy coefficient</span>
  <span class="na">vf_coef</span><span class="pi">:</span> <span class="m">0.5</span>                      <span class="c1"># Value function coefficient</span>
  <span class="na">max_grad_norm</span><span class="pi">:</span> <span class="m">0.5</span>                <span class="c1"># Gradient clipping</span>
  <span class="na">features_extractor</span><span class="pi">:</span> <span class="s2">"</span><span class="s">WeightedCNN"</span> <span class="c1"># {"ChannelCNN", BottleneckCNN", "WeightedCNN"}</span>
  <span class="na">use_gaze</span><span class="pi">:</span> <span class="s">False</span>                   <span class="c1"># Toggled for gaze experiments</span>
</code></pre></div></div> <p>Each training run:</p> <ul> <li>Used a single environment wrapped with DummyVecEnv for Stable Baselines compatibility</li> <li>Trained for 100,000 timesteps per experiment (approximately 200 episodes)</li> <li>Used a maximum episode length of 500 steps</li> <li>Logged detailed metrics including episode rewards, success rates, and navigation efficiency</li> <li>Used TensorBoard for visualizing learning progress</li> <li>Created unique experiment directories for each run with timestamped IDs</li> </ul> <p>For our gaze integration research, we conducted experiments with:</p> <ol> <li>Baseline agent (standard PPO with no gaze information)</li> <li>Channel integration (gaze as 4th input channel)</li> <li>Bottleneck integration (separate RGB and gaze processing paths)</li> <li>Weighted integration (gaze-modulated RGB input)</li> </ol> <p>All configurations were evaluated across three environment types with increasing complexity to test generalization capabilities and the impact of visual distractions on search efficiency.</p> <h5 id="evaluation-metrics"><strong>Evaluation Metrics:</strong></h5> <p>We used several metrics to evaluate the performance of our agents:</p> <p><strong>Success Rate</strong></p> <p>The primary metric was success rate, defined as the percentage of episodes where the agent successfully found the target object. Success was determined by:</p> <ul> <li>The target object being visible in the agent’s field of view</li> <li>The agent being within 1.5 meters of the object</li> <li>The object occupying at least 5% of the agent’s visual field</li> </ul> <p>This success definition ensured that the agent not only found the object but was also in a position to potentially interact with it.</p> <p><strong>Efficiency (Steps to Completion)</strong></p> <p>We measured the average number of steps taken to find the target object in successful episodes. This metric evaluates the efficiency of the search strategy, with fewer steps indicating a more direct path to the target.</p> <p><strong>Exploration Coverage</strong></p> <p>We tracked the number of unique positions visited by the agent during an episode, divided by the total number of navigable positions in the scene. This metric evaluates how thoroughly the agent explores the environment.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"</span><span class="s">exploration_coverage</span><span class="sh">"</span><span class="p">:</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">visited_positions</span><span class="p">)</span> <span class="o">/</span> <span class="mf">100.0</span>  <span class="c1"># Normalize by approximate reachable positions
</span></code></pre></div></div> <p><strong>Gaze-Object Alignment</strong></p> <p>For gaze-guided methods, we measured the average IoU (Intersection over Union) between the predicted gaze heatmap and object regions. This metric evaluates how well the gaze prediction aligns with actual object locations.</p> <p><strong>Comparison Methodology</strong></p> <p>To ensure fair comparison between methods, we:</p> <ol> <li>Used identical environment configurations for all agents</li> <li>Initialized each agent with the same network architecture (except for gaze integration)</li> <li>Used the same hyperparameters across all methods when possible</li> </ol> <p>For our final evaluation, we tested each trained agent on 50 new episodes with:</p> <ul> <li>Random starting positions</li> <li>Previously unseen kitchen configurations</li> <li>Random target object placements</li> </ul> <p>This thorough evaluation methodology allowed us to rigorously compare the different integration methods and determine which approach most effectively leveraged gaze information for visual search.</p> <hr> <h4 id="results-and-analysis"><strong>Results and Analysis</strong></h4> <p>Our experiments across different environments and integration methods yielded several key findings:</p> <p><strong>General Performance Trends</strong></p> <p>The integration of gaze information significantly improved agent performance across all tested environments. Gaze integration consistently provided a substantial baseline reward increase compared to the standard PPO agent without gaze guidance. This performance gain was evident early in training, suggesting that gaze information provides useful priors for exploration.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_2/comparison_plots_train_general_new-480.webp 480w,/assets/img/project_2/comparison_plots_train_general_new-800.webp 800w,/assets/img/project_2/comparison_plots_train_general_new-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_2/comparison_plots_train_general_new.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="comparison_plots_train_general_new" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> comparison_plots_train_general_new </div> <p>Environment complexity played a crucial role in determining the relative advantages of gaze integration:</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_2/comparison_plots_train_floorplan1_all-480.webp 480w,/assets/img/project_2/comparison_plots_train_floorplan1_all-800.webp 800w,/assets/img/project_2/comparison_plots_train_floorplan1_all-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_2/comparison_plots_train_floorplan1_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="comparison_plots_train_floorplan1_all" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_2/comparison_plots_train_floorplan30_all-480.webp 480w,/assets/img/project_2/comparison_plots_train_floorplan30_all-800.webp 800w,/assets/img/project_2/comparison_plots_train_floorplan30_all-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_2/comparison_plots_train_floorplan30_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="comparison_plots_train_floorplan30_all" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> comparison_plots_train_floorplan1_all &amp; comparison_plots_train_floorplan30_all </div> <p><strong>Simple Environments (FloorPlan1):</strong> In simpler floor plans with minimal distractions, the baseline agent learned effective policies relatively quickly. The gaze-integrated models, being more complex, required additional training time to fully leverage their capabilities. However, once trained sufficiently, they still outperformed the baseline.</p> <p><strong>Complex Environments (FloorPlan30):</strong> In more complicated environments with numerous visual distractions, the advantage of gaze integration became more pronounced. These models helped agents start with a reasonable prior rather than navigating randomly, significantly reducing the exploration space. The baseline agent often wasted time exploring irrelevant areas of the environment.</p> <h5 id="ablation-studies"><strong>Ablation Studies:</strong></h5> <p>We conducted several ablation studies to understand the contribution of different gaze integration components:</p> <p><strong>Gaze Features vs. Gaze Rewards</strong></p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_2/comparison_plots_train_floorplan30_all_table_case-480.webp 480w,/assets/img/project_2/comparison_plots_train_floorplan30_all_table_case-800.webp 800w,/assets/img/project_2/comparison_plots_train_floorplan30_all_table_case-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_2/comparison_plots_train_floorplan30_all_table_case.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="comparison_plots_train_floorplan30_all_table_case" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> comparison_plots_train_floorplan30_all_table_case </div> <p>Our experiments revealed that gaze features alone did not significantly improve performance. However, when combined with gaze-based reward incentives, performance improved substantially. This suggests a synergistic effect between perception (gaze features) and motivation (reward structure).</p> <p><strong>Integration Methods Comparison</strong></p> <p>Each integration method showed distinct advantages:</p> <ol> <li> <strong>Channel Integration:</strong> The simplest approach, providing solid performance improvements with minimal architectural changes.</li> <li> <strong>Bottleneck Integration:</strong> Offered better feature representation but required more training time to converge.</li> <li> <strong>Weighted Integration:</strong> Showed the most promising results in complex environments by effectively focusing attention on relevant regions.</li> </ol> <p><strong>Training Duration Effects</strong></p> <p>All gaze integration methods required more training timesteps to reach optimal performance compared to the baseline. This reflects the increased model complexity and the need to learn effective representations of the gaze information. However, this investment in training time translated to superior performance, especially in challenging environments.</p> <h5 id="qualitative-analysis"><strong>Qualitative Analysis:</strong></h5> <p>Visual examination of agent behavior revealed striking differences between baseline and gaze-guided agents:</p> <blockquote> <p>TODO: Add Training Videos with Caption</p> </blockquote> <p><strong>Navigation Patterns</strong></p> <ul> <li> <strong>Baseline Agents:</strong> Often exhibited wandering behavior, moving in inefficient patterns and frequently revisiting already explored areas.</li> <li> <strong>Gaze-Guided Agents:</strong> Demonstrated more directed navigation, with clear purpose in movement and fewer redundant actions.</li> </ul> <p><strong>Policy Stability</strong></p> <p>An unexpected benefit of gaze integration was increased policy stability. The variance in performance across episodes was notably reduced when using gaze guidance, suggesting that human attention patterns provide a stabilizing influence on learning.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_2/comparison_plots_eval_floorplan30_all-480.webp 480w,/assets/img/project_2/comparison_plots_eval_floorplan30_all-800.webp 800w,/assets/img/project_2/comparison_plots_eval_floorplan30_all-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_2/comparison_plots_eval_floorplan30_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="comparison_plots_eval_floorplan30_all" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> comparison_plots_eval_floorplan30_all </div> <h5 id="insights-and-interpretations"><strong>Insights and Interpretations:</strong></h5> <p>Our results provide several important insights about attention in reinforcement learning:</p> <p><strong>The Synergy Effect</strong></p> <p>The most significant finding is that gaze features must be paired with appropriate reward incentives to be effective. This reveals a fundamental principle: agents need both a map (where to look) and a reason (why to look there). This parallels human cognition, where attention and motivation are deeply interconnected.</p> <p><strong>Complexity-Performance Relationship</strong></p> <p>The advantage of gaze guidance scales with environment complexity. In simple environments, standard RL approaches can efficiently discover solutions, but as complexity increases, human-like attention becomes increasingly valuable as a focusing mechanism.</p> <p>The performance advantages demonstrated by gaze-guided agents, particularly in complex environments, strongly support the hypothesis that human attention patterns can significantly improve reinforcement learning for visual search tasks.</p> <hr> <h4 id="challenges-and-solutions"><strong>Challenges and Solutions</strong></h4> <p>Implementing gaze-guided reinforcement learning presented several significant technical challenges that required innovative solutions:</p> <p><strong>Environment Integration Challenges</strong></p> <p>One of the primary challenges was integrating the gaze prediction model with the AI2Thor environment while maintaining compatibility with Stable Baselines 3’s PPO implementation.</p> <p>AI2Thor’s observation space and Stable Baselines’ expectations didn’t align directly, creating several integration issues:</p> <ol> <li> <strong>Observation Space Incompatibility:</strong> AI2Thor provides RGB observations, but we needed to add a gaze channel while keeping everything compatible with Stable Baselines’ expected formats.</li> <li> <strong>Reward Signal Modification:</strong> Incorporating gaze-based rewards required intercepting and modifying the environment’s reward calculation without disrupting the training loop.</li> <li> <strong>Feature Extractor Complexity:</strong> Each integration method required a custom feature extractor that could process both RGB and gaze information correctly.</li> <li> <strong>Memory Management:</strong> AI2Thor instances consumed substantial memory, causing occasional crashes during extended training runs.</li> </ol> <p><strong>Gaze Integration Challenges</strong></p> <p>The integration of gaze information into the reinforcement learning pipeline presented several architectural challenges:</p> <ol> <li> <strong>Neural Network Architecture:</strong> Designing effective architectures for each integration method required balancing complexity with performance.</li> <li> <strong>Training Stability:</strong> The more complex models showed higher variance during early training phases.</li> <li> <strong>Hyperparameter Tuning:</strong> Finding optimal hyperparameters across different integration methods proved challenging.</li> <li> <strong>Feature Representation:</strong> Ensuring that gaze features were properly normalized and represented within the model.</li> </ol> <h5 id="solutions"><strong>Solutions:</strong></h5> <p>We implemented several solutions to address these challenges:</p> <p><strong>Environment Wrapper Architecture</strong></p> <p>We developed a multi-layered wrapper architecture to seamlessly integrate gaze prediction with the AI2Thor environment:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create environment with gaze
</span><span class="n">env_fn</span> <span class="o">=</span> <span class="nf">create_env</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">gaze_model</span><span class="o">=</span><span class="n">gaze_model</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="nc">DummyVecEnv</span><span class="p">([</span><span class="n">env_fn</span><span class="p">])</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">create_env</code> function implemented a series of wrappers:</p> <ul> <li> <strong>GazeEnvWrapper:</strong> This wrapper intercepted observations from AI2Thor, ran them through the gaze prediction model, and augmented the observation with a gaze heatmap.</li> <li> <strong>GazePreprocessEnvWrapper:</strong> Ensured that observations were properly formatted with the correct shape and channel ordering.</li> <li> <strong>FlattenObservationWrapper:</strong> Made the observations compatible with Stable Baselines 3’s expectations.</li> </ul> <p>This layered approach allowed us to maintain clean separation of concerns while ensuring compatibility across all components.</p> <p><strong>Custom Feature Extractors</strong></p> <p>For each integration method, we implemented specialized feature extractors that properly handled the combined RGB and gaze information:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ChannelGazeExtractor</span><span class="p">(</span><span class="n">BaseFeaturesExtractor</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">observation_space</span><span class="p">,</span> <span class="n">features_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">observation_space</span><span class="p">,</span> <span class="n">features_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cnn</span> <span class="o">=</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">use_gaze</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">observations</span><span class="p">):</span>
        <span class="c1"># Process observations
</span>        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">observations</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
        
        <span class="c1"># Normalize RGB and extract gaze
</span>        <span class="n">rgb</span> <span class="o">=</span> <span class="n">observations</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">3</span><span class="p">].</span><span class="nf">float</span><span class="p">()</span> <span class="o">/</span> <span class="mf">255.0</span>
        <span class="n">gaze</span> <span class="o">=</span> <span class="n">observations</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">:</span><span class="mi">4</span><span class="p">].</span><span class="nf">float</span><span class="p">()</span> <span class="o">/</span> <span class="mf">255.0</span> <span class="k">if</span> <span class="n">observations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">3</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(...)</span>
        
        <span class="c1"># Forward pass
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">cnn</span><span class="p">(</span><span class="n">rgb</span><span class="p">,</span> <span class="n">gaze</span><span class="p">)</span>
</code></pre></div></div> <p>Similar extractors were implemented for bottleneck and weighted integration methods, each handling the observation processing appropriately.</p> <p><strong>Reward Augmentation</strong></p> <p>To incorporate gaze guidance into the reward structure, we implemented a reward augmentation mechanism:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_augment_reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">observation</span><span class="p">,</span> <span class="n">success</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Augment reward based on gaze alignment with important regions</span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">success</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">reward</span>  <span class="c1"># Don't modify success reward
</span>    
    <span class="c1"># Generate gaze heatmap
</span>    <span class="n">gaze_heatmap</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_predict_gaze</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
    
    <span class="c1"># Calculate reward based on alignment of agent's view with gaze predictions
</span>    <span class="n">alignment_score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_compute_alignment</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="n">gaze_heatmap</span><span class="p">)</span>
    
    <span class="c1"># Scale and add to base reward
</span>    <span class="n">gaze_reward</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">gaze_reward_scale</span> <span class="o">*</span> <span class="n">alignment_score</span>
    <span class="k">return</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gaze_reward</span>
</code></pre></div></div> <p>This allowed us to guide the agent’s exploration without interfering with the base environment reward structure.</p> <p><strong>Error Handling and Training Management</strong></p> <p>To address the stability issues with AI2Thor and extended training runs, we implemented several robustness improvements:</p> <ol> <li> <strong>Checkpoint Saving:</strong> Regular checkpoints to resume training after crashes</li> <li> <strong>Error Recovery:</strong> Try-except blocks with clean environment shutdown</li> <li> <strong>Resource Monitoring:</strong> Memory usage tracking and graceful degradation</li> <li> <strong>Training Resumption:</strong> Ability to continue training from saved checkpoints</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">try</span><span class="p">:</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">learn</span><span class="p">(</span>
        <span class="n">total_timesteps</span><span class="o">=</span><span class="n">total_timesteps</span><span class="p">,</span>
        <span class="n">callback</span><span class="o">=</span><span class="n">progress_callback</span>
    <span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="n">logger</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">ERROR during training: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="kn">import</span> <span class="n">traceback</span>
    <span class="n">logger</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="n">traceback</span><span class="p">.</span><span class="nf">format_exc</span><span class="p">())</span>
<span class="k">finally</span><span class="p">:</span>
    <span class="c1"># Clean up
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="sh">'</span><span class="s">env</span><span class="sh">'</span> <span class="ow">in</span> <span class="nf">locals</span><span class="p">():</span>
            <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
            <span class="n">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="sh">"</span><span class="s">Environment closed</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">cleanup_error</span><span class="p">:</span>
        <span class="n">logger</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Error during cleanup: </span><span class="si">{</span><span class="n">cleanup_error</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h5 id="lessons-learned"><strong>Lessons Learned:</strong></h5> <p>Through the development and experimentation process, we discovered several important insights:</p> <ul> <li> <strong>Training Duration Requirements:</strong> The more complex gaze-integrated models required significantly more training time than our initial estimates. For these complex environments and neural architectures, at least a million timesteps would be necessary to see optimal performance. Our experiments of 100,000 timesteps provided promising initial results but likely undersell the full potential of gaze integration.</li> <li> <strong>Simulator Limitations:</strong> AI2Thor, while powerful for realistic indoor navigation, had stability issues during extended runs. These crashes limited our ability to conduct the longer training sessions that would likely further demonstrate the advantages of gaze integration.</li> <li> <strong>Observation Preprocessing Importance:</strong> Proper normalization and preprocessing of observations proved critical for stable training, especially with the combined RGB and gaze inputs.</li> <li> <strong>Feature Extraction Architecture Impact:</strong> The choice of feature extraction architecture had a significant impact on both training stability and ultimate performance. Simpler architectures trained more quickly but had lower performance ceilings.</li> </ul> <p>Despite the limited training duration, our results showed clear signals that gaze integration provides substantial benefits, particularly in complex environments. With more training time, the performance gap would likely widen further in favor of gaze-guided approaches.</p> <hr> <h4 id="future-directions"><strong>Future Directions</strong></h4> <p>Our work points to several promising avenues for improvement:</p> <p><strong>Extended Training Duration</strong></p> <p>The most immediate improvement would be to extend training duration to at least 1 million timesteps. Our preliminary results with 50,000 timesteps already showed significant promise, and the learning curves suggested that performance would continue to improve with additional training. More extensive training would likely reveal the full potential of gaze integration, particularly for the more complex integration methods.</p> <p><strong>Architectural Refinements</strong></p> <p>The neural network architectures could be further refined to better leverage gaze information. More sophisticated attention mechanisms could better integrate gaze with visual features. Transformer-based architectures could be particularly effective for this purpose.</p> <p><strong>Improved Gaze Prediction Models</strong></p> <p>Our current gaze prediction model could be enhanced in several ways:</p> <ul> <li>Training gaze models specifically for object search tasks could provide more relevant attention patterns.</li> <li>Incorporating additional modalities like depth information could improve gaze prediction accuracy.</li> </ul> <h5 id="scaling-considerations"><strong>Scaling Considerations:</strong></h5> <p>Scaling this approach to more complex environments presents both challenges and opportunities:</p> <ul> <li>More complex environments and extended training would require significant computational resources. Techniques like distributed training and more efficient implementations would be necessary for large-scale deployment.</li> <li>As environments become more complex (e.g., multi-room buildings, outdoor settings), the value of gaze guidance likely increases, but so does the challenge of providing accurate gaze predictions. Research into more sophisticated gaze models would be needed.</li> <li>Extending beyond object search to more complex tasks like manipulation or sequential decision-making would require more sophisticated integration of gaze information throughout the task structure.</li> </ul> <hr> <h4 id="conclusion"><strong>Conclusion</strong></h4> <p>Our project demonstrates that integrating human visual attention patterns into reinforcement learning creates agents that navigate and search more efficiently. By developing three different integration methods—channel, bottleneck, and weighted, we’ve shown how gaze information can guide visual search in various environmental contexts.</p> <h5 id="key-message"><strong>Key Message:</strong></h5> <blockquote> <p>Human attention is a powerful guide for artificial intelligence, but unlocking its full potential requires more than simply showing an agent where to look—it requires teaching the agent why looking there matters. By aligning what an agent sees with what it values through a combination of perceptual guidance and reward shaping, we can create systems that navigate complex visual environments with greater purpose and efficiency. This approach represents a step toward more human-like artificial intelligence that doesn’t just mimic human behavior but inherits the underlying cognitive principles that make human visual search so remarkably efficient.</p> </blockquote> <hr> <h4 id="resources"><strong>Resources</strong></h4> <ul> <li> <strong>Code Repository:</strong> Link to GitHub</li> <li> <strong>Data/Checkpoints:</strong> Link to Drive</li> <li> <strong>Paper/Documentation:</strong> Any publications or documentation</li> <li> <strong>Demo:</strong> Any videos or interactive demonstrations</li> <li> <strong>References:</strong> Key papers and resources that informed your work</li> </ul> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab",title:"Sharing personal reflections - Thoughts Tab",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>