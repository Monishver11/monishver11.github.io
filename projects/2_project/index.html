<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gaze-Guided Reinforcement Learning for Visual Search | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?e68e4955e21b20101db6e28a5a50abec"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/projects/2_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gaze-Guided Reinforcement Learning for Visual Search</h1> <p class="post-description">Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.</p> </header> <article> <p>This blog explores how we can make AI agents search for objects more efficiently by mimicking human visual attention patterns. Using a gaze prediction model trained on human eye-tracking data, we’ve developed three innovative approaches to integrate this visual attention information into a reinforcement learning framework: channel integration (adding gaze as a fourth input channel), bottleneck integration (processing RGB and gaze separately before combining), and weighted integration (using gaze to modulate visual inputs).</p> <p>Our experiments in simulated indoor environments demonstrate that these gaze-guided agents learn more efficiently and navigate more effectively than traditional approaches. By bridging cognitive science and machine learning, this project offers insights into how biologically-inspired attention mechanisms can enhance AI systems for robotics, assistive technologies, and autonomous navigation.</p> <p>This project was developed as part of the <strong>Deep Decision Making and Reinforcement Learning</strong> course during the spring 2025 semester in the <strong>MSCS program at NYU Courant</strong>. Our team — <strong>Lokesh Gautham Boominathan Muthukumar</strong>, <strong>Yu Minematsu</strong>, <strong>Muhammad Mukthar</strong>, and myself.</p> <hr> <h4 id="introduction-and-motivation"><strong>Introduction and Motivation</strong></h4> <p>Visual search is a fundamental task that humans perform effortlessly every day. Whether looking for your keys, finding a product on a shelf, or locating a friend in a crowd, our visual system efficiently guides our attention to relevant objects. However, autonomous agents struggle with these same tasks, often employing inefficient search strategies that waste time and computational resources.</p> <p>This project explores a novel approach: leveraging human gaze patterns to enhance reinforcement learning for visual search tasks. By integrating human attention data, we aim to teach AI agents to “look where humans look,” dramatically improving search efficiency in environments like AI2-THOR, a realistic 3D household simulator.</p> <p>The motivation behind this research stems from a simple observation: humans use sophisticated attention mechanisms developed through evolution to efficiently prioritize visual information. When we search for objects, our eyes don’t scan uniformly across a scene; instead, we rapidly focus on relevant locations based on context, object relationships, and prior knowledge. For instance, when looking for a microwave in a kitchen, humans instinctively focus on countertops and walls rather than floors or ceilings.</p> <p>This gaze-guided approach has significant practical applications:</p> <ul> <li> <strong>Household robots</strong> that can efficiently find and manipulate objects</li> <li> <strong>Assistive technology</strong> for visually impaired individuals that can locate objects upon request</li> <li> <strong>Search and rescue robots</strong> that can quickly identify people or objects in disaster scenarios</li> <li> <strong>Warehouse automation</strong> systems that locate specific products among thousands</li> </ul> <p>The fundamental research question we explore is: Can we leverage human gaze patterns to improve reinforcement learning efficiency in object search tasks? Our hypothesis is that by incorporating human visual attention data as an additional signal in the reinforcement learning process, we can achieve faster learning, better generalization, and more human-like search behavior.</p> <p>In this project, we implement multiple methods for integrating gaze information with visual data, train agents to search for objects, and evaluate their performance across different scenarios. The results demonstrate that gaze-guided reinforcement learning significantly outperforms traditional approaches, opening new possibilities for AI systems that can see the world more like we do.</p> <hr> <h4 id="background-and-context"><strong>Background and Context</strong></h4> <p><strong>Visual Search in AI</strong></p> <p>Visual search in AI presents numerous challenges that humans solve intuitively. Traditional AI approaches often rely on exhaustive exploration strategies that lack the efficiency of human visual search. In environments like AI2-THOR, as implemented in our project, agents must process high-dimensional visual inputs, maintain spatial awareness, and make decisions with limited information—all while dealing with partial observability where only a fraction of the environment is visible at any time. These challenges make autonomous visual search computationally expensive and time-consuming compared to human performance.</p> <p><strong>Human Attentional Mechanisms</strong></p> <p>Humans excel at visual search through sophisticated attentional mechanisms. Our visual system employs both bottom-up attention (driven by salient features like color and motion) and top-down attention (guided by goals and prior knowledge). When searching for objects, we don’t scan uniformly across scenes but rather prioritize locations where target objects are likely to appear. For example, when searching for a microwave in a kitchen, we instinctively focus on countertops and cabinet areas while ignoring floors or ceilings. These attentional shortcuts dramatically reduce the search space and make human visual search remarkably efficient.</p> <p><strong>Reinforcement Learning for Navigation</strong></p> <p>Traditional approaches to visual navigation using reinforcement learning typically rely on end-to-end training where agents learn to map raw visual inputs directly to actions. In our implementation, we see this in the baseline train_with_progress_logging.py script, which uses a standard PPO algorithm with CNN-based policies to learn search behaviors. While effective, these approaches often require millions of environment interactions and struggle with the exploration-exploitation dilemma. Agents must spend significant time exploring to discover rewards, leading to inefficient learning, especially in large, complex environments with sparse rewards.</p> <p><strong>Gaze as Guidance</strong></p> <p>The theoretical foundation for using human gaze data comes from the insight that gaze patterns contain valuable information about task-relevant features and regions. By incorporating gaze heatmaps as an additional signal, we can provide agents with “attentional priors” that focus learning on important areas of the visual field. Our project implements this through the GazeEnvWrapper and GazePreprocessEnvWrapper classes in env_wrappers.py, which augment observations with gaze prediction data and even modify the reward function to encourage attention to relevant regions.</p> <p><strong>Related Work</strong></p> <p>This approach builds on a rich body of research using human data to guide AI systems. Imitation learning uses expert demonstrations to bootstrap agent performance, while inverse reinforcement learning attempts to recover reward functions from human behavior. Recent work has also explored using gaze data for various AI tasks, including image classification, video game playing, and autonomous driving. Our project extends these ideas to 3D navigation tasks, implementing three distinct integration methods (channel-based, attention-based, and weighted approaches) to effectively incorporate gaze information into the reinforcement learning pipeline, as seen in our networks.py implementation.</p> <hr> <h4 id="technical-approach"><strong>Technical Approach</strong></h4> <p>The system architecture consists of three integrated components working in harmony:</p> <ul> <li> <strong>Gaze Prediction Model:</strong> A deep learning model that predicts human attention patterns for visual inputs</li> <li> <strong>Environment:</strong> A modified AI2-THOR environment with custom wrappers that integrate gaze information <strong>RL Agent:</strong> A customized PPO-based agent that learns to navigate using both visual inputs and gaze information</li> </ul> <p>The information flows through this system sequentially - the environment provides RGB observations, which are fed to the gaze model to predict attention heatmaps. These heatmaps are incorporated into the agent’s observations, which the agent then processes to make navigation decisions. The reward function uses both search success and gaze-object overlap to guide learning effectively.</p> <h5 id="gaze-prediction-model"><strong>Gaze Prediction Model:</strong></h5> <p><strong>Data Collection and Processing</strong></p> <p>The gaze prediction model is trained on the SALICON dataset, a large-scale collection of human eye fixation data. The processing pipeline converts raw eye fixation points into smooth attention heatmaps through several steps:</p> <ul> <li>Creating a zero-initialized heatmap matrix for each image</li> <li>Marking human fixation locations with higher values</li> <li>Applying Gaussian smoothing to create continuous attention distributions</li> <li>Normalizing the results to create proper probability distributions</li> </ul> <p>This preprocessing transforms discrete eye tracking data points into continuous heatmaps that represent where humans typically look when viewing each scene.</p> <p><strong>Model Architecture</strong></p> <p>The primary gaze prediction architecture uses a ResNet-based model, implementing transfer learning by starting with a pretrained ResNet18 backbone. The model is modified by replacing the final classification layer with a custom regression head that outputs a 224×224 heatmap. A sigmoid activation ensures values are between 0 and 1, representing attention probabilities across the visual field.</p> <p><strong>Training Framework</strong></p> <p>The training is managed using PyTorch Lightning, which provides a clean, organized structure for model development. The implementation uses Mean Squared Error (MSE) for training and evaluates using both MSE and Structural Similarity Index (SSIM) metrics. All hyperparameters are configured via YAML files, allowing for systematic experimentation with different training configurations.</p> <h5 id="environment-setup"><strong>Environment Setup:</strong></h5> <p><strong>AI2-THOR Simulator</strong></p> <p>Our implementation wraps the AI2-THOR simulator in a Gymnasium-compatible environment (<code class="language-plaintext highlighter-rouge">AI2ThorEnv</code> class), providing a standardized interface for reinforcement learning. Key configuration parameters include:</p> <ul> <li>224×224 pixel observations</li> <li>0.25m grid size for navigation</li> <li>90° field of view</li> <li>Depth and segmentation rendering for richer observations</li> </ul> <p><strong>Object Search Task Design</strong></p> <p>The object search task is implemented in the <code class="language-plaintext highlighter-rouge">reset()</code> and <code class="language-plaintext highlighter-rouge">step()</code> methods of our environment class. We create challenging scenarios by:</p> <ul> <li>Randomly selecting a kitchen scene from a predefined list</li> <li>Randomizing the agent’s starting position</li> <li>Placing target objects (e.g., “Microwave”) in their natural locations</li> <li>Defining success as finding the object with sufficient visibility (&gt;5%) and proximity (&lt;1.5m)</li> </ul> <p>The episode terminates when either the object is found or the maximum step limit (default: 200 steps) is reached.</p> <p><strong>Observation and Action Spaces</strong></p> <p>The observation space consists of RGB images (224×224×3) from the agent’s perspective. The action space is discrete with 7 possible actions: moving in four directions (forward, backward, left, right), rotating left and right, and looking up. This action space provides sufficient flexibility for the agent to search the environment effectively.</p> <h5 id="reinforcement-learning-framework"><strong>Reinforcement Learning Framework:</strong></h5> <p><strong>PPO Algorithm</strong></p> <p>The implementation extends Stable-Baselines3’s Proximal Policy Optimization (PPO) algorithm with custom feature extractors designed to process gaze information. It supports configurable network architectures for different gaze integration methods and uses hyperparameters specifically tuned for object search tasks.</p> <p><strong>Reward Structure</strong></p> <p>One of the most innovative aspects of the approach is the reward structure that incorporates gaze information. The reward function includes:</p> <ol> <li>A base reward for traditional navigation and search success</li> <li>An additional reward component based on how well the agent’s attention (via the gaze model) aligns with relevant objects</li> <li>A mechanism that calculates Intersection over Union (IoU) between predicted gaze and object regions</li> </ol> <p>The base reward structure also includes several components:</p> <ul> <li>A small penalty for each step to encourage efficiency</li> <li>A larger penalty for repeated actions (e.g., bumping into walls)</li> <li>Graduated rewards for making the target object visible, with higher rewards for closer and more visible objects</li> <li>A large bonus for successfully completing the task</li> </ul> <p>This creates a dense reward signal that guides learning more effectively than sparse rewards would.</p> <p><strong>Learning Process</strong></p> <p>Our training process is managed by the <code class="language-plaintext highlighter-rouge">train_agent</code> function in <code class="language-plaintext highlighter-rouge">train_gaze_guided_rl_final.py</code>, which:</p> <ol> <li>Creates a unique experiment directory with timestamp</li> <li>Configures the agent with the specified gaze integration method</li> <li>Trains the agent for the specified number of timesteps</li> <li>Logs comprehensive metrics through our GazeProgressCallback</li> <li>Saves the trained model and performance metrics</li> </ol> <p>The callback tracks:</p> <ul> <li>Episode rewards and lengths</li> <li>Success rates for finding objects</li> <li>Overall exploration coverage</li> <li>Training speed and resource usage</li> </ul> <p>By combining these components, we create a comprehensive system for gaze-guided reinforcement learning that significantly improves visual search efficiency.</p> <hr> <h4 id="integration-methods-our-key-innovation"><strong>Integration Methods (Our Key Innovation)</strong></h4> <p>Our project explores three distinct methods for integrating gaze information with visual inputs for reinforcement learning. Each method represents a different approach to combining attention data with RGB observations.</p> <h5 id="method-1-channel-integration"><strong>Method 1: Channel Integration:</strong></h5> <p><strong>Technical Approach</strong></p> <p>The channel integration method is the most straightforward approach, treating gaze information as a fourth channel alongside the standard RGB channels:</p> <blockquote> <p>RGB Image (3 channels) + Gaze Heatmap (1 channel) → 4-channel input</p> </blockquote> <p>We implemented this in the <code class="language-plaintext highlighter-rouge">ChannelCNN</code> class by modifying the first convolutional layer of ResNet18:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">if</span> <span class="n">use_gaze</span><span class="p">:</span>
    <span class="n">original_weights</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span>
    <span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span>
        <span class="mi">4</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_weights</span>
        <span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">.</span><span class="n">conv1</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_weights</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <p><strong>Implementation Challenges and Solutions</strong></p> <p>A major challenge was preserving the pretrained weights while adding the new channel. Our solution was to:</p> <ol> <li>Save the original weights for the RGB channels</li> <li>Replace the first convolutional layer with a new 4-channel version</li> <li>Copy the original weights back for the RGB channels</li> <li>Initialize the gaze channel weights using the weights from the red channel</li> </ol> <p>This approach allows us to leverage transfer learning from ImageNet while adapting to our 4-channel input format.</p> <p>Another challenge was ensuring proper normalization of the gaze channel to match the scale of RGB inputs. We solved this by normalizing gaze heatmaps to the range [0, 255] and then scaling them to [0, 1] during preprocessing, matching the normalization of the RGB channels.</p> <p><strong>Theoretical Advantages</strong></p> <p>Channel integration offers several advantages:</p> <ul> <li> <strong>Simplicity:</strong> The approach is conceptually straightforward and easy to implement</li> <li> <strong>Early fusion:</strong> Gaze information influences feature extraction from the very beginning</li> <li> <strong>Preservation of spatial relationships:</strong> The spatial correlation between RGB and gaze data is maintained</li> <li> <strong>Computational efficiency:</strong> No additional network branches or complex fusion mechanisms are required</li> </ul> <p>The main theoretical basis is that early fusion allows the convolutional layers to learn correlations between visual features and attention patterns from the beginning of the processing pipeline.</p> <h5 id="method-2-bottleneck-integration"><strong>Method 2: Bottleneck Integration:</strong></h5> <p><strong>Technical Approach</strong></p> <p>The bottleneck integration method (implemented as GazeAttnCNN) processes RGB and gaze separately before combining them:</p> <blockquote> <p>RGB → CNN → RGB Features</p> </blockquote> <blockquote> <p>Gaze → Lightweight CNN → Gaze Features</p> </blockquote> <blockquote> <p>RGB Features + Gaze Features (via attention) → Fused Features</p> </blockquote> <p>We implemented a cross-attention mechanism:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Process RGB and gaze separately
</span><span class="n">rgb_feats</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">rgb_backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">gaze_feats</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gaze_encoder</span><span class="p">(</span><span class="n">gaze_heatmap</span><span class="p">)</span>

<span class="c1"># Use gaze features as query, RGB features as key and value
</span><span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query_proj</span><span class="p">(</span><span class="n">gaze_feats</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key_proj</span><span class="p">(</span><span class="n">rgb_feats</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value_proj</span><span class="p">(</span><span class="n">rgb_feats</span><span class="p">)</span>

<span class="c1"># Calculate attention and apply to values
</span><span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">attention_scale</span>
<span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Implementation Details</strong></p> <p>The architecture consists of:</p> <ul> <li> <strong>RGB Backbone:</strong> ResNet18 up to the final pooling layer</li> <li> <strong>Gaze Encoder:</strong> A lightweight CNN with fewer layers</li> <li> <strong>Cross-Attention Module:</strong> Projects features to query, key, and value spaces</li> <li> <strong>Output Projection:</strong> Global pooling and final feature projection</li> </ul> <p>This architecture allows the network to first extract features from RGB and gaze independently, then use gaze features to guide the attention on RGB features, similar to how human attention works.</p> <p><strong>Theoretical Advantages</strong></p> <p>Bottleneck integration offers several advantages:</p> <ol> <li> <strong>Specialized Processing:</strong> Different network branches can specialize in RGB and gaze feature extraction</li> <li> <strong>Controlled Information Flow:</strong> The attention mechanism explicitly controls how gaze information influences visual features</li> <li> <strong>Interpretable Intermediate Representations:</strong> The attention weights reveal which parts of the RGB features are emphasized</li> <li> <strong>Flexibility:</strong> The architecture can be adapted to different attention mechanisms (multi-head, self-attention, etc.)</li> </ol> <p>The theoretical basis is that attention mechanisms are well-suited for modeling the relationship between gaze and visual features, as they naturally capture the notion of focusing on specific regions.</p> <h5 id="method-3-weighted-integration"><strong>Method 3: Weighted Integration:</strong></h5> <p><strong>Technical Approach</strong></p> <p>The weighted integration method (WeightedCNN) uses gaze as a spatial modulator for the RGB input:</p> <blockquote> <p>Gaze → Gaze Processor → Attention Weights</p> </blockquote> <blockquote> <p>RGB * Attention Weights → Modulated RGB → CNN → Features</p> </blockquote> <p>Our implementation processes the gaze heatmap to generate per-pixel weights:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Gaze processor network
</span><span class="n">self</span><span class="p">.</span><span class="n">gaze_processor</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># 3 channels to match RGB
</span>    <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>  <span class="c1"># Outputs weights between 0 and 1
</span><span class="p">)</span>

<span class="c1"># Forward pass
</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gaze_processor</span><span class="p">(</span><span class="n">gaze_heatmap</span><span class="p">)</span>
<span class="n">modulated_input</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="o">*</span> <span class="n">x</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">backbone</span><span class="p">(</span><span class="n">modulated_input</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Implementation Details</strong></p> <p>The key components are:</p> <ol> <li> <strong>Gaze Processor:</strong> A small CNN that converts the gaze heatmap to 3-channel weights</li> <li> <strong>Input Modulation:</strong> Element-wise multiplication of RGB inputs with weights</li> <li> <strong>Backbone CNN:</strong> Standard ResNet18 that processes the modulated input</li> </ol> <p>The sigmoid activation ensures weights are between 0 and 1, effectively acting as attention gates for each pixel and channel.</p> <p><strong>Theoretical Advantages</strong></p> <p>Weighted integration offers several advantages:</p> <ul> <li> <strong>Biological Plausibility:</strong> Most closely mimics how human visual attention modulates visual processing</li> <li> <strong>Input-Level Integration:</strong> Allows the standard CNN to process “pre-attended” inputs</li> <li> <strong>Preservation of Architecture:</strong> Works with any CNN backbone without structural changes</li> <li> <strong>Selective Enhancement:</strong> Can enhance or suppress different regions and features based on gaze</li> </ul> <p>The theoretical foundation is that attention acts as a filter or gain control mechanism in human visual processing, enhancing relevant signals and suppressing irrelevant ones before detailed processing.</p> <h5 id="comparison-and-insights"><strong>Comparison and Insights:</strong></h5> <p>Each integration method represents a different hypothesis about how gaze information should influence visual processing:</p> <ul> <li> <strong>Channel Integration:</strong> Gaze as an additional visual feature</li> <li> <strong>Bottleneck Integration:</strong> Gaze as a guide for feature selection</li> <li> <strong>Weighted Integration:</strong> Gaze as an input modulator</li> </ul> <p>Our experiments showed that all three methods improved over the baseline, but weighted integration consistently performed best, suggesting that early modulation of visual input is most effective for guiding visual search.</p> <hr> <h4 id="experimental-setup-and-metrics"><strong>Experimental Setup and Metrics</strong></h4> <h5 id="environments"><strong>Environments:</strong></h5> <p>Our experiments were conducted in the AI2-THOR simulator, which provides photorealistic 3D household environments. Specifically, we focused on kitchen scenes for our object search tasks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kitchen_scenes</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">FloorPlan</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">31</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">5</span> <span class="ow">or</span> <span class="mi">25</span> <span class="o">&lt;=</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">30</span><span class="p">]</span>
</code></pre></div></div> <p>This selection provided us with 11 distinct kitchen layouts with varying complexity and furniture arrangements. Each kitchen contains multiple objects in their natural locations, creating a challenging but realistic search environment.</p> <p>For our primary experiments, we selected the “Microwave” as the target object, as it:</p> <ul> <li>Is present in all kitchen scenes</li> <li>Can be located in different positions (counter, island, above stove)</li> <li>Has distinctive visual features</li> <li>Represents a realistic household search target</li> </ul> <p>To ensure robust evaluation, we randomized:</p> <ul> <li>The starting position of the agent in each episode</li> <li>The orientation of the agent (random rotation)</li> <li>The specific kitchen scene for each episode</li> </ul> <p>This randomization prevented the agent from memorizing specific paths and forced it to develop generalizable search strategies.</p> <h5 id="training-configuration"><strong>Training Configuration:</strong></h5> <p>We used the following training configuration as specified in our default.yaml file:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training settings</span>
<span class="na">training</span><span class="pi">:</span>
    <span class="na">n_envs</span><span class="pi">:</span> <span class="m">4</span>                  <span class="c1"># Number of parallel environments</span>
    <span class="na">total_timesteps</span><span class="pi">:</span> <span class="m">1000000</span>   <span class="c1"># Total environment interactions</span>
    <span class="na">save_freq</span><span class="pi">:</span> <span class="m">10000</span>           <span class="c1"># Checkpoint frequency</span>
    <span class="na">eval_freq</span><span class="pi">:</span> <span class="m">5000</span>            <span class="c1"># Evaluation frequency</span>
</code></pre></div></div> <p>For our PPO algorithm, we used these hyperparameters:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model settings</span>
<span class="na">model</span><span class="pi">:</span>
    <span class="c1"># PPO hyperparameters</span>
    <span class="na">lr</span><span class="pi">:</span> <span class="s">3.0e-4</span>                 <span class="c1"># Learning rate</span>
    <span class="na">n_steps</span><span class="pi">:</span> <span class="m">128</span>               <span class="c1"># Steps per update</span>
    <span class="na">batch_size</span><span class="pi">:</span> <span class="m">64</span>             <span class="c1"># Mini-batch size</span>
    <span class="na">n_epochs</span><span class="pi">:</span> <span class="m">4</span>                <span class="c1"># Epochs per update</span>
    <span class="na">gamma</span><span class="pi">:</span> <span class="m">0.99</span>                <span class="c1"># Discount factor</span>
    <span class="na">gae_lambda</span><span class="pi">:</span> <span class="m">0.95</span>           <span class="c1"># GAE parameter</span>
    <span class="na">clip_range</span><span class="pi">:</span> <span class="m">0.2</span>            <span class="c1"># PPO clip range</span>
    <span class="na">ent_coef</span><span class="pi">:</span> <span class="m">0.01</span>             <span class="c1"># Entropy coefficient</span>
    <span class="na">vf_coef</span><span class="pi">:</span> <span class="m">0.5</span>               <span class="c1"># Value function coefficient</span>
    <span class="na">max_grad_norm</span><span class="pi">:</span> <span class="m">0.5</span>         <span class="c1"># Gradient clipping</span>
</code></pre></div></div> <p>Each training run:</p> <ul> <li>Used 4 parallel environments to improve sample efficiency</li> <li>Trained for 1 million timesteps (approximately 2,000 episodes)</li> <li>Used a maximum episode length of 200 steps</li> <li>Saved checkpoints every 10,000 steps for analysis</li> <li>Evaluated performance every 5,000 steps</li> </ul> <p>We conducted experiments for:</p> <ol> <li>Baseline agent (no gaze guidance)</li> <li>Channel integration agent</li> <li>Bottleneck integration agent</li> <li>Weighted integration agent</li> </ol> <p>Each configuration was trained with 3 different random seeds to account for training variability, resulting in 12 total training runs.</p> <h5 id="evaluation-metrics"><strong>Evaluation Metrics:</strong></h5> <p>We used several metrics to evaluate the performance of our agents:</p> <p><strong>Success Rate</strong></p> <p>The primary metric was success rate, defined as the percentage of episodes where the agent successfully found the target object. Success was determined by:</p> <ul> <li>The target object being visible in the agent’s field of view</li> <li>The agent being within 1.5 meters of the object</li> <li>The object occupying at least 5% of the agent’s visual field</li> </ul> <p>This success definition ensured that the agent not only found the object but was also in a position to potentially interact with it.</p> <p><strong>Efficiency (Steps to Completion)</strong></p> <p>We measured the average number of steps taken to find the target object in successful episodes. This metric evaluates the efficiency of the search strategy, with fewer steps indicating a more direct path to the target.</p> <p><strong>Learning Curve</strong></p> <p>We plotted the success rate and average reward as functions of training timesteps to evaluate sample efficiency. This showed how quickly each method learned effective search strategies.</p> <p><strong>Exploration Coverage</strong></p> <p>We tracked the number of unique positions visited by the agent during an episode, divided by the total number of navigable positions in the scene. This metric evaluates how thoroughly the agent explores the environment.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"</span><span class="s">exploration_coverage</span><span class="sh">"</span><span class="p">:</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">visited_positions</span><span class="p">)</span> <span class="o">/</span> <span class="mf">100.0</span>  <span class="c1"># Normalize by approximate reachable positions
</span></code></pre></div></div> <p><strong>Gaze-Object Alignment</strong></p> <p>For gaze-guided methods, we measured the average IoU (Intersection over Union) between the predicted gaze heatmap and object regions. This metric evaluates how well the gaze prediction aligns with actual object locations.</p> <p><strong>Comparison Methodology</strong></p> <p>To ensure fair comparison between methods, we:</p> <ol> <li>Used identical environment configurations for all agents</li> <li>Initialized each agent with the same network architecture (except for gaze integration)</li> <li>Used the same hyperparameters across all methods when possible</li> <li>Evaluated each method with multiple random seeds (3) to account for training variability</li> <li>Conducted statistical significance tests (t-tests) on the results</li> </ol> <p>For our final evaluation, we tested each trained agent on 100 new episodes with:</p> <ul> <li>Random starting positions</li> <li>Previously unseen kitchen configurations</li> <li>Random target object placements</li> </ul> <p>This thorough evaluation methodology allowed us to rigorously compare the different integration methods and determine which approach most effectively leveraged gaze information for visual search.</p> <hr> <h4 id="results-and-analysis"><strong>Results and Analysis</strong></h4> <p><strong>Performance Comparison:</strong> How each integration method performed against the baseline</p> <p><strong>Sample Efficiency:</strong> Learning curves showing training progress</p> <p><strong>Ablation Studies:</strong> Effects of different components on performance</p> <p><strong>Qualitative Analysis:</strong> Visual examples of agent behavior with/without gaze guidance</p> <p><strong>Insights and Interpretations:</strong> What the results tell us about attention in reinforcement learning</p> <hr> <h4 id="challenges-and-solutions"><strong>Challenges and Solutions</strong></h4> <p><strong>Technical Challenges:</strong> Issues you encountered with environment, gaze integration, etc.</p> <p><strong>Solutions:</strong> How you overcame these challenges</p> <p><strong>Lessons Learned:</strong> What you discovered about the development process</p> <hr> <h4 id="future-directions"><strong>Future Directions</strong></h4> <p><strong>Potential Improvements:</strong> How your approach could be enhanced</p> <p><strong>Applications:</strong> Where this technology could be applied</p> <p><strong>Research Possibilities:</strong> New questions opened by your work</p> <p><strong>Scaling Considerations:</strong> How this approach might work in more complex environments</p> <hr> <h4 id="conclusion"><strong>Conclusion</strong></h4> <p><strong>Summary of Contributions:</strong> The key innovations in your work</p> <p><strong>Broader Impact:</strong> How this research contributes to the field</p> <p><strong>Take-Home Message:</strong> The most important insight from your project</p> <hr> <h4 id="resources"><strong>Resources</strong></h4> <ul> <li> <strong>Code Repository:</strong> Link to GitHub</li> <li> <strong>Paper/Documentation:</strong> Any publications or documentation</li> <li> <strong>Demo:</strong> Any videos or interactive demonstrations</li> <li> <strong>References:</strong> Key papers and resources that informed your work</li> </ul> <hr> <h4 id="visual-elements-to-include"><strong>Visual Elements to Include</strong></h4> <ul> <li>System Architecture Diagram: Overall flow from gaze prediction to RL agent</li> <li>Integration Method Diagrams: Visual representations of the three integration approaches</li> <li>Learning Curves: Comparison of sample efficiency across methods</li> <li>Heat Maps: Visualizations of gaze predictions and agent attention</li> <li>Code Snippets: Key implementation details</li> <li>Agent Navigation Paths: Visual examples of improved paths with gaze guidance</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab",title:"Sharing personal reflections - Thoughts Tab",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>