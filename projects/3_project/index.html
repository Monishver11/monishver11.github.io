<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Swap Regret 2.0 | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="This blog post unpacks the " swap regret paper through slide-by-slide insights showing how the treeswap algorithm closes gap between external and advancing equilibrium computation in game theory reinforcement learning.> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/projects/3_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Swap Regret 2.0</h1> <p class="post-description">This blog post unpacks the "Swap Regret 2.0" paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.</p> </header> <article> <p>In this blog post, I’ll walk through my presentation on the groundbreaking paper <a href="https://arxiv.org/pdf/2310.19786" rel="external nofollow noopener" target="_blank">“From External to Swap Regret 2.0: An Efficient Reduction for Large Action Spaces”</a> by Chen et al., which introduces significant advances in online learning algorithms and game theory.</p> <p>This paper tackles a long-standing problem in the field: the exponential gap between external regret and swap regret in online learning settings. The authors present TreeSwap, a novel algorithm that bridges this gap and enables efficient swap regret minimization even with large or infinite action spaces.</p> <h5 id="how-to-use-this-guide"><strong>How to use this guide:</strong></h5> <ol> <li> <p><strong>First steps</strong>: I recommend reading pages 1-20 of the <a href="https://arxiv.org/pdf/2310.19786" rel="external nofollow noopener" target="_blank">original paper</a> to get familiar with the basic concepts and main results.</p> </li> <li> <p><strong>Follow along</strong>: Open my <a href="https://drive.google.com/file/d/1Iv2VlHRLW3pbN4DWd4CFByxSo1p8kFGP/view?usp=sharing" rel="external nofollow noopener" target="_blank">presentation slides</a> side-by-side with this blog post. Each section below corresponds to specific slides and elaborates on the key points.</p> </li> <li> <p><strong>Slide-by-slide explanations</strong>: I’ve expanded each slide’s bullet points into coherent explanations that should make the technical concepts more accessible.</p> </li> </ol> <p>Let’s begin by understanding the fundamentals of regret minimization and why this paper represents such an important advancement in the field.</p> <h5 id="slide-1-online-learning-introduction"><strong>Slide 1: Online Learning Introduction</strong></h5> <p>No-regret learning is a fundamental paradigm in machine learning where an agent repeatedly makes decisions and aims to minimize its regret over time. What is regret exactly? It measures how much worse the agent performs compared to the best fixed strategy in hindsight. The core idea is that the agent’s cumulative regret should grow sublinearly with the number of rounds, meaning that on average, the per-round regret approaches zero as time progresses.</p> <p>This concept has profound implications for game theory. When players in a game employ no-regret learning algorithms, something remarkable happens: the empirical distribution of their strategies converges to an equilibrium. This is a powerful result that connects online learning to game-theoretic solution concepts.</p> <p>The type of equilibrium we get depends critically on the specific notion of regret that players minimize. This creates a fascinating relationship between learning dynamics and equilibrium concepts that has been central to algorithmic game theory for decades.</p> <h5 id="slide-2-continuing-on-no-regret-learning"><strong>Slide 2: Continuing on No-Regret Learning</strong></h5> <p>Let’s explore the connection between regret types and equilibrium concepts more concretely. When players minimize external regret (comparing their performance to the best single fixed action), their joint play converges to what’s called a Coarse Correlated Equilibrium (CCE).</p> <p>In a CCE, no player can improve their utility by unilaterally switching to a fixed action in hindsight. Essentially, this means that if a mediator recommends actions according to the equilibrium distribution, no player has an incentive to ignore this recommendation and play a different fixed action instead.</p> <p>CCE is a relaxation of Nash equilibrium. While Nash requires independent actions and mutual best responses, CCE only ensures that no player gains by switching to a different action. The benefit? CCE is computationally much easier to achieve than Nash, especially in large games where computing Nash equilibria becomes intractable.</p> <p>However, CCE has a significant drawback: the recommended action might reveal information about other players’ strategies, which could potentially be exploited. This is why we sometimes prefer a stronger equilibrium concept called Correlated Equilibrium (CE), which is achieved through swap regret minimization.</p> <h5 id="slide-3-swap-regret-challenges-with-large-action-spaces"><strong>Slide 3: Swap Regret Challenges with Large Action Spaces</strong></h5> <p>Now, let’s delve into swap regret. Unlike external regret (which compares against the best single action), swap regret measures the extra utility a player could have gained by replacing each action they played with another action according to a fixed transformation rule. It’s a more demanding criterion that leads to the stronger Correlated Equilibrium concept.</p> <p>In a CE, each player maximizes their expected utility against others’ action distributions while conditioning on their own sampled action. They must answer: “Given my recommended action, would switching to any other action increase my expected utility?” If the answer is no for all players, we have a Correlated Equilibrium.</p> <p>The challenge? Swap regret minimization is significantly more computationally demanding than external regret minimization, especially when dealing with large or infinite action spaces. This computational gap has been a major obstacle in applying CE to complex real-world problems.</p> <h5 id="slide-4-expert-settings-and-regret-bounds"><strong>Slide 4: Expert Settings and Regret Bounds</strong></h5> <p>Let’s examine the computational gap more precisely. In what’s called the “Expert Setting,” where a learner selects from \(N\) possible actions each round:</p> <p>For external regret, the average regret can be bounded by \(\varepsilon\) after approximately \(T \gtrsim \frac{\log N}{\varepsilon^2}\) rounds. This is a very efficient scaling – just logarithmic in the number of actions!</p> <p>In contrast, for swap regret, the best-known bounds required approximately \(T \gtrsim \frac{N \log N}{\varepsilon^2}\) rounds. Notice that crucial factor of \(N\) – it means the number of rounds needed scales linearly with the number of actions, creating an exponential gap compared to external regret.</p> <p>This gap becomes particularly problematic in applications like Poker, Diplomacy, or multi-agent reinforcement learning, which often involve moderate to large action spaces. The computational burden of swap regret minimization has made it less practical for these settings, despite its theoretical advantages.</p> <h5 id="slide-5-bandit-settings-and-challenges"><strong>Slide 5: Bandit Settings and Challenges</strong></h5> <p>The situation becomes even more challenging in bandit settings, where the learner only observes the reward of the chosen action rather than rewards for all possible actions. In this partial-information scenario, regret minimization generally requires more exploration.</p> <p>In bandit settings, the best-known algorithms for swap regret require approximately \(T \gtrsim \frac{N^2 \log N}{\varepsilon^2}\) rounds, while the previous best lower bound suggested that at least \(\frac{N \log N}{\varepsilon^2}\) rounds are necessary.</p> <p>This gap has significant implications for reinforcement learning applications, which often operate in bandit-like settings where agents receive feedback only for the actions they take. The quadratic dependence on \(N\) makes swap regret minimization particularly impractical for large-scale RL problems.</p> <p>The paper we’re discussing aims to close this gap, making swap regret minimization practical even for these challenging settings with large action spaces. The TreeSwap algorithm they introduce represents a major step forward in addressing these long-standing challenges.</p> <h5 id="slide-6-gaps-in-equilibrium-computation"><strong>Slide 6: Gaps in Equilibrium Computation</strong></h5> <p>The gaps between swap and external regret aren’t just theoretical curiosities—they manifest in significant discrepancies in equilibrium computation across various computational models. In normal-form games with \(N\) actions per player, these discrepancies become striking when comparing the complexity of computing \(\epsilon\)-approximate Correlated Equilibrium (CE) versus Coarse Correlated Equilibrium (CCE).</p> <p>For communication complexity, computing \(\epsilon\)-CCE requires \(O(\log^2 N)\) bits, while \(\epsilon\)-CE needs \(O(N \log^2 N)\) bits—an exponential increase! Similarly, query complexity jumps from \(O(N \log N)\) for \(\epsilon\)-CCE to \(O(N^2 \log N)\) for \(\epsilon\)-CE, making it quadratically worse. Even the sparsity characteristics differ dramatically—a CCE can be represented compactly with \(\text{polylog}(N)\) sparsity, while the sparsity of CE remains an open question.</p> <p>These gaps extend beyond normal-form games. In games with Littlestone dimension \(L\), we can compute \(\epsilon\)-CCE efficiently in \(O(L)\) rounds, but prior to this paper, it wasn’t even clear whether \(\epsilon\)-CE existed in these settings! Similarly, for extensive-form games with tree size description length \(n\), computing \(\epsilon\)-CCE takes polynomial time, while \(\epsilon\)-CE previously required exponential time.</p> <h5 id="slide-7-littlestone-dimension"><strong>Slide 7: Littlestone Dimension</strong></h5> <p>The Littlestone dimension is a critical complexity measure in online learning theory. It generalizes the more familiar VC dimension to sequential prediction settings, helping us characterize when no-regret learning is possible.</p> <p>In online learning, where examples arrive sequentially rather than all at once, we need a measure of how long an adversary can force a learner to make mistakes before running out of options. The Littlestone dimension precisely measures the length of the longest such sequence.</p> <p>For a function class \(H\) (which here represents action spaces), if \(\text{LDim}(H)\) is finite, then there exists a no-regret online learning algorithm for that class. Conversely, if \(\text{LDim}(H)\) is infinite, no online algorithm can achieve sublinear regret—meaning effective learning is impossible.</p> <p>This dimension plays a fundamental role in understanding the difference between batch learning (characterized by VC-dimension) and online learning (characterized by Littlestone dimension). For action spaces with finite Littlestone dimension \(L\), external regret can be bounded after \(T \geq L/\epsilon^2\) rounds, but prior to this work, it was unknown whether swap regret could be effectively minimized in such spaces.</p> <h5 id="slide-8-main-results---near-optimal-upper-and-lower-bounds-for-swap-regret"><strong>Slide 8: Main Results - Near-Optimal Upper and Lower Bounds for Swap Regret</strong></h5> <p>The paper introduces a groundbreaking reduction that converts any external-regret learning algorithm into a no-distributional swap-regret learner. This is the core innovation presented as “TreeSwap” (Algorithm 1).</p> <p>Theorem 1.1 states that if there’s a learner for some function class \(F\) that achieves external regret of \(\epsilon\) after \(M\) iterations, then TreeSwap achieves swap regret of at most \(\epsilon + 1/d\) after \(T = M^d\) iterations. Perhaps most impressively, if the per-iteration runtime complexity of the external-regret learner is \(C\), then TreeSwap maintains the same \(O(C)\) per-iteration amortized runtime complexity!</p> <p>This result is transformative because it means that any no-external-regret algorithm can be efficiently transformed into a no-swap-regret learner, regardless of action space size—even for infinite action spaces. Previous reductions required finite or bounded action spaces, severely limiting their applicability.</p> <p>The impact is substantial: if a game allows no-external-regret learning, it also allows no-swap-regret learning, leading to significantly better equilibrium computation results and removing the exponential dependence on action space size that previously hampered swap-regret algorithms.</p> <h5 id="slide-9-tree-swap-algorithm"><strong>Slide 9: Tree Swap Algorithm</strong></h5> <p>The TreeSwap algorithm organizes multiple instances of a no-external-regret algorithm in a hierarchical structure using a depth-\(d\), \(M\)-ary tree. Each node in the tree corresponds to an instance of the external-regret algorithm \(\text{Alg}\).</p> <p>For each time step \(t\), the algorithm selects a mixture of the distributions produced by different instances of \(\text{Alg}\). The key innovation lies in how these instances are updated: each instance follows a “lazy update” scheme, with instances at level \(h\) getting updated only every \(M^{d-h}\) rounds.</p> <p>This design ensures that each instance of \(\text{Alg}\) is updated a total of \(M\) times over the course of \(T = M^d\) rounds, allowing TreeSwap to leverage the external regret guarantees of the base algorithm while achieving bounded swap regret.</p> <p>The per-iteration amortized runtime complexity of TreeSwap is \(O(C)\), where \(C\) is the per-iteration runtime complexity of the underlying external-regret learner. This efficiency is crucial for practical applications in large action spaces.</p> <h5 id="slide-10-understanding-the-setup"><strong>Slide 10: Understanding the Setup</strong></h5> <p>To understand TreeSwap’s structure, consider a concrete example: a binary tree (\(M = 2\)) of depth \(d = 3\). This tree has \(T = 2^3 = 8\) leaves, each corresponding to a time step \(t\). Each node in the tree corresponds to a subproblem handled by an instance of the external-regret algorithm \(\text{Alg}\).</p> <p>This hierarchical organization helps in learning by averaging utility functions over different levels of the tree. The depth parameter \(d\) controls the trade-off between the number of rounds \(T\) and the additional term \(1/d\) in the swap regret bound.</p> <p>The arrangement of instances in this tree-like structure is central to TreeSwap’s ability to convert external regret guarantees into swap regret guarantees without incurring exponential computational costs in terms of the action space size.</p> <h5 id="slide-11-key-variables"><strong>Slide 11: Key Variables</strong></h5> <p>Understanding TreeSwap requires familiarity with several key variables:</p> <ol> <li> <p>\(\sigma_{1:d}\) represents the base-\(M\) representation of the time step \(t-1\). This representation determines the path in the tree for the current round.</p> </li> <li> <p>\(\sigma_{1:h-1}\) is the prefix of \(\sigma_{1:d}\) up to depth \(h-1\). It determines which instance of \(\text{Alg}\) is being used at a given level of the tree.</p> </li> <li> <p>Algorithm Instances (\(\text{Alg}_{\sigma_{1:h-1}}\)): Each node in the tree corresponds to an instance of the external-regret algorithm. The instance at level \(h\) gets updated every \(M^{d-h}\) rounds, creating a “lazy” update schedule.</p> </li> </ol> <p>This variable structure enables TreeSwap to distribute the learning task across multiple instances of the external-regret algorithm, with each instance focusing on a different aspect of the overall problem.</p> <h5 id="slide-12-workings-of-the-algorithm"><strong>Slide 12: Workings of the Algorithm</strong></h5> <p>For each time step \(t\), the distribution \(x^{(t)}\) played by TreeSwap is the uniform average over the distributions played by the instances of \(\text{Alg}\) at each node along the root-to-leaf path at that step.</p> <p>The instances \(\text{Alg}_{\sigma_{1:h-1}}\) are updated in a lazy fashion: every \(M^{d-h}\) rounds when \(\sigma_{1:h-1}\) lies on the current root-to-leaf path, the utility functions \(f^{(t)}\) are averaged and fed to the update procedure of the corresponding instance.</p> <p>As a result, each instance \(\text{Alg}_{\sigma_{1:h-1}}\) is updated a total of \(M\) times throughout the entire run of TreeSwap. This lazy update scheme is crucial for the algorithm’s efficiency, allowing it to maintain the same per-iteration runtime complexity as the base external-regret algorithm despite providing stronger swap regret guarantees.</p> <h5 id="slide-13-walk-through-of-full-treeswap"><strong>Slide 13: Walk-through of Full TreeSwap</strong></h5> <p>The TreeSwap algorithm operates as follows:</p> <ol> <li> <p>Initialization: For each sequence \(\sigma\) in the tree, initialize an instance of the external-regret algorithm \(\text{Alg}\) with time horizon \(M\).</p> </li> <li> <p>For each round \(t\):</p> <ul> <li>Compute \(\sigma\), the base-\(M\) representation of \(t-1\)</li> <li>Update the appropriate instances of \(\text{Alg}\) based on their position in the tree</li> <li>Output the uniform mixture of actions from the instances along the path from root to leaf</li> </ul> </li> </ol> <p>The lazy update mechanism is particularly important: instances closer to the root are updated less frequently (focusing on longer-term trends), while instances closer to the leaves update more often (adapting to shorter-term fluctuations).</p> <p>This multi-resolution approach helps the algorithm react to changes in the adversary’s strategy at various time scales, creating a robust learning process that effectively minimizes swap regret while maintaining computational efficiency.</p> <h5 id="slide-14-main-theorem-on-upper-bound"><strong>Slide 14: Main Theorem on Upper Bound</strong></h5> <p>The central result of the paper is formalized in a theorem that precisely quantifies the swap regret guarantees of TreeSwap:</p> <p><strong>Theorem</strong>: Suppose that an action set \(\mathcal{X}\) and a utility function class \(\mathcal{F}\) are given, together with an algorithm Alg satisfying the conditions of Assumption 1. Suppose that \(T, M, d \in \mathbb{N}\) are given for which \(M \geq 2\) and \(M^{d-1} \leq T \leq M^d\). Then given an adversarial sequence \(f^{(1)}, \ldots, f^{(T)} \in \mathcal{F}\), TreeSwap\((F, X, \text{Alg}, T)\) produces a sequence of iterates \(x^{(1)}, \ldots, x^{(T)}\) satisfying the following swap regret bound:</p> \[\text{SwapRegret}(x^{(1:T)}, f^{(1:T)}) \leq R_{\text{Alg}}(M) + \frac{3}{d}.\] <p>This theorem elegantly shows that the swap regret of TreeSwap can be bounded in terms of the external regret of the base algorithm (\(R_{\text{Alg}}(M)\)) plus an additional term (\(\frac{3}{d}\)) that decreases as the depth of the tree increases.</p> <h5 id="slide-15-points-to-note"><strong>Slide 15: Points to Note</strong></h5> <p>When applying the theorem in practice, several important points should be considered:</p> <ul> <li> <p>For simplicity, the theorem assumes \(T = M^d\), but the result generalizes to cases where \(T\) isn’t exactly \(M^d\).</p> </li> <li> <p>The theorem is typically applied by choosing \(M\) as a function of \(T\), then selecting \(d\) to satisfy \(M^{d-1} \leq T \leq M^d\).</p> </li> <li> <p>As long as \(T\) is sufficiently large, the additional term \(\frac{3}{d}\) can be made arbitrarily small, ensuring that SwapRegret(\(T\)) approaches the external regret bound.</p> </li> <li> <p>The depth parameter \(d\) controls the trade-off between the number of rounds \(T\) and the additional term in the swap regret bound, allowing flexibility in practical applications.</p> </li> </ul> <h5 id="slide-16-applications-concrete-swap-regret-bounds"><strong>Slide 16: Applications: Concrete Swap Regret Bounds</strong></h5> <p>The TreeSwap framework resolves multiple long-standing gaps in regret minimization and equilibrium computation. For constant \(\epsilon\), the authors address all the previously discussed gaps.</p> <p>In the expert setting with \(N\) actions, applying Theorem 1.1 with action set \(X = [N]\) and reward class defined by all [0,1]-bounded functions, i.e., \(F = [0,1]^{[N]}\), produces powerful concrete bounds.</p> <p>This application leads to significant improvements over prior work, particularly in terms of the dependence on the number of actions \(N\). Where previous approaches required a linear dependence on \(N\), TreeSwap achieves a remarkable logarithmic dependence.</p> <h5 id="slide-17-corollary-12"><strong>Slide 17: Corollary 1.2</strong></h5> <p><strong>Corollary 1.2 (Upper bound for finite action swap regret; informal version)</strong>:</p> <p>Fix \(N \in \mathbb{N}\) and \(\epsilon \in (0,1)\), and consider the setting of online learning with \(N\) actions. Then for any \(T\) satisfying \(T \geq (\log(N)/\epsilon^2)^{\Omega(1/\epsilon)}\), there is an algorithm that, when faced with any adaptive adversary, has swap regret bounded above by \(\epsilon\). Further, the amortized per-iteration runtime of the algorithm is \(O(N)\), its worst-iteration runtime is \(O(N/\epsilon)\) and its space complexity is \(O(N/\epsilon)\).</p> <p>This bound is exponentially better in \(N\) compared to prior work, which required \(T \geq \tilde{\Omega}(N/\epsilon^2)\) rounds. TreeSwap achieves an improved total runtime of \(\tilde{O}(N)\), compared to the previous \(\Omega(N^3)\) runtime of [BM07].</p> <p>The term \(O(1/\epsilon)\) in the exponent shows that as we demand lower regret (\(\epsilon\) smaller), the number of rounds increases exponentially—implying that very small \(\epsilon\) remains challenging to achieve efficiently.</p> <h5 id="slide-18-corollary-13"><strong>Slide 18: Corollary 1.3</strong></h5> <p>The authors further apply Theorem 1.1 to function classes with finite Littlestone dimension, yielding another important result:</p> <p><strong>Corollary 1.3 (Swap regret for Littlestone classes; informal version)</strong>:</p> <p>If the class \(\mathcal{X}\) has Littlestone dimension at most \(L\), then for any \(T \geq (L/\epsilon^2)^{\Omega(1/\epsilon)}\), there is a learner whose swap regret is at most \(\epsilon\). In particular, games with finite Littlestone dimension admit no-swap-regret learners and thus have \(\epsilon\)-approximate CE for all \(\epsilon &gt; 0\).</p> <p>This is a significant advancement because even the existence of approximate Correlated Equilibria in games of finite Littlestone dimension was previously unknown. The result extends swap regret minimization to continuous or infinite action spaces with structured constraints, broadening the applicability of CE to more complex decision-making domains.</p> <h5 id="slide-19-theorem-14-bandit-swap-regret-setting"><strong>Slide 19: Theorem 1.4 (Bandit swap regret setting)</strong></h5> <p>The final major result addresses the challenging bandit setting, where the learner only observes the reward of the chosen action rather than the full reward vector:</p> <p><strong>Theorem 1.4 (Bandit swap regret; informal version)</strong>:</p> <p>Let \(N \in \mathbb{N}, \epsilon \in (0,1)\) be given, and consider any \(T \geq N \cdot (\log(N)/\epsilon)^{O(1/\epsilon)}\). Then there is an algorithm in the adversarial bandit setting with \(N\) actions (BanditTreeSwap; Algorithm 4) which achieves swap regret bounded above by \(\epsilon\) after \(T\) iterations.</p> <p>This result closes a key gap in bandit learning. For \(\epsilon = O(1)\), it guarantees that \(\tilde{O}(N)\) rounds suffice to achieve swap regret of at most \(\epsilon\), creating only a polylogarithmic gap between the adversarial bandit setting and the full-information non-distributional setting.</p> <p>This is particularly noteworthy because for external regret, there’s an exponential gap between these settings: \(O(\log N)\) rounds suffice in the full-information case, while \(\Omega(N)\) rounds are needed in the bandit setting.</p> <h5 id="slide-20-applications-equilibrium-computation"><strong>Slide 20: Applications: Equilibrium Computation</strong></h5> <p>The TreeSwap results lead to transformative improvements in equilibrium computation. The authors establish:</p> <p><strong>Corollary 1.5 (Query and communication complexity upper bound; informal version)</strong>:</p> <p>In normal-form games with a constant number of players and \(N\) actions per player, the communication complexity of computing an \(\epsilon\)-approximate CE is \(\log(N)^{\tilde{O}(1/\epsilon)}\) and the query complexity of computing an \(\epsilon\)-approximate CE is \(N \cdot \log(N)^{\tilde{O}(1/\epsilon)}\).</p> <p>These bounds represent exponential improvements over previous best-known results for correlated equilibrium computation. Prior methods required quadratically more queries and exponentially more communication than what TreeSwap achieves.</p> <p>The main reduction can be used to obtain efficient algorithms for computing \(\epsilon\)-CE even when \(N\) is exponentially large, provided there are efficient external regret algorithms available. This dramatically expands the range of games for which we can practically compute correlated equilibria.</p> <h5 id="slide-21-extensive-form-games"><strong>Slide 21: Extensive Form Games</strong></h5> <p>The TreeSwap approach yields particularly impactful results for extensive-form games, which model sequential decision-making:</p> <p><strong>Corollary 1.6 (Extensive form games; informal version)</strong>:</p> <p>For any constant \(\epsilon\), there is an algorithm which computes an \(\epsilon\)-approximate CE of any given extensive form game, with a runtime polynomial in the representation of the game (i.e., polynomial in the number of nodes in the game tree and in the number of outgoing edges per node).</p> <p>This is a remarkable achievement because extensive-form games typically have action spaces that scale exponentially with the description length of the game. Previous best-known methods for \(\epsilon\)-CE in extensive-form games required exponential time in the game tree size.</p> <p>The polynomial-time algorithm for computing approximate CE in extensive-form games makes correlated equilibrium computation feasible even for large decision trees with complex information structures. This opens up new possibilities for modeling and solving realistic multi-agent interactions across domains like economics, autonomous systems, and security.</p> <h5 id="slide-22-near-matching-lower-bounds"><strong>Slide 22: Near-Matching Lower Bounds</strong></h5> <p>While TreeSwap provides powerful upper bounds, the authors also establish important lower bounds that show these results are nearly optimal:</p> <p>Theorem 1.1 and Corollary 1.2 require the number of rounds \(T\) to be exponential in \(1/\epsilon\), where \(\epsilon\) denotes the desired swap regret. The following lower bound shows this dependence is necessary, even facing an oblivious adversary that is constrained to choose reward vectors with constant \(\ell_1\) norm:</p> <p><strong>Theorem 1.7 (Lower bound for swap regret with oblivious adversary)</strong>:</p> <p>Fix \(N \in \mathbb{N}, \epsilon \in (0,1)\), and let \(T\) be any number of rounds satisfying:</p> \[T \leq O(1) \cdot \min \left\{\exp(O(\epsilon^{-1/6})), \frac{N}{\log^{12}(N) \cdot \epsilon^2} \right\}.\] <p>Then, there exists an oblivious adversary on the function class \(\mathcal{F} = \{f \in [0,1]^N \mid \|f\|_1 \leq 1\}\) such that any learning algorithm run over \(T\) steps will incur swap regret at least \(\epsilon\).</p> <p>This establishes the first \(\tilde{\Omega}\left(\min(1, \sqrt{N/T})\right)\) swap regret lower bound for distributional swap regret, achieved by an oblivious adversary. The lower bound means the exponential dependence on \(1/\epsilon\) in the upper bounds is unavoidable, confirming the near-optimality of TreeSwap.</p> <h5 id="slide-23-theorem-17"><strong>Slide 23: Theorem 1.7</strong></h5> <p>This lower bound theorem establishes fundamental limits on how efficiently swap regret can be minimized. The theorem demonstrates that any algorithm attempting to minimize swap regret must take at least a certain number of rounds (\(T\)) to achieve a desired regret bound (\(\epsilon\)).</p> <p>A key aspect of this result is that it holds even with an oblivious adversary—one that chooses the sequence of reward functions before the learning process starts and does not adapt to the learner’s actions. This makes the lower bound particularly strong, as oblivious adversaries are generally easier to learn against than adaptive ones.</p> <p>The constraint that the adversary is restricted to choosing reward functions with bounded \(\ell_1\)-norm (meaning the total sum of rewards across all actions is limited to some fixed value) is also significant. Even with this limitation, which prevents the adversary from assigning arbitrarily large rewards, minimizing swap regret still requires a large number of rounds.</p> <p>This suggests that the hardness of swap regret minimization is not due to extreme reward values but rather from the inherent complexity of tracking all possible action swaps. Bounded rewards only affect the magnitude of regret but do not reduce the structural complexity of evaluating swap regret.</p> <h5 id="slide-24-further-implications-of-lower-bounds"><strong>Slide 24: Further Implications of Lower Bounds</strong></h5> <p>The distributional swap regret lower bound established in Theorem 1.7 holds even when the learner is allowed to randomize over actions rather than picking a single action. This demonstrates that the difficulty is inherent to swap regret minimization itself, not just a restriction of the learning model.</p> <p>Remarkably, this lower bound holds even for a simple adversary and for function classes with constant Littlestone dimension. This indicates that swap regret minimization is fundamentally hard, even in the simplest possible settings.</p> <p>For external regret, upper and lower bounds are usually much closer, leading to tight characterizations of optimal algorithms. In contrast, for swap regret, the gap between the upper bound (\(\exp(\epsilon^{-1})\) from Corollary 1.2) and the lower bound (\(\exp(\epsilon^{-1/6})\) from Theorem 1.7) suggests we might be overestimating the difficulty of swap regret minimization, leaving open the possibility of improved algorithms.</p> <p>The authors also establish a stronger lower bound by allowing for an adaptive adversary—one that adjusts its rewards based on the learner’s actions. This construction shows that \(T \geq \exp(\Omega(\epsilon^{-3}))\) rounds are necessary, bringing the lower bound closer to the upper bound of \(\exp(\Omega(\epsilon^{-1}))\).</p> <h5 id="slide-25-conclusion"><strong>Slide 25: Conclusion</strong></h5> <p>The TreeSwap algorithm represents a significant breakthrough in overcoming the computational challenges of swap regret minimization. Its success lies in several key innovations:</p> <ol> <li> <p><strong>Hierarchical Structure</strong>: Instead of directly minimizing swap regret (which is computationally expensive), TreeSwap breaks down the problem by structuring it hierarchically using a tree.</p> </li> <li> <p><strong>Learning at Different Time Scales</strong>: The lazy update scheme ensures that learners at different levels of the tree operate at different time scales. Learners closer to the root are updated less frequently, focusing on longer-term trends, while learners closer to the leaves are updated more often, adapting to shorter-term fluctuations.</p> </li> <li> <p><strong>Leveraging External Regret Guarantees</strong>: Each level of the tree runs an external regret minimization algorithm, which is already well understood and efficient.</p> </li> <li> <p><strong>Combining Expert Advice</strong>: The algorithm takes a uniform mixture of the distributions suggested by the learners along the path from root to leaf, effectively combining advice from multiple “experts” operating at different time scales.</p> </li> </ol> <p>By combining these elements, TreeSwap achieves swap regret minimization as efficiently as external regret minimization in many cases. The “2.0” in the paper’s title signifies the substantial improvement over prior approaches, bringing swap regret minimization into the realm of practical applicability for large-scale problems.</p> <h5 id="slide-26-discussion--questions"><strong>Slide 26: Discussion &amp; Questions</strong></h5> <p>The near-optimality of the bounds established in this paper represents a major advancement in our understanding of swap regret. While there remains a gap between the upper and lower bounds (roughly \(\exp(\epsilon^{-1})\) versus \(\exp(\epsilon^{-1/6})\) for oblivious adversaries), these results are already close enough to be called near-optimal.</p> <p>This suggests that TreeSwap is among the most efficient known algorithms for swap regret minimization, though there may still be room for theoretical improvements. The gap indicates that we might not have fully characterized the true complexity of swap regret minimization yet.</p> <p>The paper leaves several interesting questions for future work:</p> <ol> <li>Can the gap between upper and lower bounds be further reduced?</li> <li>Are there specific classes of games or learning problems where the bounds can be tightened?</li> <li>How do these theoretical guarantees translate to practical performance in real-world applications like multi-agent reinforcement learning?</li> <li>Can the TreeSwap approach be extended to other regret notions beyond swap regret?</li> </ol> <p>These questions represent promising directions for continued research in this area, building on the foundation established by the TreeSwap algorithm.</p> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>This presentation has walked through the groundbreaking “Swap Regret 2.0” paper, which introduces TreeSwap—an algorithm that fundamentally changes our ability to minimize swap regret in large and infinite action spaces. By leveraging a hierarchical tree structure of external regret minimizers and a clever lazy update scheme, TreeSwap achieves what was previously thought impractical: efficient swap regret minimization with only logarithmic dependence on the number of actions.</p> <p>The implications of this work extend far beyond theoretical interest. By making correlated equilibrium computation feasible in extensive-form games and spaces with large action sets, this research opens new possibilities for applications in multi-agent reinforcement learning, economic modeling, and game-theoretic analysis of complex strategic interactions.</p> <p>While some gaps remain between the upper and lower bounds, particularly in the dependence on the approximation parameter ε, the near-optimality of these results represents a significant milestone in online learning theory. Future work might further tighten these bounds or explore specialized versions of TreeSwap for specific application domains.</p> <p>As machine learning continues to tackle increasingly complex multi-agent problems, algorithms like TreeSwap that efficiently bridge the gap between external and swap regret will become essential tools in our theoretical and practical toolkit.</p> <h5 id="acknowledgments"><strong>Acknowledgments</strong></h5> <p>Special thanks to the Advanced Machine Learning class at NYU’s Courant Institute for the opportunity to present this work, and to the authors of the original paper for their significant contribution to the field of online learning and game theory.</p> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Monishver11/monishver11.github.io","data-repo-id":"R_kgDONe9Wkw","data-category":"General","data-category-id":"DIC_kwDONe9Wk84Cp-bh","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab",title:"Sharing personal reflections - Thoughts Tab",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>