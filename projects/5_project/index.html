<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Baseline to DeepSeek - Single-GPU MoE Training Efficiency | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="A systems-level analysis of training Mixture-of-Experts (MoE) Transformer models under single-GPU constraints. We compare naive PyTorch MoE, ScatterMoE, MegaBlocks, and DeepSeek-inspired architectures, revealing critical trade-offs between convergence behavior, memory footprint, and training throughput."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/projects/5_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Baseline to DeepSeek - Single-GPU MoE Training Efficiency</h1> <p class="post-description">A systems-level analysis of training Mixture-of-Experts (MoE) Transformer models under single-GPU constraints. We compare naive PyTorch MoE, ScatterMoE, MegaBlocks, and DeepSeek-inspired architectures, revealing critical trade-offs between convergence behavior, memory footprint, and training throughput.</p> </header> <article> <div class="row justify-content-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_5/moe-1-480.webp 480w,/assets/img/project_5/moe-1-800.webp 800w,/assets/img/project_5/moe-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_5/moe-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="moe-intro" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Recent advances in language modeling have been driven not only by architectural innovations, but also by improvements in training efficiency at the systems level. While Mixture-of-Experts (MoE) architectures have demonstrated strong efficiency gains in large-scale, multi-GPU deployments, their practical behavior on resource-constrained hardware remains poorly understood. We present a systems-oriented empirical study comparing dense Transformers with multiple MoE architectures on a single GPU, revealing that MoE designs optimized for distributed training do not directly translate to single-device settings.</p> <p>This project was developed as part of the <strong>Big Data and Machine Learning</strong> course during the fall 2025 semester in the <strong>MSCS program at NYU Courant</strong>. Our team — <strong>Parvathi Biju</strong> and myself.</p> <hr> <h4 id="introduction-and-motivation"><strong>Introduction and Motivation</strong></h4> <p>The modded-NanoGPT speedrun project exemplifies a systems-first perspective, focusing on identifying the fastest algorithms for training Transformer models under fixed hardware constraints. Inspired by this approach, we study Mixture-of-Experts (MoE) models through a systems lens, emphasizing practical performance and feasibility rather than theoretical scaling alone.</p> <p>Transformers remain the dominant architecture for language modeling, but scaling dense models increases computation and memory costs proportionally. MoE architectures address this limitation by introducing conditional computation—routing each token to a small subset of expert networks and enabling parameter counts to scale independently of per-token compute. While MoEs have demonstrated strong efficiency gains in large-scale, multi-GPU deployments, their practical behavior on resource-constrained hardware remains poorly understood.</p> <p><strong>The Single-GPU Challenge</strong></p> <p>Training MoEs on a single GPU introduces unique challenges. Dynamic routing creates irregular computation patterns, expert load imbalance, and additional memory overhead from token dispatch and reordering. Existing MoE implementations are largely optimized for distributed environments, where large batch sizes and expert parallelism can amortize these costs. On consumer-grade GPUs, however, memory limits and kernel overhead become first-order constraints, making the choice of MoE implementation critical.</p> <p><strong>Research Questions</strong></p> <p>Our investigation focuses on three central questions:</p> <ol> <li> <strong>Memory Efficiency</strong>: How do different MoE implementations affect peak memory usage under single-GPU constraints?</li> <li> <strong>Training Throughput</strong>: What is the trade-off between kernel optimization and per-step latency?</li> <li> <strong>Convergence Quality</strong>: Do architectural choices (shared experts, routing strategies) impact final model performance independent of systems optimizations?</li> </ol> <p><strong>Key Contributions</strong></p> <ul> <li>A single-GPU systems study of MoE training, grounded in the Modded NanoGPT framework and motivated by the speedrun philosophy of maximizing training efficiency under fixed hardware constraints</li> <li>An implementation-driven comparison of MoE designs, including naive PyTorch MoE, ScatterMoE, MegaBlocks, and a DeepSeek-inspired architecture</li> <li>An empirical analysis of throughput, memory footprint, and convergence behavior across MoE variants</li> <li>Practical insights into MoE feasibility on constrained hardware, clarifying when and why certain implementations become bottlenecked by memory or kernel overhead rather than compute</li> </ul> <hr> <h4 id="background-and-context"><strong>Background and Context</strong></h4> <h5 id="mixture-of-experts-architectures"><strong>Mixture-of-Experts Architectures</strong></h5> <p>Mixture-of-Experts (MoE) architectures introduce conditional computation into neural networks by routing each token to a small subset of expert networks. This design enables model capacity to scale independently of per-token compute and has been widely adopted in large language models.</p> <p>The core MoE mechanism replaces a standard feed-forward layer with:</p> \[\text{MoE}(x) = \sum_{i=1}^{n} G(x)_i \cdot E_i(x)\] <p>where $G(x)$ is a gating function (router) that produces routing weights, and $E_i(x)$ are expert networks. In top-k routing, only the k experts with highest routing scores are computed per token.</p> <p><strong>Expert Load Imbalancing</strong></p> <p>A central challenge in MoE training is expert load imbalance, which can lead to expert underutilization and degraded optimization. Prior work addresses this through auxiliary load-balancing losses that regularize routing behavior during training. While effective at scale, these mechanisms introduce additional optimization complexity and interact with systems-level constraints, particularly in non-distributed training regimes.</p> <h5 id="modded-nanogpt-baseline"><strong>Modded NanoGPT Baseline</strong></h5> <p>Modded NanoGPT represents a systems-optimized implementation of GPT-style Transformers, developed as part of the NanoGPT speedrun effort to identify the fastest training configurations under fixed hardware constraints. The framework combines:</p> <ul> <li>Compiled execution via <code class="language-plaintext highlighter-rouge">torch.compile</code> </li> <li>Memory-efficient Flash Attention</li> <li>Mixed-precision training (bfloat16)</li> <li>Optimized optimizer configurations (AdamW with fused kernels)</li> </ul> <p>In this work, Modded NanoGPT serves as our strong dense baseline against which the systems overheads and benefits of different MoE implementations are evaluated.</p> <h5 id="moe-implementation-variants"><strong>MoE Implementation Variants</strong></h5> <p><strong>ScatterMoE</strong></p> <p>ScatterMoE proposes a routing-centric optimization for MoE layers by fusing token dispatch, expert computation, and output aggregation into a single kernel. This approach reduces routing overhead and intermediate memory traffic relative to naive MoE implementations.</p> <div class="row justify-content-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_5/moe-2-480.webp 480w,/assets/img/project_5/moe-2-800.webp 800w,/assets/img/project_5/moe-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_5/moe-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="scatter-moe-diagram" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ScatterMoE optimization: Instead of grouping tokens then routing to experts, tokens are scattered directly to expert shards with minimal data movement. </div> <p>Key benefits:</p> <ul> <li>Lower memory footprint due to fused operations</li> <li>Reduced routing overhead from eliminated intermediate tensors</li> <li>Well-suited for moderate expert counts and batch sizes</li> </ul> <p><strong>MegaBlocks</strong></p> <p>MegaBlocks reformulates MoE computation as block-sparse matrix multiplication to efficiently handle load-imbalanced expert assignments. By expressing expert execution as grouped matrix operations over dynamically constructed sparse blocks, MegaBlocks achieves high arithmetic efficiency in large-scale training regimes.</p> <div class="row justify-content-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_5/moe-3-480.webp 480w,/assets/img/project_5/moe-3-800.webp 800w,/assets/img/project_5/moe-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_5/moe-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="megablocks-diagram" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> MegaBlocks approach: Token padding and dropping enable block-sparse matrix multiplication with variable tokens per expert. </div> <p>However, this efficiency comes at the cost of:</p> <ul> <li>Increased metadata management</li> <li>Padding overhead for block alignment</li> <li>Intermediate buffer allocation</li> </ul> <p>These factors can limit practicality in memory-constrained environments.</p> <p><strong>DeepSeek-V2 Architecture</strong></p> <p>DeepSeek-V2 introduces a refined MoE design based on:</p> <ol> <li> <strong>Fine-grained experts</strong>: Smaller, more specialized expert networks</li> <li> <strong>Shared experts</strong>: Always-active experts that ensure representational capacity</li> <li> <strong>Routing bias mechanisms</strong>: Improved expert utilization without heavy auxiliary losses</li> </ol> <div class="row justify-content-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_5/moe-4-480.webp 480w,/assets/img/project_5/moe-4-800.webp 800w,/assets/img/project_5/moe-4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_5/moe-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="deepseek-diagram" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> DeepSeek-style MoE evolution: From conventional top-k routing (a) to fine-grained expert segmentation (b) to shared expert isolation (c). </div> <p>These architectural choices reduce reliance on explicit auxiliary losses and improve convergence behavior, motivating their evaluation beyond large-scale distributed settings.</p> <hr> <h4 id="experimental-design"><strong>Experimental Design</strong></h4> <h5 id="model-variants"><strong>Model Variants</strong></h5> <p>We evaluate six distinct configurations, progressively introducing MoE components and optimizations:</p> <p><strong>V0: Dense Baseline (Modded NanoGPT)</strong></p> <p>Our starting point uses the Modded NanoGPT configuration with:</p> <ul> <li>Compiled execution via <code class="language-plaintext highlighter-rouge">torch.compile</code> </li> <li>Flash Attention (scaled dot product attention)</li> <li>bfloat16 mixed precision</li> <li>Standard feed-forward layers (768 → 3072 → 768)</li> </ul> <p>This serves as the reference for throughput and memory on single-GPU hardware.</p> <p><strong>V1: Naive MoE (Masked Compute, Top-1)</strong></p> <p>The dense MLP is replaced by a Mixture-of-Experts MLP with 4 experts and top-1 routing. To keep execution compile-friendly, the implementation evaluates all experts on the full token batch and applies a mask to accumulate only the selected expert output.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MoEMLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_experts</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="n">self</span><span class="p">.</span><span class="n">experts</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="nc">MLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_experts</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">router</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_experts</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Compute routing weights
</span>        <span class="n">routing_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">router</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Evaluate ALL experts (inefficient but compile-friendly)
</span>        <span class="n">expert_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="nf">expert</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">expert</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">experts</span><span class="p">])</span>
        
        <span class="c1"># Apply routing mask
</span>        <span class="n">selected_experts</span> <span class="o">=</span> <span class="n">routing_weights</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">selected_experts</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_experts</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">expert_outputs</span> <span class="o">*</span> <span class="n">mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>This eliminates data-dependent control flow but increases activation and intermediate compute substantially.</p> <p><strong>V2: Naive MoE + Routing Losses (Top-1)</strong></p> <p>The MoE structure remains 4 experts with top-1 routing, but training adds explicit router regularization:</p> <ol> <li> <strong>Switch-style load balancing</strong>: Encourages uniform expert usage</li> <li> <strong>Router z-loss</strong>: Prevents routing logits from blowing up</li> <li> <strong>Importance-variance loss</strong>: Encourages diversity in expert selection</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load balancing loss
</span><span class="n">expert_counts</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">selected_experts</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_experts</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
<span class="n">load_balance_loss</span> <span class="o">=</span> <span class="n">expert_counts</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>

<span class="c1"># Router z-loss (logit magnitude regularization)
</span><span class="n">router_logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">router</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">z_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">logsumexp</span><span class="p">(</span><span class="n">router_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>

<span class="c1"># Combined auxiliary loss
</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">load_balance_loss</span> <span class="o">+</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">z_loss</span>
</code></pre></div></div> <p><strong>V3: ScatterMoE (Kernelized Routing)</strong></p> <p>This variant replaces naive expert execution with ScatterMoE-style token dispatch and expert computation. The core design avoids redundant expert evaluation by:</p> <ol> <li>Gathering tokens per expert</li> <li>Executing expert MLPs on compacted batches</li> <li>Scattering outputs back to original positions</li> </ol> <p><strong>V4: MegaBlocks (Grouped Block-Sparse MoE)</strong></p> <p>We integrate MegaBlocks MoE with:</p> <ul> <li>4 experts and top-1 routing</li> <li>Grouped execution (<code class="language-plaintext highlighter-rouge">mlp_impl="grouped"</code>)</li> <li>GLU experts (<code class="language-plaintext highlighter-rouge">mlp_type="glu"</code>)</li> <li>Layout transposes between batch-first and sequence-first representations</li> </ul> <p><strong>V5: DeepSeek (Shared + Routed Experts)</strong></p> <p>We implement a DeepSeek-style MoE layer with:</p> <ul> <li>1 shared expert (always active)</li> <li>4 routed experts (selected via top-2 routing)</li> <li>Two-pass computation: shared experts first, then routed experts</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DeepSeekMoE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">shared_expert</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">routed_experts</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="nc">MLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">router</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Shared expert (always active)
</span>        <span class="n">shared_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">shared_expert</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Routed experts (top-2 selection)
</span>        <span class="n">routing_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">router</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">top_k_weights</span><span class="p">,</span> <span class="n">top_k_indices</span> <span class="o">=</span> <span class="n">routing_weights</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Compute routed expert outputs
</span>        <span class="n">routed_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">top_k_indices</span> <span class="o">==</span> <span class="n">i</span><span class="p">).</span><span class="nf">any</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">mask</span><span class="p">.</span><span class="nf">any</span><span class="p">():</span>
                <span class="n">expert_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">routed_experts</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
                <span class="n">routed_out</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+=</span> <span class="n">expert_out</span> <span class="o">*</span> <span class="n">top_k_weights</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="p">:]</span>
        
        <span class="k">return</span> <span class="n">shared_out</span> <span class="o">+</span> <span class="n">routed_out</span>
</code></pre></div></div> <h5 id="training-configuration"><strong>Training Configuration</strong></h5> <p><strong>Dataset</strong>: FineWeb-10B binary shards</p> <ul> <li>Cleaned and deduplicated English web data from CommonCrawl</li> <li>0.73B tokens used for training</li> <li>Sequence length: 2048 tokens</li> </ul> <p><strong>Optimization</strong>:</p> <ul> <li>Optimizer: AdamW with fused kernels</li> <li>Learning rate: 3e-4 with cosine decay</li> <li>Batch size: 8,192 tokens (4 sequences × 2048)</li> <li>Training steps: 12,000</li> <li>Mixed precision: bfloat16</li> </ul> <p><strong>Hardware</strong>:</p> <ul> <li>NVIDIA A100 GPU (40GB)</li> <li>Single-GPU training (no distributed parallelism)</li> </ul> <p><strong>Evaluation Metrics</strong>:</p> <ul> <li>Validation loss (measured every 500 steps)</li> <li>Average step time (milliseconds per training step)</li> <li>Peak allocated memory (PyTorch CUDA allocator)</li> <li>Reserved memory (includes allocator fragmentation and caching)</li> </ul> <hr> <h4 id="results-and-analysis"><strong>Results and Analysis</strong></h4> <p>We evaluate all architectures for 12,000 training steps under identical optimization settings. All results are obtained from end-to-end training runs rather than microbenchmarks.</p> <h5 id="end-to-end-performance-comparison"><strong>End-to-End Performance Comparison</strong></h5> <div class="row justify-content-center"> <div class="col-sm-11 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project_5/moe-5-480.webp 480w,/assets/img/project_5/moe-5-800.webp 800w,/assets/img/project_5/moe-5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project_5/moe-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="results-table" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <table> <thead> <tr> <th>Variant</th> <th>Val Loss</th> <th>Step Avg (ms)</th> <th>Peak Alloc (MiB)</th> <th>Reserved (MiB)</th> </tr> </thead> <tbody> <tr> <td><strong>V0 Dense</strong></td> <td>4.0007</td> <td>80.95</td> <td>6,091</td> <td>6,112</td> </tr> <tr> <td><strong>V1 Naive MoE</strong></td> <td>3.9821</td> <td>149.08</td> <td>11,339</td> <td>13,792</td> </tr> <tr> <td><strong>V2 Naive+Loss</strong></td> <td>4.0427</td> <td>151.19</td> <td>11,340</td> <td>13,794</td> </tr> <tr> <td><strong>Scatter</strong></td> <td>4.2197</td> <td>125.39</td> <td>9,320</td> <td>9,518</td> </tr> <tr> <td><strong>MegaBlocks</strong></td> <td>4.5400</td> <td>102.84</td> <td>18,023</td> <td>19,230</td> </tr> <tr> <td><strong>DeepSeek</strong></td> <td><strong>3.9373</strong></td> <td>208.21</td> <td>13,620</td> <td>16,754</td> </tr> </tbody> </table> <p><strong>Key Observations</strong>:</p> <ol> <li> <p><strong>Memory Pressure Dominates</strong>: Even the simplest MoE variants (V1, V2) require nearly 2× the peak allocated memory of the dense baseline, despite activating only a subset of experts per token</p> </li> <li> <p><strong>Kernel Strategy Impacts Latency</strong>: Naive MoE variants (V1, V2) exhibit the highest step times among non-DeepSeek models, reflecting redundant expert computation and routing overhead</p> </li> <li> <p><strong>MegaBlocks Memory Explosion</strong>: With nearly 19 GB of reserved memory, MegaBlocks exceeds practical limits of consumer GPUs, reflecting its design focus on large-scale distributed training</p> </li> <li> <p><strong>DeepSeek Convergence vs. Efficiency</strong>: DeepSeek achieves the lowest validation loss (3.94) but also the highest step time (208ms), representing a clear quality-efficiency trade-off</p> </li> </ol> <h5 id="throughputmemory-trade-offs"><strong>Throughput–Memory Trade-offs</strong></h5> <p>Three distinct regimes emerge from our analysis:</p> <p><strong>1. Compute-Efficient but Memory-Heavy (MegaBlocks)</strong></p> <p>MegaBlocks achieves relatively low step time (102.84ms) through block-sparse operations, but incurs prohibitive memory overhead:</p> <ul> <li>Peak allocated: 18,023 MiB (3× dense baseline)</li> <li>Reserved memory: 19,230 MiB (includes allocator fragmentation)</li> </ul> <p>The design optimizes for throughput in distributed settings with large batches, but the block construction metadata and padding overhead make it impractical for single-GPU training.</p> <p><strong>2. Balanced Systems Design (ScatterMoE)</strong></p> <p>ScatterMoE offers the best compromise between efficiency and memory:</p> <ul> <li>16% faster than naive MoE (125.39ms vs 149.08ms)</li> <li>18% lower memory than naive MoE (9,320 MiB vs 11,339 MiB)</li> </ul> <p>By fusing routing operations and eliminating redundant expert computation, ScatterMoE demonstrates that careful kernel design can improve both dimensions simultaneously.</p> <p><strong>3. Optimization-Focused (DeepSeek)</strong></p> <p>DeepSeek prioritizes convergence quality over raw throughput:</p> <ul> <li>Lowest validation loss: 3.94 (vs 4.00 for dense baseline)</li> <li>Highest step time: 208.21ms (2.6× dense baseline)</li> <li>Moderate memory: 13,620 MiB (2.2× dense baseline)</li> </ul> <p>The architectural complexity—shared experts, fine-grained routing, and multiple expert passes—increases per-step cost but delivers superior final model quality.</p> <h5 id="convergence-behavior-analysis"><strong>Convergence Behavior Analysis</strong></h5> <p>While MoE architectures are primarily motivated by parameter efficiency, our results show that <strong>architecture design, not parameter count alone, determines convergence quality</strong>.</p> <p><strong>Naive MoE Variants (V1, V2)</strong></p> <p>Naive MoE achieves validation loss comparable to the dense baseline early in training (3.98 vs 4.00), but does not significantly outperform it despite activating more parameters per token. Adding explicit load-balancing losses (V2) actually degrades final validation loss to 4.04, suggesting that auxiliary routing objectives can interfere with optimization at small scale.</p> <p><strong>Kernel-Optimized Variants (Scatter, MegaBlocks)</strong></p> <p>Both ScatterMoE (4.22) and MegaBlocks (4.54) show degraded validation loss compared to the dense baseline, indicating that <strong>improving execution efficiency alone does not guarantee improved learning dynamics</strong>.</p> <p>This phenomenon likely stems from:</p> <ul> <li>Expert underutilization due to load imbalance</li> <li>Routing instability from top-1 selection</li> <li>Representational fragmentation across experts</li> </ul> <p><strong>DeepSeek Architecture</strong></p> <p>In stark contrast, the DeepSeek-style architecture achieves the strongest convergence:</p> <ul> <li>Final validation loss: <strong>3.94</strong> (1.5% improvement over dense)</li> <li>Consistent improvement throughout training</li> <li>More stable loss curves with lower variance</li> </ul> <p>This supports the hypothesis that <strong>fine-grained experts and shared capacity are more important for MoE effectiveness than routing efficiency alone</strong>. By ensuring that a portion of model capacity is always active (shared experts), the DeepSeek design avoids representational fragmentation observed in simpler MoE variants.</p> <h5 id="systems-level-insights"><strong>Systems-Level Insights</strong></h5> <p><strong>1. Memory, Not Compute, Is the Primary Bottleneck</strong></p> <p>Even modest expert counts (4 experts) quickly exhaust available VRAM:</p> <ul> <li>Activation storage for all experts (even masked ones in naive MoE)</li> <li>Routing intermediates (softmax outputs, expert assignments)</li> <li>Expert weight parameters (4× the feed-forward layer size)</li> </ul> <p>This finding contradicts the common assumption that MoE’s benefit is primarily computational efficiency. On single GPUs, the memory cost of maintaining multiple experts dominates.</p> <p><strong>2. Kernel Specialization Is Necessary but Insufficient</strong></p> <p>ScatterMoE demonstrates that fused routing kernels can improve both throughput and memory efficiency. However, these gains do not translate to better convergence. This indicates that <strong>systems optimization and architectural design must be addressed jointly</strong>.</p> <p><strong>3. Architectural Choices Dominate Convergence</strong></p> <p>The DeepSeek architecture achieves superior convergence despite higher per-step cost, demonstrating that:</p> <ul> <li>Shared experts prevent representational collapse</li> <li>Fine-grained experts improve specialization</li> <li>Top-k routing (k&gt;1) provides routing stability</li> </ul> <p>These architectural properties matter more than raw throughput for final model quality.</p> <p><strong>4. Distributed Designs Don’t Translate to Single-GPU</strong></p> <p>MegaBlocks, designed for large-scale distributed training, performs poorly in our single-GPU setting:</p> <ul> <li>Block construction overhead</li> <li>Padding waste</li> <li>Metadata management</li> </ul> <p>This highlights that <strong>MoE architectures optimized for datacenter deployments require fundamental redesign for consumer hardware</strong>.</p> <hr> <h4 id="discussion-and-practical-implications"><strong>Discussion and Practical Implications</strong></h4> <h5 id="when-to-use-moe-on-single-gpus"><strong>When to Use MoE on Single GPUs</strong></h5> <p>Our results suggest that MoE architectures are beneficial on single GPUs only when:</p> <ol> <li> <strong>Convergence quality is paramount</strong>: DeepSeek-style architectures can outperform dense baselines, but at significant computational cost</li> <li> <strong>Memory budget allows 2-3× overhead</strong>: All MoE variants require substantially more memory than dense models</li> <li> <strong>Training time is not critical</strong>: MoE variants are consistently slower per-step than dense baselines</li> </ol> <p>For most single-GPU training scenarios, <strong>dense Transformers remain the most efficient choice</strong>.</p> <h5 id="design-recommendations-for-memory-constrained-moe"><strong>Design Recommendations for Memory-Constrained MoE</strong></h5> <p>Based on our findings, we propose the following design principles for single-GPU MoE training:</p> <p><strong>1. Prioritize Shared Capacity</strong></p> <p>Always include shared experts that process all tokens. This:</p> <ul> <li>Prevents representational fragmentation</li> <li>Provides stable gradient signal</li> <li>Improves convergence quality</li> </ul> <p><strong>2. Use Fused Routing Kernels</strong></p> <p>Implement ScatterMoE-style fused operations to:</p> <ul> <li>Eliminate redundant expert evaluation</li> <li>Reduce intermediate memory allocation</li> <li>Improve kernel launch efficiency</li> </ul> <p><strong>3. Limit Expert Count</strong></p> <p>With limited memory, fewer, larger experts outperform many small experts:</p> <ul> <li>Reduces routing overhead</li> <li>Improves expert utilization</li> <li>Decreases memory fragmentation</li> </ul> <p><strong>4. Employ Top-k Routing (k ≥ 2)</strong></p> <p>Top-1 routing creates instability and load imbalance. Top-2 or top-3 routing:</p> <ul> <li>Provides routing redundancy</li> <li>Smooths expert load distribution</li> <li>Improves optimization stability</li> </ul> <h5 id="future-optimizations"><strong>Future Optimizations</strong></h5> <p>Several promising directions could improve single-GPU MoE efficiency:</p> <p><strong>1. Gradient Checkpointing for Experts</strong></p> <p>Store only routing decisions during forward pass and recompute expert outputs during backward:</p> <ul> <li>Trades compute for memory</li> <li>Particularly effective with fast expert networks</li> <li>Can reduce peak memory by 30-40%</li> </ul> <p><strong>2. Mixed Granularity Experts</strong></p> <p>Combine coarse-grained shared experts with fine-grained routed experts:</p> <ul> <li>Shared experts handle general features (large, efficient)</li> <li>Routed experts handle specialized features (small, numerous)</li> </ul> <p><strong>3. Dynamic Expert Pruning</strong></p> <p>Progressively merge underutilized experts during training:</p> <ul> <li>Reduces effective expert count over time</li> <li>Maintains initial exploration capacity</li> <li>Improves final efficiency</li> </ul> <p><strong>4. Heterogeneous Expert Sizes</strong></p> <p>Not all experts need identical capacity:</p> <ul> <li>Frequently-used experts can be smaller (cache-friendly)</li> <li>Rarely-used experts can be larger (higher capacity)</li> </ul> <hr> <h4 id="limitations-and-future-work"><strong>Limitations and Future Work</strong></h4> <h5 id="current-limitations"><strong>Current Limitations</strong></h5> <p><strong>1. Limited Training Duration</strong></p> <p>Our 12,000-step experiments capture early training dynamics but may not fully reflect long-horizon behavior. Longer runs could:</p> <ul> <li>Further amplify convergence differences</li> <li>Reveal expert specialization patterns</li> <li>Expose optimization instabilities</li> </ul> <p><strong>2. Single Expert Type</strong></p> <p>All MoE variants replace only feed-forward sublayers. Unexplored directions include:</p> <ul> <li>Attention expert mixtures</li> <li>Depth-wise routing (skip connections to different layers)</li> <li>Hybrid dense-sparse architectures</li> </ul> <p><strong>3. Fixed Batch Size</strong></p> <p>We use a single batch size (8,192 tokens) across all variants. Different MoE implementations may have different optimal batch sizes:</p> <ul> <li>Larger batches may amortize MegaBlocks overhead</li> <li>Smaller batches may reduce naive MoE memory pressure</li> </ul> <p><strong>4. Hardware-Specific Results</strong></p> <p>All experiments use NVIDIA A100 GPUs. Results may differ on:</p> <ul> <li>Consumer GPUs (3080, 4090) with different memory hierarchies</li> <li>AMD GPUs with different kernel characteristics</li> <li>Future architectures with improved sparse compute</li> </ul> <h5 id="future-research-directions"><strong>Future Research Directions</strong></h5> <p><strong>1. Adaptive Expert Allocation</strong></p> <p>Design systems that dynamically adjust expert count and size based on:</p> <ul> <li>Available memory</li> <li>Training progress</li> <li>Load imbalance metrics</li> </ul> <p><strong>2. Hybrid Parallelism Strategies</strong></p> <p>Explore combining:</p> <ul> <li>Expert parallelism (distribute experts across GPUs)</li> <li>Data parallelism (distribute batches)</li> <li>Pipeline parallelism (distribute layers)</li> </ul> <p>Even on 2-4 consumer GPUs, these strategies could unlock larger MoE models.</p> <p><strong>3. Specialized Routing Mechanisms</strong></p> <p>Develop routing strategies tailored to single-GPU constraints:</p> <ul> <li>Locality-aware routing (prefer cache-resident experts)</li> <li>Batch-aware routing (balance load within mini-batch)</li> <li>Memory-aware routing (avoid peak allocation)</li> </ul> <p><strong>4. Compiler-Level Optimizations</strong></p> <p>Leverage emerging compiler technologies:</p> <ul> <li>Triton for custom routing kernels</li> <li> <code class="language-plaintext highlighter-rouge">torch.compile</code> with expert-aware fusion</li> <li>JAX with expert parallelism primitives</li> </ul> <hr> <h4 id="conclusion"><strong>Conclusion</strong></h4> <p>This work presents a systems-oriented empirical study of Mixture-of-Experts training on a single, memory-constrained GPU, comparing dense Transformers with multiple MoE architectures and execution strategies.</p> <h5 id="key-findings"><strong>Key Findings</strong></h5> <p><strong>1. Memory Dominates Over Compute</strong></p> <p>Across all MoE variants, memory pressure—not computational throughput—emerges as the primary bottleneck. Even modest expert counts (4 experts) require 2-3× the memory of dense baselines, fundamentally limiting MoE applicability on consumer hardware.</p> <p><strong>2. Kernel Optimization Improves Efficiency but Not Convergence</strong></p> <p>ScatterMoE demonstrates that fused routing kernels can simultaneously reduce step time (16% improvement) and memory usage (18% reduction) compared to naive implementations. However, these systems-level gains do not translate to better final model quality, highlighting the independence of execution efficiency and optimization dynamics.</p> <p><strong>3. Architectural Design Determines Convergence</strong></p> <p>The DeepSeek-inspired architecture achieves the best convergence (3.94 validation loss vs 4.00 dense baseline) through architectural innovations—shared experts, fine-grained specialization, and top-k routing—rather than kernel optimizations. This demonstrates that <strong>convergence quality stems from representational capacity, not execution speed</strong>.</p> <p><strong>4. Distributed Designs Don’t Transfer to Single-GPU</strong></p> <p>MegaBlocks, optimized for large-scale distributed training, achieves relatively low step time but incurs prohibitive memory overhead (19 GB reserved), rendering it impractical for single-GPU use. This underscores that MoE systems designed for datacenter environments require fundamental redesign for consumer hardware.</p> <h5 id="practical-implications"><strong>Practical Implications</strong></h5> <p>For practitioners considering MoE on single GPUs:</p> <ul> <li> <strong>Dense models remain most efficient</strong> for typical single-GPU training scenarios</li> <li><strong>MoE is viable only when convergence quality justifies 2-3× resource overhead</strong></li> <li><strong>Architectural choices (shared experts, routing strategy) matter more than kernel optimizations</strong></li> <li><strong>Successful MoE training requires co-design of architecture and systems implementation</strong></li> </ul> <h5 id="broader-impact"><strong>Broader Impact</strong></h5> <p>Our findings challenge the conventional narrative that MoE architectures uniformly improve efficiency. Instead, we demonstrate that MoE’s benefits are highly context-dependent:</p> <ul> <li>In distributed settings with abundant memory and parallelism, MoE can dramatically improve parameter efficiency</li> <li>On single GPUs with tight memory constraints, MoE often underperforms dense baselines</li> <li>Effective MoE requires careful alignment of architectural design, systems implementation, and hardware characteristics</li> </ul> <p>This work highlights the importance of <strong>systems-aware architectural design</strong> in modern machine learning. As the field moves toward increasingly complex models, understanding the interplay between algorithm, implementation, and hardware becomes critical for practical deployment.</p> <hr> <h4 id="resources"><strong>Resources</strong></h4> <ul> <li> <strong>Code Repository:</strong> <a href="https://github.com/Monishver11/modded-nanogpt-moe" rel="external nofollow noopener" target="_blank">GitHub - Modded NanoGPT MoE</a> </li> <li> <strong>Technical Report:</strong> <a href="https://drive.google.com/file/d/1trwL6Svi8Kff0kgyABd7uk1gIfrm0vVB/view?usp=sharing" rel="external nofollow noopener" target="_blank">Full Paper (PDF)</a> </li> <li> <strong>Presentation:</strong> <a href="https://drive.google.com/file/d/1_bhe0tOPinUhAaXSTYMVyyql6s0f8nXk/view?usp=sharing" rel="external nofollow noopener" target="_blank">Slides (PDF)</a> </li> </ul> <p><strong>References</strong>:</p> <ol> <li> <p>Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., &amp; Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. <em>ICLR 2017</em>.</p> </li> <li> <p>Fedus, W., Zoph, B., &amp; Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. <em>JMLR</em>, 23(120), 1–39.</p> </li> <li> <p>Gale, T., Zaharia, M., Young, C., &amp; Shafahi, A. (2024). Scatter-MoE: Efficient sparse mixture-of-experts via token routing optimization. <em>arXiv preprint arXiv:2403.08245</em>.</p> </li> <li> <p>Gale, T., Narayanan, D., Young, C., &amp; Zaharia, M. (2023). MegaBlocks: Efficient sparse training with mixture-of-experts. <em>Proceedings of MLSys</em>, 5, 288–304.</p> </li> <li> <p>DeepSeek-AI. (2024). DeepSeek-V2: A strong, economical, and efficient mixture-of-experts language model. <em>arXiv preprint arXiv:2405.04434</em>.</p> </li> <li> <p>Dai, D., Deng, C., Zhao, C., et al. (2024). DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. <em>arXiv preprint arXiv:2401.06066</em>.</p> </li> <li> <p>Karpathy, A. (2024). <a href="https://github.com/KellerJordan/modded-nanogpt" rel="external nofollow noopener" target="_blank">Modded NanoGPT</a> - NanoGPT speedrun baseline.</p> </li> </ol> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-distributed-systems-lecture-1",title:"Distributed Systems - Lecture 1",description:"Distributed Systems Course at NYU Courant - Personal Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2026/ds1/"}},{id:"post-llmr-lecture-1",title:"LLMR - Lecture 1",description:"LLM Reasoners Course at NYU Courant - Personal Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2026/llmr1/"}},{id:"post-apache-flink",title:"Apache Flink",description:"Realtime and Big Data Analytics Course at NYU Courant - Personal Notes 10",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-11-flink/"}},{id:"post-apache-kafka",title:"Apache Kafka",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 9",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-10-kafka/"}},{id:"post-apache-zookeeper",title:"Apache ZooKeeper",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 8",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-9-zookeeper/"}},{id:"post-apache-hbase",title:"Apache HBase",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 7",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-8-hbase/"}},{id:"post-hive-amp-trino",title:"Hive &amp; Trino",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 6",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-7-hive/"}},{id:"post-mapreduce-design-patterns",title:"MapReduce Design Patterns",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 5",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-5-mr-dp/"}},{id:"post-big-data-processing-concepts-amp-mapreduce",title:"Big Data Processing Concepts &amp; MapReduce",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-4-mapreduce/"}},{id:"post-hadoop-distributed-file-system-hdfs",title:"Hadoop Distributed File System (HDFS)",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-3-hdfs/"}},{id:"post-reading-notes-from-aleksa-gordic-39-s-gpu-blogpost",title:"Reading Notes from Aleksa Gordic&#39;s GPU BlogPost",description:"Reading notes for my reference from Aleksa Gordic&#39;s GPU BlogPost",section:"Posts",handler:()=>{window.location.href="/blog/2025/aleksagordic-gpu-blog-notes/"}},{id:"post-mlp-standard-derivatives-derivation",title:"MLP Standard Derivatives Derivation",description:"MLP Standard Derivatives Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-derivatives/"}},{id:"post-simple-mlp-forward-and-backward-pass-with-einsum-derivation",title:"Simple MLP - Forward and Backward Pass (With Einsum) Derivation",description:"Simple MLP - Forward and Backward Pass Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-fw-bwd/"}},{id:"post-gpu-notes",title:"GPU Notes",description:"GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes",section:"Posts",handler:()=>{window.location.href="/blog/2025/gpu-notes/"}},{id:"post-big-data-storage",title:"Big Data Storage",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 2",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-2-storage/"}},{id:"post-introduction-to-realtime-and-big-data-analytics",title:"Introduction to Realtime and Big Data Analytics",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-1-intro/"}},{id:"post-gpu-essentials-a-concise-technical-guide",title:"GPU Essentials - A Concise Technical Guide",description:"A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.",section:"Posts",handler:()=>{window.location.href="/blog/2025/GPU-Intro/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab",title:"Sharing personal reflections - Thoughts Tab",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-smallgraphgcn-accelerating-gnn-training-on-batched-small-graphs",title:"SmallGraphGCN - Accelerating GNN Training on Batched Small Graphs",description:"Discover how fused-edge-centric CUDA kernels dramatically accelerate Graph Neural Network training on molecular datasets. By rethinking parallelism strategies for batched small graphs, our approach achieves up to 3.1\xd7 faster forward execution and 1.3\xd7 end-to-end training speedup over PyTorch Geometric.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-from-baseline-to-deepseek-single-gpu-moe-training-efficiency",title:"From Baseline to DeepSeek - Single-GPU MoE Training Efficiency",description:"A systems-level analysis of training Mixture-of-Experts (MoE) Transformer models under single-GPU constraints. We compare naive PyTorch MoE, ScatterMoE, MegaBlocks, and DeepSeek-inspired architectures, revealing critical trade-offs between convergence behavior, memory footprint, and training throughput.",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>