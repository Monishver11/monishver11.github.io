<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unveiling Probabilistic Modeling | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/probabilistic-modeling/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unveiling Probabilistic Modeling</h1> <p class="post-meta"> Created in January 17, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h4 id="why-probabilistic-modeling"><strong>Why Probabilistic Modeling?</strong></h4> <p>Probabilistic modeling offers a unified framework that underpins many machine learning methods, from linear regression to logistic regression and beyond. At its core, probabilistic modeling allows us to handle uncertainty and make informed decisions based on observed data. It provides a principled way to update our beliefs about the data-generating process as new information becomes available.</p> <p>In machine learning, we often think of learning as statistical inference, where the goal is to use data to draw conclusions about the underlying distribution or process that generated it, rather than simply fitting a model to the observed data. In this view, the goal is not just to fit a model to data, but to estimate the underlying distribution that best explains the observed data. Probabilistic methods give us a powerful tool to incorporate our beliefs about the world—often referred to as inductive biases—into the learning process. This allows us to make more informed predictions and gain deeper insights into the data.</p> <p>For example, in Bayesian inference, prior beliefs are combined with evidence from the data to update our understanding of the underlying process. This principled approach enables us to not only predict outcomes but also quantify our confidence in those predictions, making probabilistic modeling a powerful tool for developing robust, interpretable, and informed machine learning systems.</p> <h5 id="two-ways-of-generating-data"><strong>Two Ways of Generating Data</strong></h5> <p>When we think about how data is generated, there are two main perspectives to consider. The first is through <strong>conditional models</strong>, where we model the likelihood of the output \(y\) given the input \(x\). This is often denoted as \(p(y \vert x)\).</p> <p>The second perspective is through <strong>generative models</strong>, where we model the joint distribution of both the input \(x\) and the output \(y\), denoted as \(p(x, y)\).</p> <p>To understand the distinction between conditional models and generative models, let’s use the analogy of handwriting recognition:</p> <ul> <li> <p><strong>Conditional Models</strong>: Imagine you are given a handwritten letter and asked to identify the corresponding alphabet. Here, you focus only on the relationship between the handwriting (\(x\)) and the letter it represents (\(y\)). This corresponds to modeling \(p(y \mid x)\), where you predict the output \(y\) (the letter) conditioned on the input \(x\) (the handwriting). Essentially, you’re answering the question, <em>“What is the most likely letter given this handwriting?”</em></p> </li> <li> <p><strong>Generative Models</strong>: Now, imagine that instead of just recognizing handwriting, you also aim to generate realistic handwriting for any letter. To do this, you need to understand how the letters (\(y\)) and handwriting styles (\(x\)) are generated together. This involves modeling the joint distribution \(p(x, y)\), where you learn how inputs and outputs are related as part of a larger generative process. The question here becomes, <em>“How are handwriting (\(x\)) and letters (\(y\)) jointly produced?”</em></p> </li> </ul> <p>Each approach offers different advantages depending on the context. However, both share the common goal of estimating the parameters of the model, often using a technique called <strong>Maximum Likelihood Estimation (MLE)</strong>.</p> <hr> <h4 id="conditional-models"><strong>Conditional Models</strong></h4> <p>Conditional models focus on predicting the output given the input. One of the most well-known and widely used conditional models is <strong>linear regression</strong>. Let’s take a closer look at linear regression and how it fits within this probabilistic framework.</p> <h4 id="linear-regression"><strong>Linear Regression</strong></h4> <p>Linear regression is a fundamental technique in both machine learning and statistics. Its primary goal is to predict a real-valued target \(y\) (also called the response variable) from a vector of features \(x\) (also known as covariates). Linear regression is often used in situations where we want to predict a continuous value, such as:</p> <ul> <li>Predicting house prices based on factors like location, condition, and age of the house.</li> <li>Estimating medical costs of a person based on their age, sex, region, and BMI.</li> <li>Predicting someone’s age from their photograph.</li> </ul> <h5 id="the-problem-setup"><strong>The Problem Setup</strong></h5> <p>In linear regression, we are given a set of training examples, \(D = \{(x^{(n)}, y^{(n)})\}_{n=1}^N\), where \(x^{(n)} \in \mathbb{R}^d\) represents the features and \(y^{(n)} \in \mathbb{R}\) represents the target. The task is to model the relationship between the features \(x\) and the target \(y\).</p> <p>To do this, we assume that there is a linear relationship between \(x\) and \(y\), which can be expressed as:</p> \[h(x) = \theta^T x = \sum_{i=0}^{d} \theta_i x_i\] <p>Here, \(\theta \in \mathbb{R}^d\) represents the parameters (also known as the weights) of the model, and \(x_0 = 1\) is the bias term. The goal is to find the values of \(\theta\) that best explain the observed data.</p> <blockquote> <p><em>“We use superscript to denote the example id and subscript to denote the dimension id”</em></p> </blockquote> <h5 id="unveiling-probabilistic-modeling"><strong>Unveiling Probabilistic Modeling</strong></h5> <p>To estimate the parameters \(\theta\), we use the <strong>least squares method</strong>, which involves minimizing the squared loss between the predicted and observed values. The loss function is defined as:</p> \[J(\theta) = \frac{1}{N} \sum_{n=1}^{N} \left( y^{(n)} - \theta^T x^{(n)} \right)^2\] <p>This function represents the <strong>empirical risk</strong>, which quantifies the difference between the predicted and actual values across all the training examples.</p> <h5 id="matrix-formulation"><strong>Matrix Formulation</strong></h5> <p>We can also express this problem in matrix form for efficiency. Let \(X \in \mathbb{R}^{N \times d}\) be the design matrix, whose rows represent the input features for each training example. Let \(y \in \mathbb{R}^N\) be the vector of all target values. The objective is to solve for the parameter vector \(\hat{\theta}\) that minimizes the loss:</p> \[\hat{\theta} = \arg\min_\theta \left( (X\theta - y)^T (X\theta - y) \right)\] <h5 id="closed-form-solution"><strong>Closed-Form Solution</strong></h5> <p>The closed-form solution to this optimization problem is:</p> \[\hat{\theta} = (X^T X)^{-1} X^T y\] <p>This gives us the values for \(\theta\) that minimize the squared loss, and hence, provide the best linear model for the data.</p> <hr> <p>Before proceeding further, here are a few review questions. Ask yourself these and check the answers.</p> <h5 id="how-do-we-derive-the-solution-for-linear-regression"><strong>How do we derive the solution for linear regression?</strong></h5> <p>The squared loss function in matrix form is:</p> \[J(\theta) = \frac{1}{N} (X\theta - y)^T (X\theta - y)\] <p>To minimize \(J(\theta)\), we compute the gradient with respect to \(\theta\):</p> <ul> <li>Expand the quadratic term:</li> </ul> \[J(\theta) = \frac{1}{N} \left[ \theta^T X^T X \theta - 2y^T X \theta + y^T y \right]\] <ul> <li>Take the derivative with respect to \(\theta\): <ul> <li>Recall that for any vector \(a\), \(b\), and matrix \(A\), the following derivatives are useful: <ul> <li> \[\frac{\partial (a^T b)}{\partial a} = b\] </li> <li> \[\frac{\partial (a^T A a)}{\partial a} = 2A a\] </li> </ul> <p>(when \(A\) is symmetric).</p> </li> </ul> </li> </ul> <p>Applying these rules:</p> \[\nabla_\theta J(\theta) = \frac{1}{N} \left[ 2X^T X \theta - 2X^T y \right].\] <ul> <li>Set the gradient to zero to find the minimizer:</li> </ul> \[X^T X \theta = X^T y\] <ul> <li>Solve for \(\theta\):</li> </ul> \[\theta = (X^T X)^{-1} X^T y,\] <p>provided \(X^T X\) is invertible.</p> <p><strong>Note:</strong> The \(\frac{1}{N}\) normalization factor is constant and cancels out when setting the gradient to zero, so it does not affect the solution for \(\theta\).</p> <h5 id="why-do-transposes-appear-or-disappear"><strong>Why Do Transposes Appear or Disappear?</strong></h5> <ol> <li> <p><strong>Symmetry of Quadratic Terms</strong>:<br> In the term \(\theta^T X^T X \theta\), note that \(X^T X\) is a symmetric matrix (because \(X^T X = (X^T X)^T\)). This symmetry ensures that when taking the derivative, we don’t need to explicitly add or remove transposes; they naturally align.</p> </li> <li> <strong>Consistency of Vector-Matrix Multiplication</strong>:<br> When differentiating terms like \(y^T X \theta\), we use the rule \(\frac{\partial (a^T b)}{\partial a} = b\), ensuring dimensions match. This often introduces or removes a transpose based on the structure of the derivative. For example: <ul> <li>\(\nabla_\theta (-2y^T X \theta) = -2X^T y\), where \(X^T\) arises naturally to align dimensions.</li> </ul> </li> <li> <strong>Gradient Conventions</strong>:<br> The transpose changes are necessary to ensure the resulting gradient is a column vector (matching \(\theta\)’s shape), as gradients are typically represented in the same dimensionality as the parameter being differentiated.</li> </ol> <h5 id="what-happens-if-xt-x-is-not-invertible"><strong>What happens if \(X^T X\) is not invertible?</strong></h5> <p>If \(X^T X\) is not invertible (also called singular or degenerate), the normal equations do not have a unique solution. This happens in cases such as:</p> <ul> <li> <strong>Linearly dependent features</strong>: Some columns of \(X\) are linear combinations of others.</li> <li> <strong>Too few data points</strong>: If \(N &lt; d\) (more features than samples), \(X^T X\) will not be full rank.</li> </ul> <p>To address this issue, we can:</p> <ol> <li> <strong>Add regularization</strong>: Use techniques like Ridge Regression, which modifies the normal equation to include a penalty term:<br> \(\theta = (X^T X + \lambda I)^{-1} X^T y,\)<br> where \(\lambda &gt; 0\) is the regularization parameter.</li> <li> <strong>Remove redundant features</strong>: Perform feature selection or dimensionality reduction (e.g., PCA) to eliminate linear dependencies.</li> <li> <strong>Use pseudo-inverse</strong>: Compute the Moore-Penrose pseudo-inverse of \(X^T X\) to find a solution.</li> </ol> <hr> <h4 id="understanding-linear-regression-through-a-probabilistic-lens"><strong>Understanding Linear Regression Through a Probabilistic Lens</strong></h4> <p>So far, we’ve discussed how linear regression can be understood as minimizing the squared loss. But why is the squared loss a reasonable choice for regression problems? To answer this, we need to think about the assumptions we are making on the data.</p> <p>Let’s approach linear regression from a <strong>probabilistic modeling perspective</strong>.</p> <h5 id="assumptions-in-linear-regression"><strong>Assumptions in Linear Regression</strong></h5> <p>In this framework, we assume that the target \(y\) and the features \(x\) are related through a linear function, with an added error term \(\epsilon\):</p> \[y = \theta^T x + \epsilon\] <p>Here, \(\epsilon\) represents the residual error that accounts for all unmodeled effects, such as noise or other sources of variation in the data. We assume that these errors \(\epsilon\) are independent and identically distributed (iid) and follow a normal distribution:</p> \[\epsilon \sim \mathcal{N}(0, \sigma^2)\] <p>Given this assumption, the conditional distribution of \(y\) given \(x\) is a normal distribution with mean \(\theta^T x\) and variance \(\sigma^2\):</p> \[p(y | x; \theta) = \mathcal{N}(\theta^T x, \sigma^2)\] <h5 id="intuition-behind-the-gaussian-distribution"><strong>Intuition Behind the Gaussian Distribution</strong></h5> <p>This distribution suggests that, for each value of \(x\), the output \(y\) is normally distributed around the value predicted by the linear model \(\theta^T x\), with a fixed variance \(\sigma^2\) that captures the uncertainty or noise in the data. In other words, we place a Gaussian “bump” around the output of the linear predictor, reflecting the uncertainty in our prediction.</p> <p>With this, we’ve laid the groundwork for our discussion on Maximum Likelihood Estimation.</p> <hr> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>In this post, we introduced how probabilistic modeling can be used for understanding and estimating machine learning models, such as linear regression. By thinking of learning as statistical inference, we can incorporate our prior beliefs about the data-generating process and make more informed predictions.</p> <p>Next, we’ll dive into <strong>Maximum Likelihood Estimation (MLE)</strong> and examine how it can be applied to solve probabilistic linear regression and other machine learning algorithms. We’ll also explore how to formalize this understanding—stay tuned!</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>