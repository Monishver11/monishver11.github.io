<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding the Kernel Trick | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?e68e4955e21b20101db6e28a5a50abec"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/kernel-trick/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Understanding the Kernel Trick</h1> <p class="post-meta"> Created in January 13, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>When working with machine learning models, especially Support Vector Machines (SVMs), the idea of mapping data into a higher-dimensional space often comes into play. This mapping helps transform non-linearly separable data into a space where linear decision boundaries can be applied. But what happens when the dimensionality of the feature space becomes overwhelmingly large? This is where the <strong>kernel trick</strong> saves the day. In this post, we will explore the kernel trick, starting with SVMs, their reliance on feature mappings, and how inner products in feature space can be computed without ever explicitly constructing that space.</p> <hr> <h4 id="svms-with-explicit-feature-maps"><strong>SVMs with Explicit Feature Maps</strong></h4> <p>To understand the kernel trick, let’s begin with SVMs. In the simplest case, an SVM aims to find a hyperplane that separates data into classes with the largest possible margin. To handle more complex data, we map the input data \(\mathbf{x}\) into a higher-dimensional feature space using a feature map \(\psi: X \to \mathbb{R}^d\). In this space, the SVM optimization problem can be written as:</p> \[\min_{\mathbf{w} \in \mathbb{R}^d} \frac{1}{2} \|\mathbf{w}\|^2 + \frac{c}{n} \sum_{i=1}^n \max(0, 1 - y_i \mathbf{w}^T \psi(\mathbf{x}_i)).\] <p>Here, \(\mathbf{w}\) is the weight vector, \(c\) is a regularization parameter, and \(y_i\) are the labels of the data points. While this approach works well for small \(d\), it becomes computationally expensive as \(d\) increases, especially when using high-degree polynomial mappings.</p> <p>To address this issue, we turn to a reformulation of the SVM problem, derived from <strong>Lagrangian duality</strong>.</p> <h4 id="the-svm-dual-problem"><strong>The SVM Dual Problem</strong></h4> <p>Through Lagrangian duality, the SVM optimization problem can be re-expressed as a dual problem:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \psi(\mathbf{x}_j)^T \psi(\mathbf{x}_i),\] <p>subject to:</p> \[\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \in \left[ 0, \frac{c}{n} \right] \quad \forall i.\] <p>Here, \(\alpha_i\) are the dual variables (Lagrange multipliers). Once the optimal \(\boldsymbol{\alpha}^*\) is obtained, the weight vector in the feature space can be reconstructed as:</p> \[\mathbf{w}^* = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i).\] <p>The decision function for a new input \(\mathbf{x}\) is given by:</p> \[\hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i)^T \psi(\mathbf{x}).\] <h5 id="observing-the-role-of-inner-products"><strong>Observing the Role of Inner Products</strong></h5> <p>An important observation here is that the feature map \(\psi(\mathbf{x})\) appears only through inner products of the form \(\psi(\mathbf{x}_j)^T \psi(\mathbf{x}_i)\). This means we don’t actually need the explicit feature representation \(\psi(\mathbf{x})\); instead, we just need the ability to compute these inner products efficiently.</p> <hr> <h4 id="computing-inner-products-in-practice"><strong>Computing Inner Products in Practice</strong></h4> <p>Let’s explore the kernel trick with an example.</p> <h5 id="example-degree-2-monomials"><strong>Example: Degree-2 Monomials</strong></h5> <p>Suppose we are working with 2D data points \(\mathbf{x} = (x_1, x_2)\). If we map the data into a space of degree-2 monomials, the feature map becomes:</p> \[\psi: \mathbb{R}^2 \to \mathbb{R}^6, \quad (x_1, x_2) \mapsto (1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, \sqrt{2}x_1x_2, x_2^2).\] <p>The inner product in the feature space is:</p> \[\psi(\mathbf{x})^T \psi(\mathbf{x}') = 1 + 2x_1x_1' + 2x_2x_2' + (x_1x_1')^2 + 2x_1x_2x_1'x_2' + (x_2x_2')^2.\] <p>Simplifying, we observe:</p> \[\psi(\mathbf{x})^T \psi(\mathbf{x}') = (1 + x_1x_1' + x_2x_2')^2 = (1 + \mathbf{x}^T \mathbf{x}')^2.\] <p>This shows that we can compute \(\psi(\mathbf{x})^T \psi(\mathbf{x}')\) directly from the original input space without explicitly constructing \(\psi(\mathbf{x})\)—a key insight behind the kernel trick.</p> <h5 id="general-case-monomials-up-to-degree-p"><strong>General Case: Monomials Up to Degree \(p\)</strong></h5> <p>For feature maps that produce monomials up to degree \(p\), the inner product generalizes as:</p> \[\psi(x)^T \psi(x') = (1 + x^T x')^p.\] <p>It is worth noting that the coefficients of the monomials in \(\psi(x)\) may vary depending on the specific feature map.</p> <hr> <h4 id="efficiency-of-the-kernel-trick-from-exponential-to-linear-complexity"><strong>Efficiency of the Kernel Trick: From Exponential to Linear Complexity</strong></h4> <p>One of the key advantages of the kernel trick is its ability to reduce the computational complexity of working with high-dimensional feature spaces. Let’s break this down:</p> <h5 id="explicit-computation-complexity"><strong>Explicit Computation Complexity</strong></h5> <p>When we map an input vector \(\mathbf{x} \in \mathbb{R}^d\) to a feature space with monomials up to degree \(p\), the dimensionality of the feature space increases significantly. Specifically:</p> <ul> <li> <p><strong>Feature Space Dimension</strong>: The number of features in the expansion is:</p> \[\binom{d + p}{p} = \frac{(d + p)!}{d! \, p!}.\] <p>For large \(p\) or \(d\), this grows rapidly and can quickly become computationally prohibitive.</p> </li> <li> <p><strong>Explicit Inner Product</strong>: Computing the inner product directly in this expanded space has a complexity of:</p> \[O\left(\binom{d + p}{p}\right),\] <p>which is exponential in \(p\) for fixed \(d\).</p> </li> </ul> <h5 id="implicit-computation-complexity"><strong>Implicit Computation Complexity</strong></h5> <p>Using the kernel trick, we avoid explicitly constructing the feature space. For a kernel function like:</p> \[k(\mathbf{x}, \mathbf{x}') = (1 + \mathbf{x}^T \mathbf{x}')^p,\] <p>the computation operates directly in the input space.</p> <ul> <li> <p><strong>Input Space Computation</strong>: Computing the kernel function involves:</p> <ol> <li> <strong>Dot Product</strong>: \(\mathbf{x}^T \mathbf{x}'\) is computed in \(O(d)\).</li> <li> <strong>Polynomial Evaluation</strong>: Raising this result to power \(p\) is done in constant time, independent of \(d\).</li> </ol> </li> </ul> <p>Thus, the complexity is reduced to:</p> \[O(d),\] <p>which is <strong>linear</strong> in the input dimensionality \(d\), regardless of \(p\).</p> <h5 id="why-this-matters"><strong>Why This Matters</strong></h5> <ul> <li> <strong>Explicit Features</strong>: For high \(p\), the feature space grows exponentially, leading to a <strong>curse of dimensionality</strong> if explicit computation is used.</li> <li> <strong>Implicit Kernel Computation</strong>: The kernel trick sidesteps the explicit feature space, allowing efficient computation even when the feature space is high-dimensional or infinite (e.g., with RBF kernels).</li> </ul> <p>This transformation from <strong>exponential</strong> to <strong>linear complexity</strong> is one of the core reasons kernel methods are powerful tools in machine learning.</p> <p><strong>Key Takeaway</strong> : The kernel trick enables efficient computation in high-dimensional feature spaces by directly working in the input space. This reduces the complexity from \(O\left(\binom{d + p}{p}\right)\) to \(O(d)\), making it feasible to apply machine learning methods to problems with high-degree polynomial or infinite-dimensional feature spaces.</p> <hr> <h4 id="exploring-the-kernel-function"><strong>Exploring the Kernel Function</strong></h4> <p>To fully appreciate the kernel trick, we need to formalize the concept of the <strong>kernel function</strong>. In our earlier discussion, we introduced the idea of a feature map \(\psi: X \to \mathcal{H}\), which maps input data from the original space \(X\) to a higher-dimensional feature space \(\mathcal{H}\). The kernel function \(k\) corresponding to this feature map is defined as:</p> \[k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle,\] <p>where \(\langle \cdot, \cdot \rangle\) represents the inner product in \(\mathcal{H}\).</p> <h5 id="why-use-kernel-functions"><strong>Why Use Kernel Functions?</strong></h5> <p>At first glance, this notation might seem like a trivial restatement of the inner product, but it’s far more powerful. The key insight is that we can often evaluate \(k(\mathbf{x}, \mathbf{x}')\) directly, without explicitly computing \(\psi(\mathbf{x})\) and \(\psi(\mathbf{x}')\). This is crucial for efficiently working with high-dimensional or infinite-dimensional feature spaces. But this efficiency only applies to certain methods — those that can be <strong>kernelized</strong>.</p> <h4 id="kernelized-methods"><strong>Kernelized Methods</strong></h4> <p>A method is said to be <strong>kernelized</strong> if it uses the feature vectors \(\psi(\mathbf{x})\) only inside inner products of the form \(\langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle\). For such methods, we can replace these inner products with a kernel function \(k(\mathbf{x}, \mathbf{x}')\), avoiding explicit feature computation. This applies to both the optimization problem and the prediction function. Let’s revisit the SVM example to see kernelization in action.</p> <h5 id="kernelized-svm-dual-formulation"><strong>Kernelized SVM Dual Formulation</strong></h5> <p>Recall the dual problem for SVMs:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle,\] <p>subject to:</p> \[\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \in \left[ 0, \frac{c}{n} \right] \quad \forall i.\] <p><strong>Here’s the key</strong>: because every occurrence of \(\psi(\mathbf{x})\) is inside an inner product, we can replace \(\langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle\) with \(k(\mathbf{x}_i, \mathbf{x}_j)\), the kernel function. The resulting dual optimization problem becomes:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j),\] <p>subject to the same constraints.</p> <p>For predictions, the decision function can also be written in terms of the kernel:</p> \[\hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i k(\mathbf{x}_i, \mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i)^T \psi(\mathbf{x})\] <p>This reformulation is what allows SVMs to operate efficiently in high-dimensional spaces.</p> <h5 id="the-kernel-matrix"><strong>The Kernel Matrix</strong></h5> <p>A key component in kernelized methods is the <strong>kernel matrix</strong>, which encapsulates the pairwise kernel values for all data points. For a dataset \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}\), the kernel matrix \(\mathbf{K}\) is defined as:</p> \[\mathbf{K} = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}.\] <p>This \(n \times n\) matrix, also known as the <strong>Gram matrix</strong> in machine learning, summarizes all the information about the training data necessary for solving the kernelized optimization problem.</p> <p>For the kernelized SVM, we can replace \(\langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle\) with \(K_{ij}\), reducing the dual problem to:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K_{ij},\] <p>subject to the same constraints.</p> <p><strong>So, Given a kernelized ML algorithm</strong> (i.e., all \(\psi(x)\)’s show up as \(\langle \psi(x), \psi(x') \rangle\)) :</p> <ol> <li> <strong>Flexibility</strong>: By substituting the kernel function, we can implicitly use very high-dimensional or even infinite-dimensional feature spaces.</li> <li> <strong>Scalability</strong>: Once the kernel matrix is computed, the computational cost depends on the number of data points \(n\), rather than the dimension of the feature space \(d\).</li> <li> <strong>Efficiency</strong>: For many kernels, \(k(\mathbf{x}, \mathbf{x}')\) can be computed without directly accessing the high-dimensional feature representation \(\psi(\mathbf{x})\), avoiding the \(O(d)\) dependence.</li> </ol> <p>These properties make kernelized methods invaluable when \(d \gg n\), a common scenario in machine learning tasks.</p> <p>The kernel trick revolutionizes how we think about high-dimensional data. Next, we will delve into popular kernel functions, their interpretations, and how to choose the right one for your problem.</p> <hr> <h4 id="example-kernels"><strong>Example Kernels</strong></h4> <p>In many cases, it’s useful to think of the kernel function \(k(x, x')\) as a <strong>similarity score</strong> between the data points \(x\) and \(x'\). This perspective allows us to design similarity functions without explicitly considering the feature map.</p> <p>For example, we can create <strong>string kernels</strong> or <strong>graph kernels</strong>—functions that define similarity based on the structure of strings or graphs, respectively. The key question, however, is: <strong>How do we know that our kernel functions truly correspond to inner products in some feature space?</strong></p> <p>This is an essential consideration, as it ensures that the kernel method preserves the properties necessary for various machine learning algorithms to work effectively. Let’s break this down.</p> <h5 id="how-to-obtain-kernels"><strong>How to Obtain Kernels?</strong></h5> <p>There are two primary ways to define kernels:</p> <ol> <li> <p><strong>Explicit Construction</strong>: Define the feature map \(\psi(\mathbf{x})\) and use it to compute the kernel: \(k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle.\) (e.g. monomials)</p> </li> <li> <p><strong>Direct Definition</strong>: Directly define the kernel \(k(\mathbf{x}, \mathbf{x}')\) as a similarity score and verify that it corresponds to an inner product for some \(\psi\). This verification is often guided by mathematical theorems.</p> </li> </ol> <p>To understand this better, let’s first equip ourselves with some essential linear algebra concepts.</p> <h5 id="positive-semidefinite-matrices-and-kernels"><strong>Positive Semidefinite Matrices and Kernels</strong></h5> <p>To verify if a kernel corresponds to a valid inner product, we rely on the concept of <strong>positive semidefinite (PSD) matrices</strong>. Here’s a quick refresher:</p> <ul> <li> <p>A matrix \(\mathbf{M} \in \mathbb{R}^{n \times n}\) is positive semidefinite if: \(\mathbf{x}^\top \mathbf{M} \mathbf{x} \geq 0, \quad \forall \mathbf{x} \in \mathbb{R}^n.\)</p> </li> <li> <p>Equivalent conditions, each necessary and sufficient for a symmetric matrixfor \(\mathbf{M}\) being <strong>PSD</strong>:</p> <ul> <li>\(\mathbf{M} = \mathbf{R}^\top \mathbf{R}\), for some matrix \(\mathbf{R}\).</li> <li>All eigenvalues of \(\mathbf{M}\) are non-negative or \(\geq 0\).</li> </ul> </li> </ul> <p>Next, we define a <strong>positive definite (PD) kernel</strong>:</p> <h5 id="positive-definite-kernel"><strong>Positive Definite Kernel</strong></h5> <p><strong>Definition:</strong></p> <p>A symmetric function \(k: X \times X \to \mathbb{R}\) is a <strong>PD</strong> kernel if, for any finite set \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\} \subset X\), the kernel matrix:</p> \[\mathbf{K} = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}\] <p>is positive semidefinite.</p> <ol> <li>Symmetry: \(k(\mathbf{x}, \mathbf{x}') = k(\mathbf{x}', \mathbf{x})\).</li> <li>The kernel matrix needs to be positive semidefinite for any finite set of points.</li> <li>Equivalently: \(\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j k(\mathbf{x}_i, \mathbf{x}_j) \geq 0\), for all \(\alpha_i \in \mathbb{R}\) \(\forall i\).</li> </ol> <h6 id="think-of-it-this-way"><strong>Think of it this way:</strong></h6> <ol> <li> <p><strong>Symmetry</strong>:<br> Symmetry ensures the kernel measures similarity consistently between any two points: \(k(\mathbf{x}, \mathbf{x}') = k(\mathbf{x}', \mathbf{x}).\)<br> For example, the similarity between \(\mathbf{x}\) and \(\mathbf{x}'\) is the same as that between \(\mathbf{x}'\) and \(\mathbf{x}\).</p> </li> <li> <p><strong>Positive Semidefiniteness</strong>:<br> Positive semidefiniteness ensures the kernel corresponds to a <strong>valid inner product</strong> in some (possibly high-dimensional or infinite-dimensional) feature space.</p> <ul> <li>Think of the kernel as a measure of similarity: this property ensures that the relationships it captures are geometrically valid in that feature space.</li> <li>Intuitively, this means the kernel matrix does not produce “negative energy,” ensuring a consistent representation of the data.</li> </ul> </li> </ol> <p><strong>Simpler Way to State It:</strong></p> <ul> <li>A kernel is <strong>PD</strong> if it acts like an inner product in some feature space.</li> <li>For any set of points, the kernel matrix must be symmetric and positive semidefinite.</li> <li>Symmetry ensures the similarity is consistent in both directions, while positive semidefiniteness guarantees geometrically valid relationships in the feature space.</li> </ul> <h5 id="mercers-theorem"><strong>Mercer’s Theorem</strong></h5> <p>Mercer’s Theorem provides a foundational result for kernels. It states:</p> <ul> <li> <p>A symmetric function \(k(\mathbf{x}, \mathbf{x}')\) can be expressed as an inner product</p> \[k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle\] <p>if and only if \(k(\mathbf{x}, \mathbf{x}')\) is positive definite.</p> </li> </ul> <p>While proving that a kernel is <strong>PD</strong> can be challenging, we can use known kernels to construct new ones.</p> <h5 id="constructing-new-kernels-from-existing-ones"><strong>Constructing New Kernels from Existing Ones</strong></h5> <p>Given valid PD kernels \(k_1\) and \(k_2\), we can create new kernels using the following operations:</p> <ol> <li> <strong>Non-Negative Scaling</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = \alpha k(\mathbf{x}, \mathbf{x}')\), where \(\alpha \geq 0\).</li> <li> <strong>Addition</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') + k_2(\mathbf{x}, \mathbf{x}')\).</li> <li> <strong>Multiplication</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')\).</li> <li> <strong>Recursion</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k(\psi(\mathbf{x}), \psi(\mathbf{x}'))\), for any function \(\psi(\cdot)\).</li> <li> <strong>Feature Mapping</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = f(\mathbf{x}) f(\mathbf{x}')\), for any function \(f(\cdot)\).</li> </ol> <p>And, Lot more ways to help you construct new kernels from old.</p> <p>It should be noted that Mercer’s theorem only tells us when a candidate similarity function is admissible for use. It tells nothing about how good such a kernel function is.</p> <p>Next, we’ll dive into some of the most widely used kernel functions.</p> <hr> <h4 id="the-linear-kernel"><strong>The Linear Kernel</strong></h4> <p>The linear kernel is the simplest and most intuitive kernel function. Imagine working with data in an input space represented as \(X = \mathbb{R}^d\). Here, the feature space, denoted as \(\mathcal{H}\), is the same as the input space \(\mathbb{R}^d\). The feature map for this kernel is straightforward: \(\psi(x) = x\).</p> <p>The kernel function itself is defined as:</p> \[k(x, x') = \langle x, x' \rangle = x^\top x',\] <p>where \(\langle x, x' \rangle\) represents the standard inner product. This simplicity makes the linear kernel computationally efficient and ideal for linear models.</p> <h4 id="the-quadratic-kernel"><strong>The Quadratic Kernel</strong></h4> <p>The quadratic kernel takes us a step further by mapping the input space \(X = \mathbb{R}^d\) into a higher-dimensional feature space \(\mathcal{H} = \mathbb{R}^D\), where \(D\) is approximately \(d + \binom d2 \approx \frac{d^2}{2}\). This expanded feature space enables the kernel to capture quadratic relationships in the data.</p> <p>The feature map for the quadratic kernel is given by:</p> \[\psi(x) = \left(x_1, \dots, x_d, x_1^2, \dots, x_d^2, \sqrt{2}x_1x_2, \dots, \sqrt{2}x_ix_j, \dots, \sqrt{2}x_{d-1}x_d\right)^\top.\] <p>To compute the kernel function, we use the inner product of the feature maps:</p> \[k(x, x') = \langle \psi(x), \psi(x') \rangle.\] <p>Expanding this yields:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2.\] <p><strong>Derivation of the Quadratic Kernel form:</strong></p> <p>The quadratic kernel is defined as the inner product in a higher-dimensional feature space. The feature map \(\psi(x)\) includes:</p> <ol> <li>Original features: \(x_1, x_2, \dots, x_d\)</li> <li>Squared features: \(x_1^2, x_2^2, \dots, x_d^2\)</li> <li>Cross-product terms: \(\sqrt{2}x_i x_j\) for \(i \neq j\)</li> </ol> <p>Thus:</p> \[\psi(x) = \left(x_1, x_2, \dots, x_d, x_1^2, x_2^2, \dots, x_d^2, \sqrt{2}x_1x_2, \sqrt{2}x_1x_3, \dots, \sqrt{2}x_{d-1}x_d\right)^\top\] <p>The kernel is computed as:</p> \[k(x, x') = \langle \psi(x), \psi(x') \rangle\] <p>Expanding this, we have:</p> <ol> <li> <p><strong>Linear terms</strong>:<br> \(\langle x, x' \rangle = \sum_{i} x_i x_i'\)</p> </li> <li> <p><strong>Squared terms</strong>:<br> \(\sum_{i} x_i^2 x_i'^2\)</p> </li> <li> <p><strong>Cross-product terms</strong>: \(2 \sum_{i \neq j} x_i x_j x_i' x_j'\)</p> </li> </ol> <p>Combining these, the kernel becomes:</p> \[k(x, x') = \langle x, x' \rangle + \sum_{i} x_i^2 x_i'^2 + 2 \sum_{i \neq j} x_i x_j x_i' x_j'\] <p>Recognizing that:</p> \[\langle x, x' \rangle^2 = \left( \sum_{i} x_i x_i' \right)^2 = \sum_{i} x_i^2 x_i'^2 + 2 \sum_{i \neq j} x_i x_j x_i' x_j'\] <p>The kernel simplifies to:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2\] <p>One of the key advantages of kernel methods is computational efficiency. While the explicit computation of the inner product in the feature space requires \(O(d^2)\) operations, the implicit kernel calculation only requires \(O(d)\) operations.</p> <p>A good example will make it much clearer.</p> <p>Let \(x = [1, 2]\) and \(x' = [3, 4]\).</p> <p>The quadratic kernel is defined as:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2\] <p><strong>Step 1</strong>: Compute \(\langle x, x' \rangle\) \(\langle x, x' \rangle = (1)(3) + (2)(4) = 3 + 8 = 11\)</p> <p><strong>Step 2</strong>: Compute \(\langle x, x' \rangle^2\) \(\langle x, x' \rangle^2 = 11^2 = 121\)</p> <p><strong>Step 3</strong>: Compute \(k(x, x')\) \(k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2 = 11 + 121 = 132\)</p> <p><strong>Step 4</strong>: Verify with the Feature Map</p> <p>The feature map for the quadratic kernel is:</p> \[\psi(x) = [x_1, x_2, x_1^2, x_2^2, \sqrt{2}x_1x_2]\] <p>For \(x = [1, 2]\): \(\psi(x) = [1, 2, 1^2, 2^2, \sqrt{2}(1)(2)] = [1, 2, 1, 4, 2\sqrt{2}]\)</p> <p>For \(x' = [3, 4]\): \(\psi(x') = [3, 4, 3^2, 4^2, \sqrt{2}(3)(4)] = [3, 4, 9, 16, 12\sqrt{2}]\)</p> <p>Compute the inner product:</p> <p>\(\langle \psi(x), \psi(x') \rangle = (1)(3) + (2)(4) + (1)(9) + (4)(16) + (2\sqrt{2})(12\sqrt{2})\) \(= 3 + 8 + 9 + 64 + 48 = 132\)</p> <p>Thus, the quadratic kernel gives: \(k(x, x') = 132\)</p> <h4 id="the-polynomial-kernel"><strong>The Polynomial Kernel</strong></h4> <p>Building on the quadratic kernel, the polynomial kernel generalizes the concept by introducing a degree parameter \(M\). The kernel function is defined as:</p> \[k(x, x') = (1 + \langle x, x' \rangle)^M.\] <p>This kernel corresponds to a feature space that includes all monomials of the input features up to degree \(M\). Notably, the computational cost of evaluating the kernel function remains constant, regardless of \(M\). However, explicitly computing the inner product in the feature space grows rapidly as \(M\) increases.</p> <hr> <h4 id="the-radial-basis-function-rbf-kernel"><strong>The Radial Basis Function (RBF) Kernel</strong></h4> <p>The <strong>Radial Basis Function (RBF) kernel</strong>, also known as the <strong>Gaussian kernel</strong>, is one of the most widely used kernels for solving nonlinear problems. Unlike the linear and polynomial kernels, the RBF kernel maps data into an <strong>infinite-dimensional feature space</strong>, enabling it to capture highly complex relationships.</p> <p>The RBF kernel function is mathematically expressed as:</p> \[k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right),\] <p>where:</p> <ul> <li>\(x, x' \in \mathbb{R}^d\) are data points in the input space,</li> <li>\(\|x - x'\|\) is the Euclidean distance between \(x\) and \(x'\),</li> <li>\(\sigma^2\) (the <strong>bandwidth</strong>) controls how quickly the kernel value decays with distance.</li> </ul> <h5 id="how-the-rbf-kernel-works"><strong>How the RBF Kernel Works</strong></h5> <p>The RBF kernel measures <strong>similarity</strong> between two points based on their distance. Here’s a breakdown:</p> <ol> <li> <strong>Close Points</strong>: <ul> <li>If \(x\) and \(x'\) are close, \(\|x - x'\|\) is small. The exponential term \(\exp(-\|x - x'\|^2 / 2\sigma^2)\) is close to 1, meaning the points are highly similar.</li> </ul> </li> <li> <strong>Distant Points</strong>: <ul> <li>When \(x\) and \(x'\) are far apart, \(\|x - x'\|\) becomes large, and the kernel value approaches 0. This indicates little to no similarity.</li> </ul> </li> <li> <strong>Smoothness Control with \(\sigma^2\)</strong>: <ul> <li>A smaller \(\sigma^2\) leads to sharper drops in similarity, making the kernel more sensitive to nearby points.</li> <li>A larger \(\sigma^2\) smooths the decay, creating broader generalization.</li> </ul> </li> </ol> <p>The RBF kernel is powerful because it implicitly maps data into an <strong>infinite-dimensional feature space</strong>. However, thanks to the <strong>kernel trick</strong>, we don’t need to compute these features explicitly. Instead, the kernel function \(k(x, x')\) directly computes the equivalent of the dot product in this space.</p> <h6 id="what-does-this-mean"><strong>What Does This Mean?</strong></h6> <ul> <li>In this infinite space, even simple algorithms (like linear classifiers) can create highly complex and nonlinear decision boundaries in the original input space.</li> </ul> <h5 id="intuition-behind-the-rbf-kernel"><strong>Intuition Behind the RBF Kernel</strong></h5> <p>To understand the RBF kernel, let’s break it down with some simple analogies.</p> <h6 id="1-a-bubble-of-influence"><strong>1. A Bubble of Influence</strong></h6> <p>Imagine every data point in your dataset creates an invisible “bubble” around itself. The size and shape of this bubble depend on the kernel’s parameter \(\sigma^2\) (the bandwidth):</p> <ul> <li> <strong>Small \(\sigma^2\)</strong>: The bubble is tight and localized, meaning each point influences only its immediate neighbors. This captures fine-grained details.</li> <li> <strong>Large \(\sigma^2\)</strong>: The bubble is wide and smooth, allowing points to influence data further away. This leads to broader generalization.</li> </ul> <p>When we compute \(k(x, x')\), we’re essentially asking, <em>“How much does the bubble around \(x\) overlap with the bubble around \(x'\)?”</em> The more overlap, the higher the similarity score.</p> <h6 id="2-analogy-dropping-pebbles-in-a-pond"><strong>2. Analogy: Dropping Pebbles in a Pond</strong></h6> <p>Imagine dropping pebbles into a still pond:</p> <ul> <li>Each pebble creates ripples that spread outward.</li> <li>The strength of the ripples diminishes as they travel further from the pebble.</li> </ul> <p>The kernel function \(k(x, x')\) measures how much the ripples from one pebble (data point \(x\)) interfere or overlap with those from another pebble (\(x'\)).</p> <ul> <li> <strong>Close pebbles</strong>: Their ripples interfere constructively (high similarity, \(k(x, x')\) close to 1).</li> <li> <strong>Distant pebbles</strong>: Their ripples barely touch (low similarity, \(k(x, x')\) close to 0).</li> </ul> <p>The parameter \(\sigma^2\) controls the rate at which the ripples fade:</p> <ul> <li> <strong>Small \(\sigma^2\)</strong>: Ripples fade quickly, leading to sharp, localized interference.</li> <li> <strong>Large \(\sigma^2\)</strong>: Ripples fade slowly, allowing broader interference.</li> </ul> <h6 id="3-the-infinite-dimensional-perspective"><strong>3. The Infinite-Dimensional Perspective</strong></h6> <p>Now imagine these ripples aren’t confined to the surface of the pond but instead exist in an infinite-dimensional space. Each data point generates a unique “wave” in this space.</p> <p>The RBF kernel computes the similarity between these waves without explicitly constructing them. It’s like a shortcut for comparing the interference patterns of ripples in an infinitely deep and wide pond.</p> <h6 id="4-why-does-this-matter"><strong>4. Why Does This Matter?</strong></h6> <p>This ripple analogy helps explain why the RBF kernel is so effective:</p> <ul> <li> <strong>Localized Influence</strong>: Points that are closer together naturally exert more influence on each other.</li> <li> <strong>Nonlinear Relationships</strong>: The ripple effect in the transformed feature space allows the kernel to capture intricate patterns in data.</li> <li> <strong>Flexibility</strong>: By tuning \(\sigma^2\), you can adjust the model to balance between fine details (small \(\sigma^2\)) and broad generalization (large \(\sigma^2\)).</li> </ul> <p><strong>Key Takeaway:</strong> The RBF kernel creates a ripple effect around every data point and measures how much these ripples overlap. This process enables us to handle nonlinear relationships and create complex decision boundaries, all while staying computationally efficient.</p> <h5 id="why-is-the-rbf-kernel-infinite-dimensional"><strong>Why is the RBF Kernel Infinite-Dimensional?</strong></h5> <p>The RBF kernel maps data into an <strong>infinite-dimensional feature space</strong> because of its connection to the <strong>Taylor series expansion</strong> of the exponential function. The kernel is expressed as:</p> \[k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)\] <p>The exponential function can be expanded as a Taylor series:</p> \[\exp(-t) = \sum_{n=0}^{\infty} \frac{(-t)^n}{n!}\] <p>Applying this to the RBF kernel:</p> \[k(x, x') = \sum_{n=0}^{\infty} \frac{1}{n!} \left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)^n\] <p>Each term in this infinite series corresponds to a basis function in the feature space. Since the series includes terms of all powers \(n\), the feature space has an <strong>infinite number of dimensions</strong>.</p> <h6 id="key-intuition"><strong>Key Intuition:</strong></h6> <ol> <li> <p><strong>Infinite Series</strong>: The kernel includes contributions from all possible degrees of interaction between features (e.g., quadratic, cubic, quartic terms, etc.), up to infinity.</p> </li> <li> <p><strong>Feature Representation</strong>: The mapping \(\phi(x)\) to the feature space involves infinitely many components derived from the series expansion.</p> </li> <li> <p><strong>Kernel Trick</strong>: Instead of explicitly constructing these infinite features, the RBF kernel directly computes their inner product, \(\langle \phi(x), \phi(x') \rangle\), through \(k(x, x')\).</p> </li> </ol> <p>This infinite-dimensional nature is what gives the RBF kernel its remarkable flexibility to model complex, nonlinear patterns.</p> <hr> <h4 id="kernelization-the-recipe"><strong>Kernelization: The Recipe</strong></h4> <p>To effectively leverage kernel methods, follow this general recipe:</p> <ol> <li>Recognize problems that can benefit from kernelization. These are cases where the feature map \(\psi(x)\) only appears in inner products \(\langle \psi(x), \psi(x') \rangle\).</li> <li>Select an appropriate kernel function(‘similarity score’) that suits the data and the task at hand.</li> <li>Compute the kernel matrix, a symmetric matrix of size \(n \times n\) for a dataset with \(n\) data points.</li> <li>Use the kernel matrix to optimize the model and make predictions.</li> </ol> <p>This approach allows us to solve problems in high-dimensional feature spaces without the computational burden of explicit mappings.</p> <h5 id="whats-next"><strong>What’s Next?</strong></h5> <p>We explored the theoretical foundations of kernel functions, how to construct valid kernels, and the properties of popular kernels. But, a key question remains: <strong>under what conditions can we apply kernelization effectively?</strong> Understanding this requires exploring one more crucial concept: how the solution to certain optimization problems is spanned by the input data itself. Next, we’ll delve into this idea and explore how it connects with kernels to solve the SVM problem we’ve been discussing. Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Add some visualization for kernels intuition</li> <li><a href="https://www.cs.princeton.edu/~bee/courses/scribe/lec_10_09_2013.pdf" rel="external nofollow noopener" target="_blank">Kernels and Kernel Methods - Princeton University</a></li> <li><a href="https://towardsdatascience.com/radial-basis-function-rbf-kernel-the-go-to-kernel-acf0d22c798a" rel="external nofollow noopener" target="_blank">Radial Basis Function (RBF) Kernel: The Go-To Kernel</a></li> <li><a href="https://medium.com/@suvigya2001/the-gaussian-rbf-kernel-in-non-linear-svm-2fb1c822aae0" rel="external nofollow noopener" target="_blank">The Gaussian RBF Kernel in Non Linear SVM</a></li> <li><a href="https://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf" rel="external nofollow noopener" target="_blank">The Radial Basis Function Kernel: University of Wisconsin–Madison</a></li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>