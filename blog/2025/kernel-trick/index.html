<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding the Kernel Trick | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/kernel-trick/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding the Kernel Trick</h1> <p class="post-meta"> Created in January 13, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>When working with machine learning models, especially Support Vector Machines (SVMs), the idea of mapping data into a higher-dimensional space often comes into play. This mapping helps transform non-linearly separable data into a space where linear decision boundaries can be applied. But what happens when the dimensionality of the feature space becomes overwhelmingly large? This is where the <strong>kernel trick</strong> saves the day. In this post, we will explore the kernel trick, starting with SVMs, their reliance on feature mappings, and how inner products in feature space can be computed without ever explicitly constructing that space.</p> <hr> <h4 id="svms-with-explicit-feature-maps"><strong>SVMs with Explicit Feature Maps</strong></h4> <p>To understand the kernel trick, let’s begin with SVMs. In the simplest case, an SVM aims to find a hyperplane that separates data into classes with the largest possible margin. To handle more complex data, we map the input data \(\mathbf{x}\) into a higher-dimensional feature space using a feature map \(\psi: X \to \mathbb{R}^d\). In this space, the SVM optimization problem can be written as:</p> \[\min_{\mathbf{w} \in \mathbb{R}^d} \frac{1}{2} \|\mathbf{w}\|^2 + \frac{c}{n} \sum_{i=1}^n \max(0, 1 - y_i \mathbf{w}^T \psi(\mathbf{x}_i)).\] <p>Here, \(\mathbf{w}\) is the weight vector, \(c\) is a regularization parameter, and \(y_i\) are the labels of the data points. While this approach works well for small \(d\), it becomes computationally expensive as \(d\) increases, especially when using high-degree polynomial mappings.</p> <p>To address this issue, we turn to a reformulation of the SVM problem, derived from <strong>Lagrangian duality</strong>.</p> <h4 id="the-svm-dual-problem"><strong>The SVM Dual Problem</strong></h4> <p>Through Lagrangian duality, the SVM optimization problem can be re-expressed as a dual problem:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \psi(\mathbf{x}_j)^T \psi(\mathbf{x}_i),\] <p>subject to:</p> \[\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \in \left[ 0, \frac{c}{n} \right] \quad \forall i.\] <p>Here, \(\alpha_i\) are the dual variables (Lagrange multipliers). Once the optimal \(\boldsymbol{\alpha}^*\) is obtained, the weight vector in the feature space can be reconstructed as:</p> \[\mathbf{w}^* = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i).\] <p>The decision function for a new input \(\mathbf{x}\) is given by:</p> \[\hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i)^T \psi(\mathbf{x}).\] <h5 id="observing-the-role-of-inner-products"><strong>Observing the Role of Inner Products</strong></h5> <p>An important observation here is that the feature map \(\psi(\mathbf{x})\) appears only through inner products of the form \(\psi(\mathbf{x}_j)^T \psi(\mathbf{x}_i)\). This means we don’t actually need the explicit feature representation \(\psi(\mathbf{x})\); instead, we just need the ability to compute these inner products efficiently.</p> <h4 id="computing-inner-products-in-practice"><strong>Computing Inner Products in Practice</strong></h4> <p>Let’s explore the kernel trick with an example.</p> <h5 id="example-degree-2-monomials"><strong>Example: Degree-2 Monomials</strong></h5> <p>Suppose we are working with 2D data points \(\mathbf{x} = (x_1, x_2)\). If we map the data into a space of degree-2 monomials, the feature map becomes:</p> \[\psi: \mathbb{R}^2 \to \mathbb{R}^6, \quad (x_1, x_2) \mapsto (1, \sqrt{2}x_1, \sqrt{2}x_2, x_1^2, \sqrt{2}x_1x_2, x_2^2).\] <p>The inner product in the feature space is:</p> \[\psi(\mathbf{x})^T \psi(\mathbf{x}') = 1 + 2x_1x_1' + 2x_2x_2' + (x_1x_1')^2 + 2x_1x_2x_1'x_2' + (x_2x_2')^2.\] <p>Simplifying, we observe:</p> \[\psi(\mathbf{x})^T \psi(\mathbf{x}') = (1 + x_1x_1' + x_2x_2')^2 = (1 + \mathbf{x}^T \mathbf{x}')^2.\] <p>This shows that we can compute \(\psi(\mathbf{x})^T \psi(\mathbf{x}')\) directly from the original input space without explicitly constructing \(\psi(\mathbf{x})\)—a key insight behind the kernel trick.</p> <h5 id="general-case-monomials-up-to-degree-p"><strong>General Case: Monomials Up to Degree \(p\)</strong></h5> <p>For feature maps that produce monomials up to degree \(p\), the inner product generalizes as:</p> \[\psi(x)^T \psi(x') = (1 + x^T x')^p.\] <p>It is worth noting that the coefficients of the monomials in \(\psi(x)\) may vary depending on the specific feature map.</p> <h4 id="efficiency-of-the-kernel-trick-from-exponential-to-linear-complexity"><strong>Efficiency of the Kernel Trick: From Exponential to Linear Complexity</strong></h4> <p>One of the key advantages of the kernel trick is its ability to reduce the computational complexity of working with high-dimensional feature spaces. Let’s break this down:</p> <h5 id="explicit-computation-complexity"><strong>Explicit Computation Complexity</strong></h5> <p>When we map an input vector \(\mathbf{x} \in \mathbb{R}^d\) to a feature space with monomials up to degree \(p\), the dimensionality of the feature space increases significantly. Specifically:</p> <ul> <li> <p><strong>Feature Space Dimension</strong>: The number of features in the expansion is:</p> \[\binom{d + p}{p} = \frac{(d + p)!}{d! \, p!}.\] <p>For large \(p\) or \(d\), this grows rapidly and can quickly become computationally prohibitive.</p> </li> <li> <p><strong>Explicit Inner Product</strong>: Computing the inner product directly in this expanded space has a complexity of:</p> \[O\left(\binom{d + p}{p}\right),\] <p>which is exponential in \(p\) for fixed \(d\).</p> </li> </ul> <h5 id="implicit-computation-complexity"><strong>Implicit Computation Complexity</strong></h5> <p>Using the kernel trick, we avoid explicitly constructing the feature space. For a kernel function like:</p> \[k(\mathbf{x}, \mathbf{x}') = (1 + \mathbf{x}^T \mathbf{x}')^p,\] <p>the computation operates directly in the input space.</p> <ul> <li> <p><strong>Input Space Computation</strong>: Computing the kernel function involves:</p> <ol> <li> <strong>Dot Product</strong>: \(\mathbf{x}^T \mathbf{x}'\) is computed in \(O(d)\).</li> <li> <strong>Polynomial Evaluation</strong>: Raising this result to power \(p\) is done in constant time, independent of \(d\).</li> </ol> </li> </ul> <p>Thus, the complexity is reduced to:</p> \[O(d),\] <p>which is <strong>linear</strong> in the input dimensionality \(d\), regardless of \(p\).</p> <h5 id="why-this-matters"><strong>Why This Matters</strong></h5> <ul> <li> <strong>Explicit Features</strong>: For high \(p\), the feature space grows exponentially, leading to a <strong>curse of dimensionality</strong> if explicit computation is used.</li> <li> <strong>Implicit Kernel Computation</strong>: The kernel trick sidesteps the explicit feature space, allowing efficient computation even when the feature space is high-dimensional or infinite (e.g., with RBF kernels).</li> </ul> <p>This transformation from <strong>exponential</strong> to <strong>linear complexity</strong> is one of the core reasons kernel methods are powerful tools in machine learning.</p> <p><strong>Key Takeaway</strong> : The kernel trick enables efficient computation in high-dimensional feature spaces by directly working in the input space. This reduces the complexity from \(O\left(\binom{d + p}{p}\right)\) to \(O(d)\), making it feasible to apply machine learning methods to problems with high-degree polynomial or infinite-dimensional feature spaces.</p> <hr> <h4 id="exploring-the-kernel-function"><strong>Exploring the Kernel Function</strong></h4> <p>To fully appreciate the kernel trick, we need to formalize the concept of the <strong>kernel function</strong>. In our earlier discussion, we introduced the idea of a feature map \(\psi: X \to \mathcal{H}\), which maps input data from the original space \(X\) to a higher-dimensional feature space \(\mathcal{H}\). The kernel function \(k\) corresponding to this feature map is defined as:</p> \[k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle,\] <p>where \(\langle \cdot, \cdot \rangle\) represents the inner product in \(\mathcal{H}\).</p> <h5 id="why-use-kernel-functions"><strong>Why Use Kernel Functions?</strong></h5> <p>At first glance, this notation might seem like a trivial restatement of the inner product, but it’s far more powerful. The key insight is that we can often evaluate \(k(\mathbf{x}, \mathbf{x}')\) directly, without explicitly computing \(\psi(\mathbf{x})\) and \(\psi(\mathbf{x}')\). This is crucial for efficiently working with high-dimensional or infinite-dimensional feature spaces. But this efficiency only applies to certain methods — those that can be <strong>kernelized</strong>.</p> <h4 id="kernelized-methods"><strong>Kernelized Methods</strong></h4> <p>A method is said to be <strong>kernelized</strong> if it uses the feature vectors \(\psi(\mathbf{x})\) only inside inner products of the form \(\langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle\). For such methods, we can replace these inner products with a kernel function \(k(\mathbf{x}, \mathbf{x}')\), avoiding explicit feature computation. This applies to both the optimization problem and the prediction function. Let’s revisit the SVM example to see kernelization in action.</p> <h5 id="kernelized-svm-dual-formulation"><strong>Kernelized SVM Dual Formulation</strong></h5> <p>Recall the dual problem for SVMs:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle,\] <p>subject to:</p> \[\sum_{i=1}^n \alpha_i y_i = 0, \quad \alpha_i \in \left[ 0, \frac{c}{n} \right] \quad \forall i.\] <p><strong>Here’s the key</strong>: because every occurrence of \(\psi(\mathbf{x})\) is inside an inner product, we can replace \(\langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle\) with \(k(\mathbf{x}_i, \mathbf{x}_j)\), the kernel function. The resulting dual optimization problem becomes:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j),\] <p>subject to the same constraints.</p> <p>For predictions, the decision function can also be written in terms of the kernel:</p> \[\hat{f}(\mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i k(\mathbf{x}_i, \mathbf{x}) = \sum_{i=1}^n \alpha_i^* y_i \psi(\mathbf{x}_i)^T \psi(\mathbf{x})\] <p>This reformulation is what allows SVMs to operate efficiently in high-dimensional spaces.</p> <h5 id="the-kernel-matrix"><strong>The Kernel Matrix</strong></h5> <p>A key component in kernelized methods is the <strong>kernel matrix</strong>, which encapsulates the pairwise kernel values for all data points. For a dataset \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\}\), the kernel matrix \(\mathbf{K}\) is defined as:</p> \[\mathbf{K} = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}.\] <p>This \(n \times n\) matrix, also known as the <strong>Gram matrix</strong> in machine learning, summarizes all the information about the training data necessary for solving the kernelized optimization problem.</p> <p>For the kernelized SVM, we can replace \(\langle \psi(\mathbf{x}_i), \psi(\mathbf{x}_j) \rangle\) with \(K_{ij}\), reducing the dual problem to:</p> \[\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K_{ij},\] <p>subject to the same constraints.</p> <p><strong>So, Given a kernelized ML algorithm</strong> (i.e., all \(\psi(x)\)’s show up as \(\langle \psi(x), \psi(x') \rangle\)) :</p> <ol> <li> <strong>Flexibility</strong>: By substituting the kernel function, we can implicitly use very high-dimensional or even infinite-dimensional feature spaces.</li> <li> <strong>Scalability</strong>: Once the kernel matrix is computed, the computational cost depends on the number of data points \(n\), rather than the dimension of the feature space \(d\).</li> <li> <strong>Efficiency</strong>: For many kernels, \(k(\mathbf{x}, \mathbf{x}')\) can be computed without directly accessing the high-dimensional feature representation \(\psi(\mathbf{x})\), avoiding the \(O(d)\) dependence.</li> </ol> <p>These properties make kernelized methods invaluable when \(d \gg n\), a common scenario in machine learning tasks.</p> <p>The kernel trick revolutionizes how we think about high-dimensional data. Next, we will delve into popular kernel functions, their interpretations, and how to choose the right one for your problem.</p> <hr> <h4 id="example-kernels"><strong>Example Kernels</strong></h4> <p>In many cases, it’s useful to think of the kernel function \(k(x, x')\) as a <strong>similarity score</strong> between the data points \(x\) and \(x'\). This perspective allows us to design similarity functions without explicitly considering the feature map.</p> <p>For example, we can create <strong>string kernels</strong> or <strong>graph kernels</strong>—functions that define similarity based on the structure of strings or graphs, respectively. The key question, however, is: <strong>How do we know that our kernel functions truly correspond to inner products in some feature space?</strong></p> <p>This is an essential consideration, as it ensures that the kernel method preserves the properties necessary for various machine learning algorithms to work effectively. Let’s break this down.</p> <h5 id="how-to-obtain-kernels"><strong>How to Obtain Kernels?</strong></h5> <p>There are two primary ways to define kernels:</p> <ol> <li> <p><strong>Explicit Construction</strong>: Define the feature map \(\psi(\mathbf{x})\) and use it to compute the kernel: \(k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle.\) (e.g. monomials)</p> </li> <li> <p><strong>Direct Definition</strong>: Directly define the kernel \(k(\mathbf{x}, \mathbf{x}')\) as a similarity score and verify that it corresponds to an inner product for some \(\psi\). This verification is often guided by mathematical theorems.</p> </li> </ol> <p>To understand this better, let’s first equip ourselves with some essential linear algebra concepts.</p> <h5 id="positive-semidefinite-matrices-and-kernels"><strong>Positive Semidefinite Matrices and Kernels</strong></h5> <p>To verify if a kernel corresponds to a valid inner product, we rely on the concept of <strong>positive semidefinite (PSD) matrices</strong>. Here’s a quick refresher:</p> <ul> <li> <p>A matrix \(\mathbf{M} \in \mathbb{R}^{n \times n}\) is positive semidefinite if: \(\mathbf{x}^\top \mathbf{M} \mathbf{x} \geq 0, \quad \forall \mathbf{x} \in \mathbb{R}^n.\)</p> </li> <li> <p>Equivalent conditions, each necessary and sufficient for a symmetric matrixfor \(\mathbf{M}\) being <strong>PSD</strong>:</p> <ul> <li>\(\mathbf{M} = \mathbf{R}^\top \mathbf{R}\), for some matrix \(\mathbf{R}\).</li> <li>All eigenvalues of \(\mathbf{M}\) are non-negative or \(\geq 0\).</li> </ul> </li> </ul> <p>Next, we define a <strong>positive definite (PD) kernel</strong>:</p> <h5 id="positive-definite-kernel"><strong>Positive Definite Kernel</strong></h5> <p><strong>Definition:</strong></p> <p>A symmetric function \(k: X \times X \to \mathbb{R}\) is a <strong>PD</strong> kernel if, for any finite set \(\{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_n\} \subset X\), the kernel matrix:</p> \[\mathbf{K} = \begin{bmatrix} k(\mathbf{x}_1, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_1, \mathbf{x}_n) \\ \vdots &amp; \ddots &amp; \vdots \\ k(\mathbf{x}_n, \mathbf{x}_1) &amp; \cdots &amp; k(\mathbf{x}_n, \mathbf{x}_n) \end{bmatrix}\] <p>is positive semidefinite.</p> <ol> <li>Symmetry: \(k(\mathbf{x}, \mathbf{x}') = k(\mathbf{x}', \mathbf{x})\).</li> <li>The kernel matrix needs to be positive semidefinite for any finite set of points.</li> <li>Equivalently: \(\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j k(\mathbf{x}_i, \mathbf{x}_j) \geq 0\), for all \(\alpha_i \in \mathbb{R}\) \(\forall i\).</li> </ol> <p>[How, better way of stating it!]</p> <h5 id="mercers-theorem"><strong>Mercer’s Theorem</strong></h5> <p>Mercer’s Theorem provides a foundational result for kernels. It states:</p> <ul> <li> <p>A symmetric function \(k(\mathbf{x}, \mathbf{x}')\) can be expressed as an inner product</p> \[k(\mathbf{x}, \mathbf{x}') = \langle \psi(\mathbf{x}), \psi(\mathbf{x}') \rangle\] <p>if and only if \(k(\mathbf{x}, \mathbf{x}')\) is positive definite.</p> </li> </ul> <p>While proving that a kernel is <strong>PD</strong> can be challenging, we can use known kernels to construct new ones.</p> <h5 id="constructing-new-kernels-from-existing-ones"><strong>Constructing New Kernels from Existing Ones</strong></h5> <p>Given valid PD kernels \(k_1\) and \(k_2\), we can create new kernels using the following operations:</p> <ol> <li> <strong>Non-Negative Scaling</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = \alpha k(\mathbf{x}, \mathbf{x}')\), where \(\alpha \geq 0\).</li> <li> <strong>Addition</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') + k_2(\mathbf{x}, \mathbf{x}')\).</li> <li> <strong>Multiplication</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k_1(\mathbf{x}, \mathbf{x}') k_2(\mathbf{x}, \mathbf{x}')\).</li> <li> <strong>Recursion</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = k(\psi(\mathbf{x}), \psi(\mathbf{x}'))\), for any function \(\psi(\cdot)\).</li> <li> <strong>Feature Mapping</strong>: \(k_{\text{new}}(\mathbf{x}, \mathbf{x}') = f(\mathbf{x}) f(\mathbf{x}')\), for any function \(f(\cdot)\).</li> </ol> <p>And, Lots more theorems to help you construct new kernels from old.</p> <p>[Add reference to mercer theorem]</p> <hr> <h4 id="popular-kernel-functions"><strong>Popular Kernel Functions</strong></h4> <h5 id="the-linear-kernel"><strong>The Linear Kernel</strong></h5> <p>The linear kernel is the simplest and most intuitive kernel function. Imagine working with data in an input space represented as \(X = \mathbb{R}^d\). Here, the feature space, denoted as \(\mathcal{H}\), is the same as the input space \(\mathbb{R}^d\). The feature map for this kernel is straightforward: \(\psi(x) = x\).</p> <p>The kernel function itself is defined as:</p> \[k(x, x') = \langle x, x' \rangle = x^\top x',\] <p>where \(\langle x, x' \rangle\) represents the standard inner product. This simplicity makes the linear kernel computationally efficient and ideal for linear models.</p> <h5 id="the-quadratic-kernel"><strong>The Quadratic Kernel</strong></h5> <p>The quadratic kernel takes us a step further by mapping the input space \(X = \mathbb{R}^d\) into a higher-dimensional feature space \(\mathcal{H} = \mathbb{R}^D\), where \(D\) is approximately \(d + \binom d2 \approx \frac{d^2}{2}\). This expanded feature space enables the kernel to capture quadratic relationships in the data.</p> <p>The feature map for the quadratic kernel is given by:</p> \[\psi(x) = \left(x_1, \dots, x_d, x_1^2, \dots, x_d^2, \sqrt{2}x_1x_2, \dots, \sqrt{2}x_ix_j, \dots, \sqrt{2}x_{d-1}x_d\right)^\top.\] <p>To compute the kernel function, we use the inner product of the feature maps:</p> \[k(x, x') = \langle \psi(x), \psi(x') \rangle.\] <p>Expanding this yields:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2.\] <p><strong>Derivation of the Quadratic Kernel form:</strong></p> <p>The quadratic kernel is defined as the inner product in a higher-dimensional feature space. The feature map \(\psi(x)\) includes:</p> <ol> <li>Original features: \(x_1, x_2, \dots, x_d\)</li> <li>Squared features: \(x_1^2, x_2^2, \dots, x_d^2\)</li> <li>Cross-product terms: \(\sqrt{2}x_i x_j\) for \(i \neq j\)</li> </ol> <p>Thus:</p> \[\psi(x) = \left(x_1, x_2, \dots, x_d, x_1^2, x_2^2, \dots, x_d^2, \sqrt{2}x_1x_2, \sqrt{2}x_1x_3, \dots, \sqrt{2}x_{d-1}x_d\right)^\top\] <p>The kernel is computed as:</p> \[k(x, x') = \langle \psi(x), \psi(x') \rangle\] <p>Expanding this, we have:</p> <ol> <li> <p><strong>Linear terms</strong>:<br> \(\langle x, x' \rangle = \sum_{i} x_i x_i'\)</p> </li> <li> <p><strong>Squared terms</strong>:<br> \(\sum_{i} x_i^2 x_i'^2\)</p> </li> <li> <p><strong>Cross-product terms</strong>: \(2 \sum_{i \neq j} x_i x_j x_i' x_j'\)</p> </li> </ol> <p>Combining these, the kernel becomes:</p> \[k(x, x') = \langle x, x' \rangle + \sum_{i} x_i^2 x_i'^2 + 2 \sum_{i \neq j} x_i x_j x_i' x_j'\] <p>Recognizing that:</p> \[\langle x, x' \rangle^2 = \left( \sum_{i} x_i x_i' \right)^2 = \sum_{i} x_i^2 x_i'^2 + 2 \sum_{i \neq j} x_i x_j x_i' x_j'\] <p>The kernel simplifies to:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2\] <p>One of the key advantages of kernel methods is computational efficiency. While the explicit computation of the inner product in the feature space requires \(O(d^2)\) operations, the implicit kernel calculation only requires \(O(d)\) operations.</p> <p>A good example will make it much clearer.</p> <p>Let \(x = [1, 2]\) and \(x' = [3, 4]\).</p> <p>The quadratic kernel is defined as:</p> \[k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2\] <p><strong>Step 1</strong>: Compute \(\langle x, x' \rangle\) \(\langle x, x' \rangle = (1)(3) + (2)(4) = 3 + 8 = 11\)</p> <p><strong>Step 2</strong>: Compute \(\langle x, x' \rangle^2\) \(\langle x, x' \rangle^2 = 11^2 = 121\)</p> <p><strong>Step 3</strong>: Compute \(k(x, x')\) \(k(x, x') = \langle x, x' \rangle + \langle x, x' \rangle^2 = 11 + 121 = 132\)</p> <p><strong>Step 4</strong>: Verify with the Feature Map</p> <p>The feature map for the quadratic kernel is:</p> \[\psi(x) = [x_1, x_2, x_1^2, x_2^2, \sqrt{2}x_1x_2]\] <p>For \(x = [1, 2]\): \(\psi(x) = [1, 2, 1^2, 2^2, \sqrt{2}(1)(2)] = [1, 2, 1, 4, 2\sqrt{2}]\)</p> <p>For \(x' = [3, 4]\): \(\psi(x') = [3, 4, 3^2, 4^2, \sqrt{2}(3)(4)] = [3, 4, 9, 16, 12\sqrt{2}]\)</p> <p>Compute the inner product:</p> <p>\(\langle \psi(x), \psi(x') \rangle = (1)(3) + (2)(4) + (1)(9) + (4)(16) + (2\sqrt{2})(12\sqrt{2})\) \(= 3 + 8 + 9 + 64 + 48 = 132\)</p> <p>Thus, the quadratic kernel gives: \(k(x, x') = 132\)</p> <h5 id="the-polynomial-kernel"><strong>The Polynomial Kernel</strong></h5> <p>Building on the quadratic kernel, the polynomial kernel generalizes the concept by introducing a degree parameter \(M\). The kernel function is defined as:</p> \[k(x, x') = (1 + \langle x, x' \rangle)^M.\] <p>This kernel corresponds to a feature space that includes all monomials of the input features up to degree \(M\). Notably, the computational cost of evaluating the kernel function remains constant, regardless of \(M\). However, explicitly computing the inner product in the feature space grows rapidly as \(M\) increases.</p> <h5 id="the-radial-basis-function-rbf-or-gaussian-kernel"><strong>The Radial Basis Function (RBF) or Gaussian Kernel</strong></h5> <p>The RBF kernel, also known as the Gaussian kernel, is one of the most widely used kernels for nonlinear problems. The input space remains \(X = \mathbb{R}^d\), but the feature space is infinite-dimensional, making it capable of capturing complex relationships in the data.</p> <p>The kernel function is expressed as:</p> \[k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right),\] <p>where \(\sigma^2\) is a parameter known as the bandwidth, controlling the smoothness of the kernel.</p> <p>One might wonder if this kernel still adheres to the principle of inner products in a feature space. The answer is both yes and no. While it acts like a similarity score, it corresponds to the inner product of feature vectors in an infinite-dimensional space.</p> <p>[Explain it intuitively, and how to make sense of it]</p> <p>[some visualization needed]</p> <hr> <h4 id="kernelization-the-recipe"><strong>Kernelization: The Recipe</strong></h4> <p>To effectively leverage kernel methods, follow this general recipe:</p> <ol> <li>Recognize problems that can benefit from kernelization. These are cases where the feature map \(\psi(x)\) only appears in inner products \(\langle \psi(x), \psi(x') \rangle\).</li> <li>Select an appropriate kernel function(‘similarity score’) that suits the data and the task at hand.</li> <li>Compute the kernel matrix, a symmetric matrix of size \(n \times n\) for a dataset with \(n\) data points.</li> <li>Use the kernel matrix to optimize the model and make predictions.</li> </ol> <p>This approach allows us to solve problems in high-dimensional feature spaces without the computational burden of explicit mappings.</p> <h5 id="whats-next"><strong>What’s Next?</strong></h5> <p>We explored the theoretical foundations of kernel functions, how to construct valid kernels, and the properties of popular kernels. But, a key question remains: under what conditions can we apply kernelization effectively? Understanding this involves diving deeper into the properties of kernel functions and their applicability to different problem domains. In the next post, we will explore these conditions in detail and discuss their implications for solving SVM problems.</p> <p>Stay tuned!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li>Add some visualization for kernels intuition</li> <li> </li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>