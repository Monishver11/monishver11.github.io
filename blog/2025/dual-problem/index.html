<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Dual Problem of SVM | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/dual-problem/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">The Dual Problem of SVM</h1> <p class="post-meta"> Created in January 08, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In machine learning, optimization problems often present themselves as key challenges. For example, when training a <strong>Support Vector Machine (SVM)</strong>, we are tasked with finding the optimal hyperplane that separates two classes in a high-dimensional feature space. While we can solve this directly using methods like subgradient descent, we can also leverage a more analytical approach through <strong>Quadratic Programming (QP)</strong> solvers.</p> <p>Moreover, for convex optimization problems, there is a powerful technique known as solving the <strong>dual problem</strong>. Understanding this duality is not only essential for theory, but it also offers computational advantages. In this blog, we’ll dive into the dual formulation of SVM and its implications.</p> <hr> <h4 id="svm-as-a-quadratic-program"><strong>SVM as a Quadratic Program</strong></h4> <p>To understand the dual problem of SVM, let’s first revisit the primal optimization problem for SVMs. The goal of an SVM is to find the hyperplane that maximizes the margin between two classes. The optimization problem can be written as:</p> \[\begin{aligned} \min_{w, b, \xi} \quad &amp; \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \xi_i \\ \text{subject to} \quad &amp; -\xi_i \leq 0 \quad \text{for } i = 1, \dots, n, \\ &amp; 1 - y_i (w^T x_i + b) - \xi_i \leq 0 \quad \text{for } i = 1, \dots, n, \end{aligned}\] <p><strong>Primal</strong>—sounds technical, right? Here’s what it means: The <strong>primal problem</strong> refers to the original formulation of the optimization problem in terms of the decision variables (in this case, \(w\), \(b\), and \(\xi\)) and the objective function. It is called “primal” because it directly represents the problem without transformation. The primal form of SVM is concerned with finding the optimal hyperplane parameters that minimize the classification error while balancing the margin size.</p> <h5 id="breakdown-of-the-problem"><strong>Breakdown of the Problem</strong></h5> <ul> <li> <p><strong>Objective Function</strong>: The term \(\frac{1}{2} \|w\|^2\) is a regularization term that aims to minimize the complexity of the model, ensuring that the hyperplane is as “wide” as possible. The second term, \(C \sum_{i=1}^n \xi_i\), penalizes the violations of the margin (through slack variables \(\xi_i\)) and controls the trade-off between margin size and misclassification.</p> </li> <li> <p><strong>Constraints</strong>: The constraints consist of two parts:</p> <ul> <li>The first part ensures that slack variables are non-negative: \(-\xi_i \leq 0\), meaning that each slack variable must be at least zero. <strong>Why?</strong> The slack variables represent the margin violations, and they must be non-negative because they quantify how much a data point violates the margin, which cannot be negative.</li> <li>The second part enforces that the data points are correctly classified or their margin violations are captured by \(\xi_i\). For each data point \(i\), the condition \(1 - y_i (w^T x_i + b) - \xi_i \leq 0\) must hold. Here, \(y_i\) is the true label of the data point, and \(w^T x_i + b\) represents the signed distance of the point from the hyperplane. If the point is correctly classified and lies outside the margin (i.e., its signed distance from the hyperplane is greater than 1), the constraint holds true. If the point is misclassified or falls inside the margin, the slack variable \(\xi_i\) will account for this violation.</li> </ul> </li> </ul> <p>This problem has a <strong>differentiable objective function</strong>, <strong>affine constraints</strong>, and includes a number of <strong>unknowns</strong> that can be solved using Quadratic Programming (QP) solvers.</p> <p>So, <strong>Quadratic Programming (QP)</strong> is an optimization problem where the objective function is quadratic (i.e., includes squared terms like \(\|w\|^2\)), and the constraints are linear. In the context of SVM, QP is utilized because the objective function involves the squared norm of \(w\) (which is quadratic), and the constraints are linear inequalities.</p> <p>The QP formulation for SVM involves minimizing a quadratic objective function (with respect to \(w\), \(b\), and \(\xi\)) subject to linear constraints. Now, while QP solvers provide an efficient way to tackle this problem, let’s explore the <strong>dual problem(next)</strong> to gain further insights.</p> <p>But, why bother with the <strong>dual problem</strong>? Here’s why it’s worth the dive:</p> <ol> <li> <p><strong>Efficient Computation with Kernels</strong>: The dual formulation focuses on Lagrange multipliers and inner products between data points, rather than directly optimizing \(w\) and \(b\). This is particularly beneficial when using kernels, as it avoids explicit computation in high-dimensional spaces. For non-linear problems or datasets where relationships are better captured in transformed spaces, the dual approach enables efficient computation while leveraging the kernel trick.</p> </li> <li> <p><strong>Geometrical Insights</strong>: The dual formulation emphasizes the relationship between support vectors and the margin, offering a clearer geometrical interpretation. It shows that only the support vectors (the points closest to the decision boundary) determine the optimal hyperplane.</p> </li> <li> <p><strong>Convexity and Global Optimality</strong>: The dual problem is convex, ensuring that solving it leads to the global optimal solution. This is particularly beneficial when the primal problem has a large number of variables and constraints.</p> </li> </ol> <p>In short, while QP solvers can efficiently solve the primal problem, the dual problem formulation offers computational benefits, the potential for kernel methods, and a clearer understanding of the SVM model’s properties. This makes the dual approach a powerful tool in SVM optimization.</p> <p>No need to worry about the details above—we’ll cover them step by step. For now, keep reading!</p> <hr> <h4 id="the-lagrangian"><strong>The Lagrangian</strong></h4> <p>To begin understanding the dual problem, we need to define the <strong>Lagrangian</strong> of the optimization problem. For general inequality-constrained optimization problems, the goal is:</p> \[\begin{aligned} \min_x \quad &amp; f_0(x) \\ \text{subject to} \quad &amp; f_i(x) \leq 0, \quad i = 1, \dots, m. \end{aligned}\] <p>The corresponding <strong>Lagrangian</strong> is defined as:</p> \[L(x, \lambda) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x),\] <p>where:</p> <ul> <li>\(\lambda_i\) are the <strong>Lagrange multipliers</strong> (also known as <strong>dual variables</strong>).</li> <li>The Lagrangian function combines the objective function \(f_0(x)\) with the constraints \(f_i(x)\), weighted by the Lagrange multipliers \(\lambda_i\). These multipliers represent how much the objective function will change if we relax or tighten the corresponding constraint.</li> </ul> <h5 id="why-do-we-use-the-lagrangian"><strong>Why Do We Use the Lagrangian?</strong></h5> <p>The Lagrangian serves as a bridge between constrained and unconstrained optimization. Here’s the intuition behind its design and necessity:</p> <ol> <li> <strong>Softening Hard Constraints</strong>: <ul> <li>In constrained optimization, the solution must strictly satisfy all constraints, which can make direct optimization challenging.</li> <li>By introducing Lagrange multipliers, the constraints are “softened” into penalties. This means that instead of strictly enforcing constraints during every step of optimization, we penalize deviations from the constraints in the objective function.</li> </ul> </li> <li> <strong>Unified Objective</strong>: <ul> <li>The Lagrangian integrates the objective function and constraints into a single function. This allows us to handle both aspects of the problem (maximizing or minimizing while respecting constraints) in one unified framework.</li> </ul> </li> <li> <strong>Flexibility</strong>: <ul> <li>The Lagrange multipliers \(\lambda_i\) provide a mechanism to adjust the influence of each constraint. If a constraint is more critical, its corresponding multiplier will have a larger value, increasing its contribution to the Lagrangian.</li> </ul> </li> <li> <strong>Theoretical Insights</strong>: <ul> <li>The Lagrangian formulation is foundational to deriving the <strong>dual problem</strong>, which can sometimes simplify the original (primal) problem. It also provides deeper insights into the sensitivity of the solution to changes in the constraints.</li> </ul> </li> </ol> <p><strong>Think of it this way:</strong></p> <p>Imagine you’re managing a budget to purchase items for a project. Your primary goal is to minimize costs (the objective function), but you have constraints like a maximum budget and a minimum quality requirement for each item.</p> <ul> <li>Without the Lagrangian: You’d need to find solutions that satisfy the budget and quality constraints explicitly, which could be cumbersome.</li> <li>With the Lagrangian: You assign a penalty (via Lagrange multipliers) to every dollar you overspend or every unit of quality you fail to meet. Now, your goal is to minimize the combined cost (original cost + penalties), which naturally leads you to solutions that respect the constraints.</li> </ul> <p>In optimization terms, the Lagrangian lets us trade off between violating constraints and optimizing the objective function. This trade-off is critical in complex problems where perfect feasibility might not always be achievable during intermediate steps.</p> <h5 id="the-role-of-lagrange-multipliers"><strong>The Role of Lagrange Multipliers</strong></h5> <p>The multipliers \(\lambda_i\) play a dual role:</p> <ol> <li> <p>They measure the <strong>sensitivity</strong> of the objective function to the corresponding constraint. For instance, if increasing the limit of a constraint by a small amount significantly improves the objective, its multiplier will have a large value.</p> </li> <li> <p>They ensure that the optimal solution respects the constraints. If a constraint is not active at the solution (i.e., it is satisfied without being tight), its multiplier will be zero. This is formalized in the concept of <strong>complementary slackness</strong>, which we’ll explore later.</p> </li> </ol> <p>In essence, the Lagrangian is not just a mathematical tool; it reflects the natural balance between objectives and constraints, making it indispensable in optimization theory.</p> <hr> <h4 id="lagrange-dual-function"><strong>Lagrange Dual Function</strong></h4> <p>Next, we define the <strong>Lagrange dual function</strong>, which plays a crucial role in deriving the dual problem. The dual function is obtained by minimizing the Lagrangian with respect to the primal variables (denoted as \(x\)):</p> \[g(\lambda) = \inf_x L(x, \lambda) = \inf_x \left[ f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) \right].\] <h5 id="what-does-this-mean"><strong>What Does This Mean?</strong></h5> <p>The Lagrange dual function \(g(\lambda)\) gives us the smallest possible value of the Lagrangian \(L(x, \lambda)\) for a given set of Lagrange multipliers \(\lambda_i \geq 0\). It represents the “best” value of the Lagrangian when we optimize over the primal variables \(x\).</p> <p>In simpler terms:</p> <ul> <li>The primal variables \(x\) are those we’re directly trying to optimize in the original problem.</li> <li>By minimizing over \(x\) for fixed \(\lambda\), we explore how well the Lagrangian balances the objective function \(f_0(x)\) and the weighted constraints \(f_i(x)\).</li> </ul> <h5 id="why-do-we-minimize-over-x"><strong>Why Do We Minimize Over \(x\)?</strong></h5> <p>The reason for this step is that it helps us decouple the influence of the primal variables \(x\) and the dual variables \(\lambda\). By focusing only on \(x\), we shift our attention to understanding the properties of the dual variables, which simplifies the problem and provides valuable insights.</p> <p>Minimizing the Lagrangian with respect to \(x\) ensures that:</p> <ol> <li> <strong>Feasibility of Constraints</strong>: <ul> <li>The minimization respects the constraints \(f_i(x) \leq 0\) through the penalization mechanism introduced by \(\lambda_i\).</li> </ul> </li> <li> <strong>Dual Representation</strong>: <ul> <li>The dual function \(g(\lambda)\) captures how “good” a particular choice of \(\lambda\) is in approximating the original problem.</li> </ul> </li> <li> <strong>Foundation for the Dual Problem</strong>: <ul> <li>The minimization step builds the foundation for solving the optimization problem via its <strong>dual formulation</strong>, which is often simpler than the primal.</li> </ul> </li> </ol> <h5 id="why-do-we-need-this"><strong>Why Do We Need This?</strong></h5> <p>The goal of introducing the dual function is to exploit the following properties, which help us solve the original optimization problem more efficiently:</p> <ul> <li> <p><strong>Lower Bound Property</strong>: The dual function \(g(\lambda)\) provides a lower bound on the optimal value of the primal problem. If \(p^*\) is the optimal value of the primal problem, then: \(g(\lambda) \leq p^*, \quad \forall \lambda \geq 0.\) This property is useful because even if we cannot solve the primal problem directly, we can approximate its solution by maximizing \(g(\lambda)\).</p> </li> <li> <p><strong>Convexity of \(g(\lambda)\)</strong>: The dual function is always concave, regardless of whether the primal problem is convex. This makes the dual problem easier to solve using convex optimization techniques.</p> </li> </ul> <h5 id="how-does-this-work-why-does-it-work"><strong>How Does This Work? Why Does It Work?</strong></h5> <ol> <li> <strong>Lower Bound Property</strong>: <ul> <li>When we minimize the Lagrangian \(L(x, \lambda)\) with respect to the primal variables \(x\), we’re essentially finding the “best possible value” of the objective function \(f_0(x)\) for a fixed choice of \(\lambda\).</li> <li>Since the dual function \(g(\lambda)\) is derived from a relaxed version of the primal problem (allowing \(\lambda_i \geq 0\) to penalize constraint violations), it cannot exceed the true optimal value \(p^*\) of the primal problem. This creates the lower bound.</li> </ul> <p><strong>Intuition</strong>: Think of the dual function as a “proxy” for the primal problem. By maximizing \(g(\lambda)\), we try to approach the primal solution as closely as possible from below.</p> </li> <li> <strong>Convexity of \(g(\lambda)\)</strong>: <ul> <li>The concavity of \(g(\lambda)\) follows from its definition as the infimum (or greatest lower bound) of a family of affine functions. In optimization, operations involving infima tend to preserve convexity (or result in concavity for maximization problems).</li> <li>This property ensures that maximizing \(g(\lambda)\) is computationally efficient, even if the original primal problem is non-convex.</li> </ul> <p><strong>Intuition</strong>: The concave structure of \(g(\lambda)\) creates an inverted “bowl-shaped” surface, making it easier to find the maximum using gradient-based optimization methods.</p> </li> </ol> <h5 id="what-is-inf-and-how-is-it-different-from-min"><strong>What is \(\inf\) and How is it Different from \(\min\)?</strong></h5> <p>The <strong>infimum</strong> (denoted as \(\inf\)) is a generalization of the minimum (\(\min\)) in optimization and analysis. Here’s the distinction:</p> <ol> <li> <strong>Minimum (\(\min\))</strong>: <ul> <li>The minimum is the smallest value attained by a function within its domain. It must be achieved by some point \(x\) in the domain.</li> <li>Example: For \(f(x) = x^2\) on \([0, 2]\), the minimum is \(f(0) = 0\).</li> </ul> </li> <li> <strong>Infimum (\(\inf\))</strong>: <ul> <li>The infimum is the greatest lower bound of a function, but it may not be attained by any point in the domain. It represents the “smallest possible value” the function can approach, even if it doesn’t reach it.</li> <li>Example: For \(f(x) = 1/x\) on \((0, 2]\), the infimum is \(\inf f(x) = 0\), but \(f(x)\) never actually equals \(0\) within the domain.</li> </ul> </li> </ol> <h5 id="why-use-inf-instead-of-min"><strong>Why Use \(\inf\) Instead of \(\min\)?</strong></h5> <p>In the context of the dual function:</p> <ul> <li>The infimum \(\inf_x L(x, \lambda)\) is used because the Lagrangian \(L(x, \lambda)\) might not achieve a true minimum for certain values of \(\lambda\) (e.g., the domain could be open or unbounded).</li> <li>Using \(\inf\) ensures that the dual function \(g(\lambda)\) is well-defined, even in cases where \(\min_x L(x, \lambda)\) doesn’t exist.</li> </ul> <h5 id="a-lighter-take"><strong>A Lighter Take</strong></h5> <p>Think of this as “playing with math language” to get to the result we want. Just as you might rephrase a sentence to make it clearer or more persuasive, in mathematics, we transform the problem into a new form (the dual) that’s easier to work with.</p> <p>For now, trust the process. This step might seem abstract, but it leads us to a form of the problem where powerful mathematical tools can come into play. Once we see the bigger picture, the reasoning behind these transformations will become clear.</p> <p>In essence, the Lagrange dual function \(g(\lambda)\) gives us a way to shift our perspective on the optimization problem, helping us solve it through duality principles. As we proceed, you’ll see how this approach simplifies the original problem and why it’s such a powerful concept.</p> <hr> <h4 id="the-primal-and-the-dual"><strong>The Primal and the Dual</strong></h4> <p>For any general primal optimization problem:</p> \[\begin{aligned} \min_x \quad &amp; f_0(x) \\ \text{subject to} \quad &amp; f_i(x) \leq 0, \quad i = 1, \dots, m, \end{aligned}\] <p>we can formulate the corresponding <strong>dual problem</strong> as:</p> \[\begin{aligned} \max_\lambda \quad &amp; g(\lambda) \\ \text{subject to} \quad &amp; \lambda_i \geq 0, \quad i = 1, \dots, m. \end{aligned}\] <p>The dual problem has some remarkable properties:</p> <ul> <li> <strong>Convexity</strong>: The dual problem is always a <strong>convex optimization problem</strong>, even if the primal problem is not convex.</li> <li> <strong>Simplification</strong>: In some cases, solving the dual problem is easier than solving the primal problem directly. This is particularly true when the primal problem is difficult to solve or the number of constraints is large.</li> </ul> <h5 id="contract-negotiation-analogy"><strong>Contract Negotiation Analogy</strong></h5> <ul> <li> <p><strong>Primal Problem (Your Terms)</strong>: Think of the primal problem as negotiating a contract where you aim to minimize costs while respecting certain constraints (like budget or timelines). You’re focused on getting the best deal for yourself under these limitations.</p> </li> <li> <p><strong>Lagrangian (Penalties for Violations)</strong>: During the negotiation, you introduce penalties — if you can’t meet a certain term, you adjust other aspects of the contract. This is similar to using Lagrange multipliers in the Lagrangian function to penalize constraint violations.</p> </li> <li> <p><strong>Dual Problem (Value Assessment)</strong>: In the dual problem, you step into the other party’s shoes and assess how much value they assign to the contract’s terms, maximizing the value they place on the constraints (such as how much they’d pay for more time or resources).</p> </li> <li> <p><strong>Duality</strong>: The primal (minimizing costs) and dual (maximizing value) problems balance each other. Weak duality means the dual value is a lower bound to the primal, and strong duality means the best deal is the same for both sides when the optimal values align.</p> </li> </ul> <p>Below are the next two important results derived from the above formulation.</p> <h4 id="weak-and-strong-duality"><strong>Weak and Strong Duality</strong></h4> <ol> <li> <p><strong>Weak Duality</strong>: This property tells us that the optimal value of the primal problem is always greater than or equal to the optimal value of the dual problem:</p> \[p^* \geq d^*,\] <p>where \(p^*\) and \(d^*\) are the optimal values of the primal and dual problems, respectively. This is a fundamental result in optimization theory. <strong>Why so?</strong> Because the dual problem is designed to provide a lower bound on the primal objective through the Lagrangian.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Lagrangian_1-480.webp 480w,/assets/img/Lagrangian_1-800.webp 800w,/assets/img/Lagrangian_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Lagrangian_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lagrangian_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ol> <li> <p><strong>Strong Duality</strong>: In some special cases (such as when the problem satisfies <strong>Slater’s condition</strong>), <strong>strong duality</strong> holds, meaning the optimal values of the primal and dual problems are equal:</p> \[p^* = d^*.\] <p>Strong duality is particularly useful because it allows us to solve the dual problem instead of the primal one, often simplifying the problem or reducing computational complexity.</p> </li> </ol> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Lagrangian_2-480.webp 480w,/assets/img/Lagrangian_2-800.webp 800w,/assets/img/Lagrangian_2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Lagrangian_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Lagrangian_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h5 id="slaters-conditions"><strong>Slater’s Conditions</strong></h5> <ul> <li>They are a set of conditions that <strong>guarantee strong duality</strong> in certain types of constrained optimization problems (particularly convex problems).</li> <li>They state that for strong duality to hold, there must exist a <strong>strictly feasible point</strong> (a point where all inequality constraints are strictly satisfied).</li> <li>In simpler terms, Slater’s conditions ensure that there is at least one point where all the constraints are strictly satisfied, which <strong>enables strong duality</strong> and ensures that the primal and dual solutions align.</li> <li>These conditions are crucial in convex optimization as they help guarantee the optimal solutions for both the primal and dual problems are the same.</li> </ul> <h4 id="complementary-slackness"><strong>Complementary Slackness</strong></h4> <p>When <strong>strong duality</strong> holds, we can derive the <strong>complementary slackness</strong> condition. This condition provides deeper insight into the relationship between the primal and dual solutions. Specifically, if \(x^*\) is the optimal primal solution and \(\lambda^*\) is the optimal dual solution, we have:</p> \[f_0(x^*) = g(\lambda^*) = \inf_x L(x, \lambda^*) \leq L(x^*, \lambda^*) = f_0(x^*) + \sum_{i=1}^m \lambda_i^* f_i(x^*).\] <p>For this equality to hold, the term \(\sum_{i=1}^m \lambda_i^* f_i(x^*)\) must be zero for each constraint. This leads to the following <strong>complementary slackness condition</strong>:</p> <ol> <li>If \(\lambda_i^* &gt; 0\), then \(f_i(x^*) = 0\), meaning that the corresponding constraint is <strong>active</strong> at the optimal point. Also, it means the constraint is tight and directly affects the optimal solution.</li> <li>If \(f_i(x^*) &lt; 0\), then \(\lambda_i^* = 0\), meaning that the corresponding constraint is <strong>inactive</strong>. This indicates that the constraint doesn’t influence the optimal solution at all.</li> </ol> <p>This condition tells us which constraints are binding (active) at the optimal solution and which are not, providing critical information about the structure of the optimal solution.</p> <h5 id="contract-negotiation-analogy-duality-and-complementary-slackness"><strong>Contract Negotiation Analogy: Duality and Complementary Slackness</strong></h5> <h6 id="strong-duality-and-weak-duality">Strong Duality and Weak Duality:</h6> <ul> <li> <p><strong>Weak Duality</strong>: In a contract negotiation, <strong>weak duality</strong> is like the <strong>minimum acceptable price</strong> for the other party. The price (dual value) they would accept for your offer will always be a <strong>lower bound</strong> to what you are willing to pay (primal cost). The other party cannot ask for more than what you’re offering, but they may accept less.</p> </li> <li> <p><strong>Strong Duality</strong>: <strong>Strong duality</strong> happens when both parties agree on the same <strong>optimal terms</strong> for the contract. Both your <strong>best offer</strong> (primal) and the <strong>value assigned</strong> to the terms (dual) align perfectly, resulting in the best possible contract for both sides.</p> </li> </ul> <h6 id="complementary-slackness-1">Complementary Slackness:</h6> <ul> <li> <strong>Complementary Slackness</strong> tells us which terms of the contract are really influencing the negotiation outcome. <ul> <li>If a <strong>dual variable</strong> (e.g., the price or terms the other party values) is <strong>positive</strong>, then the corresponding <strong>primal constraint</strong> (e.g., a term you care about, like the delivery time or cost) is <strong>active</strong> — it <strong>must</strong> be part of the final agreement.</li> <li>If a <strong>primal constraint</strong> (e.g., a timeline or budget) is not strict enough (it’s not a deal-breaker), then the <strong>dual variable</strong> (e.g., the value the other party assigns to it) is <strong>zero</strong>, meaning it doesn’t impact the outcome.</li> </ul> </li> </ul> <p><strong>Example:</strong></p> <ul> <li>Suppose you’re negotiating a <strong>project deadline</strong>. If the other party <strong>values</strong> the deadline highly (dual value is positive), it <strong>must</strong> be part of the contract (the deadline is <strong>active</strong>). If they don’t care much about it, then <strong>it doesn’t matter</strong> to the final deal (dual value is zero), and you can relax that constraint.</li> </ul> <hr> <p>Finally, there might be one question left—this is the one I had, so here’s the explanation:</p> <h5 id="if-the-lagrangian-dual-function-is-concave-how-is-the-lagrangian-dual-problem-always-a-convex-optimization-problem"><strong>If the Lagrangian dual function is concave, how is the Lagrangian dual problem always a convex optimization problem?</strong></h5> <p>To understand why the dual problem is always a convex optimization problem, let’s revisit its structure: \(\max_{\lambda \geq 0} \; g(\lambda).\)</p> <ol> <li> <p><strong>Maximizing a Concave Function:</strong><br> The dual function \(g(\lambda)\) is <strong>concave</strong> by construction. In optimization, maximizing a concave function is equivalent to minimizing a convex function (negating the objective). Therefore, the objective of the dual problem aligns with the structure of a convex optimization problem.</p> </li> <li> <p><strong>Convex Feasible Region:</strong><br> The constraints in the dual problem (\(\lambda \geq 0\)) define a <strong>convex set</strong>, as the non-negative orthant in \(\mathbb{R}^m\) is convex.</p> </li> </ol> <h6 id="definition-of-a-convex-optimization-problem"><strong>Definition of a Convex Optimization Problem:</strong></h6> <p>A problem is a convex optimization problem if:</p> <ul> <li>The objective function is <strong>convex</strong> (for minimization) or <strong>concave</strong> (for maximization).</li> <li>The feasible region is a <strong>convex set</strong>.</li> </ul> <p>The dual problem satisfies these conditions because:</p> <ul> <li>The objective function \(g(\lambda)\) is concave.</li> <li>The constraint \(\lambda \geq 0\) defines a convex feasible region.</li> </ul> <p>Thus, the dual problem is always a <strong>convex optimization problem</strong>, regardless of whether the primal problem is convex or not.</p> <h6 id="intuition-behind-convexity-of-the-dual-problem"><strong>Intuition Behind Convexity of the Dual Problem</strong></h6> <p>The dual problem’s convexity comes from the way it is constructed:</p> <ol> <li>The Lagrangian combines the primal objective and constraints into a single function that penalizes constraint violations.</li> <li>By minimizing the Lagrangian over the primal variables \(x\), the dual function \(g(\lambda)\) captures the <strong>tightest lower bound</strong> of the primal objective.</li> <li>The pointwise infimum of affine functions (as in \(g(\lambda)\)) is guaranteed to be concave.</li> <li>The dual problem maximizes this concave function over a convex set (\(\lambda \geq 0\)), making it a convex optimization problem.</li> </ol> <p>This ensures that solving the dual problem is computationally efficient and well-structured, even when the primal problem is non-convex.</p> <hr> <h4 id="conclusion"><strong>Conclusion</strong></h4> <p>By exploring the <strong>dual problem</strong> of SVM, we gain both theoretical insights and practical benefits. The dual formulation provides a new perspective on the original optimization problem, and solving it can sometimes be more efficient or insightful. The duality between the primal and dual problems underpins many of the optimization techniques used in machine learning, particularly in the context of support vector machines.</p> <p>Pat yourselves on the back for making it to the end of this blog! Take a well-deserved break, and stay tuned for the next one, where we’ll apply everything we’ve learned so far to formulate the SVM dual problem.</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://www.youtube.com/watch?v=thuYiebq1cE&amp;t=136s" rel="external nofollow noopener" target="_blank">https://www.youtube.com/watch?v=thuYiebq1cE&amp;t=136s - David S. Rosenberg</a></li> <li><a href="https://www.youtube.com/watch?v=8mjcnxGMwFo" rel="external nofollow noopener" target="_blank">Lagrange Multipliers | Geometric Meaning &amp; Full Example - Dr. Trefor Bazett</a></li> <li><a href="https://www.youtube.com/watch?v=d0CF3d5aEGc&amp;t=216s" rel="external nofollow noopener" target="_blank">Convexity and The Principle of Duality - Visually Explained</a></li> <li><a href="https://math.stackexchange.com/questions/515812/pointwise-infimum-of-affine-functions-is-concave?noredirect=1&amp;lq=1" rel="external nofollow noopener" target="_blank">Pointwise infimum of affine functions is concave</a></li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>