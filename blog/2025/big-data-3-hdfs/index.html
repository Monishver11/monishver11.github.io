<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hadoop Distributed File System (HDFS) | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/big-data-3-hdfs/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Hadoop Distributed File System (HDFS)</h1> <p class="post-meta"> Created in December 15, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/rbda-nyu"> <i class="fa-solid fa-tag fa-sm"></i> RBDA-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <ul> <li>The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware.</li> <li>It provides an interface similar to a POSIX file system (files and directories), but with relaxed requirements.</li> <li>It scales up to 100+ PB of storage and thousands of servers, supporting close to a billion files and blocks.</li> <li>Small gist about POSIX: POSIX (Portable Operating System Interface) is a standard that defines how file systems and operating systems should behave, ensuring consistency across Unix-like systems such as Linux and macOS. In file systems, POSIX specifies that every write must be immediately visible to all readers, files can be modified at any position (supporting random writes), and operations like open, close, read, write, delete, and rename must appear atomic and consistent. It also enforces strict locking and consistency rules for concurrent access, ensuring multiple processes can safely read and write files simultaneously without data corruption.</li> </ul> <h4 id="assumptions-and-goals"><strong>Assumptions and goals:</strong></h4> <ul> <li>Commodity hardware: Hardware failure is the norm rather than the exception. An HDFS instance may consist of thousands of servers. Each component has a non-trivial probability of failure. As a result, some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS.</li> <li>Streaming data access: HDFS is designed more for batch processing (e.g., MapReduce) rather than interactive use by users. The emphasis is on high throughput of data access rather than low latency of data access. Therefore, some POSIX semantics has been relaxed to increase data throughput rates.</li> <li>Large datasets: A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. It should provide high aggregate data bandwidth. It should scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.</li> <li>Simple coherency model: HDFS applications need a write-once-read-many (WORM) access model for files. Files cannot be modified except for appends and truncates. This design avoids complex consistency and synchronization problems that arise from random writes or concurrent updates. By restricting how data can change, HDFS simplifies data coherency management across replicas and achieves higher throughput for large-scale, sequential data access. Ex: a MapReduce application or a web crawler application fits perfectly with this model.</li> <li>Moving computation is cheaper than moving data: A computation requested by an application is much more efficient if it is executed near the data it operates on, especially when the size of the dataset is huge. This minimizes network congestion and increases the overall throughput of the system. So, it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. HDFS provides interfaces for applications to move themselves closer to where the data is located.</li> <li> <p>Portability across heterogeneous hardware &amp; software platforms: HDFS has been designed to be easily portable from on platform to another. It is implemented as a user-level filesystem in Java. Because it’s written in Java, it runs anywhere a Java Virtual Machine (JVM) is available, such as Linux, Windows, or macOS. The JVM acts as a layer between the Java program and the underlying operating system, translating Java instructions into native instructions the OS can execute. And unlike traditional file systems built directly into the operating system kernel, HDFS is a user-level file system, meaning it operates as a regular application process instead of requiring kernel-level changes. This makes it easier to install, update, and move between environments. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.</p> </li> <li>A gist on how Java program is executed from code to instructions in device: When a Java program like HDFS is executed, the process starts with writing source code in .java files, which is then compiled by the Java compiler (javac) into bytecode stored in .class files. These class files, along with necessary resources and configuration, are often bundled into a JAR (Java ARchive) file for easy distribution and deployment. On the target machine, the Java Virtual Machine (JVM) loads the bytecode from the class or JAR files and interprets or Just-In-Time (JIT) compiles it into native machine instructions that the operating system and hardware can execute. This abstraction provided by the JVM allows the same Java program or JAR file to run on any device or OS with a JVM installed, making Java programs, including HDFS, portable across heterogeneous platforms.</li> </ul> <h4 id="architecture"><strong>Architecture</strong></h4> <ul> <li>HDFS has a master-workers architecture.</li> <li>One NameNode: Manages the file system metadata.</li> <li>Many DataNodes: Stores the actual data blocks</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-1-480.webp 480w,/assets/img/hdfs-1-800.webp 800w,/assets/img/hdfs-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hdfs-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>The HDFS namespace is a hierarchy of files and dirs. It supports user quotas and access permissions.</li> <li>The file content is split into large blocks (typically 128MB). Large blocks can minimize seek time. A file smaller than a block does not occupy a full block’s worth of storage.</li> <li>Clarification: Disk seek time is the delay caused by moving the read/write head to the data location. By using large blocks, HDFS allows more data to be read sequentially once the head is positioned, reducing the number of seeks needed. Fewer, larger blocks overall improve throughput for reading large files, which is especially beneficial for batch processing like MapReduce.</li> <li>Benefits of blocks as primitive unit: <ul> <li>Support very large files, even larger than any single disk in the network.</li> <li>Simplify storage management and decouple metadata from blocks.</li> <li>Blocks can be replicated for fault tolerance and availability.</li> </ul> </li> <li>NameNode: <ul> <li>The NameNode manages the HDFS namespace.</li> <li>It maintains the file system tree and the metadata for all the files and directories. This information is persisted on disk. The NameNode loads the entire namespace image into memory at startup.</li> <li>The NameNode also knows where every block is located. This information is in memory only, not persisted on disk. It can be reconstructed from DataNodes when the system starts (via heartbeats).</li> <li>Without the NameNode, the file system cannot be used.</li> </ul> </li> <li>DataNode: <ul> <li>DataNodes are the workhorses of HDFS.</li> <li>Each block is independently replicated at multiple DataNodes. An application can specify the number of replicas of a file that should be maintained by HDFS. It is called the replication factor of that file (typically 3).</li> <li>DataNodes store and retrieve blocks when asked by clients or the NameNode.</li> <li>DataNodes sends heartbeats to the NameNode (typically every 3 seconds).</li> <li>DataNodes also report to the NameNode periodically with the lists of blocks they are storing (at startup and every hour).</li> </ul> </li> <li>HDFS client: <ul> <li>The HDFS client is a library that exports the HDFS file system interface.</li> <li>Applications access the file system using the HDFS client.</li> <li>The user application does not need to know that the filesystem metadata and storage are on different servers, and that blocks have multiple replicas.</li> <li>However, the block locations are exposed to the client, so that applications like MapReduce can schedule tasks to where the data are located.</li> </ul> </li> <li>Reading a file <ul> <li>First, the HDFS clients asks the NameNode for the list of DataNodes that host replicas of the blocks of the file.</li> <li>The list is sorted by the network topology distance from the client.</li> <li>Then the client contacts a DataNode directly and requests the transfer of the desired block.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-2-480.webp 480w,/assets/img/hdfs-2-800.webp 800w,/assets/img/hdfs-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hdfs-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Network topology <ul> <li>The distance between two nodes is the sum of their distances to their closest common ancestor.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-3-480.webp 480w,/assets/img/hdfs-3-800.webp 800w,/assets/img/hdfs-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hdfs-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Writing a file <ul> <li>First, the HDFS client asks the NameNode to choose DataNodes to host replicas of the first block of the file.</li> <li>The client organizes a pipeline from node to node and sends the data.</li> <li>When the first block is filled, the client requests new DataNodes to be chosen to host replicas of the next block. A new pipeline is organized, and the client sends the further bytes of the file.</li> <li>Choice of DataNodes for each block is likely to be different.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-4-480.webp 480w,/assets/img/hdfs-4-800.webp 800w,/assets/img/hdfs-4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hdfs-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Block placement <ul> <li>Trade-oﬀ between minimizing the write cost, and maximizing data reliability, availability and aggregate read bandwidth. <ul> <li>#1: same node as the client.</li> <li>#2: diﬀerent rack from #1.</li> <li>#3: same rack as #2, but diﬀerent node.</li> <li>This default strategy gives a good balance among: <ul> <li>Reliability: blocks are stored on two racks.</li> <li>Write bandwidth: writes only have to traverse a single network switch.</li> <li>Read performance: choice of two racks to read from.</li> <li>Block distribution across the cluster: clients only write a single block on the local rack.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-5-480.webp 480w,/assets/img/hdfs-5-800.webp 800w,/assets/img/hdfs-5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hdfs-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>The single-writer, multiple-reader model <ul> <li>The HDFS client that opens a file for writing is granted a lease (lock) for the file; no other client can write to the file. The writer’s lease doesn’t prevent other clients from reading the file; a file may have many concurrent readers.</li> <li>The writing client periodically renews the lease by sending a heartbeat to the NameNode.</li> <li>When the file is closed, the lease is revoked.</li> </ul> </li> <li>Coherency model <ul> <li>A coherency model for a filesystem describes the data visibility of reads and writes for a file.</li> <li>HDFS trades off some POSIX semantics for performance.</li> <li>After creating a file, it is visible in the filesystem namespace. However, the last block’s content may not be visible until the file is closed.</li> <li>If the client needs the visibility guarantee, it can call hflush() explicitly. It guarantees that the data written up to that point in the file has reached all the DataNodes in the write pipeline and is visible to all new readers. However, the data may be in the DataNodes’ memory only.</li> <li>To guarantee that the DataNodes have written the data to disk, call hsync().</li> <li>So, in short: HDFS’s coherency model is tied to its block structure - a file is visible immediately, but the last block may be partially written. Visibility and durability of that block are controlled explicitly via hflush() (memory) and hsync() (disk).</li> </ul> </li> </ul> <h4 id="resilience"><strong>Resilience</strong></h4> <ul> <li>NameNode <ul> <li>NameNode persists checkpoint + journal on disk.</li> <li>Checkpoint: the file system tree and metadata at the specific point in time.</li> <li>Journal (“edit log”): All changes to HDFS since the last checkpoint.</li> <li>CheckpointNode periodically combines the existing checkpoint and journal into a new checkpoint and sends it back to NameNode, which truncates the journal.</li> <li>BackupNode maintains a read-only, synchronized namespace state of the NameNode without block locations.</li> </ul> </li> <li>DataNode <ul> <li>When writing a file, the client computes the checksum for each data block. DataNodes store the checksums locally in a metadata file.</li> <li>When reading a file, the client verifies the checksum. If a block is corrupted, the client notifies the NameNode and fetches another replica of the block.</li> <li>If a DataNode fails or a block is corrupted: <ul> <li>Data can be retrieved from another DataNode storing the block replica.</li> <li>The NameNode marks the replica as unavailable/corrupt.</li> <li>The NameNode schedules creation of new replicas on other DataNodes.</li> </ul> </li> </ul> </li> <li>A small gist about checksum: A checksum is a small value computed from a block of data to verify its integrity. It is generated by applying a hash function (e.g., CRC32) to a block of data. The resulting number changes if even a single bit of the data changes. In HDFS, when data is written, a checksum is generated and stored alongside it. During reads, the checksum is recalculated and compared with the stored value to detect any corruption. If they don’t match, HDFS identifies the block as damaged and retrieves a healthy replica from another DataNode. This ensures reliable and consistent data storage across the system.</li> </ul> <h4 id="optimizations"><strong>Optimizations</strong></h4> <ul> <li>Block caching <ul> <li>Frequently accessed blocks may be explicitly cached in DataNode’s memory.</li> <li>By default, a block is cached in only “one” DataNode’s memory. This # of DataNode’s is configurable on a per-file basis.</li> <li>Users or applications tell the NameNode which files to cache, and for how long.</li> <li>Applications (e.g., MapReduce) can schedule tasks on the DataNode where a block is cached, for increased read performance.</li> <li>The NameNode maintains which DataNodes holds the cache of a specific block, along with other bookkeeping information.</li> </ul> </li> <li>HDFS federation <ul> <li>On a very large clusters with many files, the NameNode’s memory becomes the limiting factor for scaling. This is because the NameNode keeps a reference to every file and block in the filesystem in memory.</li> <li>HDFS federation allows a cluster to scale by adding NameNodes. Each NameNode manages a portion of the filesystem namespace(its own part of the directory tree).</li> <li>Under federation, each NameNode manages a namespace volume, which contains the metadata for the namespace.</li> <li>Namespace volumes are independent of each other. NameNodes do not communicate with one another. If one NameNode fails, the availability of the namespaces managed by other NameNodes will not be affected.</li> <li>Under federation, each NameNode also manages a block pool, which contains all the blocks for the files in the namespace.</li> <li>Block pool storage is not partitioned among DataNodes. DataNodes register with each NameNode in the cluster and store blocks from multiple block pools.</li> <li>The conceptual flow: In very large HDFS clusters, the single NameNode becomes a scalability bottleneck because it must keep metadata for every file and block in memory. HDFS Federation solves this by allowing multiple independent NameNodes, each managing its own namespace volume—a separate portion of the filesystem’s directory structure. Along with the namespace, each NameNode also manages a corresponding block pool, which contains all the physical data blocks belonging to files in that namespace. The DataNodes in the cluster are shared among all NameNodes and register with each of them, storing blocks from multiple block pools simultaneously. Note that the DataNode storage is shared physically, but logically partitioned into multiple block pools - one per NameNode. This design separates metadata management from physical storage, allowing the system to scale horizontally, isolate faults (a failure in one NameNode doesn’t affect others), and support multiple namespaces while still using the same underlying DataNodes for efficient shared storage.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-6-480.webp 480w,/assets/img/hdfs-6-800.webp 800w,/assets/img/hdfs-6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hdfs-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/hdfs-7-480.webp 480w,/assets/img/hdfs-7-800.webp 800w,/assets/img/hdfs-7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/hdfs-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="hdfs-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>HDFS High Availability (HA) <ul> <li>Although checkpoints of the NameNode protect against data loss, they don’t provide high availability of the filesystem.</li> <li>The NameNode is still a single point of failure (SPOF).</li> <li>If the NameNode fails or performs routine maintenance, the entire Hadoop ecosystem becomes out of service until a new NameNode is brought online. Manual intervention is required. On large clusters with many files and blocks, starting a new NameNode can take 30 minutes or more.</li> <li>With HDFS high availability (HA), there are a pair of NameNodes in an active-standby configuration.</li> <li>If the Active NameNode fails, the Standby NameNode takes over its duties to continue servicing client requests without a significant interruption (~1 minute).</li> <li>Even if the Standby NameNode is down when the Active NameNode fails, the sysadmin can still start the Standby NameNode from cold (same as non-HA).</li> <li>HDFS High Availability (HA) requires a few architectural changes: <ul> <li>The NameNodes must use highly available shared storage (e.g., NFS or the Quorum Journal Manager) to share the journal.</li> <li>DataNodes must send block reports to both NameNodes because the block mappings are stored in a NameNode’s memory, not on disk.</li> <li>Clients must be configured to handle NameNode failover transparently.</li> <li>Checkpoint/BackupNode’s role is subsumed by the Standby NameNode, which takes periodic checkpoints of the Active NameNode’s namespace.</li> </ul> </li> <li>The transition from the Active NameNode to the Standby NameNode is managed by a failover controller. By default, it uses ZooKeeper to ensure that only one NameNode is active.</li> <li>Each NameNode runs a lightweight failover controller process, which monitors its NameNode for failures (using a simple heartbeating mechanism) and triggers a failover should a NameNode fail.</li> <li>Failover may also be initiated manually (e.g., for routine maintenance). This is known as a graceful failover, since the failover controller arranges an orderly transition for both NameNodes to switch roles.</li> <li>However, in the case of an ungraceful failover, it’s impossible to be sure that the failed NameNode has stopped running.</li> <li>For example, a slow network or a network partition can trigger a failover transition, even though the previous Active NameNode is still running and thinks it’s still the Active NameNode.</li> <li>The HA implementation employs fencing methods to ensure that the previous Active NameNode is prevented from doing any damage or causing corruption.</li> <li>Have you heard of STONITH (shoot the other node in the head)?</li> <li>Q: How does each NameNode monitor failures in HDFS High Availability? A: Each NameNode runs a lightweight Failover Controller (ZKFC) that monitors the health of its own NameNode using a heartbeat mechanism. Both ZKFCs communicate with ZooKeeper, which coordinates and ensures that only one NameNode is Active at any time. If the Active NameNode fails, ZooKeeper triggers the Standby’s ZKFC to take over and become Active, ensuring continuous service.</li> <li>Q: What are fencing methods and why are they needed? A: Fencing is a safety mechanism used during failover to prevent the old Active NameNode from making changes after a Standby takes over. This is crucial because the old Active might still be running but disconnected (e.g., due to a network partition). Fencing isolates or disables the old node, commonly by killing its process, revoking access to shared storage, or even powering off the machine, ensuring that only one NameNode can write to the filesystem metadata, preventing corruption.</li> <li>Q: What is STONITH and how is it related? A: STONITH (Shoot The Other Node In The Head) is a type of fencing where the old node is forcibly shut down before the Standby takes over. In HDFS HA, it guarantees that the previous Active NameNode cannot interfere with the new Active, providing a strong safeguard against simultaneous writes and ensuring data consistency.</li> </ul> </li> <li>Balancer <ul> <li>Over time, the distribution of blocks across DataNodes can become unbalanced. An unbalanced cluster can aﬀect locality for applications (e.g., MapReduce), and it puts a greater strain on the highly utilized DataNodes.</li> <li>The balancer program is a daemon… <ul> <li>It redistributes blocks by moving them from overutilized DataNodes to underutilized DataNodes.</li> <li>It adheres to the block replica placement policy that makes data loss unlikely by placing block replicas on diﬀerent racks.</li> <li>It minimizes inter-rack data copying in the balancing process.</li> </ul> </li> <li>Q: What is the HDFS Balancer daemon and where does it run? A: The Balancer is a separate user-level daemon that runs on the Hadoop cluster, typically launched from a client node or NameNode host. It is not part of the NameNode or DataNode processes.</li> <li>Q: How does the Balancer work? A: It communicates with the NameNode to get cluster metadata, identifies overutilized and underutilized DataNodes, and moves blocks accordingly. It follows replica placement rules to avoid data loss and minimize inter-rack transfers.</li> <li>Q: What happens if there are failovers or crashes? A: If the Active NameNode fails, the Balancer reconnects to the new Active NameNode. If a DataNode fails, block moves to that node are paused or rescheduled. If the Balancer itself crashes, it can be restarted safely, resuming block moves without affecting cluster integrity.</li> <li>Q: If the Balancer runs outside HDFS, how does it access block information? A: The Balancer accesses block information through the HDFS client interface. It communicates with the NameNode using HDFS RPC APIs to retrieve metadata about all blocks, including which DataNodes store them and their disk utilization.</li> <li>Q: How does the Balancer move blocks between DataNodes? A: After obtaining metadata from the NameNode, the Balancer schedules block transfers between DataNodes. The actual data movement happens directly between DataNodes, while the Balancer only coordinates the process based on the metadata it received.</li> <li>Q: Why can the Balancer manage blocks even though it’s outside HDFS processes? A: Because it uses the official HDFS client APIs, the Balancer can read cluster metadata, monitor utilization, and orchestrate block redistribution without being part of the NameNode or DataNode processes.</li> </ul> </li> <li>Block scanner <ul> <li>Every DataNode runs a block scanner, which periodically verifies all the blocks stored on the DataNode.</li> <li>This allows bad blocks to be detected and fixed before they are read by clients.</li> <li>The scanner maintains a list of blocks to verify and scans them one by one for checksum errors.</li> <li>It also employs a throttling mechanism to preserve disk bandwidth on the DataNode. This throttling mechanism limits the speed or resource usage of the block scanner so it does not overwhelm the DataNode’s disk or network.</li> </ul> </li> </ul> <h4 id="usage"><strong>Usage</strong></h4> <ul> <li>The Hadoop FS shell provides commands that directly interact with HDFS.</li> <li>Ex: hadoop fs -command <args></args> </li> </ul> <p><strong>Doubt: Is HDFS C+P or A+P under CAP theorem?</strong></p> <ul> <li>From the sources, I feel it falls under C+P. One of the answer/argument that makes sense is below.</li> <li>The cluster is consistent as long as the primary namenode is available. when a namenode becomes unavailable, the secondary namenode gets queried, however writes can become delayed / rejected. so despite not being an explicit “single point of failure”, it will still affect the “perceived availability and/or consistency of data” to external interfaces. At the end of the day, the primary namenode handles data consistency, and is synced to the secondary namenode (i.e. it does not require consensus). This is only “partially available” in practice.</li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Monishver11/monishver11.github.io","data-repo-id":"R_kgDONe9Wkw","data-category":"General","data-category-id":"DIC_kwDONe9Wk84Cp-bh","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-apache-flink",title:"Apache Flink",description:"Realtime and Big Data Analytics Course at NYU Courant - Personal Notes 10",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-11-flink/"}},{id:"post-apache-kafka",title:"Apache Kafka",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 9",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-10-kafka/"}},{id:"post-apache-zookeeper",title:"Apache ZooKeeper",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 8",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-9-zookeeper/"}},{id:"post-apache-hbase",title:"Apache HBase",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 7",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-8-hbase/"}},{id:"post-hive-amp-trino",title:"Hive &amp; Trino",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 6",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-7-hive/"}},{id:"post-mapreduce-design-patterns",title:"MapReduce Design Patterns",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 5",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-5-mr-dp/"}},{id:"post-big-data-processing-concepts-amp-mapreduce",title:"Big Data Processing Concepts &amp; MapReduce",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-4-mapreduce/"}},{id:"post-hadoop-distributed-file-system-hdfs",title:"Hadoop Distributed File System (HDFS)",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-3-hdfs/"}},{id:"post-reading-notes-from-aleksa-gordic-39-s-gpu-blogpost",title:"Reading Notes from Aleksa Gordic&#39;s GPU BlogPost",description:"Reading notes for my reference from Aleksa Gordic&#39;s GPU BlogPost",section:"Posts",handler:()=>{window.location.href="/blog/2025/aleksagordic-gpu-blog-notes/"}},{id:"post-mlp-standard-derivatives-derivation",title:"MLP Standard Derivatives Derivation",description:"MLP Standard Derivatives Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-derivatives/"}},{id:"post-simple-mlp-forward-and-backward-pass-with-einsum-derivation",title:"Simple MLP - Forward and Backward Pass (With Einsum) Derivation",description:"Simple MLP - Forward and Backward Pass Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-fw-bwd/"}},{id:"post-gpu-notes",title:"GPU Notes",description:"GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes",section:"Posts",handler:()=>{window.location.href="/blog/2025/gpu-notes/"}},{id:"post-big-data-storage",title:"Big Data Storage",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 2",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-2-storage/"}},{id:"post-introduction-to-realtime-and-big-data-analytics",title:"Introduction to Realtime and Big Data Analytics",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-1-intro/"}},{id:"post-gpu-essentials-a-concise-technical-guide",title:"GPU Essentials - A Concise Technical Guide",description:"A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.",section:"Posts",handler:()=>{window.location.href="/blog/2025/GPU-Intro/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey-blog-2025-wrapping-ml-basics",title:"[Wrapping Up Our ML Foundations Journey](/blog/2025/wrapping-ml-basics/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-smallgraphgcn-accelerating-gnn-training-on-batched-small-graphs",title:"SmallGraphGCN - Accelerating GNN Training on Batched Small Graphs",description:"Discover how fused-edge-centric CUDA kernels dramatically accelerate Graph Neural Network training on molecular datasets. By rethinking parallelism strategies for batched small graphs, our approach achieves up to 3.1\xd7 faster forward execution and 1.3\xd7 end-to-end training speedup over PyTorch Geometric.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>