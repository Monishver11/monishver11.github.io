<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Random Forests | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/random-forest/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Random Forests</h1> <p class="post-meta"> Created in April 27, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>After understanding the power of bagging to reduce variance by combining multiple models, a natural question arises: <strong>Can we make this idea even stronger?</strong></p> <p>Bagging helps, but when the individual models (like decision trees) are still correlated, variance reduction is not as efficient as it could be. This brings us to the motivation for a more advanced ensemble method — <strong>Random Forests</strong>.</p> <hr> <h5 id="motivating-random-forests-handling-correlated-trees"><strong>Motivating Random Forests: Handling Correlated Trees</strong></h5> <p>Let’s revisit an important insight we learned from bagging.</p> <p>Suppose we have independent estimates \(\hat{\theta}_1, \dots, \hat{\theta}_n\) satisfying:</p> \[\mathbb{E}[\hat{\theta}_i] = \theta, \quad \text{Var}(\hat{\theta}_i) = \sigma^2\] <p>Then:</p> <ul> <li> <p>The mean of the estimates is:</p> \[\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n \hat{\theta}_i\right] = \theta\] </li> <li> <p>And the variance of the mean is:</p> \[\text{Var}\left(\frac{1}{n} \sum_{i=1}^n \hat{\theta}_i\right) = \frac{\sigma^2}{n}\] </li> </ul> <p>This shows that averaging independent estimators reduces variance effectively.</p> <p>However, in practice, if the estimators \(\hat{\theta}_i\) are <strong>correlated</strong>, the variance reduction is <strong>less effective</strong>. Why is that? Let’s break it down.</p> <p><strong>What Happens When Estimators are Correlated?</strong></p> <p>To understand this, consider that each estimator \(\hat{\theta}_i\) has some <strong>variance</strong> on its own. If these estimators are independent, averaging them reduces the total variance by a factor of \(\frac{1}{n}\) (as we saw earlier in bagging). This is because <strong>independence</strong> means that the errors or fluctuations in one model don’t affect the others.</p> <p>But, when estimators are <strong>correlated</strong>, this doesn’t hold true anymore. In fact, the variance reduction becomes less effective. The reason for this is that correlation introduces <strong>covariance terms</strong>.</p> <ul> <li> <strong>Covariance</strong> is a measure of how much two random variables change together. If two estimators are correlated, the error in one model is likely to be reflected in the other. This reduces the benefit of averaging since both estimators will likely “make similar mistakes.”</li> </ul> <p><strong>A Simple Example</strong></p> <p>Imagine you’re trying to estimate the weight of an object. Suppose you have two different methods (estimators) to measure this weight:</p> <ol> <li> <strong>Estimator 1</strong> measures the weight but has some error due to a systematic bias (e.g., the scale is always slightly off by 1kg).</li> <li> <strong>Estimator 2</strong> is another scale that also has a bias, but it happens to be the same as Estimator 1.</li> </ol> <p>Now, if these two estimators have <strong>the same bias</strong>, they are correlated because they both tend to make the same mistake. Averaging the two estimations won’t help much because both estimators have the same error. So, the resulting variance after averaging will not reduce as much as we would expect with independent estimators.</p> <p><strong>Why Are Bootstrap Samples Not Fully Independent?</strong></p> <p>Now, let’s return to bagging, where we train multiple trees on <strong>bootstrap samples</strong>.</p> <ul> <li>A <strong>bootstrap sample</strong> is generated by randomly sampling with replacement from the original dataset. This means that each point in the original dataset has a chance of being included multiple times, and some points may not be included at all.</li> <li>These <strong>bootstrap samples</strong> are independent <strong>from each other</strong> in terms of how they are created, but they are <strong>not independent</strong> from the true population distribution \(P_{X \times Y}\).</li> </ul> <p>Why? Because all the bootstrap samples come from the <strong>same original dataset</strong>, which means they contain the same underlying distribution. If the original dataset has certain features that are particularly strong predictors, these features will often appear at the top of many decision trees across different bootstrap samples. This can make the individual trees more <strong>similar</strong> to each other than we would like.</p> <p>For example, consider a dataset where the feature “age” strongly predicts whether a customer will buy a product. If all the bootstrap samples include “age” as a key feature, the decision trees trained on these samples will end up making similar decisions based on “age”. This creates <strong>correlation</strong> between the predictions of the trees because they are all making similar splits based on the same strong features.</p> <p><strong>What Does This Mean for Bagging?</strong></p> <p>In bagging, since the trees are still correlated (due to the shared features across bootstrap samples), the reduction in variance is not as significant as we would expect if the trees were fully independent. This limits the effectiveness of bagging.</p> <p><strong>Can We Reduce This Correlation?</strong></p> <p>This is the key challenge addressed by <strong>Random Forests</strong>. By introducing additional randomness into the process — specifically, by randomly selecting a subset of features at each node when building each tree — we can reduce the correlation between the trees. This leads to a more diverse set of trees, improving the overall performance of the ensemble.</p> <p>Thus, reducing the correlation between trees is one of the main innovations of Random Forests that makes it more powerful than bagging alone.</p> <hr> <p>To clearly understand how correlation between trees impacts variance reduction, let’s break down the two scenarios with a full mathematical setup.</p> <h5 id="setup"><strong>Setup:</strong></h5> <ul> <li>Suppose we have a dataset \(D = \{x_1, x_2, \dots, x_n\}\), where each \(x_i\) is a feature vector and each corresponding output \(y_i\) is from a true population distribution \(P_{X \times Y}\).</li> <li>We are training decision trees, and we want to understand how their predictions are affected by the sampling method.</li> <li>We’ll define two key scenarios: <ul> <li> <strong>Independent sampling</strong> (ideal case): Each tree in the ensemble is trained on independently drawn samples from the true population \(P_{X \times Y}\).</li> <li> <strong>Bootstrap sampling</strong>: Each tree is trained on a bootstrap sample, which is created by sampling <em>with replacement</em> from the original dataset \(D\).</li> </ul> </li> </ul> <p><strong>Case 1: Independent Sampling</strong></p> <p>Let’s assume that we have two estimators (trees) \(\hat{f}_1(x)\) and \(\hat{f}_2(x)\), both trained independently from the true population. Their predictions are unbiased, and their variances are \(\text{Var}(\hat{f}_1(x)) = \sigma_1^2\) and \(\text{Var}(\hat{f}_2(x)) = \sigma_2^2\). Since they are trained on independent samples, the covariance between their predictions is zero:</p> \[\text{Cov}(\hat{f}_1(x), \hat{f}_2(x)) = 0\] <p>This means that the two trees are completely independent of each other. When we combine their predictions (by averaging), we can reduce the overall variance of the ensemble:</p> \[\text{Var}(\hat{f}_{\text{avg}}(x)) = \frac{1}{2} \left( \text{Var}(\hat{f}_1(x)) + \text{Var}(\hat{f}_2(x)) \right) = \frac{\sigma_1^2 + \sigma_2^2}{2}\] <p>Since the predictions are independent, the variance reduces nicely without any issues.</p> <p><strong>Case 2: Bootstrap Sampling</strong></p> <p>Now, let’s look at the case of bootstrap sampling. Each decision tree is trained on a bootstrap sample of the original data, which means that the training samples are drawn <em>with replacement</em> from the dataset. This results in <strong>correlated trees</strong> because:</p> <ul> <li>Bootstrap samples are independent of each other (each sample is drawn from the dataset), but they are <strong>not independent</strong> of the true population distribution \(P_{X \times Y}\).</li> <li>As a result, the predictions from different trees \(\hat{f}_1(x)\) and \(\hat{f}_2(x)\) are <strong>correlated</strong> with each other.</li> </ul> <p>Let’s define the correlation coefficient between \(\hat{f}_1(x)\) and \(\hat{f}_2(x)\) as \(\rho\), where \(0 &lt; \rho &lt; 1\). This correlation arises because both trees are trained on slightly different subsets of the data, which means they will make similar predictions on the same inputs.</p> <p>Now, the <strong>variance of the ensemble</strong> will depend on both the variance of individual trees and the covariance between them:</p> \[\text{Var}(\hat{f}_{\text{avg}}(x)) = \frac{1}{2} \left( \text{Var}(\hat{f}_1(x)) + \text{Var}(\hat{f}_2(x)) \right) + \text{Cov}(\hat{f}_1(x), \hat{f}_2(x))\] <p>Since \(\text{Cov}(\hat{f}_1(x), \hat{f}_2(x)) = \rho \cdot \sigma_1 \cdot \sigma_2\), we get:</p> \[\text{Var}(\hat{f}_{\text{avg}}(x)) = \frac{\sigma_1^2 + \sigma_2^2}{2} + \rho \cdot \sigma_1 \cdot \sigma_2\] <p>Notice that the correlation \(\rho\) causes the variance to <strong>not reduce as effectively</strong> as in the independent case. The more correlated the trees are, the less variance reduction we achieve, and the ensemble may not perform as well as expected.</p> <p><strong>Key Differences and Conclusion:</strong></p> <ol> <li> <p><strong>Independent Sampling</strong>: When the trees are independent, we see <strong>maximum variance reduction</strong> because there is no covariance between the models. The variance of the average prediction is simply the average of the individual variances.</p> </li> <li> <p><strong>Bootstrap Sampling</strong>: When the trees are trained on bootstrap samples, the <strong>correlation</strong> between the trees reduces the potential for variance reduction. This is because the trees share a common structure due to being trained on similar data. The variance of the average prediction is larger because of the covariance term.</p> </li> </ol> <p>This setup clearly shows how correlation between trees in bootstrap sampling impacts the variance reduction in bagging. The next step to address this issue is through <strong>Random Forests</strong>, where we introduce further randomness to decorrelate the trees.</p> <hr> <h5 id="random-forests"><strong>Random Forests</strong></h5> <p>Random Forests build upon the foundation of bagging decision trees, but introduce an extra layer of randomness to improve performance and reduce correlation between trees. Here’s how:</p> <ul> <li> <strong>Grow trees independently</strong>, just as in bagging, by training each tree on a different bootstrap sample.</li> <li> <strong>At each split</strong> in the tree, instead of considering all available features, <strong>randomly select a subset of \(m\) features</strong> and split based only on these.</li> </ul> <p><strong>What Does This Change Do?</strong></p> <p>This adjustment has a significant impact on the performance of the ensemble:</p> <ul> <li> <strong>Reduces correlation between trees</strong>: By limiting the set of features considered at each split, trees become less likely to make the same decisions and thus become less correlated with each other.</li> <li> <strong>Increases diversity among trees</strong>: Different features lead to different decision boundaries in each tree, creating a diverse set of models that are not overly similar to each other.</li> <li> <strong>Improves ensembling effectiveness</strong>: With greater diversity, the ensemble as a whole becomes stronger. The averaged predictions from these less correlated trees lead to more robust and accurate results.</li> </ul> <p><strong>Typical Values for \(m\)</strong></p> <p>The parameter \(m\) determines the number of features considered at each split. The value of \(m\) is chosen based on the type of task:</p> <ul> <li> <p>For <strong>classification tasks</strong>, it is common to set:<br> \(m \approx \sqrt{p}\)</p> </li> <li> <p>For <strong>regression tasks</strong>, we typically set:<br> \(m \approx \frac{p}{3}\)</p> </li> </ul> <p>(where \(p\) is the total number of features.)</p> <p>These values help strike a balance between randomness and the amount of information available at each decision node.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-3-480.webp 480w,/assets/img/ensemble-3-800.webp 800w,/assets/img/ensemble-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ensemble-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Random Forests: Effect of Feature Subset Size (m) </div> <p><strong>Important Note:</strong></p> <p>If you set \(m = p\) (i.e., if each tree is allowed to use all the features at each split), then Random Forests will behave just like <strong>bagging</strong> — i.e., there will be no additional randomness, and the trees will be fully correlated.</p> <p>By introducing this random selection of features, <strong>Random Forests</strong> overcome one of the limitations of bagging (the correlation between trees) and unlock the full power of ensemble learning. This makes Random Forests one of the most powerful and widely used machine learning techniques today.</p> <hr> <h5 id="review-recap-of-key-concepts"><strong>Review: Recap of Key Concepts</strong></h5> <p>In summary, here’s a quick review of the key points we’ve covered:</p> <ul> <li> <p><strong>Deep decision trees</strong> generally have <strong>low bias</strong> (they can fit the training data very well) but <strong>high variance</strong> (small changes in the data can lead to significant changes in the tree structure).</p> </li> <li> <p><strong>Ensembling</strong> multiple models helps <strong>reduce variance</strong>. The rationale behind this is that the <strong>mean of i.i.d. estimates</strong> tends to have <strong>smaller variance</strong> than a single estimate.</p> </li> <li> <p><strong>Bootstrap sampling</strong> allows us to simulate many different datasets from a single training set, which is the foundation of <strong>bagging</strong>.</p> </li> </ul> <p>However, while bagging uses <strong>bootstrap samples</strong> to train individual models, these models (the decision trees) are <strong>correlated</strong>, which limits the reduction in variance.</p> <ul> <li> <strong>Random Forests</strong> address this by <strong>increasing the diversity</strong> of the ensemble. They achieve this by selecting a <strong>random subset of features</strong> at each split of the decision trees, reducing correlation and enhancing performance.</li> </ul> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>To wrap it up, Random Forests combine the strengths of bagging (reduced variance) with the added benefit of increased diversity among trees. By introducing randomness in feature selection, they make each tree in the ensemble more independent, leading to a <strong>stronger, more robust model</strong>.</p> <p>Random Forests stand out as one of the most powerful and widely used techniques in machine learning, thanks to their ability to handle complex data patterns while mitigating overfitting through ensembling and randomness.</p> <p>Next, we’ll explore <strong>Boosting</strong> - another powerful ensemble technique that builds models sequentially to improve accuracy by focusing on the mistakes made by previous models.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>