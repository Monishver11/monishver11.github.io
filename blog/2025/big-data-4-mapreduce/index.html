<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Big Data Processing Concepts &amp; MapReduce | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/big-data-4-mapreduce/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Big Data Processing Concepts &amp; MapReduce</h1> <p class="post-meta"> Created in December 15, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/rbda-nyu"> <i class="fa-solid fa-tag fa-sm"></i> RBDA-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h4 id="big-data-processing-concepts"><strong>Big data processing concepts</strong></h4> <ul> <li>Parallel data processing <ul> <li>Parallel data processing reduces the execution time by dividing a single large job into multiple smaller tasks that run concurrently. Ex: a single machine with multiple processors or cores.</li> </ul> </li> <li>Distributed data processing <ul> <li>Distributed data processing is achieved through physically separate machines that are networked together as a cluster.</li> </ul> </li> <li>Hadoop <ul> <li>Hadoop is an open-source framework for large-scale data storage and data processing that is compatible with commodity hardware.</li> <li>It can be used as an analytics engine for processing large amounts of structured, semi-structured and unstructured data.</li> <li>It implements the MapReduce processing framework.</li> </ul> </li> <li>Processing workloads <ul> <li>A processing workload in Big Data is defined as the amount and nature of data that is processed within a certain amount of time.</li> <li>Workloads are usually divided into two types: Batch processing and transactional processing.</li> <li>Batch processing: <ul> <li>(A.k.a. offline processing) involves processing data in batches and usually imposes delays, which results in high-latency responses.</li> <li>Batch workloads typically involve large quantities of data with sequential read/writes and comprise of groups of read or write queries.</li> <li>Queries can be complex and involve multiple joins.</li> <li>OLAP (online analytical processing) systems commonly process workloads in batches.</li> <li>Gist: In batch processing, data is read and written in large, continuous chunks on disk (sequentially), which is faster than random access. Instead of many small operations, the system groups multiple read/write queries—such as aggregations, joins, or filters—into a single large job (like MapReduce), processing them together for high throughput but with higher latency.</li> </ul> </li> <li>Transactional processing: <ul> <li>(A.k.a online processing) follows an approach whereby data is processed interactively without delay, resulting in low-latency responses.</li> <li>Transaction workloads involve small amounts of data with random reads and writes and fewer joins.</li> <li>OLTP (online transaction processing) system fall within this category.</li> </ul> </li> </ul> </li> </ul> <h4 id="mapreduce"><strong>MapReduce</strong></h4> <ul> <li>MapReduce is a widely used implementation of a batch processing framework.</li> <li>It is highly scalable and reliable and is based on the principle of divide-and-conquer, which provides built-in fault tolerance and redundancy.</li> <li>MapReduce does not require that the input data conform to any particular data model. Therefore, it can be used to process schema-less datasets.</li> <li>A dataset is broken down into multiple smaller parts, and operations are performed on each part independently and in parallel.</li> <li>The results from all operations are then summarized to arrive at the answer.</li> <li>Traditionally, data processing requires moving data from the storage node to the processing node that runs the data processing algorithm. This approach works fine for smaller datasets. However, with large datasets, moving data can incur more overhead than the actual processing of the data.</li> <li>With MapReduce, the data processing algorithm is instead moved to the nodes that store the data. The data processing algorithm executes in parallel on these nodes, thereby eliminating the need to move the data first. It saves network bandwidth and reduces processing time for large datasets.</li> <li>Terminology: <ul> <li>A MapReduce job is a unit of work that the client wants to be performed.</li> <li>Hadoop runs the job by dividing it into tasks: map tasks and reduce tasks.</li> <li>Hadoop divides the input to a MapReduce job into fixed-size splits.</li> <li>Hadoop creates one map task for each split, which runs the user-defined map function for each record in the split.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-1-480.webp 480w,/assets/img/mapreduce-1-800.webp 800w,/assets/img/mapreduce-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Map: <ul> <li>The dataset is divided into multiple smaller splits. Each split contains multiple key-value pairs.</li> <li>The map function (“mapper”) executes user-defined logic on each split: (K1, V1) -&gt; list(K2, V2)</li> <li>A mapper may generate zero(filtering) or more than one(demultiplexing) key-value pairs.</li> </ul> </li> <li>Combine: <ul> <li>With larger datasets, moving data between map and reduce stages is more expensive than the actual processing.</li> <li>The optional combiner function summarizes a mapper’s output before it gets processed by the reducer: (K2, list(V2)) -&gt; list(K2, V2)</li> </ul> </li> <li>Partition: <ul> <li>If more than one reducer is involved, a partitioning function (“partitioner”) divides the output from the mapper or combiner (if used) into partitions between reducer instances.</li> <li>Although each partition contains multiple key-value pairs, all records for a particular key are assigned to the same partition.</li> </ul> </li> <li>Shuffle &amp; Sort: <ul> <li>Output from all partitioners is copied across the network to the nodes running the reduce task.</li> <li>The key-value pairs are grouped and sorted according to the keys, list(K2, V2) -&gt; (K2, list(V2))</li> </ul> </li> <li>Reduce: <ul> <li>The reduce function (“reducer”) further summarizes its input: (K2, list(V2)) -&gt; list(K3, V3).</li> <li>The output of the reducer is then written as a separate file. One file per reducer.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-2-480.webp 480w,/assets/img/mapreduce-2-800.webp 800w,/assets/img/mapreduce-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>MapReduce in action (Ex: NCDC weather dataset) <ul> <li>Note: In Hadoop’s TextInputFormat, each line of a file is given a key representing its byte offset from the start of the entire file, not just the split. Offsets are used instead of line numbers because files are split and processed in parallel across multiple mappers, and byte offsets allow each mapper to locate its data efficiently without reading previous lines, ensuring scalability and consistency.</li> <li>Note: Line numbers can’t be used in Hadoop because the input files are split and processed in parallel by multiple mappers. Each mapper starts reading from a different byte position in the file, so it has no way to know how many lines came before its split without scanning the entire file sequentially. Byte offsets, on the other hand, can be determined directly from the file’s position on disk, making them independent, efficient, and uniquely identifiable across splits — perfect for distributed processing.</li> <li>The map function extracts the year and the air temperature, and emits them as its output.</li> <li>The output from the map function is processed by the MapReduce framework before being sent to the reduce function. This processing sorts and groups the key-value pairs by key.</li> <li>For each input, the reduce function iterates through the list and picks up the maximum reading ￼</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-3-480.webp 480w,/assets/img/mapreduce-3-800.webp 800w,/assets/img/mapreduce-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-4-480.webp 480w,/assets/img/mapreduce-4-800.webp 800w,/assets/img/mapreduce-4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼ ￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-5-480.webp 480w,/assets/img/mapreduce-5-800.webp 800w,/assets/img/mapreduce-5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼</p> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-6-480.webp 480w,/assets/img/mapreduce-6-800.webp 800w,/assets/img/mapreduce-6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼ ￼</p> <ul> <li>Data flow <ul> <li>Where to run the map task?</li> <li>Data locality: run the map task on a node where the input data resides in HDFS, because it doesn’t use valuable cluster bandwidth.</li> <li>If not possible, the job scheduler will look for a free map slot on a node in the same rack as one of the blocks, and the required data block is transferred over the rack’s local network.</li> <li>If still not possible, an oﬀ-rack node is used, which results in an inter-rack network transfer.</li> <li>So, what’s transferred is the actual HDFS block data required by the Map task to perform its computation. ￼</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-7-480.webp 480w,/assets/img/mapreduce-7-800.webp 800w,/assets/img/mapreduce-7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼ </p> <ul> <li>How large is an input split? <ul> <li>Map tasks process input splits in parallel. So the processing is better load balanced when the splits are small.</li> <li>However, if splits are too small, the overhead of managing the splits and map task creation begins to dominate the total job execution time.</li> <li>For most jobs, a good split size tends to be the size of an HDFS block, which is 128 MB by default. It is the largest size of input that can be guaranteed to be stored on a single node.</li> </ul> </li> <li>Where to store the map output? <ul> <li>Map tasks write their output to the local disk, not to HDFS. Why? Map output is intermediate output. It’s processed by reduce tasks to produce the final output. Once the job is complete, the map output can be thrown away.</li> <li>Storing it in HDFS with replication would be overkill.</li> <li>What if the node running the map task fails before the map output has been consumed by the reduce task? If a node fails before the reduce phase reads its map output, Hadoop simply re-runs the failed map task on another node. Since the map output is intermediate and deterministic, it can be regenerated from the original input data stored safely in HDFS.</li> </ul> </li> <li>Why doesn’t Hadoop move the map task to another node that holds the same HDFS block instead of transferring the data? Why move data instead of computation? <ul> <li>A: Hadoop’s scheduler tries first to move computation (the map task) to where the data already resides — this is called data locality, and it’s the preferred option. However, if all nodes holding that block are busy (no free map slots), the scheduler can’t wait indefinitely because it would delay the job. In that case, it assigns the task to another available node and transfers the required data block over the network. So, Hadoop only moves data as a fallback when moving computation isn’t immediately possible, balancing performance and cluster utilization.</li> </ul> </li> <li>Do reduce tasks enjoy data locality?</li> <li>The input to each reduce task is normally the output from all mappers. ￼</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-8-480.webp 480w,/assets/img/mapreduce-8-800.webp 800w,/assets/img/mapreduce-8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-8" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Minimizing the data transferred between map and reduce tasks <ul> <li>The combiner function is an optimization that runs on the map output to reduce the amount of data transferred to the reducers by performing local aggregation. Hadoop does not provide a guarantee of how many times it will call it for a particular map output record. Calling the combiner function zero, one, or many times should produce the same output from the reducer.</li> <li>It works best for associative and commutative operations, where partial results can be safely combined.</li> <li>Which of the following data processing can benefit from a combiner function? <ul> <li>Count the number of occurrences. YES</li> <li>Find the maximum value. YES</li> <li>Find the average value. NO, since the combiner may process subsets differently, leading to incorrect results unless additional logic (like tracking sums and counts separately) is used.</li> <li>Filter values based on a predicate. YES</li> </ul> </li> </ul> </li> </ul> <h4 id="architecture"><strong>Architecture</strong></h4> <p><strong>YARN (Yet Another Resource Negotiator)</strong></p> <ul> <li>YARN is Hadoop’s cluster resource management system.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-9-480.webp 480w,/assets/img/mapreduce-9-800.webp 800w,/assets/img/mapreduce-9-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-9" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼ ￼</p> <ul> <li>YARN provides its core services via two types of long-running daemons.</li> <li>Resource manager (only one in the cluster): manage the use of resources across the cluster.</li> <li>Node managers (on each node): launch and monitor containers. A container executes an application-specific process with a constrained set of resources (memory, CPU, …). ￼</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-10-480.webp 480w,/assets/img/mapreduce-10-800.webp 800w,/assets/img/mapreduce-10-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-10" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼</p> <ul> <li>Running an application: A client contacts the resource manager and asks it to run an application master process. The resource manager finds a node manager that can launch the application master in a container. The application master may request more containers from the resource manager. The application master use them to run a distributed computation (e.g., MapReduce).</li> <li>Types of YARN scheduler: (FIFO, Capacity, FAIR) scheduler.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-11-480.webp 480w,/assets/img/mapreduce-11-800.webp 800w,/assets/img/mapreduce-11-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-11" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼￼</p> <p><strong>Hadoop</strong></p> <ul> <li>Running a MapReduce job:</li> <li>Client: submit the MapReduce job.</li> <li>YARN resource manager: coordinate the allocation of compute resources in cluster.</li> <li>YARN node managers: launch and monitor the containers on machines in the cluster.</li> <li>MapReduce application master: coordinate the tasks running the MR job. The application master and the MapReduce tasks run in containers scheduled by the resource manager and managed by the node managers.</li> <li>HDFS: share job files between the other entities.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-12-480.webp 480w,/assets/img/mapreduce-12-800.webp 800w,/assets/img/mapreduce-12-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-12" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼￼</p> <ul> <li>Progress &amp; status updates:</li> <li>A job and each of its tasks have a status.</li> <li>The state of the job or task (e.g., running, successfully completed, failed).</li> <li>The progress of maps and reduces.</li> <li>The values of the job’s counters.</li> <li>A status message or description.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-13-480.webp 480w,/assets/img/mapreduce-13-800.webp 800w,/assets/img/mapreduce-13-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-13" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼￼</p> <ul> <li>Q: Does each node hold HDFS blocks and containers for task execution, managed by the NodeManager?</li> <li>A: Yes. Each node stores blocks as part of HDFS and can also run containers, which execute tasks (Map or Reduce) scheduled by YARN. The NodeManager on each node handles launching, monitoring, and reporting on these containers, while the ResourceManager coordinates cluster-wide resource allocation.</li> <li>Q: What does the YARN scheduler do? Does it schedule Map and Reduce tasks, and how is memory/CPU utilization handled?</li> <li>A: The YARN scheduler (FIFO, Capacity, or FAIR) manages the allocation of resources—CPU, memory, and containers—across the cluster. It decides which nodes get containers for tasks. While it schedules containers for Map and Reduce tasks indirectly via the ApplicationMaster, the actual memory and CPU usage is constrained per container on individual nodes as specified by the scheduler.</li> <li>Q: Does the ResourceManager use HDFS block information to improve data locality and distributed efficiency?</li> <li>A: Yes. The ResourceManager, through the ApplicationMaster, tries to schedule tasks on nodes that already hold the input HDFS blocks to exploit data locality, reducing network transfer and improving performance. If local nodes aren’t available, it may schedule tasks on the same rack or another node as a fallback.</li> <li>Q: What are job counters in Hadoop, and what do they mean?</li> <li>A: Counters are metrics collected during job execution. They track things like the number of bytes read/written, number of records processed, map/reduce task attempts, and custom user-defined counters. Counters provide insight into job progress, efficiency, and can help debug or optimize jobs.</li> <li>Q: What is a status message, and how is it used?</li> <li>A: A status message is a short description of the current state of a job or task (e.g., “Reading input”, “Merging outputs”, “Task failed”). It helps users and administrators monitor progress, understand failures, and debug issues during job execution.</li> <li>Gist: In Hadoop with YARN, each node stores HDFS blocks and also runs containers for executing tasks, with the NodeManager handling container lifecycle and reporting. The ResourceManager coordinates cluster-wide resource allocation, while the YARN scheduler (FIFO, Capacity, or FAIR) decides which nodes get containers, controlling CPU and memory usage per container. To maximize efficiency, tasks are ideally scheduled on nodes holding the relevant HDFS blocks, leveraging data locality; if unavailable, tasks may run on the same rack or another node. During execution, job counters track metrics like bytes read/written, records processed, and task attempts, providing insight for monitoring and optimization. Status messages report the current state of jobs or tasks, helping users and administrators monitor progress and debug issues.</li> </ul> <p><strong>Resilience</strong></p> <ul> <li>Where can a failure happen? It can happen in 4 places: A MapReduce task, the MapReduce application master, YARN node manager or the YARN resource manager.</li> <li>Task failure: <ul> <li>Possible causes: Mapper/reducer bug, JVM bug, Hanging tasks: timeout (default: 10 min) exceeded without a progress update.</li> <li>The application master will reschedule the task on another node manager.</li> <li>If a task fails too many times (default: 4), it will not be retried again, and the whole job will fail.</li> </ul> </li> <li>Application master failure: <ul> <li>An application master sends periodic heartbeats to the resource manager.</li> <li>In the event of application master failure, the resource manager will detect the failure and start a new instance of the master running in a new container (managed by a node manager).</li> <li>In the case of the MapReduce application master, it’ll use the job history to recover the state of the tasks that were already run by the (failed) application, so they don’t have to be rerun.</li> <li>If a MapReduce application master fails too many times (default: 2), it will not be retried again, and the whole job will fail.</li> <li>Q: Where is the MapReduce job history stored and retrieved if the application master fails?</li> <li>A: The job history is persisted in HDFS, not in the application master’s memory. When a new application master is started after a failure, it reads the job history from HDFS to recover the state of already completed or partially completed tasks, so they don’t have to be rerun.</li> </ul> </li> <li>Node manager failure: <ul> <li>The resource manager will notice a node manager that has stopped sending heartbeats (default: 10 min) and remove it from its pool of nodes to schedule containers on.</li> <li>Q: If a NodeManager fails, do we need to rerun completed Map tasks?</li> <li>Yes, because the intermediate results are only stored in the node’s disk and not in hdfs. So, we’ve to rerun, so as to collect all the intermediate results from all mappers before the reducer part.</li> <li>Q: If a NodeManager fails, do we need to rerun completed Reduce tasks?</li> <li>A: No. Completed Reduce tasks write their output to HDFS, which is replicated and durable. Only in-progress Reduce tasks on the failed node need to be rescheduled on another node.</li> </ul> </li> <li>Resource manager failure: <ul> <li>Failure of the resource manager is serious, because without it, neither jobs nor task containers can be launched.</li> <li>In the default configuration, the resource manager is a single point of failure, since in the (unlikely) event of machine failure, all running jobs fails, and can’t be recovered.</li> <li>For high availability (HA), we need to configure a standby resource manager.</li> <li>Q: In ResourceManager failure, do we need to store information about all running applications?A: Yes. To recover after a ResourceManager failure, information about all running applications—including job metadata and application master states—needs to be persisted(not in HDFS). In HA setups, a standby ResourceManager keeps this information in ZooKeeper or shared storage(NFS) to resume operations without losing job information.</li> <li>Q: In ResourceManager failure, do we need to store NodeManager information?A: No. It can be reconstructed back again from the NodeManager’s heartbeats.</li> <li>Q: In ResourceManager failure, what about task information?A: Task information (status, progress, container allocation) is primarily tracked by the ApplicationMaster. The ResourceManager coordinates resources but doesn’t store detailed task outputs. In HA setups, the standby ResourceManager works with running ApplicationMasters to continue scheduling containers and managing tasks without losing track of progress.</li> </ul> </li> <li>Shuffle and sort (Most important part) <ul> <li>MapReduce guarantees that the input to every reducer is sorted by key.</li> <li>Flow: In MapReduce, each map task processes an input split, which contains multiple records. The map function runs on each record and generates intermediate key-value pairs, which are initially buffered in memory rather than written to disk immediately. This in-memory buffering allows the system to perform local partitioning, which determines which reducer each key belongs to, as well as optional combining, which reduces the number of intermediate records by aggregating data locally before transfer. Additionally, the intermediate data is sorted in memory by key to facilitate efficient merging later. Once the buffer reaches a configured threshold (default 100 MB), it is spilled to disk as a temporary file. Large map outputs may generate multiple spill files, which are later merged using a merge sort into a single sorted file per partition. During the shuffle phase, reducers fetch the intermediate data from all mappers, ensuring that each reducer receives all data corresponding to its assigned partition. Because the map output is already sorted locally, reducers perform a multi-way merge rather than a full sort, which improves efficiency. The final merged data is then streamed directly to the reduce function without writing it back to disk, minimizing I/O. If the intermediate data is too large to fit in memory, MapReduce applies external sorting, reading chunks of data from disk, merging them in memory, and writing back to disk. After merging, the reducer processes the records using the reduce function and writes the final output to HDFS. Throughout this process, combiners may optionally reduce the volume of data transferred, merges are optimized with multi-way strategies, and copying of map outputs can begin even before all map tasks finish, though the reduce function only runs after all intermediate data for that partition has been fetched.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mapreduce-14-480.webp 480w,/assets/img/mapreduce-14-800.webp 800w,/assets/img/mapreduce-14-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mapreduce-14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mapreduce-14" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>￼￼</p> <p><strong>Speculative execution</strong></p> <ul> <li>The job execution time is sensitive to slow-running tasks (“stragglers”).</li> <li>Hadoop doesn’t try to diagnose and fix slow-running tasks; instead, it tries to detect when a task is running slower than expected and launches another equivalent task as a backup.</li> <li>The scheduler tracks the progress of all tasks of the same type (map and reduce) in a job, and only launches speculative duplicates for the small portion that are running significantly slower than the average.</li> <li>When a task completes successfully, any duplicate tasks that are running are killed since they are no longer needed.</li> <li>Q: Which part of Hadoop handles speculative execution?</li> <li>A: Speculative execution is managed by the Hadoop MapReduce framework itself, specifically by the ApplicationMaster in YARN (Hadoop 2+).</li> <li>Q: Is the YARN scheduler part of the ApplicationMaster, and how do they interact?</li> <li>A: No, the YARN scheduler is part of the ResourceManager and operates cluster-wide, deciding which nodes get containers based on available resources and scheduling policies (FIFO, Capacity, FAIR). It does not manage task-level details like progress or stragglers. The ApplicationMaster, on the other hand, is job-specific. It requests containers from the ResourceManager (which allocates them via the scheduler), monitors task progress, handles failures, and manages speculative execution. Essentially, the ApplicationMaster depends on the scheduler for container allocation, and once allocated, it schedules and manages tasks within those containers, coordinating retries, merges, and data flow. The scheduler handles resource allocation, while the ApplicationMaster handles job and task management.</li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Monishver11/monishver11.github.io","data-repo-id":"R_kgDONe9Wkw","data-category":"General","data-category-id":"DIC_kwDONe9Wk84Cp-bh","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-llmr-lecture-1-wip",title:"LLMR - Lecture 1(WIP)",description:"LLM Reasoners Course at NYU Courant - Personal Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2026/llmr1/"}},{id:"post-apache-flink",title:"Apache Flink",description:"Realtime and Big Data Analytics Course at NYU Courant - Personal Notes 10",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-11-flink/"}},{id:"post-apache-kafka",title:"Apache Kafka",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 9",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-10-kafka/"}},{id:"post-apache-zookeeper",title:"Apache ZooKeeper",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 8",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-9-zookeeper/"}},{id:"post-apache-hbase",title:"Apache HBase",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 7",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-8-hbase/"}},{id:"post-hive-amp-trino",title:"Hive &amp; Trino",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 6",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-7-hive/"}},{id:"post-mapreduce-design-patterns",title:"MapReduce Design Patterns",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 5",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-5-mr-dp/"}},{id:"post-big-data-processing-concepts-amp-mapreduce",title:"Big Data Processing Concepts &amp; MapReduce",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-4-mapreduce/"}},{id:"post-hadoop-distributed-file-system-hdfs",title:"Hadoop Distributed File System (HDFS)",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-3-hdfs/"}},{id:"post-reading-notes-from-aleksa-gordic-39-s-gpu-blogpost",title:"Reading Notes from Aleksa Gordic&#39;s GPU BlogPost",description:"Reading notes for my reference from Aleksa Gordic&#39;s GPU BlogPost",section:"Posts",handler:()=>{window.location.href="/blog/2025/aleksagordic-gpu-blog-notes/"}},{id:"post-mlp-standard-derivatives-derivation",title:"MLP Standard Derivatives Derivation",description:"MLP Standard Derivatives Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-derivatives/"}},{id:"post-simple-mlp-forward-and-backward-pass-with-einsum-derivation",title:"Simple MLP - Forward and Backward Pass (With Einsum) Derivation",description:"Simple MLP - Forward and Backward Pass Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-fw-bwd/"}},{id:"post-gpu-notes",title:"GPU Notes",description:"GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes",section:"Posts",handler:()=>{window.location.href="/blog/2025/gpu-notes/"}},{id:"post-big-data-storage",title:"Big Data Storage",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 2",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-2-storage/"}},{id:"post-introduction-to-realtime-and-big-data-analytics",title:"Introduction to Realtime and Big Data Analytics",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-1-intro/"}},{id:"post-gpu-essentials-a-concise-technical-guide",title:"GPU Essentials - A Concise Technical Guide",description:"A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.",section:"Posts",handler:()=>{window.location.href="/blog/2025/GPU-Intro/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey-blog-2025-wrapping-ml-basics",title:"[Wrapping Up Our ML Foundations Journey](/blog/2025/wrapping-ml-basics/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-smallgraphgcn-accelerating-gnn-training-on-batched-small-graphs",title:"SmallGraphGCN - Accelerating GNN Training on Batched Small Graphs",description:"Discover how fused-edge-centric CUDA kernels dramatically accelerate Graph Neural Network training on molecular datasets. By rethinking parallelism strategies for batched small graphs, our approach achieves up to 3.1\xd7 faster forward execution and 1.3\xd7 end-to-end training speedup over PyTorch Geometric.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-from-baseline-to-deepseek-single-gpu-moe-training-efficiency",title:"From Baseline to DeepSeek - Single-GPU MoE Training Efficiency",description:"A systems-level analysis of training Mixture-of-Experts (MoE) Transformer models under single-GPU constraints. We compare naive PyTorch MoE, ScatterMoE, MegaBlocks, and DeepSeek-inspired architectures, revealing critical trade-offs between convergence behavior, memory footprint, and training throughput.",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>