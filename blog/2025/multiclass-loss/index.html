<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?e68e4955e21b20101db6e28a5a50abec"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/multiclass-loss/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm</h1> <p class="post-meta"> Created in April 12, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In real-world machine learning problems, we often need to classify data into multiple categories, not just two. While binary classification is a fundamental building block, it’s crucial to understand how we can extend these ideas to handle multiple classes. This transition from binary to multiclass classification is what we’ll explore in this blog. We’ll start by revisiting <strong>binary logistic regression</strong>, then step into <strong>multiclass logistic regression</strong>, and finally discuss how we can generalize algorithms like the perceptron for multiclass classification.</p> <hr> <h5 id="binary-logistic-regression-recap"><strong>Binary Logistic Regression Recap</strong></h5> <p>Let’s begin with the most basic form of classification: binary logistic regression.</p> <p>Given an input \(x\), our goal is to predict whether it belongs to class 1 or class 0. The function we use for binary classification is called the <strong>sigmoid function</strong>, which outputs a probability between 0 and 1:</p> \[f(x) = \sigma(z) = \frac{1}{1 + \exp(-z)} = \frac{1}{1 + \exp(-w^\top x - b)} \quad (1)\] <p>The output \(f(x)\) represents the probability of class 1. The probability of the other class (class 0) is simply:</p> \[1 - f(x) = \frac{\exp(-w^\top x - b)}{1 + \exp(-w^\top x - b)} = \frac{1}{1 + \exp(w^\top x + b)} = \sigma(-z) \quad (2)\] <p>Another way to think about this is that one class corresponds to the parameters \(w\) and \(b\), while the other class corresponds to the parameters \(-w\) and \(-b\). This helps set the foundation for extending this concept to multiple classes.</p> <hr> <h5 id="extending-to-multiclass-logistic-regression"><strong>Extending to Multiclass Logistic Regression</strong></h5> <p>Now that we have a solid understanding of binary logistic regression, let’s consider the case where we have more than two classes. This is where <strong>multiclass logistic regression</strong> comes in. For each class \(c\), we assign a weight vector \(w_c\) and a bias \(b_c\). The probability of belonging to class \(c\) given an input \(x\) is computed using the <strong>softmax function</strong>:</p> \[f_c(x) = \frac{\exp(w_c^\top x + b_c)}{\sum_{c'} \exp(w_{c'}^\top x + b_{c'})} \quad (3)\] <p>This formulation, known as <strong>softmax regression</strong>, allows us to calculate the probability for each class and select the class with the highest probability.</p> <h5 id="the-loss-function"><strong>The Loss Function</strong></h5> <p>To train the model, we use a <strong>cross-entropy loss</strong> function, which measures how well the model’s predicted probabilities match the true labels. Given a dataset \(\{(x^{(i)}, y^{(i)})\}\), the loss is defined as:</p> \[L = \sum_i -\log f_{y^{(i)}}(x^{(i)})\] <p>This loss function encourages the model to assign higher probabilities to the correct class. The gradient of the loss with respect to the pre-activation (logits) is:</p> \[\frac{\partial L}{\partial z} = f - y\] <p><strong>Derivation:</strong></p> <p>Assume the true class is \(k = y^{(i)}\). The loss for this example is:</p> \[\ell^{(i)} = -\log f_k^{(i)}\] <p>Substituting in the softmax definition:</p> \[\ell^{(i)} = -\log\left( \frac{\exp(z_k^{(i)})}{\sum_{j=1}^C \exp(z_j^{(i)})} \right) = -z_k^{(i)} + \log \left( \sum_{j=1}^C \exp(z_j^{(i)}) \right)\] <p>We differentiate the loss \(\ell^{(i)}\) with respect to each logit \(z_c^{(i)}\). There are two cases:</p> <ul> <li><strong>Case 1: \(c = k\) (the correct class)</strong></li> </ul> \[\frac{\partial \ell^{(i)}}{\partial z_k^{(i)}} = -1 + \frac{\exp(z_k^{(i)})}{\sum_{j=1}^C \exp(z_j^{(i)})} = f_k^{(i)} - 1\] <ul> <li><strong>Case 2: \(c \ne k\)</strong></li> </ul> \[\frac{\partial \ell^{(i)}}{\partial z_c^{(i)}} = \frac{\exp(z_c^{(i)})}{\sum_{j=1}^C \exp(z_j^{(i)})} = f_c^{(i)}\] <p>We can express both cases together using the one-hot encoded label vector \(y^{(i)}\):</p> \[\frac{\partial \ell^{(i)}}{\partial z^{(i)}} = f^{(i)} - y^{(i)}\] <p>Now, let \(f\) and \(y\) now represent the matrices of predicted probabilities and one-hot labels over the entire dataset. Then the total loss is:</p> \[L = \sum_{i=1}^N \ell^{(i)} = -\sum_{i=1}^N \log f_{y^{(i)}}^{(i)}\] <p>By stacking all gradients, the overall gradient of the loss with respect to the logits becomes:</p> \[\frac{\partial L}{\partial z} = f - y\] <p>This fully vectorized form allows efficient implementation and is similar to the gradient descent update used in binary logistic regression but generalized to multiple classes.</p> <hr> <h5 id="quick-comparison-to-one-vs-all-ova-approach"><strong>Quick Comparison to One-vs-All (OvA) Approach</strong></h5> <p>In many multiclass problems, instead of learning a separate model for each class, we can use the <strong>One-vs-All (OvA)</strong> strategy. In OvA, we train a binary classifier for each class, where the classifier tries to distinguish one class from all others. The base hypothesis space in this case is:</p> \[\mathcal{H} = \{ h: \mathcal{X} \to \mathbb{R} \} \quad \text{(score functions)}\] <p>For \(k\) classes, the <strong>multiclass hypothesis space</strong> is:</p> \[\mathcal{F} = \left\{ x \mapsto \arg\max_i h_i(x) \ \big| \ h_1, \ldots, h_k \in \mathcal{H} \right\}\] <p>Intuitively, each function \(h_i(x)\) scores how likely \(x\) belongs to class \(i\). During training, we want each classifier to output positive values for examples from its own class and negative values for examples from all other classes. At test time, the classifier that outputs the highest score determines the predicted class.</p> <hr> <h5 id="multiclass-perceptron-generalizing-the-perceptron-algorithm"><strong>Multiclass Perceptron: Generalizing the Perceptron Algorithm</strong></h5> <p>The classic Perceptron algorithm is designed for binary classification, but it can be naturally extended to multiclass problems. In the multiclass setting, instead of a single weight vector, we maintain <strong>one weight vector per class</strong>.</p> <p>For each class \(i\), we define a <strong>linear scoring function</strong>:</p> \[h_i(x) = w_i^\top x, \quad w_i \in \mathbb{R}^d\] <p>Given an input \(x\), the model predicts the class with the highest score:</p> \[\hat{y} = \arg\max_{i} w_i^\top x\] <p>The algorithm proceeds iteratively, updating the weights when it makes a mistake:</p> <ol> <li> <strong>Initialize</strong>: Set all weight vectors to zero, \(w_i = 0\) for all classes $i$.</li> <li>For \(T\) iterations over the training set: <ul> <li>For each training example \((x, y)\): <ul> <li>Predict the label: \(\hat{y} = \arg\max_{i} w_i^\top x\)</li> <li>If \(\hat{y} \neq y\) (i.e., the prediction is incorrect): <ul> <li> <strong>Promote</strong> the correct class:<br> \(w_y \leftarrow w_y + x\)</li> <li> <strong>Demote</strong> the incorrect prediction: \(w_{\hat{y}} \leftarrow w_{\hat{y}} - x\)</li> </ul> </li> </ul> </li> </ul> </li> </ol> <p>This update increases the score for the true class and decreases the score for the incorrect one, helping the model learn to separate them better in future iterations.</p> <h5 id="rewrite-the-scoring-function"><strong>Rewrite the scoring function</strong></h5> <p>When the number of classes \(k\) is large, storing and updating \(k\) separate weight vectors can become computationally expensive. To address this, we can rewrite the scoring function in a more compact form using a <strong>shared weight vector</strong>.</p> <p>We define a <strong>joint feature map</strong> \(\psi(x, i)\) that combines both the input \(x\) and a class label \(i\). Then, the score for class \(i\) can be written as:</p> \[h_i(x) = w_i^\top x = w^\top \psi(x, i) \tag{4}\] <p>Now, instead of maintaining a separate \(w_i\) for each class, we use <strong>a single global weight vector</strong> \(w\) that interacts with \(\psi(x, i)\) to compute scores for all classes:</p> \[h(x, i) = w^\top \psi(x, i) \tag{5}\] <p>This transformation allows us to use a single weight vector for all classes, which significantly reduces memory usage and computational complexity.</p> <p><strong>Concrete Example</strong></p> <p>Let:</p> <ul> <li>Input vector \(x \in \mathbb{R}^2\), e.g.,</li> </ul> \[x = \begin{bmatrix} 1 \\ 2 \end{bmatrix}\] <ul> <li>Number of classes \(k = 3\)</li> </ul> <p>We define \(\psi(x, i)\) as a vector in \(\mathbb{R}^{2k}\) (i.e., 6 dimensions). It places \(x\) into the block corresponding to class \(i\) and zeros elsewhere.</p> <p>For example, for class \(i = 2\):</p> \[\psi(x, 2) = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 2 \\ 0 \\ 0 \\ \end{bmatrix}\] <p>Let \(w \in \mathbb{R}^6\) (since \(x \in \mathbb{R}^2\) and \(k = 3\)):</p> \[w = \begin{bmatrix} 0.5 \\ -1.0 \\ 0.2 \\ 0.3 \\ -0.4 \\ 1.0 \\ \end{bmatrix}\] <p>To compute the score for class 2:</p> \[h(x, 2) = w^\top \psi(x, 2)\] <p>Only the block for class 2 is active:</p> \[h(x, 2) = [0.2, 0.3]^\top \cdot [1, 2] = 0.2 \cdot 1 + 0.3 \cdot 2 = 0.8\] <p>We can now compute scores for all classes:</p> <ul> <li> <p>Class 1 uses block: \([0.5, -1.0]\)</p> \[h(x, 1) = 0.5 \cdot 1 + (-1.0) \cdot 2 = -1.5\] </li> <li> <p>Class 2: (already computed) \(0.8\)</p> </li> <li> <p>Class 3 uses block: \([-0.4, 1.0]\)</p> \[h(x, 3) = -0.4 \cdot 1 + 1.0 \cdot 2 = 1.6\] </li> </ul> <p>For final prediction, we select the class with the highest score:</p> \[\hat{y} = \arg\max_i h(x, i) = 3\] <p>So, for input \(x = [1, 2]\), the predicted class is <strong>3</strong>.</p> <p>And suppose the true label is:</p> \[y = 2\] <p>Since \(\hat{y} \ne y\), the prediction is incorrect.</p> <p>The classic multiclass Perceptron updates:</p> <ul> <li> <strong>Promote</strong> the correct class (add input to the correct class block)</li> <li> <strong>Demote</strong> the predicted class (subtract input from the predicted class block)</li> </ul> <p>Using the joint feature map:</p> \[w \leftarrow w + \psi(x, y) - \psi(x, \hat{y})\] <p>In our case:</p> <ul> <li> \[\psi(x, y) = \psi(x, 2) = [0, 0, 1, 2, 0, 0]^\top\] </li> <li> \[\psi(x, \hat{y}) = \psi(x, 3) = [0, 0, 0, 0, 1, 2]^\top\] </li> </ul> <p>Then:</p> \[w_{\text{new}} = w + \psi(x, 2) - \psi(x, 3)\] <p>Apply this update:</p> \[w = \begin{bmatrix} 0.5 \\ -1.0 \\ 0.2 \\ 0.3 \\ -0.4 \\ 1.0 \\ \end{bmatrix} + \begin{bmatrix} 0 \\ 0 \\ 1 \\ 2 \\ 0 \\ 0 \\ \end{bmatrix} - \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 2 \\ \end{bmatrix} = \begin{bmatrix} 0.5 \\ -1.0 \\ 1.2 \\ 2.3 \\ -1.4 \\ -1.0 \\ \end{bmatrix}\] <p>This update increases the score for the correct class (2) and decreases the score for the incorrect prediction (3), just like the original multiclass Perceptron but using a single shared weight vector and structured feature representation.</p> <hr> <h5 id="formalising-via-multivector-construction"><strong>Formalising via Multivector Construction</strong></h5> <p>Consider a simple example where \(x \in \mathbb{R}^2\) and we have 3 classes \(Y = \{1, 2, 3\}\). Suppose we stack the weight vectors for each class together in the following way:</p> \[w = \begin{pmatrix} -\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}, 0, 1, \frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2} \end{pmatrix}^\top\] <p>Now, define the feature map \(\Psi(x, y)\) as follows:</p> <ul> <li> \[\Psi(x, 1) = (x_1, x_2, 0, 0, 0, 0)\] </li> <li> \[\Psi(x, 2) = (0, 0, x_1, x_2, 0, 0)\] </li> <li> \[\Psi(x, 3) = (0, 0, 0, 0, x_1, x_2)\] </li> </ul> <p>The dot product between the weight vector \(w\) and the feature map \(\Psi(x, y)\) is then:</p> \[\langle w, \Psi(x, y) \rangle = \langle w_y, x \rangle\] <p>This approach allows us to represent all classes using a single weight vector, which is more efficient and scalable.</p> <p>With the multivector construction in place, the multiclass perceptron algorithm can be rewritten as follows:</p> <ol> <li> <strong>Initialize</strong> the weight vector \(w = 0\).</li> <li>For \(T\) iterations, repeat the following for each training example \((x, y)\): <ul> <li>Predict \(\hat{y} = \arg\max_{y'} w^\top \psi(x, y')\) (choose the class with the highest score).</li> <li>If \(\hat{y} \neq y\): <ul> <li>Update the weight vector: \(w \leftarrow w + \psi(x, y)\).</li> <li>Update the weight vector: \(w \leftarrow w - \psi(x, \hat{y})\).</li> </ul> </li> </ul> </li> </ol> <p>This version of the algorithm is computationally efficient and scales well to large datasets.</p> <p><strong>Question</strong>: What is the <strong>base binary classification problem</strong> in multiclass perceptron?</p> <p><strong>Answer</strong>: At each update step, the multiclass Perceptron reduces to a binary classification problem between the correct class \(y\) and the predicted class \(\hat{y}\). The model must adjust the weights so that \(y\) scores higher than \(\hat{y}\) — just like in a binary classification setting where one class must be separated from another.</p> <hr> <h5 id="feature-engineering-for-multiclass-tasks"><strong>Feature Engineering for Multiclass Tasks</strong></h5> <p>To apply the multivector construction in practice, we need to define meaningful and informative features that capture the relationship between the input and each possible class. This is especially important in structured prediction tasks like <strong>part-of-speech (POS) tagging</strong>.</p> <p>Suppose our input space \(X\) consists of all possible words, and our label space \(Y\) contains the categories {NOUN, VERB, ADJECTIVE, ADVERB, etc.}. Each input word needs to be classified into one of these grammatical categories.</p> <p>We can define features that depend on both the input word and the target label — a natural fit for the joint feature map \(\Psi(x, y)\) introduced earlier. For example, some useful features might include:</p> <ul> <li>Whether the word is exactly a specific token (e.g., “run”, “apple”)</li> <li>Whether the word ends in certain suffixes (e.g., “ly” for adverbs)</li> <li>Capitalization or presence of digits (in named entity recognition)</li> </ul> <p>Here are a few sample features written in the multivector style:</p> <ul> <li> \[\psi_1(x, y) = 1[x = \text{apple} \land y = \text{NOUN}]\] </li> <li> \[\psi_2(x, y) = 1[x = \text{run} \land y = \text{NOUN}]\] </li> <li> \[\psi_3(x, y) = 1[x = \text{run} \land y = \text{VERB}]\] </li> <li> \[\psi_4(x, y) = 1[x \text{ ends in } \text{ly} \land y = \text{ADVERB}]\] </li> </ul> <p>Each of these features “activates” only when both the input word and the predicted class match a certain pattern. This is perfectly aligned with the multivector framework, where the model learns weights for specific combinations of features and labels.</p> <h5 id="feature-templates"><strong>Feature Templates</strong></h5> <p>In real-world applications, especially in natural language processing (NLP), we rarely hand-code features for every word. Instead, we use <strong>feature templates</strong> that automatically generate features from observed patterns.</p> <p><strong>What is a Feature Template?</strong></p> <p>A feature template is a <strong>rule or function</strong> that, given an input and a label, produces one or more binary features of the form \(\psi(x, y)\).</p> <p>Templates help create thousands or even millions of features in a structured and consistent way.</p> <p>Let’s say we want to predict the POS tag for the word <strong>“running”</strong> in the sentence:</p> <blockquote> <p>I am <strong>running</strong> late.</p> </blockquote> <p>We might use the following templates:</p> <hr> <table> <thead> <tr> <th>Template Description</th> <th>Template Rule</th> <th>Example Feature</th> </tr> </thead> <tbody> <tr> <td>Current word</td> <td>\(\psi(x, y) = 1[x = w \land y = y']\)</td> <td>\(x = \text{"running"}, y = \text{VERB}\)</td> </tr> <tr> <td>Word suffix (3 chars)</td> <td>\(\psi(x, y) = 1[x[-3:] = s \land y = y']\)</td> <td>\(x[-3:] = \text{"ing"}, y = \text{VERB}\)</td> </tr> <tr> <td>Previous word is “am”</td> <td>\(\psi(x, y) = 1[\text{prev}(x) = \text{"am"} \land y = y']\)</td> <td>\(y = \text{VERB}\)</td> </tr> <tr> <td>Is capitalized</td> <td>\(\psi(x, y) = 1[x[0].\text{isupper()} \land y = y']\)</td> <td>—</td> </tr> <tr> <td>Prefix (first 2 letters)</td> <td>\(\psi(x, y) = 1[x[:2] = p \land y = y']\)</td> <td>\(x[:2] = \text{"ru"}, y = \text{VERB}\)</td> </tr> </tbody> </table> <hr> <p>Each of these templates would produce many feature instances across a dataset — and each instance activates only when the corresponding condition holds.</p> <p><strong>Integration with the Model</strong></p> <p>In the multivector model, we don’t store a giant feature matrix explicitly. Instead, we treat each <strong>feature-label pair</strong> \(\psi(x, y)\) as a key that can be mapped to an <strong>index</strong> in a long feature vector. This is done using either:</p> <ul> <li>A <strong>dictionary lookup</strong>, if we predefine all feature-label pairs, or</li> <li>A <strong>hash function</strong>, if we want to compute the index on the fly (common in online or large-scale settings)</li> </ul> <p><strong>Why is this needed?</strong></p> <p>When \(\psi(x, y)\) is represented as a very large sparse vector (e.g. size 100,000+), we don’t want to store all zeros. So instead, we store only the <strong>nonzero features</strong> — each one identified by its <strong>feature name and associated label</strong>.</p> <p>Say we define a feature template:</p> <ul> <li>“Does the word end with ‘ing’?”</li> </ul> <p>Then for the input word <strong>“running”</strong>, and possible labels:</p> <ul> <li> \[\psi(x = \text{running}, y = \text{VERB}) = 1[\text{suffix} = ing \land y = \text{VERB}]\] </li> <li> \[\psi(x = \text{running}, y = \text{NOUN}) = 1[\text{suffix} = ing \land y = \text{NOUN}]\] </li> </ul> <p>These are <strong>two different features</strong>, because they are tied to different labels.</p> <p>We can assign an index to each:</p> <table> <thead> <tr> <th>Feature Name</th> <th>Label</th> <th>Combined Key</th> <th>Index</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">suffix=ing</code></td> <td>VERB</td> <td><code class="language-plaintext highlighter-rouge">suffix=ing_VERB</code></td> <td>1921</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">suffix=ing</code></td> <td>NOUN</td> <td><code class="language-plaintext highlighter-rouge">suffix=ing_NOUN</code></td> <td>2390</td> </tr> </tbody> </table> <p>So \(\psi(x, y)\) is implemented as:</p> <ul> <li>A vector of size (say) 50,000,</li> <li>With a single non-zero at position 1921 or 2390, depending on the label,</li> <li>And the model’s weight vector \(w\) has learned weights at those positions.</li> <li> <p>During prediction, we compute:</p> \[\hat{y} = \arg\max_{y'} w^\top \psi(x, y')\] </li> </ul> <p>This is how the model can <strong>distinguish between “ing” being a verb signal vs a noun signal</strong>, just by associating label-specific versions of the feature. And this feature-to-index mapping is what makes it possible to use linear classifiers with sparse high-dimensional features efficiently.</p> <p><strong>So, Why Feature Templates Matter?</strong></p> <ul> <li>They <strong>automate</strong> feature construction and ensure consistency across training and test data.</li> <li>They <strong>generalize well</strong> — e.g., instead of memorizing that “running” is a verb, a suffix-based feature can generalize that any word ending in “ing” is likely a verb.</li> <li>They are <strong>language-agnostic</strong> to some extent — and can be extended to other structured tasks like NER, chunking, or even machine translation.</li> </ul> <p>This feature-based view, combined with the multivector construction, gives us a powerful and scalable way to build multiclass classifiers, especially in domains like NLP where feature engineering plays a key role.</p> <hr> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>We covered how multiclass classification can be tackled using multiclass loss and perceptron algorithms. We highlighted the importance of feature engineering, specifically through feature templates, which help automatically create relevant features for each class. This approach enables efficient, scalable models, especially in tasks like POS tagging. By mapping feature-label pairs to indices, we can handle large datasets without excessive memory usage.</p> <p>Having seen how to generalize the perceptron algorithm, we’ll now move on to explore how <strong>Support Vector Machines (SVMs)</strong> can be extended to handle multiclass classification. Stay tuned and Take care!</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>