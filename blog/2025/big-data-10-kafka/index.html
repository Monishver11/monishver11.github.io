<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Apache Kafka | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 9"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/big-data-10-kafka/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Apache Kafka</h1> <p class="post-meta"> Created in December 17, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/rbda-nyu"> <i class="fa-solid fa-tag fa-sm"></i> RBDA-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>Data at rest versus data in motion</strong></p> <ul> <li>So far in this semester, we have been focusing on data at rest. <ul> <li>The data is stored in a storage system.</li> <li>Examples: HDFS, Hive, HBase, …</li> </ul> </li> <li>Today, we will focus on data in motion.</li> <li>The data is in route between source and destination in the network.</li> <li>We need to handle the continuous flow of data.</li> </ul> <p><strong>Why is it important?</strong></p> <ul> <li>We need to get the data from where it is created to where it can be analyzed. <ul> <li>The faster we can do this, the more agile and responsive our business can be.</li> <li>The less eﬀort we spend on moving data around, the more we can focus on the core business at hand.</li> </ul> </li> <li>This is why the pipeline is a critical component in the data-driven business.</li> <li>How we move the data becomes nearly as important as the data itself.</li> </ul> <p><strong>A critical component of data-driven applications</strong></p> <ul> <li>Publish/subscribe (pub/sub) messaging is a pattern that is characterized by the sender (publisher) of a piece of data (message) not specifically directing it to a receiver.</li> <li>Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages.</li> <li>Publish/subscribe systems often have a broker, a central point where messages are published, to facilitate this pattern.</li> </ul> <p><strong>How it starts</strong></p> <ul> <li>Many use cases for publish/subscribe start out the same way: with a simple message queue or interprocess communication channel.</li> <li>Example: Suppose you create an application that needs to send monitoring information somewhere… <ul> <li>You open a direct connection from your application to an application that displays your metrics on a dashboard.</li> <li>Then, you push metrics over that connection.</li> </ul> </li> <li>As you have more applications that are using those servers to get individual metrics and use them for various purposes, your architecture soon becomes…</li> <li>To clean up that mess, you set up a single application that receives metrics from all the applications out there, and provide a server to query those metrics for any system that needs them.</li> <li>Maybe you need to process more than metrics data…</li> <li>There is still a lot of duplication in the system!</li> <li>What you would like to have is a single centralized system that allows for publishing generic types of data, which will grow as your business grows. Kafka to the rescue!</li> </ul> <div class="row justify-content-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-1-480.webp 480w,/assets/img/kafka-1-800.webp 800w,/assets/img/kafka-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-2-480.webp 480w,/assets/img/kafka-2-800.webp 800w,/assets/img/kafka-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-3-480.webp 480w,/assets/img/kafka-3-800.webp 800w,/assets/img/kafka-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="kafka-basics"><strong>Kafka basics</strong></h4> <p><strong>Overview</strong></p> <ul> <li>Kafka was developed as a publish/subscribe messaging system designed to solve this problem.</li> <li>It is often described as a distributing streaming platform.</li> <li>Data within Kafka is stored durably, in order, and can be read deterministically.</li> <li>In addition, the data can be distributed within the system to provide additional protections against failures, as well as significant opportunities for scaling performance.</li> <li>Q: What does “distributed streaming platform” mean in Kafka?</li> <li>A: A distributed streaming platform is a system that can collect, store, process, and deliver continuous streams of data across multiple machines. In Kafka’s case, “streaming” means data is produced and consumed continuously (not as one-time batches), and “distributed” means the data and workload are spread across many brokers for scalability and fault tolerance. Kafka keeps streams durably on disk, ordered within partitions, and replicated so that failures of individual machines do not cause data loss, while allowing many producers and consumers to operate in parallel.</li> </ul> <p><strong>Messages</strong></p> <ul> <li>The unit of data within Kafka is called a message. It’s similar to a row or a record in a database.</li> <li>A message is simply an array of bytes. The data contained within it does not have a specific format or meaning to Kafka.</li> <li>A message can have an optional piece of metadata, referred to as a key. <ul> <li>The key is also a byte array and also has no specific meaning to Kafka.</li> <li>Keys are used when messages are to be written to partitions in a more controlled manner.</li> </ul> </li> <li>Q: What does it mean that a message is an “array of bytes”?</li> <li>A: When Kafka says a message (and its key) is an array of bytes, it means Kafka treats message data as raw binary data (byte[]) without interpreting its structure or semantics. Kafka does not care whether those bytes represent a string, JSON, Avro, Protobuf, an image, or anything else. It simply stores and transfers bytes efficiently. It is the responsibility of producers to serialize objects into bytes and consumers to deserialize those bytes back into meaningful data using an agreed-upon format.</li> </ul> <p><strong>Batches</strong></p> <ul> <li>For eﬃciency, messages are written into Kafka in batches.</li> <li>A batch is just a collection of messages, all of which are being produced to the same topic and partition.</li> <li>This is a trade-oﬀ between latency and throughput. The larger the batches, the more messages that can be handled per unit of time, but the longer it takes an individual message to propagate.</li> <li>Batches are typically compressed, providing more eﬃcient data transfer and storage at the cost of some processing power.</li> </ul> <p><strong>Schemas</strong></p> <ul> <li>Although Kafka itself treats messages as opaque byte arrays, in practice applications need a schema to describe the structure and meaning of the data inside those bytes. A schema defines fields, data types, and sometimes rules, making messages easier to interpret, validate, and evolve over time. Without schemas, producers and consumers must rely on implicit agreements, which easily break as systems change.</li> <li>Formats like JSON or XML are popular because they are simple and human readable, but they lack strong typing and make it hard to safely evolve data formats without breaking consumers. Apache Avro, on the other hand, provides compact binary serialization, strict data types, and built-in support for schema evolution. Avro keeps schemas separate from the message payload and does not require regenerating code when schemas change, which fits well with Kafka’s long-lived data streams.</li> <li>A consistent schema allows producers and consumers to evolve independently. If schemas are versioned and stored in a shared repository (such as a Schema Registry), producers can start writing data with a new schema version while consumers continue to read older versions or gradually adapt. This avoids tight coordination where all producers and consumers must be updated at the same time, enabling safer upgrades and more flexible, scalable Kafka-based systems.</li> <li>Conceptual Flow: In Kafka, the message itself is just a byte array (both the value and the optional key). Kafka does not understand or enforce any schema. The schema is used by the producer and consumer, not by Kafka, to serialize data into bytes when writing and deserialize bytes back into structured data when reading. Typically, the producer serializes an object according to a schema (for example, Avro) into a byte array and sends it to Kafka, and the consumer uses the same (or a compatible) schema to interpret those bytes. When a schema registry is used, the schema information is stored separately and referenced by the message, enabling versioning and compatibility without embedding the full schema in every message.</li> </ul> <p><strong>Topics and partitions</strong></p> <ul> <li>Messages in Kafka are categorized into topics. They’re similar to tables in a database or folders in a filesystem.</li> <li>Topics are additionally broken down into a number of partitions.</li> <li>Example: Suppose we use Kafka to store the commit log. <ul> <li>Then, a partition would be a single log.</li> <li>Messages are written to it in an append-only fashion and are read in order from beginning to end.</li> </ul> </li> <li>A topic typically has multiple partitions.</li> <li>Kafka guarantees message ordering within each partition, but there is no guarantee of message ordering across the entire topic.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-4-480.webp 480w,/assets/img/kafka-4-800.webp 800w,/assets/img/kafka-4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Partitions are the mechanism Kafka uses to achieve scalability and fault tolerance. Because each partition of a topic can be placed on a different broker (server), Kafka can spread the load of reads and writes across many machines, allowing a single topic to scale horizontally beyond the limits of one server. To handle failures, partitions can also be replicated, meaning the same partition is stored on multiple brokers; if one broker fails, another replica can take over without data loss.</li> <li>The term stream is a logical concept used when talking about how data flows through Kafka-based systems. A stream refers to all the data in a topic as a whole, even though that topic may be physically split across many partitions. From the perspective of producers and consumers, this represents a continuous flow of records over time.</li> <li>This terminology becomes especially important in stream processing, where frameworks (such as Kafka Streams, Flink, or Spark Streaming) process data as it arrives, rather than in batches. These frameworks treat a topic as a single stream of events, abstracting away the underlying partitions while still leveraging them internally for parallelism and scalability.</li> <li>More clearly: a Kafka topic is logically one stream of messages, but physically it is split into partitions. When replication is not used, each partition holds different messages, and together all partitions make up the complete data for the topic. No two partitions contain the same records. Messages are assigned to partitions (based on key or round-robin), and once written, they exist only in that one partition. Replication, when enabled, simply creates copies of a partition’s data on other brokers for fault tolerance; it does not change the fact that partitions themselves divide the topic’s data uniquely.</li> </ul> <p><strong>Producers and consumers</strong></p> <ul> <li>In Kafka, clients are applications that interact with the Kafka cluster, and they come in two fundamental types: producers and consumers. A producer publishes (writes) messages to Kafka topics, while a consumer subscribes to topics and reads messages from them.</li> <li>On top of these basic clients, Kafka provides advanced client APIs. Kafka Connect is used for data integration, allowing Kafka to reliably move data between Kafka and external systems like databases, filesystems, or cloud services with minimal custom code. Kafka Streams is a stream-processing library that lets applications read from Kafka topics, process the data in real time (such as filtering, aggregating, or joining streams), and write the results back to Kafka.</li> <li>Internally, both Kafka Connect and Kafka Streams are built using producers and consumers, but they expose higher-level abstractions so developers can focus on integration or processing logic rather than low-level messaging details.</li> </ul> <p><strong>Producers</strong></p> <ul> <li>Producers (a.k.a., publishers or writers) create new messages.</li> <li>A message will be produced to a specific topic.</li> <li>By default, the producer will balance messages over all partitions of a topic.</li> <li>In some cases, the producer will direct messages to specific partitions. <ul> <li>This is typically done using the message key and a partitioner that will generate a hash of the key and map it to a specific partition. This ensures that all messages produced with the same key will get written to the same partition.</li> <li>The producer could also use a custom partitioner that follows other business rules for mapping messages to partitions.</li> </ul> </li> </ul> <p><strong>Consumers</strong></p> <ul> <li>Consumers (a.k.a., subscribers or readers) read messages.</li> <li>The consumer subscribes to one or more topics and reads the messages in the order in which they were produced to each partition.</li> <li>The consumer keeps track of which messages it has already consumed by keeping track of the oﬀset of messages.</li> </ul> <p><strong>Oﬀsets</strong></p> <ul> <li>Kafka tracks a consumer’s progress using offsets, which are monotonically increasing integer values that Kafka assigns to messages as they are appended to a partition.</li> <li>Each message in a partition has a unique offset, and later messages always have larger offsets (though some numbers may be skipped).</li> <li>A consumer records the next offset to read for each partition it consumes from; this offset is stored persistently (earlier in ZooKeeper, now typically in Kafka itself). Because offsets are stored outside the consumer process, a consumer can stop, crash, or restart and then resume reading from exactly where it left off, without losing or reordering data.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-5-480.webp 480w,/assets/img/kafka-5-800.webp 800w,/assets/img/kafka-5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Consumer group</strong></p> <ul> <li>Consumers work as part of a consumer group, which is one or more consumers that work together to consume a topic.</li> <li>The group ensures that each partition is only consumed by one member.</li> <li>The mapping of a consumer to a partition is often called ownership of the partition by the consumer.</li> <li>In this way, consumers can horizontally scale to consume topics with a large number of messages.</li> <li>If a single consumer fails, the remaining members of the group will reassign the partitions being consumed to take over for the missing member.</li> <li>Note: <ul> <li>A consumer can consume from multiple partitions if there are fewer consumers than partitions.</li> <li>If there are more consumers than partitions, some consumers will be idle.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-6-480.webp 480w,/assets/img/kafka-6-800.webp 800w,/assets/img/kafka-6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Brokers</strong></p> <ul> <li>A single Kafka server is called a broker.</li> <li>The broker receives messages from producers, assigns oﬀsets to them, and writes the messages to storage on disk.</li> <li>It also services consumers, responding to fetch requests for partitions and responding with the messages that have been published.</li> <li>A single broker can usually handle thousands of partitions and millions of messages per second.</li> </ul> <p><strong>Clusters</strong></p> <ul> <li>Kafka brokers are designed to operate as part of a cluster.</li> <li>Within a cluster of brokers, one broker will also function as the cluster controller (automatically elected from the live members of the cluster).</li> <li>The controller is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures.</li> </ul> <p><strong>Leader and followers</strong></p> <ul> <li>A partition is owned by a single broker in the cluster. That broker is called the leader of the partition.</li> <li>A replicated partition is assigned to additional brokers. They are called followers of the partition.</li> <li>Replication provides redundancy of messages in the partition. If there is a broker failure, one of the followers can take over leadership.</li> <li>All producers must connect to the leader in order to publish messages, but consumers may fetch from either the leader or one of the followers.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-7-480.webp 480w,/assets/img/kafka-7-800.webp 800w,/assets/img/kafka-7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Retention</strong></p> <ul> <li>Retention is the durable storage of messages for some period of time.</li> <li>Kafka brokers are configured with a default retention setting for topics. <ul> <li>Time-based retention: retaining messages for some period of time (e.g., 7 days)</li> <li>Space-based retention: retaining until the partition reaches a certain size in bytes (e.g., 1 GB).</li> </ul> </li> <li>Once these limits are reached, messages are expired and deleted.</li> <li>Individual topics can also be configured with their own retention settings so that messages are stored for only as long as they are useful.</li> <li>Example <ul> <li>A tracking topic might be retained for several days.</li> <li>Application metrics might be retained for only a few hours.</li> </ul> </li> <li>Topics can also be configured as log compacted. <ul> <li>Kafka will retain only the last message produced with a specific key.</li> <li>Useful for changelog data, where only the last update is interesting.</li> </ul> </li> </ul> <p><strong>Multiple clusters</strong></p> <ul> <li>As Kafka deployments grow, it also supports multiple clusters.</li> <li>Why is it useful? <ul> <li>Segregation of types of data.</li> <li>Isolation for security requirements.</li> <li>Multiple data centers (disaster recovery).</li> </ul> </li> <li>When working with multiple data centers in particular, it is often required that messages be copied between them. However, the replication mechanisms within the Kafka clusters are designed only to work within a single cluster, not between multiple clusters.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-8-480.webp 480w,/assets/img/kafka-8-800.webp 800w,/assets/img/kafka-8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-8" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>MirrorMaker is a tool for replicating data to other clusters.</li> <li>At its core, MirrorMaker is simply a Kafka consumer and producer, linked together with a queue.</li> <li>Messages are consumed from one Kafka cluster and produced to another.</li> </ul> <p><strong>What makes Kafka a good choice?</strong></p> <ul> <li>Multiple producers: Ideal for aggregating data from many frontend systems and making it consistent.</li> <li>Multiple consumers: Multiple consumers can read any single stream of messages without interfering with other clients.</li> <li>Disk-based retention: If a consumer falls behind or temporarily becomes oﬄine, there is no danger of losing data.</li> <li>Scalable: Expansions can be performed while the cluster is online, without impacting whole system availability.</li> <li>High performance: Kafka provides sub-second message latency from producing a message to availability to consumers.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-9-480.webp 480w,/assets/img/kafka-9-800.webp 800w,/assets/img/kafka-9-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-9" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Note:</strong></p> <ul> <li>If a topic has 2 partitions and 4 consumers in the same consumer group, Kafka assigns at most one consumer per partition, so only 2 consumers actively read data (one per partition) while the remaining 2 stay idle; adding more consumers does not increase throughput because parallelism is bounded by the number of partitions. If one of the active consumers fails, Kafka automatically triggers a rebalance and assigns the affected partition to one of the idle consumers, which then continues consuming from the last committed offset. If the 4 consumers belong to different consumer groups, each group independently consumes all messages from both partitions, meaning all 4 consumers receive the full data stream.</li> <li>Follow-up: What happens when partitions are replicated?</li> <li>When partitions are replicated, each partition has one leader replica and one or more follower replicas on different brokers. Consumers always read from the leader replica. If the broker hosting the leader fails, Kafka automatically elects one of the in-sync follower replicas as the new leader, and consumers continue reading transparently without data loss (assuming the replica was in sync). Replication therefore provides fault tolerance and high availability, but it does not increase consumer parallelism—only the number of partitions does.</li> </ul> <p><strong>Use cases</strong></p> <ul> <li>Activity tracking <ul> <li>The original use case for Kafka at LinkedIn is user activity tracking.</li> <li>A website’s users interact with frontend applications, which generate messages regarding actions the user is taking. <ul> <li>Passive information: page views, click tracking, …</li> <li>More complex information: a user adds to their profile, …</li> </ul> </li> <li>The messages are published to one or more topics, which are then consumed by applications on the backend. These applications generate reports, feed machine learning systems, update search results, or perform other operations to provide a rich user experience.</li> </ul> </li> <li>Messaging <ul> <li>Kafka is also used for messaging, where applications need to send notifications (e.g., emails) to users.</li> <li>Those applications can produce messages without needing to be concerned about formatting or how the messages will actually be sent.</li> <li>A single application can then read all the messages to be sent and handle them consistently, including: <ul> <li>Formatting the messages (a.k.a., decorating) using a common look and feel.</li> <li>Collecting multiple messages into a single notification to be sent.</li> <li>Applying a user’s preferences for how they want to receive messages.</li> </ul> </li> </ul> </li> <li>Metrics and logging <ul> <li>Kafka is also ideal for collecting application and system metrics and logs.</li> <li>Applications publish metrics on a regular basis to a Kafka topic, and those metrics can be consumed by systems for monitoring and alerting.</li> <li>They can also be used in an oﬄine system (e.g., Hadoop) to perform longer-term analysis (e.g., growth projections), or routed to dedicated log search systems (e.g., Elasticsearch, security analysis applications).</li> <li>When the destination system needs to change (e.g., it’s time to update the log storage system), there is no need to alter the frontend applications or the means of aggregation.</li> </ul> </li> <li>Commit log <ul> <li>Database changes can be published to Kafka, and applications can easily monitor this stream to receive live updates as they happen.</li> <li>This changelog stream can also be used for replicating database updates to a remote system, or for consolidating changes from multiple applications into a single database view.</li> <li>Durable retention is useful here for providing a buﬀer for the changelog, so it can be replayed in the event of a failure of the consuming applications.</li> <li>Alternately, log-compacted topics can be used to provide longer retention by only retaining a single change per key.</li> </ul> </li> <li>Stream processing <ul> <li>Stream processing refers to applications that provide similar functionality to map/reduce processing in Hadoop.</li> <li>Hadoop usually relies on aggregation of data over a long time frame, either hours or days. On the other hand, stream processing operates on data in real time, as quickly as messages are produced.</li> <li>Stream frameworks allow users to write small applications to operate on Kafka messages, performing tasks such as counting metrics, partitioning messages for eﬃcient processing by other applications, or transforming messages using data from multiple sources.</li> </ul> </li> </ul> <h4 id="kafka-internals"><strong>Kafka internals</strong></h4> <p><strong>Cluster membership</strong></p> <ul> <li>Kafka uses ZooKeeper to maintain the list of brokers that are currently members of a cluster. Recall the group membership example from ZooKeeper</li> <li>Every broker has a unique identifier. Every time a broker process starts, it registers itself with its ID in ZooKeeper by creating an ephemeral znode under the path “/brokers/ids”.</li> <li>Kafka brokers, the controller, and some of the ecosystem tools place a watch on that path so that they get notified when brokers are added or removed.</li> <li>When a broker loses connectivity to ZooKeeper, the ephemeral znode that the broker created when starting will be automatically removed from ZooKeeper.</li> <li>Kafka components that are watching the list of brokers will be notified that the broker is gone.</li> <li>Although the znode representing the broker is gone when the broker is stopped, the broker ID still exists in Kafka’s internal data structures.</li> <li>This way, if you completely lose a broker and start a brand-new broker with the ID of the old one, it will immediately join the cluster in place of the missing broker with the same partitions and topics assigned to it.</li> </ul> <p><strong>Controller</strong></p> <ul> <li>The controller is one of the Kafka brokers that, in addition to the usual broker functionality, is responsible for electing partition leaders. <ul> <li>The first broker that starts in the cluster becomes the controller by creating an ephemeral znode in ZooKeeper called “/controller”.</li> <li>When other brokers start, they also try to create this node but receive a “node already exists” exception, which causes them to “realize” that the controller node already exists and that the cluster already has a controller.</li> <li>The brokers create a ZooKeeper watch on the controller znode so they get notified of changes to this znode.</li> </ul> </li> <li>This way, we guarantee that the cluster will only have one controller at a time.</li> <li>When the controller broker is stopped or loses connectivity to ZooKeeper, the ephemeral znode will disappear.</li> <li>Other brokers in the cluster will be notified through the ZooKeeper watch that the controller is gone and will attempt to create the controller znode in ZooKeeper themselves.</li> <li>The first node to create the new controller in ZooKeeper becomes the next controller, while the other nodes will receive a “node already exists” exception and re-create the watch on the new controller znode.</li> <li>When the controller first comes up, it has to read the latest replica state map from ZooKeeper before it can start managing the cluster metadata and performing leader elections.</li> <li>When the controller notices that a broker left the cluster, it knows that all the partitions that had a leader on that broker will need a new leader.</li> <li>It goes over all the partitions that need a new leader and determines who the new leader should be (simply the next replica in the replica list of that partition).</li> <li>Then it persists the new state to ZooKeeper and sends information about the leadership change to all the brokers that contain replicas for those partitions.</li> <li>Example flow: <ul> <li>Suppose a Kafka cluster has three brokers: B1, B2, and B3. When the cluster starts, B1 starts first and creates the ephemeral znode /controller in ZooKeeper, becoming the controller. When B2 and B3 start, they try to create the same znode but get a “node already exists” exception, so they know B1 is already the controller and set a watch on /controller. Now, imagine B1 crashes or loses connectivity. Its ephemeral znode disappears, and B2 and B3 are notified via their watches. Both try to create /controller; whichever succeeds first becomes the new controller (say B2), while the other (B3) gets an exception and resets its watch on /controller.</li> <li>After B2 becomes the new controller, it first reads the latest replica state from ZooKeeper to understand the current mapping of partitions and their leaders. Suppose the cluster has a topic orders with 3 partitions: <ul> <li>Partition 0 → replicas [B1, B2, B3], leader was B1</li> <li>Partition 1 → replicas [B1, B3, B2], leader was B1</li> <li>Partition 2 → replicas [B2, B1, B3], leader was B2</li> </ul> </li> <li>Since B1 has failed, partitions 0 and 1 need new leaders. B2 examines the replica lists: <ul> <li>For Partition 0, the next replica in the list after B1 is B2, so B2 becomes the new leader.</li> <li>For Partition 1, the next replica after B1 is B3, so B3 becomes the new leader.</li> <li>Partition 2 is unaffected because its leader, B2, is still active.</li> </ul> </li> <li>The controller then updates ZooKeeper with the new leadership information for partitions 0 and 1. It also notifies all brokers that host replicas of these partitions (B2 and B3 for Partition 0, B2 and B3 for Partition 1) about the changes.</li> <li>This ensures that producers and consumers can continue sending and receiving messages without disruption. By following this process, Kafka guarantees automatic failover, consistent leadership, and uninterrupted availability of all partitions in the cluster.</li> </ul> </li> </ul> <p><strong>A Quick Review:</strong></p> <ul> <li>Broker as leader: <ul> <li>Each partition in a topic has exactly one leader at a time.</li> <li>A broker can be the leader for multiple partitions, even across multiple topics. So if a broker handles multiple topics, it can simultaneously be the leader for several partitions of different topics.</li> <li>Followers replicate the leader’s data for fault tolerance, but only the leader handles reads/writes for that partition.</li> </ul> </li> <li>Consumers and partitions: <ul> <li>A consumer in a consumer group can read from one or more partitions, but each partition can be read by only one consumer in the same group at a time.</li> <li>For a single topic with multiple partitions, multiple consumers in the same group allow parallel consumption, but each partition is still assigned to only one consumer.</li> <li>If a consumer subscribes to multiple topics, it can consume from one or more partitions from each topic, depending on how partitions are assigned to it.</li> </ul> </li> <li>Example: <ul> <li>Topic A has 3 partitions: P0, P1, P2. Topic B has 2 partitions: Q0, Q1.</li> <li>Broker B1 might be leader for P0 and Q1; B2 leader for P1 and Q0; B3 leader for P2.</li> <li>Consumer C1 in a consumer group could be assigned P0 (topic A) and Q0 (topic B), while C2 gets P1 and Q1.</li> </ul> </li> <li>So, brokers can be leaders for multiple partitions across topics, and consumers can read from multiple partitions across topics, but within a consumer group, each partition is served by only one consumer at a time.</li> </ul> <p><strong>Replication</strong></p> <ul> <li>Replication is critical because it is the way Kafka guarantees availability and durability when individual nodes inevitably fail.</li> <li>As we’ve already discussed, data in Kafka is organized by topics. <ul> <li>Each topic is partitioned.</li> <li>Each partition can have multiple replicas.</li> </ul> </li> <li>Those replicas are stored on brokers, and each broker typically stores hundreds or even thousands of replicas belonging to diﬀerent topics and partitions.</li> <li>There are two types of replicas.</li> <li>Leader replica <ul> <li>Each partition has a single replica designated as the leader.</li> <li>All produce requests go through the leader to guarantee consistency.</li> <li>Clients can consume from either the leader replica or its followers.</li> </ul> </li> <li>Follower replica <ul> <li>All replicas for a partition that are not leaders are called followers.</li> <li>If a leader replica for a partition fails, one of the follower replicas will be promoted to become the new leader for the partition.</li> </ul> </li> <li>Kafka provides the following reliability guarantees: <ul> <li>Kafka provides order guarantee of messages in a partition.</li> <li>Produced messages are considered “committed” when they were written to the partition on all its in-sync replicas (but not necessarily flushed to disk). In-sync replicas are the leader and followers that lags within 10 seconds (configurable).</li> <li>Messages that are committed will not be lost as long as at least one replica remains alive.</li> <li>Consumers can only read messages that are committed.</li> </ul> </li> <li>Q: Can Kafka consumers read from any replica of a partition, or only from the leader?</li> <li>A: In Kafka, each partition has a single leader replica and zero or more follower replicas. By default, consumers read from the leader replica, which guarantees that they see all committed messages in order and avoids inconsistencies. Followers replicate the leader’s data and can serve as read sources only if the system is explicitly configured to allow reading from followers, for example, to balance read load. However, followers that are out-of-sync may not have all committed messages or may lag behind the leader, so reading from them can result in missing or stale data. Therefore, reading from the leader is the default and safest approach, ensuring correctness, ordering, and durability, while followers primarily provide redundancy and high availability in case of leader failure.</li> </ul> <p><strong>Request processing</strong></p> <ul> <li>A Kafka broker processes requests sent to the partition leaders from clients, partition replicas, and the controller.</li> <li>Clients always initiate connections and send requests, and the broker processes the requests and responds to them.</li> <li>All requests sent to the broker from a specific client will be processed in the order in which they were received This guarantee allows Kafka to behave as a message queue and provide ordering guarantees on the messages it stores.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-10-480.webp 480w,/assets/img/kafka-10-800.webp 800w,/assets/img/kafka-10-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-10" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>How do clients know where to send the requests?</li> <li>Clients know where to send requests because each partition has a designated leader, and brokers maintain metadata about which broker is the leader for which partition. When a client wants to produce or consume messages, it first fetches the cluster metadata (from ZooKeeper in older versions or from the Kafka bootstrap servers). This metadata includes the list of brokers, topics, partitions, and the leader for each partition. Using this information, the client can send requests directly to the broker that is the leader for the partition it wants to read from or write to.</li> <li>This ensures that requests always reach the correct partition leader, maintaining message ordering and consistency guarantees, and reduces unnecessary network hops through other brokers.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-11-480.webp 480w,/assets/img/kafka-11-800.webp 800w,/assets/img/kafka-11-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-11.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-11" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Clients can set an upper/lower bound on the amount of data the broker can return, as well as a timeout.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-12-480.webp 480w,/assets/img/kafka-12-800.webp 800w,/assets/img/kafka-12-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-12.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-12" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Consumers only see messages that were replicated to in-sync replicas.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-13-480.webp 480w,/assets/img/kafka-13-800.webp 800w,/assets/img/kafka-13-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-13.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-13" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Physical storage</strong></p> <ul> <li>The basic storage unit of Kafka is a partition replica.</li> <li>Partitions cannot be split between multiple brokers, and not even between multiple disks on the same broker.</li> <li>Therefore, the size of a partition is limited by the space available on a single mount point.</li> <li>Kafka 3.9 (November 2024) supports “tiered storage”. (see KIP-405) <ul> <li>The local tier: store the log segments on the local disks. (same as current)</li> <li>The remote tier: store the completed log segments on HDFS, S3, … (new)</li> </ul> </li> <li>When you create a topic, Kafka first decides how to allocate the partitions between brokers. <ul> <li>The default replication factor is 3.</li> <li>Users can add or remove replicas even after a topic exists.</li> </ul> </li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-14-480.webp 480w,/assets/img/kafka-14-800.webp 800w,/assets/img/kafka-14-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-14.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-14" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Kafka does not keep data forever. It doesn’t even wait for all consumers to read a message before deleting it.</li> <li>Instead, the Kafka administrator configures a retention period for each topic. <ul> <li>Time-based: how long to store messages before deleting them.</li> <li>Space-based: how much data to store before older messages are purged.</li> </ul> </li> <li>Each partition is split into segments. By default, each segment contains either 1 GB of data or a week of data, whichever is smaller.</li> <li>As a Kafka broker is writing to a partition, if the segment limit is reached, it closes the file and starts a new one.</li> <li>Each segment is stored in a single data file, which stores Kafka messages and their oﬀsets.</li> <li>The format of the data on the disk is identical to the format of the messages that we send from the producer to the broker and later from the broker to the consumers (i.e., the wire protocol). <ul> <li>This allows Kafka to use a zero-copy method to send messages to clients. Kafka sends messages from the file (or more likely, the Linux filesystem cache) directly to the network channel without any intermediate buﬀers.</li> <li>It also avoids decompressing and recompressing messages that the producer already compressed.</li> </ul> </li> <li> <p>Note: Zero-copy is an optimization in which Kafka sends data from disk to the network without multiple memory copies between user space and kernel space. Normally, sending data involves copying from disk to kernel buffers, then to user-space buffers, and back to kernel network buffers before transmission. With zero-copy (using OS features like Linux sendfile), Kafka can send messages directly from the on-disk segment files or the Linux page cache to the network socket. This reduces CPU and memory usage, speeds up message delivery, and works efficiently because Kafka’s on-disk segment format matches the wire protocol, so no data transformation is needed. The wire protocol is the format and rules that define how data is encoded and transmitted over the network between clients and servers.</p> </li> <li>Kafka provides flexible message retrieval and retention mechanisms. Consumers can start reading messages from any offset, and to make this efficient, each partition maintains an offset-to-segment index, which maps offsets to their exact position within segment files.</li> <li>Additionally, Kafka maintains a timestamp-to-offset index to quickly locate messages by time, which is useful for Kafka Streams and certain failover scenarios.</li> <li>For retention, topics can be configured in three ways: delete, which removes messages older than a configured time; compact, which keeps only the latest value for each key; or delete and compact, combining compaction with time-based deletion.</li> <li>Note: Both compaction and deletion operate on the messages stored on the brokers for a given topic.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-15-480.webp 480w,/assets/img/kafka-15-800.webp 800w,/assets/img/kafka-15-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-15.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-15" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h4 id="kafka-producer-flow"><strong>Kafka Producer Flow</strong></h4> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-16-480.webp 480w,/assets/img/kafka-16-800.webp 800w,/assets/img/kafka-16-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-16" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>When a producer wants to send messages to a Kafka topic, it first connects to one of the bootstrap brokers configured in its setup. It then requests metadata from the broker, which includes the list of partitions for the topic, the leader broker for each partition, and the replicas for each partition. The producer caches this metadata so that it can send records directly to the correct leader broker for the target partition.</li> <li>If the leader changes due to a failover or if the cached metadata expires, the producer automatically refreshes the metadata by querying the brokers again. This ensures that the producer always knows which broker to contact for each partition and guarantees that messages are sent to the appropriate leader.</li> </ul> <h4 id="kafka-consumer---commits-and-oﬀsets"><strong>Kafka Consumer - Commits and oﬀsets</strong></h4> <div class="row justify-content-center"> <div class="col-sm-7 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kafka-17-480.webp 480w,/assets/img/kafka-17-800.webp 800w,/assets/img/kafka-17-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/kafka-17.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="kafka-17" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>In Kafka, consumers track which messages they have read by maintaining offsets, which represent their current position in each partition. Whenever a consumer calls poll(), it retrieves all messages from the last committed offset up to the latest.</li> <li>Unlike traditional message queues, Kafka does not track individual acknowledgments for each message. Instead, a consumer commits offsets, which updates a special internal topic called __consumer_offsets. This commit records the last successfully processed message in a partition, implicitly marking all prior messages as processed. By default, Kafka consumers commit offsets automatically every few seconds, but this can be disabled to allow explicit commits, which helps prevent message loss or duplication during rebalancing events.</li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Monishver11/monishver11.github.io","data-repo-id":"R_kgDONe9Wkw","data-category":"General","data-category-id":"DIC_kwDONe9Wk84Cp-bh","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-apache-flink",title:"Apache Flink",description:"Realtime and Big Data Analytics Course at NYU Courant - Personal Notes 10",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-11-flink/"}},{id:"post-apache-kafka",title:"Apache Kafka",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 9",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-10-kafka/"}},{id:"post-apache-zookeeper",title:"Apache ZooKeeper",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 8",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-9-zookeeper/"}},{id:"post-apache-hbase",title:"Apache HBase",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 7",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-8-hbase/"}},{id:"post-hive-amp-trino",title:"Hive &amp; Trino",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 6",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-7-hive/"}},{id:"post-mapreduce-design-patterns",title:"MapReduce Design Patterns",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 5",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-5-mr-dp/"}},{id:"post-big-data-processing-concepts-amp-mapreduce",title:"Big Data Processing Concepts &amp; MapReduce",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-4-mapreduce/"}},{id:"post-hadoop-distributed-file-system-hdfs",title:"Hadoop Distributed File System (HDFS)",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-3-hdfs/"}},{id:"post-reading-notes-from-aleksa-gordic-39-s-gpu-blogpost",title:"Reading Notes from Aleksa Gordic&#39;s GPU BlogPost",description:"Reading notes for my reference from Aleksa Gordic&#39;s GPU BlogPost",section:"Posts",handler:()=>{window.location.href="/blog/2025/aleksagordic-gpu-blog-notes/"}},{id:"post-mlp-standard-derivatives-derivation",title:"MLP Standard Derivatives Derivation",description:"MLP Standard Derivatives Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-derivatives/"}},{id:"post-simple-mlp-forward-and-backward-pass-with-einsum-derivation",title:"Simple MLP - Forward and Backward Pass (With Einsum) Derivation",description:"Simple MLP - Forward and Backward Pass Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-fw-bwd/"}},{id:"post-gpu-notes",title:"GPU Notes",description:"GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes",section:"Posts",handler:()=>{window.location.href="/blog/2025/gpu-notes/"}},{id:"post-big-data-storage",title:"Big Data Storage",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 2",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-2-storage/"}},{id:"post-introduction-to-realtime-and-big-data-analytics",title:"Introduction to Realtime and Big Data Analytics",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-1-intro/"}},{id:"post-gpu-essentials-a-concise-technical-guide",title:"GPU Essentials - A Concise Technical Guide",description:"A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.",section:"Posts",handler:()=>{window.location.href="/blog/2025/GPU-Intro/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey-blog-2025-wrapping-ml-basics",title:"[Wrapping Up Our ML Foundations Journey](/blog/2025/wrapping-ml-basics/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-smallgraphgcn-accelerating-gnn-training-on-batched-small-graphs",title:"SmallGraphGCN - Accelerating GNN Training on Batched Small Graphs",description:"Discover how fused-edge-centric CUDA kernels dramatically accelerate Graph Neural Network training on molecular datasets. By rethinking parallelism strategies for batched small graphs, our approach achieves up to 3.1\xd7 faster forward execution and 1.3\xd7 end-to-end training speedup over PyTorch Geometric.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>