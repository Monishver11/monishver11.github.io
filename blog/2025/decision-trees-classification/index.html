<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decision Trees for Classification | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?e68e4955e21b20101db6e28a5a50abec"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/decision-trees-classification/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Decision Trees for Classification</h1> <p class="post-meta"> Created in April 20, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In the last post, we explored how decision trees can grow deep and complex—leading to overfitting—and how strategies like limiting tree depth or pruning can help us build simpler, more generalizable models.</p> <p>In this post, we turn our focus to <strong>classification trees</strong>, where our goal isn’t just fitting the data—it’s finding splits that <strong>create pure, confident predictions</strong> in each region.</p> <p>We’ll look at what makes a split “good,” explore different <strong>impurity measures</strong>, and understand how decision trees use these ideas to grow meaningfully.</p> <hr> <h5 id="what-makes-a-good-split-for-classification"><strong>What Makes a Good Split for Classification?</strong></h5> <p>In classification tasks, a decision tree predicts the <strong>majority class</strong> in each region. So, a good split is one that increases the <strong>purity</strong> of the resulting nodes—that is, each node contains mostly (or only) examples from a single class.</p> <p>Let’s walk through a concrete example. Suppose we’re trying to classify data points into <strong>positive (+)</strong> and <strong>negative (−)</strong> classes. We consider two possible ways to split the data:</p> <p><strong>Split Option 1</strong></p> <ul> <li> \[R_1: 8+,\ 2−\] </li> <li> \[R_2: 2+,\ 8−\] </li> </ul> <p>Here, each region contains a clear majority: 8 out of 10 are of the same class. Not bad!</p> <p><strong>Split Option 2</strong></p> <ul> <li> \[R_1: 6+,\ 4−\] </li> <li> \[R_2: 4+,\ 6−\] </li> </ul> <p>This is more mixed—each region has a 60-40 class split, making it <strong>less pure</strong> than Split 1.</p> <p>Now, let’s consider a <strong>better version of Split 2</strong>:</p> <p><strong>Split Option 3 (Refined)</strong></p> <ul> <li> \[R_1: 6+,\ 4−\] </li> <li> \[R_2: 0+,\ 10−\] </li> </ul> <p>Now things look different! Region \(R_2\) contains <strong>only negatives</strong>, making it a <strong>perfectly pure node</strong>. Even though \(R_1\) isn’t completely pure, this split overall is more desirable than the others.</p> <p>This example shows that a good split isn’t just about balance—it’s about <strong>maximizing purity</strong>, ideally pushing each node toward containing only one class.</p> <h5 id="misclassification-error-in-a-node"><strong>Misclassification Error in a Node</strong></h5> <p>Once we split the data, how do we measure how well a node performs in <strong>classification</strong>?</p> <p>Suppose we’re working with <strong>multiclass classification</strong>, where the possible labels are:</p> \[Y = \{1, 2, \ldots, K\}\] <p>Let’s focus on a particular <strong>node</strong> \(m\) (i.e., a region \(R_m\) of the input space) that contains \(N_m\) data points.</p> <p>For each class \(k \in \{1, \ldots, K\}\), we compute the <strong>proportion</strong> of points in node \(m\) that belong to class \(k\):</p> \[\hat{p}_{mk} = \frac{1}{N_m} \sum_{i: x_i \in R_m} \mathbb{1}[y_i = k]\] <p>This gives us the <strong>empirical class distribution</strong> within the node.</p> <p>To make a prediction, we choose the <strong>majority class</strong> in that node:</p> \[k(m) = \arg\max_k \hat{p}_{mk}\] <p>This means the class with the highest proportion becomes the predicted label for <strong>all points</strong> in that region.</p> <hr> <h5 id="node-impurity-measures"><strong>Node Impurity Measures</strong></h5> <p>In classification, our goal is to make each region (or <strong>node</strong>) as <strong>pure</strong> as possible—i.e., containing data points from mostly <strong>one class</strong>.</p> <p>To quantify how <strong>impure</strong> a node is, we use <strong>impurity measures</strong>. Let’s explore the three most common ones:</p> <p><strong>1. Misclassification Error</strong></p> <p>This measures the fraction of points <strong>not belonging to the majority class</strong> in a node:</p> \[\text{Misclassification Error} = 1 - \hat{p}_{mk(m)}\] <p>Here, \(\hat{p}_{mk(m)}\) is the proportion of the majority class in node \(m\).</p> <ul> <li>If a node has 90% of points from class A and 10% from class B, misclassification error = \(1 - 0.9 = 0.1\)</li> <li>It’s simple, but not very sensitive to class distribution beyond the majority vote.</li> </ul> <p><strong>2. Gini Index</strong></p> <p>The Gini index gives us a better sense of how <strong>mixed</strong> a node is:</p> \[\text{Gini}(m) = \sum_{k=1}^K \hat{p}_{mk}(1 - \hat{p}_{mk})\] <p>This value is <strong>0</strong> when the node is perfectly pure (i.e., all points belong to one class) and <strong>maximum</strong> when all classes are equally likely.</p> <ul> <li>For example, if a node has: <ul> <li>50% class A, 50% class B → Gini = \(0.5\)</li> <li>90% class A, 10% class B → Gini = \(0.18\)</li> <li>100% class A → Gini = \(0\) (pure)</li> </ul> </li> </ul> <p>The Gini index is widely used in practice because it’s differentiable and more sensitive to class proportions than misclassification error.</p> <p><strong>3. Entropy (a.k.a. Information Gain)</strong></p> <p>Entropy comes from information theory and captures the amount of <strong>uncertainty</strong> or <strong>disorder</strong> in the class distribution:</p> \[\text{Entropy}(m) = - \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}\] <p>Like Gini, it’s <strong>0</strong> for a pure node and <strong>higher</strong> for more mixed distributions.</p> <ul> <li>If a node has: <ul> <li>50% class A, 50% class B → Entropy = \(0.693\)</li> <li>90% class A, 10% class B → Entropy ≈ \(0.325\)</li> <li>100% class A → Entropy = \(0\)</li> </ul> </li> </ul> <p>Entropy grows slower than Gini but still encourages purity. It’s used in algorithms like <strong>ID3</strong> and <strong>C4.5</strong>.</p> <p><strong>Summary: When Are Nodes “Pure”?</strong></p> <p>All three measures hit <strong>zero</strong> when the node contains data from only one class. But Gini and Entropy give a smoother, more nuanced view of how mixed the classes are—helpful for greedy splitting.</p> <hr> <table> <thead> <tr> <th>Class Distribution</th> <th>Misclassification</th> <th>Gini</th> <th>Entropy</th> </tr> </thead> <tbody> <tr> <td>100% A</td> <td>0</td> <td>0</td> <td>0</td> </tr> <tr> <td>90% A / 10% B</td> <td>0.1</td> <td>0.18</td> <td>0.325</td> </tr> <tr> <td>50% A / 50% B</td> <td>0.5</td> <td>0.5</td> <td>0.693</td> </tr> </tbody> </table> <hr> <p>So, when deciding <strong>where to split</strong>, we prefer splits that lead to <strong>lower impurity</strong> in the resulting nodes.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DT-5-480.webp 480w,/assets/img/DT-5-800.webp 800w,/assets/img/DT-5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/DT-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DT-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Both Gini and Entropy encourage the class proportions to be close to 0 or 1—i.e., pure nodes. </div> <p><strong>Analogy: Sorting Colored Balls into Boxes</strong></p> <p>Imagine you’re sorting colored balls into boxes. Each ball represents a data point, and the color represents its class label.</p> <ul> <li>A <strong>perfectly sorted box</strong> has balls of only one color—this is a <strong>pure</strong> node.</li> <li>A box with a mix of colors is <strong>impure</strong>—you’re less certain what color a randomly chosen ball will be.</li> </ul> <p>Now think of impurity measures as ways to <strong>score</strong> each box. To ground these interpretations with some math, let’s calculate each impurity measure using a simple example.</p> <p>Suppose we’re at a node where the class distribution is:</p> <ul> <li>8 red balls (Class A)</li> <li>2 blue balls (Class B)</li> </ul> <p>This gives us the class probabilities:</p> <ul> <li> \[\hat{p}_A = \frac{8}{10} = 0.8\] </li> <li> \[\hat{p}_B = \frac{2}{10} = 0.2\] </li> </ul> <p>Let’s compute each impurity measure and interpret what it tells us:</p> <p><strong>Misclassification Error:</strong> <em>“What’s the chance I assign the wrong class if I always predict the majority class?”</em></p> <ul> <li>The majority class is red (Class A), so we’ll predict red for all inputs.</li> <li>The only mistakes occur when the true class is blue.</li> <li>So, the misclassification error is:</li> </ul> \[1 - \hat{p}_{\text{majority}} = 1 - 0.8 = 0.2\] <p><strong>Gini Index:</strong> <em>“If I randomly pick two balls (with replacement), what’s the chance they belong to different classes?”</em></p> <ul> <li>Formula:</li> </ul> \[G = \sum_k \hat{p}_k (1 - \hat{p}_k)\] <ul> <li>For our example:</li> </ul> \[G = 0.8(1 - 0.8) + 0.2(1 - 0.2) = 0.8(0.2) + 0.2(0.8) = 0.16 + 0.16 = 0.32\] <ul> <li>Interpretation: There’s a 32% chance of getting different classes when picking two random samples. The more balanced the classes, the higher the Gini.</li> </ul> <p><strong>Entropy:</strong> <em>“How surprised am I when I pick a ball and see its class?”</em></p> <ul> <li>Formula:</li> </ul> \[H = -\sum_k \hat{p}_k \log_2 \hat{p}_k\] <ul> <li>For our example:</li> </ul> \[H = -0.8 \log_2(0.8) - 0.2 \log_2(0.2)\] <ul> <li>Approximating:</li> </ul> \[H \approx -0.8(-0.32) - 0.2(-2.32) = 0.256 + 0.464 = 0.72\] <ul> <li>Interpretation: Entropy is a measure of uncertainty. A pure node has zero entropy. Here, there’s moderate uncertainty because the node isn’t completely pure.</li> </ul> <p>This example shows how the impurity measures behave when the node is somewhat pure but not perfectly. Gini and entropy are more sensitive to changes in class proportions than misclassification error, which is why they’re often preferred during tree building.</p> <hr> <h5 id="quantifying-the-impurity-of-a-split"><strong>Quantifying the Impurity of a Split</strong></h5> <p>Once we’ve chosen an impurity measure (like Gini, Entropy, or Misclassification Error), how do we decide if a split is good?</p> <p>When a split divides a node into two regions—left (\(R_L\)) and right (\(R_R\))—we compute the <strong>weighted average impurity</strong> of the child nodes:</p> \[\text{Impurity}_{\text{split}} = \frac{N_L \cdot Q(R_L) + N_R \cdot Q(R_R)}{N_L + N_R}\] <p>Where:</p> <ul> <li>\(N_L, N_R\) are the number of samples in the left and right nodes.</li> <li>\(Q(R)\) is the impurity of region \(R\), measured using Gini, Entropy, or Misclassification Error.</li> </ul> <p>We want to <strong>minimize</strong> this weighted impurity. A good split is one that sends the data into two groups that are as pure as possible.</p> <p><strong>Example:</strong></p> <p>Suppose we split a node of 10 samples into:</p> <ul> <li>Left node (\(R_L\)): 6 samples, Gini impurity = 0.1</li> <li>Right node (\(R_R\)): 4 samples, Gini impurity = 0.3</li> </ul> <p>Then the weighted impurity is:</p> \[\frac{6 \cdot 0.1 + 4 \cdot 0.3}{10} = \frac{0.6 + 1.2}{10} = 0.18\] <p>We’d compare this value with the weighted impurity of other candidate splits and choose the split with the <strong>lowest</strong> value.</p> <p>This process is repeated greedily at each step of the tree-building process.</p> <hr> <h5 id="interpretability-of-decision-trees"><strong>Interpretability of Decision Trees</strong></h5> <div class="row justify-content-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DT-6-480.webp 480w,/assets/img/DT-6-800.webp 800w,/assets/img/DT-6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/DT-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DT-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>One of the biggest advantages of decision trees is their <strong>interpretability</strong>.</p> <p>Each internal node represents a question about a feature, and each leaf node gives a prediction. You can <strong>follow a path</strong> from the root to a leaf to understand exactly how a prediction was made.</p> <p>This makes decision trees particularly useful when model transparency is important—such as in healthcare or finance.</p> <p>Even people without a technical background can often understand a <strong>small decision tree</strong> just by reading it. However, as trees grow deeper and wider, they can become <strong>harder to interpret</strong>, especially if overfit to the training data.</p> <h5 id="discussion-trees-vs-linear-models"><strong>Discussion: Trees vs. Linear Models</strong></h5> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DT-7-480.webp 480w,/assets/img/DT-7-800.webp 800w,/assets/img/DT-7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/DT-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="DT-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Unlike models like logistic regression or SVMs, <strong>decision trees don’t rely on geometric concepts</strong> such as distances, angles, or dot products. Instead, they work by recursively splitting the input space.</p> <p><strong>Decision trees are:</strong></p> <ul> <li> <strong>Non-linear</strong>: They can carve out complex, axis-aligned regions in the input space.</li> <li> <strong>Non-metric</strong>: No need to define or compute distances between points.</li> <li> <strong>Non-parametric</strong>: They don’t assume a fixed form for the underlying function—tree complexity grows with data.</li> </ul> <p><strong>But there are tradeoffs:</strong></p> <ul> <li>Trees may <strong>struggle with problems that have linear decision boundaries</strong>, which linear models handle easily.</li> <li>They are <strong>high-variance models</strong>—small changes in the training data can lead to very different trees.</li> <li>Without constraints (like pruning or depth limits), they are prone to <strong>overfitting</strong>, especially on noisy data.</li> </ul> <hr> <h5 id="recap--conclusion"><strong>Recap &amp; Conclusion</strong></h5> <ul> <li> <p>Trees partition the input space into regions, unlike linear models that rely on fixed decision boundaries.</p> </li> <li> <p>Split the data to minimize the <strong>sum of squared errors</strong> in each region.</p> </li> <li> <p>Building the best tree is computationally infeasible. We use a <strong>greedy algorithm</strong> to build the tree step-by-step.</p> </li> <li> <p>Fully grown trees overfit. We prevent this by limiting depth, size, or pruning based on validation performance.</p> </li> <li> <p>For classfication, a good split increases <strong>class purity</strong> in the nodes. We explored this with intuitive +/− examples.</p> </li> <li> <p><strong>Impurity Measures:</strong> Misclassification Error, Gini Index and Entropy</p> </li> <li> <p>We pick splits that <strong>reduce weighted impurity</strong> the most.</p> </li> <li> <p>Small trees are easy to understand; large ones can become complex.</p> </li> <li> <p>Trees are non-linear, non-parametric, and don’t need distance—but they may overfit or struggle with linear patterns.</p> </li> </ul> <p>This sets the stage for the next step: <strong>ensembles</strong> like Random Forests and Boosted Trees.</p> <p>Stay tuned!</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab",title:"Sharing personal reflections - Thoughts Tab",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>