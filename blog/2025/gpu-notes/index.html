<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> GPU Notes | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/gpu-notes/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">GPU Notes</h1> <p class="post-meta"> Created in December 11, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/gpu"> <i class="fa-solid fa-hashtag fa-sm"></i> GPU</a>   ·   <a href="/blog/category/gpu-nyu"> <i class="fa-solid fa-tag fa-sm"></i> GPU-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h4 id="concepts"><strong>Concepts:</strong></h4> <ul> <li>Shared memory is divided into banks (usually 32 banks), so warp-level accesses can be parallelized if threads don’t conflict. If warp threads access different banks → all 32 loads happen in parallel. If multiple threads hit the same bank → bank conflict, serialized access, slower. That’s why memory access patterns matter (e.g. row-major vs col-major).</li> <li>There is an upper bound on the # threads that can within a block. Similarly, there is an upper bound on the # blocks per SM. This is because of the hardware as it needs to schedule the warps based on them and the hardware is fixed. SOTA is like 2048 threads per block for now. But, this upper bound is good enough and even higher for most applications.</li> <li>Compute capability: a property of the hardware</li> <li>Pay for branch divergence, instead of resolving idle SPs or inactive threads in a warp, as the former is less expensive.</li> <li>If you need a global/kernel level sync, then it’s a good point to cut to form a new kernel starting from that point.</li> <li>Don’t use __syncthreads in an if-else block, it leads to undefined behavior.</li> <li>CUDA only assigns a block to a SM, if all of the block resources are available beforehand. This is needed for zero context switch overhead for GPUs.</li> <li>Thread scheduling is totally outside our control. So, don’t make assumption on which warps finishes before other</li> <li>Maximize threads per SM to the upper bound, instead of # blocks per SM -&gt; latency hiding (how?)</li> <li>Warps are the unit of scheduling inside a SM and execution of a warp is the definition of SIMD.</li> <li>Branch prediction - applicable in every multi-core systems. In GPU, we try to avoid this if-else conditions, as it causes warp divergence.</li> <li>Latency hiding/ Keeping the gpu busy by switching between warps while other warps wait for data. This is also called latency tolerance.</li> <li>No. of warps and problems size are two key factors that play a important role in deciding the threads/block.</li> <li>Rule of thumb - use cuda get device properties and maximize threads/SM for max parallelism and best performance. Because this causes highest latency tolerance and as there will be some warps to execute.</li> <li>Maxing block/SM is not in your control, that scheduling happens at the chip level.</li> <li>Never make your code dependable on a hardware feature, like warp size.</li> <li>CGMA ratio - Compute to global memory access. Also called arithmetic intensity.</li> <li>Global memory persists for an application and not just the kernel execution</li> <li>Shared memory of a SM is virtualized, meaning if two blocks are assigned to a SM, then each block sees and accesses its own shared memory block only.</li> <li>Constant memory is written only by the host and its read only for the device. It’s much faster accessible than global memory. It’s accessed by everyone and in our direct control. The size of this memory is comparable to that of shared memory.</li> <li>Registers, shared memory, constant memory and global memory are in our direct control. Cache is not in our direct control/in-direct control.</li> <li>The life-time of a register is related to the thread that holds it.</li> <li>Each access to registers involves fewer machine instructions than global memory.</li> <li>Register files means group of registers inside a SM.</li> <li>Address space is the set of addresses your program can access.</li> <li>If in a kernel, I’ve int x, then every thread has that variable in its set of registers.</li> <li>If its <strong>shared</strong> int y, then every block in a SM, has their own y variable.</li> <li> <strong>device</strong> int z, is in global memory, so everyone in the grid can access it.</li> <li>Automatic array variables local to a thread reside in local memory. This local memory doesn’t physically exist. It’s an abstractions to the local scope of a thread and put in global memory by the compiler. Again, since this is in global memory, its takes 100x more instructions to access this even though it appear local. So, try to make the variable put in registers as much as possible. Reduce this lmem(local memory) for performance improvements.</li> <li>Global memory access is a performance bottleneck. So, we try to reduce this. A common strategy is tiling, in which we partition the data we want to use into subset called tiles, such that each tile fits into the shared memory.</li> <li>In GPU, we want the performance/watt to be high and they’re quite good given the right problem.</li> <li> <p>Global synchronization across blocks of a grid isn’t possible and not expected. This is due to the possibility of deadlock and the fact that there can be many blocks waiting to be scheduled on the SM. In this case, blocks executing in a SM, will finish and reach the barrier, they wait for the others that are waiting to be scheduled in the SM, but can’t because the finished ones are waiting too and this is a deadlock.</p> </li> <li>Registers are faster, but those values are fetched from global memory. So, if I have a variable x in a thread execution, then each of x value for each thread will need to fetched by global memory. So, putting x in shared memory will only fetch once from the global memory. So, in this case putting in shared memory is best.</li> <li>A kernel that is memory bound is an indication that it must be optimized for performance. Always make sure the kernel is compute bound and most of its work is in this part. Also, to determine whether some kernel is compute/memory bound, we need both SW and HW details.</li> <li>In thread divergence, you lose 50%, even if 31 falls in if case and 1 falls in else case and 16 in if case and 16 in else case. The power consumption is double, as both cases are executed.</li> <li>System to device memory bandwidth link is a bottleneck. The GPU(SM &amp; shared memory) to device memory is better as its handled by NVIDIA an optimized for GPU usage.</li> <li>Sending large amount of data in one transfer is better than sending several small transfers. The reason is we avoid the overheads of the each individual transfer.</li> <li>Coalescing of the memory access is done at a per warp level, as the threads in the warp execute the same instructions at once.</li> <li>Accessing col wise in a 2D matrix of its memory locations is better than accessing the memory locations row wise. Draw a diagram to understand. So, in a matmul, accessing both col wise of its input is better. And there is a way to do, check about it. This is at a warp level and not at each individual thread level.</li> <li>Less global memory instructions -&gt; more opportunities to coalesce. Meaning, if there are less trips to the memory, we can get a coalesced block of data that we require in less trips.</li> <li>Making the memory accessed to be contiguous as much as possible. There is an alignment that is done by the OS, but as a programmer, you also need to make sure it’s aligned to get better performance. It reduces the no. of memory trips to the memory, there by increasing coalescing. Also, the cache picks a block of data from memory, so if the data you want is already contiguoos, you avoid a lot of cache misses.</li> <li>The # registers used by the thread/warp is directly based on the code we write, the variables and the structure determine it.</li> <li>Latency hiding - something must be executing while you’re retrieving the memory.</li> <li>If you’ve many instructions between the (instruction to memory access) and the actual usage, then for those many instructions you can schedule the computations, thus making it compute bound a little more.</li> <li> <p>Loop unrolling reduces the # instructions that are executed by the GPU assembly. Do this, once your code is already optimized for memory, resources etc. Follow the order of optimizations as we learned in class slides.</p> </li> <li>Reduce global memory trips using tiling, shared memory etc.</li> <li>Data prefetching becomes less beneficial as thread granularity increases. The reason is the shared memory is already used by the threads in this case to the max.</li> <li>What if we don’t know the shared memory size beforehand? Dynamic shared memory. Even here you need to know the size before kernel launch, but after the program launch. And you can’t have more than one dynamic allocation like this explicitly. But, we can play with pointers, but that’s not used much as the shared memory is small for this sophistication.</li> <li>If you’ve two files (c/c++) with int x; Then the linker, picks one at random, if we want to run both files. This can be a source of bug. To avoid this, we use extern keyword to tell the linker where to check for.</li> <li>The memory is cut into pieces to avoid it being big, power-hungry and slow. So, we’ve many pieces called banks. How about the address spaces for each?</li> <li>Shared memory accesses that span b distinct banks yield an effective bandwidth that is b times as high as the bandwidth of when accesses map to the same bank. Shared memory is now 32 banks with fixed width of four bytes. Consecutive four-byte data map to consecutive banks. So, if the threads is a warp access contiguous memory locations, it’ll take the memory from the 32 banks, which are independent and results b times higher than accessing the same bank. So, memory coalescing is important in this aspect too.</li> <li>Shared memory bank conflicts - since the width of the bank is minimal, only one at a time is possible for transfer, so it takes more time than if the accesses are from different banks which can be accessed in parallel.</li> <li>IEEE floating point standard (single-precision): (–1)^sign x (1+mantissa) x 2^(exponent – 127)</li> <li>With just this encoding, we can’t define 0 in this format. So, we’ve 3 encoding schemes based on exp.</li> <li>In tensor cores, the FMA is between 3 matrices, multiply two and add with third. The multiply is single precision and the additions is half precision.</li> <li>In multiplying two vectors, parallel compute approach results in more accurate than serial approach, but FMA is the most accurate when compared to the exact value. The GPU always uses FMA and the CPU is serial, and that’s the reason why GPU is more favorable in-terms of accuracy of floating point operations than CPU. Also, SP’s are optimized to do FMA in a single cycle.</li> <li>Asynchronous = returns to host right-away and doesn’t wait for device. (Non-blocking).</li> <li>Some CUDA API calls and all kernel launches are asynchronous with respect to the host code. This means error-reporting is also asynchronous.</li> <li>If the kernel launch is non-blocking and if I have a mem copy to host from device is next statement after the kernel launch statement, then it might lead to error right, because the kernel isn’t done yet, but we’re trying to load the result from device to host. So, what happens under the hood is that, there is a queue(streams) maintained in the device, and this queue holds the kernel launch first and then the mem copy, and the queue’s function will be executed sequentially, this is the reason, the prior works.</li> <li>You can have many queues, called streams and with CUDA, you’ve control to put functions in different streams. Until now, we’ve dealt with only 1 stream, so all operations are executed sequentially in that queue of the device.</li> <li>Operations in different streams can be interleaved and, when possible, they can even run concurrently. We can have several open streams to the same device at once. Need GPUs with concurrent transfer/execution capability.</li> <li>All operations to non-default streams are non-blocking wrt the host code.</li> <li>Some times for correctness reasons, you need to synchronize the host code with operations in a stream.</li> <li>Three options: cudaDeviceSynchronize() → blocks host for all streams, cudaStreamSynchronize(stream) → blocks host for this particular stream and cudaStreamQuery(stream) → does not block host for this stream.</li> <li>Streams are good way to overlap execution and transfer, hardware permits.</li> <li>Accessing host memory from device without explicit copy is called zero-copy mechanism. If it’s a small data access, then zero-copy is better, as it avoids the overhead of transferring the data to global memory and then accessing it within the device.</li> <li>As we program GPUs we need to pay attention to several performance bottlenecks: <ul> <li>Branch diversion</li> <li>Global memory latency</li> <li>Global memory bandwidth</li> <li>Shared memory bank conflicts</li> <li>Communication</li> <li>Limited resources</li> </ul> </li> <li>We have several techniques in our arsenal to enhance performance <ul> <li>Try to make threads in the same warp follow the same control flow</li> <li>Tiling</li> <li>Coalescing</li> <li>Loop unrolling</li> <li>Increase thread granularity</li> <li>Trade one resource for another</li> <li>Memory access pattern</li> <li>Streams</li> </ul> </li> <li>_syncthreads() - within one block - inside a kernel - waits for all threads in the same block to reach that point before continuing.</li> <li>cudaDeviceSynchronize() - across entire device - one host side - waits until all previously launched kernels on the GPU are complete.</li> <li> <p>cudaEvent_t - calculates the actual GPU execution time (when GPU was busy running your kernels). Its measures pure GPU time.</p> </li> <li>General rule of thumb: as the problem size increases, increasing the # threads must be done and that needs to increase performance, and not amount of work done by each thread. This applies to parallel computing in general.</li> <li>Order of cost: communication &amp; memory access &gt; computation</li> <li>In a CUDA program, if we suspect an error has occurred during a kernel launch, then we must explicitly check for it after the kernel has executed.</li> <li>If debugging, compile with: nvcc -DDEBUG code.cu -o code. This invokes the cuda error handling implicitly.</li> <li> <p>MPI, OPENMP and CUDA - Multicore + MultiGPU communication setup. MPI is for Multi-node GPU communication.</p> </li> <li>Zero-copy: nothing is copied, instead accessed directly. Between host to device or device to device. Transfer happens between the other memory to the current device’s L2 cache directly.</li> <li>Unified virtual address: puts all the host, device memory of all devices into a single address space.</li> <li>Unified memory: creates a pool of managed memory that is shared between the CPU and GPU. Under the hood, the data(pages) automatically migrates from CPU to GPU and among GPU’s for which ever needs that data.</li> <li>All this, is for ease of use and not for performance reasons. So, sometimes manual control is better/best as in the prior you might not exactly know the actual place where the data resides.</li> <li>UM is built on top of UVA, and UM has this extra capability of data(page) movements.</li> <li>All this, is only with respect to the GPU’s global memory. The shared memory, tiling, L1 cache are still within and unique for each devices.</li> <li>Zero-Copy: use it when you have a small piece of data for reading a few times. Manual: complex pattern between host and device. If it’s structured regular thing, use unified memory.</li> <li>Within the UVA, when we need data, we use the copy… option, but it actually transfers the pages. Without, this the copy can easily fill up the space and its redundant.</li> <li>UM is performant than UVA.</li> <li>Coherence: will not allow writes to the same page at the same time, even when it leads to low performance.</li> <li>Dynamic parallelism: something related to nested code/recursion. In GPUs, it means, the kernel can start new kernel. Streams can spawn new streams. It permits dynamic run time decisions.</li> <li>Until the child kernel finishes, the parent kernel isn’t done, subject to the availability of the resources. If there’s less resources, then there is a possibility of a deadlock.</li> <li>To speed-up, start a dummy kernel first. Because, the stage setting takes time with the CUDA runtime, so the first kernel launch takes more time than the subsequent ones.</li> <li>CudaThreadSynchronize() - the device waits for the work to be done on another device, for the work sent to the child kernel by the parent kernel. Even without this, the parent waits till the child kernel finishes. But does the parent kernel block, or it does some work itself? Check??</li> <li>Alignment with Cuda Pitch for 2D arrays/kernels. CUDA ensures that each row starts at an address aligned to 64 B or 128 B multiples. Aligned memory → fewer memory transactions → higher bandwidth utilization.</li> <li>Cuda Compilation: CUDA file(.cu)-&gt; PTX(Intermediate representation) -&gt; SASS(or other assemblies) -&gt; CuBit(Cuda binary) -&gt; execute</li> <li>-arch: for virtual compute architecture for generation of PTX code.</li> <li>-code: specifies the actual device that will be targeted by SASS ad the cuBin binary.</li> <li>Without -code, the final form is PTX, so every time for the GPU code generation, it uses a JIT compiler.</li> <li>Fat binary: an executable or object file that contains multiple versions of GPU code. One or more machine-specific binaries (SASS), compiled for concrete GPU architectures like sm_30, sm_35, etc. Optionally, one or more PTX versions, virtual assembly for JIT-compilation on future GPUs.</li> <li> <p>When a CUDA kernel is launched, the driver checks the GPU’s SM version and looks in the fatbinary for a matching compiled binary (cubin). If it finds one, it runs it directly for maximum speed; otherwise, it falls back to the embedded PTX, JIT-compiles it into a cubin, and caches it (in ~/.nv/ComputeCache) for future use. This allows the driver to automatically choose the optimal version at runtime with no code changes needed.</p> </li> <li>In the nvcc cmd, Arch gets you the PTX for that architecture and code gives you the assembly and binary for your specific device.</li> <li>(-arch=compute_Xi works with -code=sm_Xj, where i&lt;=j) - Check</li> <li>With just -arch and without -code, it’ll give you the ptx only. And at runtime, it uses the JIT compiler to compile this into an assembly and binary as executable. So, it takes a bit more time.</li> <li>Write code, that’s wrap friendly(that reduces thread divergence) and cache friendly for good memory access. This can be cultivated with experience and system design thinking.</li> <li> <strong>shfl_sync</strong>: fastest way to share data between threads within a warp, instead of going into shared memory or cache. This goes into low-level hardware and used by Nvidia libraries.</li> <li>Thread block cluster - a group of thread blocks. We introduce this a layer in between the blocks and grids. The next layer is grid with cluster. For a thread block cluster, all blocks within must be assigned to each of the SMs for the single thread block to be executed. The shared memory of all the blocks within a thread block cluster are accessible and called distributed shared memory. This needs C++, Cuda Blackwell architecture. One advantage is that this now can give us block level synchronization. - Check last point.</li> <li>How to reduce the performance dip because of AtomicAdd, but still maintaining the atomicity? Solution: Privatization. This is always good when we’ve severe collision. But, it’s costly. There’s overhead for creating and initializing private copies for each thread block and the overhead for accumulating the contents of private copies into the final copy. The benefit is much less contention and serialization in accessing both the private copies and the final copy. The overall performance can often be improved more than 10x. These private copies are stored in shared memory. Even in AtomicAdd, now we’ve variations based on the level of atomicity we want. For block level, we can use AtomicAdd_Block with privatization for best performance.</li> <li>What if the copy is too large to privatize? Sometimes one can partially privatize an output copy and use range testing to go to either global memory or shared memory.</li> </ul> <h4 id="lecture-doubts"><strong>Lecture doubts:</strong></h4> <ul> <li>If the kernel call API is non-blocking, then the after steps like bring the results back to host will start immediately, how is it possible? Or why kernel call API is non-blocking? - A: CUDA streams.</li> <li>What is codaMalloc(void<strong>) -&gt; what’s this void</strong> means? cudaMalloc() allocates memory on the GPU and writes the GPU address into your device pointer variable. To let CUDA modify that pointer, you must pass its address (i.e., a pointer to your pointer). Since cudaMalloc() expects a void<strong>, we cast our variable’s address — e.g., (void</strong>)&amp;d_curr — to match its signature. This cast simply tells CUDA, “here’s the address of my pointer; fill it with the device memory location.”</li> <li>As a programmer, do we have access to coalesce the memory access before the actual accessing from the memory?</li> <li>What is data prefetch? technique used to hide memory access latency by loading data into faster memory (like shared memory or registers) before it is actually needed for computation.</li> <li>What is presorting overhead in floating point operations?</li> <li>For mem copy from host to device, will it be done via pinned pages or the usage of pinned pages in in programmers control? If so, what are the specific api’s that allows us to do this?</li> <li>Nvidia doesn’t support backward compatibility??</li> <li>Access vs Transfer in GPU peer-to-peer ??. Access is unto L2 cache and transfer goes till the GPU global memory. Check more??</li> <li>What is zero copy and how its different from/related to peer-to-peer copy??</li> <li>cudaHostAlloc() ??</li> <li>By default, grids launched within a thread block are executed sequentially. • This happens even if grids are launched by different threads within the block. • To deal with this drawback → streams • streams created on the host cannot be used on the device. • Streams created in a block can be used by all threads in that block. ??</li> <li> <strong>shfl_sync</strong> and its variations. How its used and what happens due to this under the hood and what evens without this?</li> <li>Q: If threads in a warp should access contiguous memory for performance, but shared memory accesses must avoid bank conflicts to be parallel, isn’t this contradictory?</li> <li>A: No, because these rules apply to two different memory systems. Contiguous access refers to global memory, where consecutive addresses allow requests from a warp to be coalesced into fewer DRAM transactions, maximizing bandwidth. Bank conflicts apply to shared memory, which is divided into banks; here, threads must access different banks to avoid serialization. In practice, an optimal kernel loads data from global memory using coalesced (contiguous) accesses, then rearranges it in shared memory into a layout that avoids bank conflicts for computation. Both conditions are required: coalescing ensures efficient data movement into the SM, while conflict-free shared memory ensures fast, parallel use of that data once it is on-chip.</li> </ul> <h4 id="c--cuda"><strong>C++ &amp; CUDA:</strong></h4> <ul> <li>std::shared_ptr<t> → smart pointer, auto memory cleanup.</t> </li> <li>Custom deleter = control how memory is freed (e.g. cudaFree).</li> <li>Constructor initializer list (: h(h_), w(w_)) → compact way to set members.</li> <li>Templates (template <typename t="">) → write generic code for any type.</typename> </li> <li>Macros (#define Index(…)) → text substitution, quick shorthand.</li> <li>Exceptions (throw std::runtime_error(“msg”)) → safe error reporting.</li> <li>Header files: <ul> <li>#include = “paste the file here.”</li> <li>&lt;…&gt; = system/standard headers</li> <li>“…” = local project headers</li> <li>#pragma once -&gt; Ensures header file is included only once and prevents duplicate definitions.</li> </ul> </li> <li>Files: <ul> <li>.hh / .hpp → C++ headers (project-specific)</li> <li>.cuh → CUDA headers (contain CUDA-related declarations)</li> <li>.cpp / .cu → source files (where you usually put bigger function definitions, kernels, training loops)</li> </ul> </li> <li>operator() makes an object callable like a function</li> <li>functors (function objects)</li> <li>kernels can’t take references (&amp;) — arguments must be passed by value</li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Monishver11/monishver11.github.io","data-repo-id":"R_kgDONe9Wkw","data-category":"General","data-category-id":"DIC_kwDONe9Wk84Cp-bh","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-hadoop-distributed-file-system-hdfs",title:"Hadoop Distributed File System (HDFS)",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-3-hdfs/"}},{id:"post-reading-notes-from-aleksa-gordic-39-s-gpu-blogpost",title:"Reading Notes from Aleksa Gordic&#39;s GPU BlogPost",description:"Reading notes for my reference from Aleksa Gordic&#39;s GPU BlogPost",section:"Posts",handler:()=>{window.location.href="/blog/2025/aleksagordic-gpu-blog-notes/"}},{id:"post-mlp-standard-derivatives-derivation",title:"MLP Standard Derivatives Derivation",description:"MLP Standard Derivatives Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-derivatives/"}},{id:"post-simple-mlp-forward-and-backward-pass-with-einsum-derivation",title:"Simple MLP - Forward and Backward Pass (With Einsum) Derivation",description:"Simple MLP - Forward and Backward Pass Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-fw-bwd/"}},{id:"post-gpu-notes",title:"GPU Notes",description:"GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes",section:"Posts",handler:()=>{window.location.href="/blog/2025/gpu-notes/"}},{id:"post-big-data-storage",title:"Big Data Storage",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 2",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-2-storage/"}},{id:"post-introduction-to-realtime-and-big-data-analytics",title:"Introduction to Realtime and Big Data Analytics",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-1-intro/"}},{id:"post-gpu-essentials-a-concise-technical-guide",title:"GPU Essentials - A Concise Technical Guide",description:"A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.",section:"Posts",handler:()=>{window.location.href="/blog/2025/GPU-Intro/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey-blog-2025-wrapping-ml-basics",title:"[Wrapping Up Our ML Foundations Journey](/blog/2025/wrapping-ml-basics/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>