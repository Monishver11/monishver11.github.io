<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Boosting and AdaBoost | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/adaboost/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Boosting and AdaBoost</h1> <p class="post-meta"> Created in April 27, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Boosting is a powerful machine learning technique that focuses on reducing the error rate of high-bias estimators by combining many weak learners, typically trained sequentially. Unlike bagging, which trains classifiers in parallel to reduce variance, boosting focuses on improving performance by training classifiers sequentially on reweighted data. The core idea behind boosting is simple: rather than using a large and complex model that may overfit, we train a series of simpler models (typically decision trees) to improve accuracy gradually.</p> <p>In contrast to bagging’s emphasis on parallel training of multiple models on different data subsets, boosting systematically reweights the training examples after each classifier is added, directing the model’s attention to examples that previous classifiers struggled with.</p> <p><strong>Key Intuition:</strong></p> <ul> <li>A <strong>weak learner</strong> is a classifier that performs slightly better than random chance. <ul> <li>Example: A rule such as “If <code class="language-plaintext highlighter-rouge">&lt;keyword&gt;</code> then spam” or “From a friend” then “not spam”.</li> </ul> </li> <li>Weak learners focus on different parts of the data, which may be misclassified by previous models.</li> <li>The final model is a weighted combination of these weak learners, with each learner contributing differently based on its performance.</li> </ul> <p>We will explore a specific boosting algorithm: <strong>AdaBoost</strong> (Freund &amp; Schapire, 1997), which is commonly used with decision trees as weak learners.</p> <hr> <h5 id="adaboost-setting"><strong>AdaBoost: Setting</strong></h5> <p>For binary classification, where the target variable \(Y = \{-1, 1\}\), AdaBoost uses a base hypothesis space \(H = \{h : X \rightarrow \{-1, 1\}\}\). Common choices for weak learners include:</p> <ul> <li> <strong>Decision stumps</strong>: A tree with a single split.</li> <li> <strong>Decision trees</strong> with a few terminal nodes.</li> <li> <strong>Linear decision functions</strong>.</li> </ul> <p><strong>Weighted Training Set</strong></p> <p>Each weak learner in AdaBoost is trained on a <strong>weighted</strong> version of the training data. The training set \(D = \{(x_1, y_1), \dots, (x_n, y_n)\}\) has weights associated with each example: \(w_1, w_2, \dots, w_n\).</p> <p>The <strong>weighted empirical risk</strong> is defined as:</p> \[\hat{R}_n^w(f) \overset{\text{def}}{=} \frac{1}{W} \sum_{i=1}^{n} w_i \cdot \ell(f(x_i), y_i)\] <p>where \(W = \sum_{i=1}^{n} w_i\), and \(\ell\) is a loss function (typically 0-1 loss in the case of classification).</p> <p>Examples with larger weights have a more significant impact on the loss, guiding the model to focus on harder-to-classify examples.</p> <h5 id="adaboost-sketch-of-the-algorithm"><strong>AdaBoost: Sketch of the Algorithm</strong></h5> <p>AdaBoost works by combining several weak learners to create a strong classifier.<br> Here’s the high-level process:</p> <ol> <li> <strong>Start by assigning equal weights</strong> to all training examples:</li> </ol> \[w_1 = w_2 = \cdots = w_n = 1\] <ol> <li> <p><strong>For each boosting round</strong> \(m = 1, \dots, M\) (where \(M\) is the number of classifiers we want to train):</p> <ul> <li> <strong>Train a base classifier</strong> \(G_m(x)\) on the <strong>current weighted</strong> training data.</li> <li> <strong>Evaluate</strong> how well \(G_m(x)\) performs.</li> <li> <strong>Increase the weights</strong> of examples that were <strong>misclassified</strong>, so the next classifier focuses more on those harder examples.</li> </ul> </li> <li> <p><strong>Aggregate the predictions</strong> from all classifiers, weighted by their accuracy:</p> </li> </ol> \[G(x) = \text{sign}\left( \sum_{m=1}^{M} \alpha_m G_m(x) \right)\] <p>The key idea is: <strong>the more accurate a base learner, the higher its influence in the final prediction</strong>.</p> <h5 id="adaboost-how-to-compute-classifier-weights"><strong>AdaBoost: How to Compute Classifier Weights</strong></h5> <p>In AdaBoost, each base classifier \(G_m\) contributes to the final prediction with a weight \(\alpha_m\).<br> We want the following:</p> <ul> <li>\(\alpha_m\) should be <strong>non-negative</strong>.</li> <li>\(\alpha_m\) should be <strong>larger</strong> when \(G_m\) fits its weighted training data well.</li> </ul> <p>The <strong>weighted 0-1 error</strong> of the base classifier \(G_m(x)\) is computed as:</p> \[\text{err}_m = \frac{1}{W} \sum_{i=1}^{n} w_i \, \mathbf{1}\left[ y_i \neq G_m(x_i) \right]\] <p>where:</p> <ul> <li>\(W = \sum_{i=1}^{n} w_i\) is the total sum of weights.</li> <li>\(\mathbf{1}[\cdot]\) is an indicator function, equal to 1 if the condition is true, and 0 otherwise.</li> </ul> <p>Since the error is normalized by the total weight, we always have:</p> \[\text{err}_m \in [0, 1]\] <p>Once we know the error \(\text{err}_m\), we compute the weight of the classifier \(G_m\) as:</p> \[\alpha_m = \ln\left( \frac{1 - \text{err}_m}{\text{err}_m} \right)\] <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-4-480.webp 480w,/assets/img/ensemble-4-800.webp 800w,/assets/img/ensemble-4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ensemble-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-4" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Higher weighted error ⇒ lower weight </div> <p><strong>Interpretation</strong>:</p> <ul> <li>If \(\text{err}_m\) is <strong>small</strong> (good classifier), then \(\alpha_m\) is <strong>large</strong>.</li> <li>If \(\text{err}_m\) is <strong>large</strong> (poor classifier), then \(\alpha_m\) is <strong>small</strong>.</li> </ul> <p>Thus, <strong>more accurate classifiers get higher voting power</strong> in the final decision.</p> <h5 id="adaboost-how-example-weights-are-updated"><strong>AdaBoost: How Example Weights Are Updated</strong></h5> <p>After training a base classifier, we update the weights of the examples to <strong>focus more on mistakes</strong>.</p> <p>Suppose \(w_i\) is the weight of example \(x_i\) <strong>before</strong> training \(G_m\). After training:</p> <ul> <li>If \(G_m\) <strong>correctly classifies</strong> \(x_i\), <strong>keep \(w_i\) the same</strong>.</li> <li>If \(G_m\) <strong>misclassifies</strong> \(x_i\), <strong>increase \(w_i\)</strong>:</li> </ul> \[w_i \leftarrow w_i \times e^{\alpha_m}\] <p>This adjustment ensures that:</p> <ul> <li> <strong>Hard examples</strong> (previously misclassified) <strong>get more weight</strong> and are more likely to be correctly classified by future classifiers.</li> <li>If \(G_m\) is a <strong>strong classifier</strong> (large \(\alpha_m\)), the weight update for misclassified examples is <strong>more significant</strong>.</li> </ul> <p>Alternatively, you can think of it this way:</p> \[w_i \leftarrow w_i \times \left( \frac{1}{\text{err}_m} - 1 \right)\] <p>This reweighting step is what drives AdaBoost to sequentially <strong>correct</strong> the errors of the previous learners.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-5-480.webp 480w,/assets/img/ensemble-5-800.webp 800w,/assets/img/ensemble-5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ensemble-5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-5" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> AdaBoost: Schematic </div> <p><strong>Intuition Behind AdaBoost: Analogy</strong></p> <p>To better internalize AdaBoost, imagine the process as <strong>training a team of tutors</strong> to help a student (the model) pass an exam (classification task):</p> <ul> <li> <p><strong>First Tutor</strong>: The first tutor teaches the entire syllabus equally. After the first test, they realize the student struggles with some topics (mistakes/misclassifications).</p> </li> <li> <p><strong>Second Tutor</strong>: The second tutor <strong>focuses more heavily</strong> on the topics where the student made mistakes, spending extra time on them.</p> </li> <li> <p><strong>Third Tutor</strong>: The third tutor notices there are still lingering problems on certain topics, so they <strong>focus even more narrowly</strong> on the hardest concepts.</p> </li> <li> <p><strong>And so on…</strong></p> </li> </ul> <p>Each tutor is <strong>not perfect</strong>, but by <strong>combining their focused efforts</strong>, the student gets a much more complete understanding — better than what any single tutor could achieve alone.</p> <hr> <h5 id="simple-mathematical-example"><strong>Simple Mathematical Example</strong></h5> <p>Let’s walk through a <strong>tiny AdaBoost example</strong> to see everything in action.</p> <p>Suppose we have 4 data points:</p> <hr> <table> <thead> <tr> <th style="text-align: center">\(x_i\)</th> <th style="text-align: center">\(y_i\) (True Label)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">+1</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">+1</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">-1</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">-1</td> </tr> </tbody> </table> <hr> <p><strong>Step 1: Initialization</strong></p> <p>All examples start with <strong>equal weight</strong>:</p> \[w_i = \frac{1}{4} = 0.25 \quad \text{for each } i\] <p><strong>Step 2: First Classifier \(G_1(x)\)</strong></p> <p>Suppose \(G_1\) predicts:</p> <hr> <table> <thead> <tr> <th style="text-align: center">\(x_i\)</th> <th style="text-align: center">\(G_1(x_i)\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">+1</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">-1 ❌</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">-1</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">-1</td> </tr> </tbody> </table> <hr> <p>It misclassifies \(x_2\).</p> <p>Compute weighted error:</p> \[\text{err}_1 = \frac{w_2}{\sum_{i=1}^{4} w_i} = \frac{0.25}{1} = 0.25\] <p>Classifier weight:</p> \[\alpha_1 = \ln\left( \frac{1 - 0.25}{0.25} \right) = \ln(3) \approx 1.0986\] <p><strong>Step 3: Update Weights</strong></p> <p>Increase the weight for the misclassified example:</p> <ul> <li>For correctly classified points \(w_i\) stays the same.</li> <li>For misclassified points:</li> </ul> \[w_i \leftarrow w_i \times e^{\alpha_1}\] <p>Thus:</p> <ul> <li>\(w_2\) (misclassified) becomes:</li> </ul> \[w_2' = 0.25 \times e^{1.0986} \approx 0.25 \times 3 = 0.75\] <ul> <li>\(w_1, w_3, w_4\) stay \(0.25\).</li> </ul> <p><strong>Normalization step</strong> (so weights sum to 1):</p> <p>Total weight:</p> \[W' = 0.25 + 0.75 + 0.25 + 0.25 = 1.5\] <p>New normalized weights:</p> <hr> <table> <thead> <tr> <th style="text-align: center">\(x_i\)</th> <th style="text-align: center">New Weight</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">\(\frac{0.25}{1.5} \approx 0.167\)</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">\(\frac{0.75}{1.5} = 0.5\)</td> </tr> <tr> <td style="text-align: center">3</td> <td style="text-align: center">\(\frac{0.25}{1.5} \approx 0.167\)</td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">\(\frac{0.25}{1.5} \approx 0.167\)</td> </tr> </tbody> </table> <hr> <p><strong>Step 4: Second Classifier \(G_2(x)\)</strong></p> <p>Train next classifier \(G_2\) <strong>on the new weights</strong>.</p> <p>Now, \(x_2\) has the highest weight (0.5), so the model focuses more on predicting \(x_2\) correctly!</p> <p>(And the process repeats…)</p> <p><strong>Key Takeaways</strong></p> <ul> <li>AdaBoost <strong>punishes mistakes</strong> by increasing weights of misclassified examples.</li> <li>Future classifiers <strong>focus</strong> on the harder examples.</li> <li>Classifier weight \(\alpha\) depends on how good the classifier is (lower error → higher weight).</li> <li>Final prediction is:</li> </ul> \[G(x) = \text{sign}\left( \alpha_1 G_1(x) + \alpha_2 G_2(x) + \cdots + \alpha_M G_M(x) \right)\] <p>Thus, even if each individual classifier is weak, <strong>together they become strong</strong>!</p> <hr> <h5 id="adaboost-algorithm"><strong>AdaBoost: Algorithm</strong></h5> <p>Given a training set:</p> \[\mathcal{D} = \{(x_1, y_1), \ldots, (x_n, y_n)\}.\] <p>The AdaBoost procedure works as follows:</p> <p><strong>Steps:</strong></p> <ol> <li> <p><strong>Initialize observation weights</strong>:</p> <p>Set:</p> \[w_i = 1, \quad \text{for all } i = 1, 2, \ldots, n.\] </li> <li> <p><strong>For \(m = 1\) to \(M\) (number of base classifiers)</strong>:</p> <ul> <li> <p><strong>Train</strong> a base learner on the weighted training data, obtaining a classifier \(G_m(x)\).</p> </li> <li> <p><strong>Compute the weighted empirical 0-1 risk</strong>:</p> </li> </ul> \[\text{err}_m = \frac{1}{W} \sum_{i=1}^{n} w_i \, \mathbf{1}[y_i \neq G_m(x_i)],\] <p>where:</p> \[W = \sum_{i=1}^{n} w_i.\] <ul> <li> <p><strong>Compute classifier weight</strong>:</p> <p>Assign a weight to the classifier based on its error:</p> \[\alpha_m = \ln\left( \frac{1 - \text{err}_m}{\text{err}_m} \right).\] </li> <li> <p><strong>Update example weights</strong>:</p> <p>Update the training example weights to emphasize misclassified examples:</p> \[w_i \leftarrow w_i \times \exp\left( \alpha_m \mathbf{1}[y_i \neq G_m(x_i)] \right).\] </li> </ul> </li> <li> <p><strong>Final classifier</strong>:</p> <p>After \(M\) rounds, return the final classifier:</p> \[G(x) = \text{sign}\left( \sum_{m=1}^{M} \alpha_m G_m(x) \right).\] </li> </ol> <p><strong>To put it shortly:</strong></p> <ul> <li> <strong>Start</strong>: Treat every sample equally.</li> <li> <strong>Learn</strong>: Focus the learner on samples that previous classifiers got wrong.</li> <li> <strong>Combine</strong>: Build a strong final classifier by combining the weighted votes of all the base classifiers.</li> </ul> <p>Each \(\alpha_m\) ensures that <strong>better-performing classifiers get a stronger say</strong> in the final decision!</p> <hr> <h5 id="weighted-error-vs-classifiers-true-error"><strong>Weighted Error vs. Classifier’s True Error</strong></h5> <p>In AdaBoost, the error \(\text{err}_m\) computed at each iteration is the <strong>weighted error</strong> based on the current distribution of sample weights, <strong>not</strong> the classifier’s true (unweighted) error rate over the data.</p> <p>This distinction is important: a classifier might have a low overall misclassification rate but could still have a <strong>high weighted error</strong> if it misclassifies examples that currently have large weights (i.e., harder or previously misclassified points).</p> <p>AdaBoost intentionally shifts focus toward difficult examples, so <strong>do not confuse the weighted empirical error used in boosting with the base learner’s standard classification error</strong>.</p> <h5 id="how-is-the-base-learner-optimized-at-each-iteration"><strong>How is the Base Learner Optimized at Each Iteration?</strong></h5> <p>At each iteration \(m\) of AdaBoost, the goal is to find the base classifier \(G_m(x)\) that <strong>minimizes the weighted empirical error</strong>:</p> \[\text{err}_m = \frac{1}{W} \sum_{i=1}^{n} w_i \, \mathbf{1}[y_i \neq G_m(x_i)].\] <p>Here’s the key idea:</p> <ul> <li>We <strong>don’t</strong> try to find a classifier that perfectly fits the original (unweighted) training data.</li> <li>Instead, we <strong>optimize for the current weighted dataset</strong> — meaning examples with larger weights influence the learning process more.</li> <li>The base learner is trained to focus on <strong>minimizing mistakes</strong> on the examples that have been <strong>harder to classify</strong> so far.</li> </ul> <p><strong>Typical Optimization Process</strong>:</p> <ul> <li>If using <strong>decision stumps</strong> (one-level decision trees), the learner searches for the split that minimizes the weighted classification error.</li> <li>In general, the base model uses the <strong>sample weights</strong> as importance scores to guide its fitting.</li> </ul> <p>Thus, at each step, AdaBoost adapts the learning problem to focus on what the previous classifiers struggled with, gradually building a strong ensemble.</p> <div class="row justify-content-center"> <div class="col-sm-4 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-6-480.webp 480w,/assets/img/ensemble-6-800.webp 800w,/assets/img/ensemble-6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ensemble-6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-6" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-7-480.webp 480w,/assets/img/ensemble-7-800.webp 800w,/assets/img/ensemble-7-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ensemble-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-7" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-8-480.webp 480w,/assets/img/ensemble-8-800.webp 800w,/assets/img/ensemble-8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ensemble-8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-8" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> AdaBoost with Decision Stumps. (I)After 1 round, (II)After 3 rounds &amp; (III)After 120 rounds. Size of plus sign represents weight of example. Blackness represents preference for red class; whiteness represents preference for blue class. </div> <h5 id="does-adaboost-overfit"><strong>Does AdaBoost Overfit?</strong></h5> <p>While boosting generally performs well, it’s natural to ask: <strong>Does AdaBoost overfit with many rounds?</strong></p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-9-480.webp 480w,/assets/img/ensemble-9-800.webp 800w,/assets/img/ensemble-9-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ensemble-9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-9" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> General learning curves if we were overfitting </div> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ensemble-10-480.webp 480w,/assets/img/ensemble-10-800.webp 800w,/assets/img/ensemble-10-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ensemble-10.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ensemble-10" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Learning Curves for AdaBoost </div> <p>The learning curves for AdaBoost typically show that the test error continues to decrease even after the training error reaches zero, which indicates that AdaBoost is <strong>resistant to overfitting</strong>. This is one of the reasons why AdaBoost is so powerful: it can maintain good generalization even with many weak learners.</p> <h5 id="adaboost-in-real-world-applications"><strong>AdaBoost in Real-World Applications</strong></h5> <p>A famous application of AdaBoost is <strong>face detection</strong>, as demonstrated in Viola &amp; Jones (2001). In this case, AdaBoost uses pre-defined weak classifiers and employs a smart way of doing real-time inference, even on hardware from 2001. This demonstrates the efficiency and applicability of AdaBoost in practical scenarios.</p> <h5 id="wrapping-up"><strong>Wrapping Up</strong></h5> <p>Boosting is an ensemble technique aimed at reducing bias by combining multiple weak learners. The sequential nature of boosting means that each learner focuses on errors made by previous ones, ultimately improving the model’s performance. <strong>AdaBoost</strong> is a specific and highly effective boosting algorithm that can be used with decision trees as weak learners to achieve powerful classification results.</p> <p><strong>Next Steps:</strong></p> <p>In the next section, we’ll explore the <strong>objective function</strong> of AdaBoost in more detail, along with some <strong>generalizations</strong> to other loss functions and the popular <strong>Gradient Boosting</strong> algorithm.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey-blog-2025-wrapping-ml-basics",title:"[Wrapping Up Our ML Foundations Journey](/blog/2025/wrapping-ml-basics/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>