<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Loss Functions - Regression and Classification | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/loss-functions/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Loss Functions - Regression and Classification</h1> <p class="post-meta"> Created in January 02, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Loss functions are of critical importance to machine learning, guiding models to minimize errors and improve predictions. They quantify how far off a model’s predictions are from the actual outcomes and serve as the basis for optimization. In this post, we’ll explore loss functions for <strong>regression</strong> and <strong>classification</strong> problems, breaking down their mathematical foundations and building intuitive understanding along the way. We will then transition our focus to logistic regression, examining its relationship with loss functions in classification tasks.</p> <hr> <h4 id="loss-functions-for-regression"><strong>Loss Functions for Regression</strong></h4> <p>Regression tasks focus on predicting continuous values. Think about forecasting stock prices, estimating medical costs based on patient details, or predicting someone’s age from their photograph. These problems share a common requirement: accurately measuring how close the predicted values are to the true values.</p> <h6 id="setting-the-stage-notation"><strong>Setting the Stage: Notation</strong></h6> <p>Before diving in, let’s clarify the notation:</p> <ul> <li>\(\hat{y}\) represents the predicted value (the model’s output).</li> <li>\(y\) denotes the actual observed value (the ground truth).</li> </ul> <p>A <strong>loss function</strong> for regression maps the predicted and actual values to a real number: \(\ell(\hat{y}, y) \in \mathbb{R}.\) Most regression losses are based on the <strong>residual</strong>, defined as:</p> \[r = y - \hat{y}\] <p>The residual captures the difference between the true value and the prediction.</p> <h6 id="what-makes-a-loss-function-distance-based"><strong>What Makes a Loss Function Distance-Based?</strong></h6> <p>A loss function is <strong>distance-based</strong> if it meets two criteria:</p> <ol> <li>It depends solely on the residual: \(\ell(\hat{y}, y) = \psi(y - \hat{y}),\) where \(\psi: \mathbb{R} \to \mathbb{R}.\)</li> <li>It equals zero when the residual is zero: \(\psi(0) = 0.\)</li> </ol> <p>Such loss functions are <strong>translation-invariant</strong>, meaning they remain unaffected if both the prediction and the actual value are shifted by the same amount: \(\ell(\hat{y} + b, y + b) = \ell(\hat{y}, y), \quad \forall b \in \mathbb{R}.\)</p> <p>However, in some scenarios, translation invariance may not be desirable. For example, using the <strong>relative error</strong>: \(\text{Relative error} = \frac{\hat{y} - y}{y},\) provides a loss function better suited to cases where proportional differences matter.</p> <p>For instance:</p> <ul> <li>If the actual stock price is $100, and your model predicts $110, the absolute error is $10, but the relative error is 10%.</li> <li>But, if the actual stock price is $10, and your model predicts $11, the absolute error is still $1, but the relative error is 10%.</li> </ul> <h5 id="exploring-common-loss-functions-for-regression"><strong>Exploring Common Loss Functions for Regression</strong></h5> <h6 id="1-squared-loss-l2-loss"><strong>1. Squared Loss (L2 Loss)</strong></h6> <p>Squared loss is one of the most widely used loss functions:</p> \[\ell(r) = r^2 = (y - \hat{y})^2\] <p>This loss penalizes large residuals more heavily, making it sensitive to outliers. Its simplicity and differentiability make it popular in linear regression and similar models.</p> <h6 id="2-absolute-loss-l1-loss"><strong>2. Absolute Loss (L1 Loss)</strong></h6> <p>Absolute loss measures the magnitude of the residual:</p> \[\ell(r) = |r| = |y - \hat{y}|\] <p>Unlike squared loss, absolute loss is robust to outliers but lacks smooth differentiability.</p> <p><strong>Think of it this way</strong>: Imagine predicting house prices based on size. If one house in the dataset has an extremely high price (an outlier), using absolute loss will make the model focus more on the typical pricing pattern of most houses and ignore the outlier. In contrast, least squares regression would try to minimize the error caused by that outlier, potentially distorting the model.</p> <h6 id="3-huber-loss"><strong>3. Huber Loss</strong></h6> <p>The Huber loss combines the best of both worlds:</p> \[\ell(r) = \begin{cases} \frac{1}{2}r^2 &amp; \text{if } |r| \leq \delta, \\ \delta |r| - \frac{1}{2}\delta^2 &amp; \text{if } |r| &gt; \delta. \end{cases}\] <p>For small residuals, it behaves like squared loss, while for large residuals, it switches to absolute loss, providing robustness without sacrificing differentiability. <strong>Note</strong>: Equal values and slopes at \((r = \delta)\).</p> <p><strong>Understanding Robustness</strong>: It describes a loss function’s resistance to the influence of outliers.</p> <ul> <li> <strong>Squared loss</strong> is highly sensitive to outliers.</li> <li> <strong>Absolute loss</strong> is much more robust.</li> <li> <strong>Huber loss</strong> strikes a balance between sensitivity and robustness. Meaning, it is sensitive enough to provide a useful gradient for smaller errors (via L2), but becomes more robust to large residuals, preventing them from disproportionately influencing the model (via L1).</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Regression_Losses-480.webp 480w,/assets/img/Regression_Losses-800.webp 800w,/assets/img/Regression_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Regression_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Regression_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center"> Regression Loss Functions </div> <hr> <h4 id="loss-functions-for-classification"><strong>Loss Functions for Classification</strong></h4> <p>Classification tasks involve predicting discrete labels. For instance, we might want to decide whether an email is spam or if an image contains a cat. The challenge lies in guiding the model to make accurate predictions while quantifying the degree of correctness.</p> <h6 id="the-role-of-the-score-function"><strong>The Role of the Score Function</strong></h6> <p>In binary classification, the model predicts a score, \(f(x)\), for each input \(x\):</p> <ul> <li>If \(f(x) &gt; 0\), the model predicts the label \(1\).</li> <li>If \(f(x) &lt; 0\), the model predicts the label \(-1\).</li> </ul> <p>This score represents the model’s confidence, and its magnitude indicates how certain the prediction is.</p> <h6 id="what-is-the-margin"><strong>What is the Margin?</strong></h6> <p>The <strong>margin</strong> captures the relationship between the predicted score and the true label:</p> \[m = y\hat{y}\] <p>or equivalently:</p> \[m = yf(x)\] <p>The margin measures correctness:</p> <ul> <li> <strong>Positive margin</strong>: The prediction is correct.</li> <li> <strong>Negative margin</strong>: The prediction is incorrect.</li> </ul> <p>The goal of many classification tasks is to maximize this margin, ensuring confident and accurate predictions.</p> <h5 id="common-loss-functions-for-classification"><strong>Common Loss Functions for Classification</strong></h5> <h6 id="1-0-1-loss"><strong>1. 0-1 Loss</strong></h6> <p>The 0-1 loss is a simple yet impractical loss function:</p> \[\ell(y, \hat{y}) = \begin{cases} 0 &amp; \text{if } y = \hat{y} \\ 1 &amp; \text{if } y \neq \hat{y} \end{cases}\] <p>Alternatively,</p> \[\ell_{0-1}(f(x), y) = \mathbf{1}[yf(x) \leq 0]\] <p>Here, \(\mathbf{1}\) is the indicator function, which equals 1 if the condition is true and 0 otherwise.</p> <p>Although intuitive, the 0-1 loss is:</p> <ul> <li> <strong>Non-convex</strong>, making optimization difficult, because its value is either 0 or 1, which creates a step-like behavior.</li> <li> <strong>Non-differentiable</strong>, rendering gradient-based methods inapplicable. For instance, if \(\hat{y} = 0.5\), the loss could change abruptly from 0 to 1 depending on whether the true label \(y\) is 0 or 1, leading to no gradient at this boundary.</li> </ul> <h6 id="2-hinge-loss"><strong>2. Hinge Loss</strong></h6> <p>Hinge loss, commonly used in Support Vector Machines (SVMs), addresses the limitations of 0-1 loss:</p> \[\ell_{\text{Hinge}}(m) = \max(1 - m, 0)\] <p>It is a convex, upper bound on 0-1 loss and encourages a positive margin. However, it is not differentiable at \(m = 1\).</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Classification_Losses-480.webp 480w,/assets/img/Classification_Losses-800.webp 800w,/assets/img/Classification_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Classification_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Classification_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center"> Classification Loss Functions </div> <hr> <h4 id="diving-deeper-logistic-regression"><strong>Diving Deeper: Logistic Regression</strong></h4> <p>In our exploration above, we’ve covered the basics of regression and classification losses. Now, let’s shift our focus to <strong>logistic regression</strong> and its corresponding loss functions, which are pivotal in classification problems. We’ll also touch on why square loss isn’t typically used for classification.</p> <p>Despite its name, <strong>logistic regression</strong> is not actually a regression algorithm—it’s a <strong>linear classification</strong> method. Logistic regression predicts probabilities, making it well-suited for binary classification problems.</p> <p>The predictions are modeled using the <strong>sigmoid function</strong>, denoted by \(\sigma(z)\), where:</p> \[\sigma(z) = \frac{1}{1 + \exp(-z)}\] <p>and \(z = f(x) = w^\top x\) is the score computed from the input features and weights.</p> <h5 id="logistic-regression-with-labels-as-0-or-1"><strong>Logistic Regression with Labels as 0 or 1</strong></h5> <p>When the labels are in \(\{0, 1\}\):</p> <ul> <li> <p>The predicted probability is: \(\hat{y} = \sigma(z)\)</p> </li> <li> <p>The loss function for logistic regression in this case is the <strong>binary cross-entropy loss</strong>:</p> \[\ell_{\text{Logistic}} = -y \log(\hat{y}) - (1 - y) \log(1 - \hat{y})\] </li> </ul> <p>Here’s how it works based on different predicted values of \(\hat{y}\):</p> <ul> <li> <strong>If \(y = 1\) (True label is 1)</strong>: <ul> <li> <p>The loss is:</p> \[\ell_{\text{Logistic}} = -\log(\hat{y})\] <p>This means if the predicted probability \(\hat{y}\) is close to 1 (i.e., the model is confident that the class is 1), the loss will be very small (approaching 0). On the other hand, if \(\hat{y}\) is close to 0, the loss becomes large, penalizing the model for being very wrong.</p> </li> </ul> </li> <li> <strong>If \(y = 0\) (True label is 0)</strong>: <ul> <li> <p>The loss is:</p> \[\ell_{\text{Logistic}} = -\log(1 - \hat{y})\] <p>In this case, if the predicted probability \(\hat{y}\) is close to 0 (i.e., the model correctly predicts the class as 0), the loss will be very small (approaching 0). However, if \(\hat{y}\) is close to 1, the loss becomes large, penalizing the model for incorrectly predicting class 1.</p> </li> </ul> </li> </ul> <h6 id="example-of-different-predicted-values">Example of Different Predicted Values:</h6> <ol> <li> <strong>For a true label \(y = 1\):</strong> <ul> <li> <p>If \(\hat{y} = 0.9\): \(\ell_{\text{Logistic}} = -\log(0.9) \approx 0.105\) This is a small loss, since the model predicted a high probability for class 1, which is correct.</p> </li> <li> <p>If \(\hat{y} = 0.1\): \(\ell_{\text{Logistic}} = -\log(0.1) \approx 2.302\) This is a large loss, since the model predicted a low probability for class 1, which is incorrect.</p> </li> </ul> </li> <li> <strong>For a true label \(y = 0\):</strong> <ul> <li> <p>If \(\hat{y} = 0.1\): \(\ell_{\text{Logistic}} = -\log(1 - 0.1) \approx 0.105\) This is a small loss, since the model predicted a low probability for class 1, which is correct.</p> </li> <li> <p>If \(\hat{y} = 0.9\): \(\ell_{\text{Logistic}} = -\log(1 - 0.9) \approx 2.302\) This is a large loss, since the model predicted a high probability for class 1, which is incorrect.</p> </li> </ul> </li> </ol> <h6 id="key-points">Key Points:</h6> <ul> <li>The <strong>negative sign</strong> in the loss function ensures that when the model predicts correctly (i.e., \(\hat{y}\) is close to the true label), the loss is minimized (approaching 0).</li> <li>The loss grows as the predicted probability \(\hat{y}\) moves away from the true label \(y\), and it grows more rapidly as the predicted probability becomes more confident but incorrect.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Binary_Cross_Entropy_Loss-480.webp 480w,/assets/img/Binary_Cross_Entropy_Loss-800.webp 800w,/assets/img/Binary_Cross_Entropy_Loss-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Binary_Cross_Entropy_Loss.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Binary_Cross_Entropy_Loss" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center"> Binary Cross Entropy Loss Function(https://www.desmos.com/calculator/ygciza1leg) </div> <h5 id="logistic-regression-with-labels-as--1-or-1"><strong>Logistic Regression with Labels as -1 or 1</strong></h5> <p>When the labels are in \(\{-1, 1\}\), the sigmoid function simplifies using the property: \(1 - \sigma(z) = \sigma(-z).\)</p> <p>This allows us to express the loss equivalently as:</p> \[\ell_{\text{Logistic}} = \begin{cases} -\log(\sigma(z)) &amp; \text{if } y = 1, \\ -\log(\sigma(-z)) &amp; \text{if } y = -1. \end{cases}\] <p>Simplifying further:</p> \[\ell_{\text{Logistic}} = -\log(\sigma(yz)) = -\log\left(\frac{1}{1 + e^{-yz}}\right) = \log(1 + e^{-m})\] <p>where \(m = yz\) is the margin.</p> <h6 id="key-insights-of-logistic-loss"> <strong>Key Insights of Logistic loss</strong>:</h6> <ul> <li>Is differentiable, enabling gradient-based optimization.</li> <li>Always rewards larger margins, encouraging more confident predictions.</li> <li>Never becomes zero, ensuring continuous optimization pressure.</li> </ul> <h6 id="what-about-square-loss-for-classification"><strong>What About Square Loss for Classification?</strong></h6> <p>Square loss, while effective for regression, is rarely used for classification. Let’s break it down:</p> \[\ell(f(x), y) = (f(x) - y)^2\] <p>For binary classification where \(y \in \{-1, 1\}\), we can rewrite this in terms of the margin:</p> \[\ell(f(x), y) = (f(x) - y)^2 = f^2(x) - 2f(x)y + y^2.\] <p>Using the fact that \(y^2 = 1\):</p> \[\ell(f(x), y) = f^2(x) - 2f(x)y + 1 = (1 - f(x)y)^2 = (1 - m)^2.\] <h6 id="why-not-use-square-loss"><strong>Why Not Use Square Loss?</strong></h6> <p>Square loss heavily penalizes outliers, such as mislabeled examples, making it unsuitable for classification tasks where robust performance on noisy data is crucial.</p> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Logistic_Regression_Losses-480.webp 480w,/assets/img/Logistic_Regression_Losses-800.webp 800w,/assets/img/Logistic_Regression_Losses-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Logistic_Regression_Losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Logistic_Regression_Losses" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center"> Logistic Regression Loss Functions </div> <hr> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>Loss functions form the backbone of machine learning, providing a mathematical framework for optimization. A quick recap:</p> <ul> <li> <strong>Regression Losses</strong>: <ul> <li>Squared (L2) loss: Sensitive to outliers.</li> <li>Absolute (L1) loss: Robust but non-differentiable.</li> <li>Huber loss: Balances robustness and smoothness.</li> </ul> </li> <li> <strong>Classification Losses</strong>: <ul> <li>Hinge loss: Encourages a large positive margin (used in SVMs).</li> <li>Logistic loss: Differentiable and rewards confidence.</li> </ul> </li> </ul> <p>These concepts tie back to critical components of machine learning workflows, such as <strong>gradient descent</strong>, which relies on the properties of loss functions to update model parameters effectively.</p> <p>Up next, we’ll dive into <strong>Regularization</strong>, focusing on how it combats overfitting and improves model performance. Stay tuned!</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>