<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> My Understanding of "Efficient Algorithms for Online Decision Problems" Paper | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala’s (2005) paper, " efficient algorithms for online decision problems. this blog explores how fpl improves decision-making minimizes regret and extends to structured problems like shortest paths adaptive huffman coding.> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?e68e4955e21b20101db6e28a5a50abec"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/FPL-proof/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">My Understanding of "Efficient Algorithms for Online Decision Problems" Paper</h1> <p class="post-meta"> Created in February 05, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/paper"> <i class="fa-solid fa-hashtag fa-sm"></i> Paper</a>   ·   <a href="/blog/category/adv-ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ADV-ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Here is the link to the paper - <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/2005-Efficient_Algorithms_for_Online_Decision_Problems.pdf" rel="external nofollow noopener" target="_blank">Efficient Algorithms for Online Decision Problems</a>. I highly recommend reading through the entire paper once or twice before continuing with this blog.</p> <h4 id="1-introduction"><strong>1. Introduction</strong></h4> <p>This paper explores <strong>online decision problems</strong>, where decisions must be made sequentially without knowing future costs. The goal is to ensure that the total cost incurred is <strong>close to the best fixed decision in hindsight</strong>.</p> <h5 id="key-contributions"><strong>Key Contributions:</strong></h5> <ul> <li>Introducing <strong>Follow the Perturbed Leader (FPL)</strong>, a computationally efficient alternative to <strong>exponential weighting</strong> methods like Weighted Majority.</li> <li>Extending FPL to <strong>structured problems</strong> (e.g., <strong>shortest paths</strong>, <strong>tree-based search</strong>, <strong>adaptive Huffman coding</strong>).</li> <li>Demonstrating that FPL achieves <strong>low regret</strong> while remaining computationally efficient.</li> <li>Showing how FPL can be applied <strong>even when exact offline solutions are infeasible</strong>, by leveraging approximation algorithms.</li> </ul> <h5 id="2-online-decision-problem-setup"><strong>2. Online Decision Problem Setup</strong></h5> <p><strong>Problem Definition</strong></p> <ul> <li>At each time step \(t\), a <strong>cost vector</strong> \(s_t\) is revealed.</li> <li>The algorithm picks a <strong>decision \(d_t\)</strong> from a set \(D\).</li> <li> <p>The objective is to minimize the cumulative cost: \(\sum_{t=1}^{T} d_t \cdot s_t\) compared to the <strong>best single decision in hindsight</strong>:</p> \[\min_{d \in D} \sum_{t=1}^{T} d \cdot s_t.\] </li> </ul> <p><strong>Example: The Experts Problem</strong></p> <ul> <li>Suppose there are <strong>\(n\) experts</strong> providing recommendations.</li> <li>Each expert incurs a cost at each time step.</li> <li>The goal is to perform <strong>as well as the best expert</strong>.</li> </ul> <p>Traditional solutions use <strong>exponential weighting</strong> but are computationally expensive. <strong>FPL</strong> provides a <strong>simpler and faster</strong> alternative.</p> <h5 id="3-follow-the-perturbed-leader-fpl-algorithm"><strong>3. Follow the Perturbed Leader (FPL) Algorithm</strong></h5> <p><strong>Key Idea</strong></p> <ul> <li>Instead of following the exact best decision so far (<strong>Follow the Leader, FTL</strong>), which can lead to excessive switching, we introduce <strong>random perturbations</strong>.</li> <li>This smooths decision-making and prevents adversarial manipulation.</li> </ul> <p><strong>Algorithm</strong></p> <ol> <li>Compute cumulative costs for each decision: \(c_t(e) = \sum_{\tau=1}^{t} s_\tau(e)\)</li> <li>Add <strong>random perturbation</strong> \(p_t(e)\) drawn from an exponential distribution: \(\tilde{c}_t(e) = c_t(e) + p_t(e).\)</li> <li>Choose the decision <strong>with the lowest perturbed cost</strong>.</li> </ol> <p><strong>Why Does This Work?</strong></p> <ul> <li>Reduces <strong>frequent switching</strong> of decisions.</li> <li>Makes the algorithm <strong>less predictable</strong> to adversarial environments.</li> <li>Maintains a <strong>low regret bound</strong>.</li> </ul> <h5 id="4-regret-analysis-of-fpl"><strong>4. Regret Analysis of FPL</strong></h5> <p>The regret measures how much worse FPL is compared to the <strong>best decision in hindsight</strong>.</p> \[E[\text{Cost of FPL}] - \min_{d \in D} \sum_{t=1}^{T} d \cdot s_t.\] <p><strong>Step 1: The “Be the Leader” Algorithm</strong></p> <ul> <li>The <strong>hypothetical</strong> “Be the Leader” algorithm always picks the best decision so far: \(d_t = M(s_{1:t})\)</li> <li>It incurs <strong>zero regret</strong>: \(\sum_{t=1}^{T} M(s_{1:t}) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T}.\)</li> <li>However, it <strong>switches too often</strong>.</li> </ul> <p><strong>Step 2: Introducing Perturbations</strong></p> <p>FPL <strong>adds perturbations</strong> to cumulative costs before selecting the leader: \(M(s_{1:t-1} + p_t).\) The regret bound becomes:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + \eta R A T + D / \eta.\] <p><strong>Step 3: Choosing the Optimal Perturbation Scale</strong></p> <p>To balance stability and adaptation, we set: \(\eta = \sqrt{\frac{D}{RAT}}.\) This leads to the <strong>final regret bound</strong>: \(O(\sqrt{T}).\) This ensures <strong>sublinear regret</strong>, meaning FPL performs nearly as well as the best expert over time.</p> <h5 id="5-applying-fpl-to-structured-problems"><strong>5. Applying FPL to Structured Problems</strong></h5> <p>FPL can be extended to <strong>problems beyond expert selection</strong>, such as <strong>shortest paths</strong>.</p> <p><strong>Online Shortest Paths Problem</strong></p> <ul> <li>Given a <strong>graph</strong> with edge costs changing over time.</li> <li>Each round, the algorithm must select a <strong>path from source \(s\) to destination \(t\)</strong>.</li> <li>The naive approach (treating paths as independent experts) is <strong>computationally infeasible</strong>.</li> </ul> <p><strong>Efficient Approach: FPL at the Edge Level</strong></p> <ol> <li>Instead of applying FPL to entire paths, apply <strong>perturbations at the edge level</strong>.</li> <li>Compute <strong>perturbed edge costs</strong>: \(\tilde{c}_t(e) = c_t(e) + p_t(e).\)</li> <li> <strong>Compute the shortest path</strong> based on these perturbed edge costs.</li> <li>Select the <strong>shortest path</strong>.</li> </ol> <p><strong>Why Does This Work?</strong></p> <ul> <li> <strong>Avoids exponential complexity</strong> by working at the edge level.</li> <li> <strong>Efficiently computed</strong> using shortest path algorithms (e.g., Dijkstra’s).</li> <li> <strong>Regret bound remains low</strong>: \((1 + \epsilon) \times (\text{Best Path Cost}) + O(m n \log n).\)</li> </ul> <p><strong>Other Structured Problems</strong></p> <p>FPL has also been applied to:</p> <ul> <li> <strong>Tree search</strong>: Efficiently updating search trees.</li> <li> <strong>Adaptive Huffman coding</strong>: Dynamically optimizing prefix codes.</li> <li> <strong>Online approximation algorithms</strong>: Extending FPL when exact offline solutions are infeasible.</li> </ul> <h5 id="6-summary"><strong>6. Summary</strong></h5> <ul> <li>FPL is a simple, efficient alternative to exponential weighting methods.</li> <li>It achieves regret \(O(\sqrt{T})\), ensuring performance close to the best decision in hindsight.</li> <li>The perturbation method generalizes to structured problems like shortest paths.</li> <li>FPL is computationally feasible even when the number of decisions is large.</li> </ul> <hr> <blockquote> <p>Follow Up Questions;</p> </blockquote> <h4 id="understanding-the-additive-analysis-and-regret-bounds"><strong>Understanding the Additive Analysis and Regret Bounds</strong></h4> <p>The <strong>Additive Analysis</strong> section in the paper derives a regret bound for the <strong>Follow the Perturbed Leader (FPL)</strong> algorithm. The key goal is to compare the <strong>cumulative cost of FPL</strong> to the <strong>best fixed decision in hindsight</strong>.</p> <h5 id="key-notation"><strong>Key Notation</strong></h5> <p>Before deriving the regret bound, let’s clarify the notation:</p> <ul> <li>\(s_t\): <strong>State (cost vector) at time \(t\)</strong> <ul> <li>At each time step, we observe a cost vector \(s_t\), where each component represents the cost of a different decision.</li> </ul> </li> <li>\(d_t\): <strong>Decision made at time \(t\)</strong> <ul> <li>The action chosen by the algorithm at time \(t\).</li> </ul> </li> <li>\(M(x)\): <strong>Best fixed decision in hindsight</strong> <ul> <li>Given a total cost vector \(x\), \(M(x)\) returns the best decision: \(M(x) = \arg\min_{d \in D} d \cdot x\)</li> </ul> </li> <li>\(s_{1:T}\): <strong>Total cost vector over all \(T\) time steps</strong> <ul> <li>This is simply the sum of all cost vectors: \(s_{1:T} = s_1 + s_2 + \dots + s_T\)</li> </ul> </li> <li>\(p_t\): <strong>Random perturbation added at time \(t\)</strong> <ul> <li>Introduced to smooth out decision-making and prevent frequent switches.</li> </ul> </li> </ul> <h5 id="goal-regret-minimization"><strong>Goal: Regret Minimization</strong></h5> <p>We define regret as the difference between FPL’s total cost and the best fixed decision in hindsight:</p> \[E \left[ \sum_{t=1}^{T} d_t \cdot s_t \right] - \min_{d \in D} \sum_{t=1}^{T} d \cdot s_t.\] <p><strong>Step 1: The Hypothetical “Be the Leader” Algorithm</strong></p> <p>To analyze FPL, we first consider a <strong>hypothetical “Be the Leader” algorithm</strong>, which always picks the <strong>best decision so far</strong>:</p> \[d_t = M(s_{1:t}).\] <p>The key property of this algorithm is:</p> \[\sum_{t=1}^{T} M(s_{1:t}) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T}.\] <p><strong>Why Does This Hold?</strong></p> <ul> <li>The <strong>best decision in hindsight</strong> is optimal for the full sequence.</li> <li>If we could always “be the leader,” we would incur <strong>zero regret</strong>.</li> <li>However, this algorithm <strong>switches too frequently</strong>, making it unstable.</li> </ul> <p><strong>Step 2: Introducing Perturbations</strong></p> <p>FPL smooths out decision-making by adding <strong>random perturbations</strong> before selecting the leader:</p> \[M(s_{1:t-1} + p_t).\] <p>The analysis shows:</p> \[\sum_{t=1}^{T} M(s_{1:t} + p_t) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T} + D \sum_{t=1}^{T} |p_t - p_{t-1}|_\infty.\] <p><strong>Key Insight</strong></p> <ul> <li>The first term on the RHS is the cost of the <strong>best decision in hindsight</strong>.</li> <li>The second term represents the <strong>additional cost due to perturbations</strong>.</li> <li>This term grows at most as \(O(\sqrt{T})\), ensuring that regret remains <strong>sublinear</strong>.</li> </ul> <p><strong>Step 3: Bounding the Impact of Perturbations</strong></p> <p>The final step is to bound the effect of perturbations:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + \eta R A T + D / \eta.\] <p>where:</p> <ul> <li>\(\eta\) is a tuning parameter controlling the size of perturbations.</li> <li>\(R, A, D\) are problem-dependent constants.</li> </ul> <p><strong>Choosing the Optimal \(\eta\)</strong></p> <p>To minimize regret, we set:</p> \[\eta = \sqrt{\frac{D}{RAT}}.\] <p>Plugging this into the regret bound:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + 2 \sqrt{DRAT}.\] <p><strong>Final Regret Bound</strong> \(O(\sqrt{T}).\)</p> <p>This ensures <strong>sublinear regret</strong>, meaning that over time, <strong>FPL performs nearly as well as the best fixed decision</strong>.</p> <p><strong>Key Takeaways</strong></p> <ol> <li>“Be the Leader” is optimal but switches too often.</li> <li>FPL adds perturbations to smooth decision-making.</li> <li>The regret bound is controlled by the trade-off between making stable choices and avoiding excessive randomness.</li> <li>Choosing \(\eta\) optimally gives a regret bound of \(O(\sqrt{T})\), ensuring long-term efficiency.</li> </ol> <hr> <h4 id="understanding-the-key-notation-and-be-the-leader-algorithm"><strong>Understanding the Key Notation and “Be the Leader” Algorithm</strong></h4> <p>This section provides a clear explanation of the <strong>Key Notation</strong> and the <strong>“Be the Leader” algorithm</strong> used in the <strong>Additive Analysis</strong> of the paper.</p> <h5 id="1-key-notation-clarified"><strong>1. Key Notation (Clarified)</strong></h5> <p>To understand the regret bound derivation, let’s first clarify the key mathematical symbols.</p> <h5 id="online-decision-problem-setup"><strong>Online Decision Problem Setup</strong></h5> <p>In an <strong>online decision problem</strong>, we repeatedly make decisions without knowing future costs. Our goal is to minimize <strong>total cost</strong> over time.</p> <hr> <table> <thead> <tr> <th>Symbol</th> <th>Definition</th> </tr> </thead> <tbody> <tr> <td>\(s_t\)</td> <td> <strong>State (cost vector) at time \(t\)</strong>, representing the cost of each decision at step \(t\).</td> </tr> <tr> <td>\(d_t\)</td> <td> <strong>Decision chosen at time \(t\)</strong> (e.g., selecting an expert or a path).</td> </tr> <tr> <td>\(M(x)\)</td> <td> <strong>Best fixed decision in hindsight</strong>, meaning the best decision if we knew all costs in advance.</td> </tr> <tr> <td>\(s_{1:T}\)</td> <td> <strong>Total cost vector over \(T\) rounds</strong>, defined as \(s_{1:T} = \sum_{t=1}^{T} s_t\).</td> </tr> <tr> <td>\(p_t\)</td> <td> <strong>Random perturbation added at time \(t\)</strong> to smooth decision-making.</td> </tr> </tbody> </table> <hr> <h5 id="example-experts-problem"><strong>Example: Experts Problem</strong></h5> <ul> <li>Suppose we are choosing between <strong>two experts</strong> (A and B).</li> <li>Each expert has a different cost at each time step.</li> <li>We want to <strong>pick the expert that minimizes the total cost over time</strong>.</li> </ul> <hr> <table> <thead> <tr> <th>Time \(t\)</th> <th>Expert A’s Cost \(s_t(A)\)</th> <th>Expert B’s Cost \(s_t(B)\)</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> </tr> <tr> <td>\(t = 2\)</td> <td>0.5</td> <td>0.2</td> </tr> <tr> <td>\(t = 3\)</td> <td>0.4</td> <td>0.5</td> </tr> <tr> <td>\(t = 4\)</td> <td>0.2</td> <td>0.3</td> </tr> </tbody> </table> <hr> <ul> <li> <strong>Without perturbations</strong>, we would always select the expert with the lowest cumulative cost.</li> <li>However, <strong>this can cause excessive switching</strong>, which leads to instability.</li> </ul> <p><strong>2. Step 1: The “Be the Leader” Algorithm</strong></p> <p>The <strong>“Be the Leader” algorithm</strong> is a <strong>hypothetical strategy</strong> where we always choose the <strong>best decision so far</strong>.</p> <p><strong>How It Works</strong></p> <p>At time \(t\), select:</p> \[d_t = M(s_{1:t}),\] <p>meaning:</p> <ul> <li>Choose the <strong>decision that has had the lowest total cost so far</strong>.</li> <li>This ensures <strong>no regret</strong> because we are always picking the best option up to that point.</li> </ul> <p><strong>Key Property</strong></p> \[\sum_{t=1}^{T} M(s_{1:t}) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T}.\] <p><strong>Why is this true?</strong></p> <ol> <li>The <strong>best decision in hindsight</strong> is optimal for the full sequence.</li> <li>If we could always “be the leader,” we would incur <strong>zero regret</strong>.</li> <li>However, this algorithm <strong>switches decisions too frequently</strong>, making it unstable.</li> </ol> <p><strong>Example Calculation</strong></p> <hr> <table> <thead> <tr> <th>Time \(t\)</th> <th>Cumulative Cost \(A\)</th> <th>Cumulative Cost \(B\)</th> <th>Chosen Expert</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> <td><strong>A</strong></td> </tr> <tr> <td>\(t = 2\)</td> <td>0.8</td> <td>0.6</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 3\)</td> <td>1.2</td> <td>1.1</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 4\)</td> <td>1.4</td> <td>1.4</td> <td><strong>Switching rapidly!</strong></td> </tr> </tbody> </table> <hr> <ul> <li>The leader <strong>switches frequently</strong> whenever cumulative costs change slightly.</li> <li>This is problematic, especially in <strong>adversarial settings</strong>, because an adversary can force unnecessary switches.</li> </ul> <p><strong>3. Why Do We Need Perturbations?</strong></p> <p>The <strong>“Be the Leader”</strong> algorithm <strong>switches too often</strong>, making it inefficient.</p> <p><strong>Follow the Perturbed Leader (FPL) Fixes This</strong></p> <p>To avoid excessive switching, <strong>FPL adds random perturbations</strong> before selecting the leader:</p> \[M(s_{1:t-1} + p_t).\] <p>This <strong>smooths out decision-making</strong>:</p> <ul> <li> <strong>Prevents rapid switches</strong> caused by small cost changes.</li> <li> <strong>Balances stability and adaptability</strong>.</li> <li> <strong>Maintains a low regret bound</strong>.</li> </ul> <p><strong>Key Insight</strong></p> <ul> <li> <strong>FPL ensures that decisions do not fluctuate excessively</strong>.</li> <li> <strong>Adding perturbations leads to a regret bound of \(O(\sqrt{T})\), ensuring long-term efficiency</strong>.</li> </ul> <hr> <h5 id="understanding-step-2-and-step-3-in-additive-analysis"><strong>Understanding Step 2 and Step 3 in Additive Analysis</strong></h5> <p>In the <strong>Additive Analysis</strong> section, Steps 2 and 3 are crucial for deriving the regret bound for <strong>Follow the Perturbed Leader (FPL)</strong>. These steps show how perturbations help smooth decision-making while maintaining low regret.</p> <hr> <p><strong>Step 2: Introducing Perturbations</strong></p> <p>The issue with the <strong>“Be the Leader”</strong> algorithm is that it <strong>switches too frequently</strong>, leading to instability. To fix this, <strong>Follow the Perturbed Leader (FPL)</strong> <strong>adds small random perturbations</strong> to past costs before selecting the leader:</p> \[d_t = M(s_{1:t-1} + p_t).\] <p><strong>Effect of Perturbations</strong></p> <p>Instead of choosing the decision with the exact lowest cumulative cost, FPL selects the decision <strong>with the lowest perturbed cost</strong>:</p> \[\tilde{c}_t(e) = c_t(e) + p_t(e).\] <p>This ensures:</p> <ol> <li> <strong>Fewer unnecessary switches</strong>: Small cost fluctuations no longer cause frequent decision changes.</li> <li> <strong>Better robustness against adversarial cost sequences</strong>.</li> </ol> <p><strong>Key Inequality</strong></p> <p>The analysis shows:</p> \[\sum_{t=1}^{T} M(s_{1:t} + p_t) \cdot s_t \leq M(s_{1:T}) \cdot s_{1:T} + D \sum_{t=1}^{T} |p_t - p_{t-1}|_\infty.\] <p><strong>Breaking It Down</strong></p> <ul> <li> <strong>LHS</strong>: The total cost incurred by FPL.</li> <li> <strong>RHS</strong>: The total cost of the best decision in hindsight <strong>plus an extra term due to perturbations</strong>.</li> <li> <strong>The second term</strong> captures the additional cost introduced by randomness.</li> </ul> <p>Since perturbations are drawn from a well-chosen distribution, their effect remains <strong>small</strong> (bounded by \(O(\sqrt{T})\)).</p> <p><strong>Step 3: Bounding the Impact of Perturbations</strong></p> <p>The final step is to <strong>quantify how much extra cost perturbations introduce</strong>.</p> <p>The key regret bound derived is:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + \eta R A T + D / \eta.\] <p>where:</p> <ul> <li>\(\eta\) controls <strong>how large the perturbations are</strong>.</li> <li>\(R, A, D\) are constants depending on the problem setup.</li> </ul> <p><strong>Choosing the Optimal Perturbation Scale</strong></p> <p>To balance stability and adaptation, they choose:</p> \[\eta = \sqrt{\frac{D}{RAT}}.\] <p>Plugging this into the regret bound:</p> \[E[\text{Cost of FPL}] \leq \min_{\text{fixed decision } d} \sum_{t=1}^{T} d \cdot s_t + 2 \sqrt{DRAT}.\] <p><strong>Final Regret Bound</strong> \(O(\sqrt{T}).\)</p> <p>This means that <strong>the regret grows sublinearly</strong>, ensuring that over time, <strong>FPL performs nearly as well as the best fixed decision</strong>.</p> <hr> <h5 id="understanding-fpl-with-a-worked-out-example"><strong>Understanding FPL with a Worked-Out Example</strong></h5> <p>To better understand <strong>Follow the Perturbed Leader (FPL)</strong>, let’s go through a <strong>step-by-step numerical example</strong>.</p> <p><strong>Problem Setup</strong></p> <ul> <li>We have <strong>two experts</strong>: <strong>A</strong> and <strong>B</strong>.</li> <li>Each expert incurs a cost at each time step.</li> <li>We must <strong>pick one expert per round</strong> without knowing future costs.</li> <li>Our goal is to minimize <strong>total cost over \(T = 4\) rounds</strong>.</li> </ul> <p><strong>Cost Sequence</strong></p> <hr> <table> <thead> <tr> <th>Time \(t\)</th> <th>Expert A’s Cost \(s_t(A)\)</th> <th>Expert B’s Cost \(s_t(B)\)</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> </tr> <tr> <td>\(t = 2\)</td> <td>0.5</td> <td>0.2</td> </tr> <tr> <td>\(t = 3\)</td> <td>0.4</td> <td>0.5</td> </tr> <tr> <td>\(t = 4\)</td> <td>0.2</td> <td>0.3</td> </tr> </tbody> </table> <hr> <ul> <li> <strong>Without perturbations</strong>, FPL would always pick the expert with the lowest cumulative cost.</li> <li>However, this leads to <strong>frequent switching</strong>.</li> </ul> <p><strong>Step 1: Follow the Leader (FTL) - No Perturbation</strong></p> <p>If we naively follow the leader <strong>without perturbations</strong>, we get:</p> <table> <thead> <tr> <th>Time \(t\)</th> <th>Cumulative Cost \(A\)</th> <th>Cumulative Cost \(B\)</th> <th>Chosen Expert</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> <td><strong>A</strong></td> </tr> <tr> <td>\(t = 2\)</td> <td>0.8</td> <td>0.6</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 3\)</td> <td>1.2</td> <td>1.1</td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 4\)</td> <td>1.4</td> <td>1.4</td> <td><strong>Switching rapidly!</strong></td> </tr> </tbody> </table> <p><strong>Problem with FTL</strong></p> <ul> <li>The algorithm <strong>switches too frequently</strong>, making it unstable.</li> <li>Small cost differences cause unnecessary <strong>leader changes</strong>.</li> <li>This is <strong>bad in adversarial settings</strong>, where cost sequences can be manipulated.</li> </ul> <p><strong>Step 2: Follow the Perturbed Leader (FPL) - Adding Perturbations</strong></p> <p>Now, let’s <strong>add perturbations</strong>.</p> <ul> <li>We <strong>randomly sample perturbations</strong> \(p_t(A)\) and \(p_t(B)\) from an <strong>exponential distribution</strong>.</li> <li>Suppose we get: \(p_1(A) = 0.1, \quad p_1(B) = 0.2\)</li> </ul> <p><strong>Step 2.1: Compute Perturbed Costs</strong></p> <hr> <table> <thead> <tr> <th>Time \(t\)</th> <th>Cumulative Cost \(A\)</th> <th>Cumulative Cost \(B\)</th> <th>Perturbed \(A\)</th> <th>Perturbed \(B\)</th> <th>Chosen Expert</th> </tr> </thead> <tbody> <tr> <td>\(t = 1\)</td> <td>0.3</td> <td>0.4</td> <td><strong>0.4</strong></td> <td><strong>0.6</strong></td> <td><strong>A</strong></td> </tr> <tr> <td>\(t = 2\)</td> <td>0.8</td> <td>0.6</td> <td><strong>0.9</strong></td> <td><strong>0.8</strong></td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 3\)</td> <td>1.2</td> <td>1.1</td> <td><strong>1.3</strong></td> <td><strong>1.2</strong></td> <td><strong>B</strong></td> </tr> <tr> <td>\(t = 4\)</td> <td>1.4</td> <td>1.4</td> <td><strong>1.5</strong></td> <td><strong>1.6</strong></td> <td><strong>A</strong></td> </tr> </tbody> </table> <hr> <p><strong>Step 2.2: What Changed?</strong></p> <ul> <li> <strong>FPL smooths out decisions</strong>: The perturbations prevent unnecessary switching.</li> <li> <strong>Perturbations reduce instability</strong>: Instead of switching too frequently (like FTL), FPL <strong>stabilizes</strong>.</li> <li><strong>Better stability → Lower regret!</strong></li> </ul> <p><strong>Step 3: Computing the Regret</strong></p> <p><strong>Step 3.1: Compute Cost of FPL</strong></p> <p>The total cost incurred by <strong>FPL</strong>: \(\sum_{t=1}^{T} \text{Chosen Expert's Cost}\) Using the decisions above:</p> \[\text{Total Cost of FPL} = 0.3 + 0.2 + 0.5 + 0.2 = 1.2.\] <p><strong>Step 3.2: Compute Cost of Best Expert in Hindsight</strong></p> <p>If we had <strong>perfect hindsight</strong>, we would choose the best expert who had the <strong>lowest total cost over all \(T\) rounds</strong>.</p> \[\text{Total Cost of Best Expert} = \min(1.4, 1.4) = 1.4.\] <p><strong>Step 3.3: Compute Regret</strong></p> <p>Regret is the difference between FPL and the best fixed decision:</p> \[\text{Regret} = E[\text{Cost of FPL}] - \text{Cost of Best Expert}.\] <p>Since FPL <strong>performs better than the best single expert</strong>, it actually has <strong>negative regret in this case!</strong> In general, the regret is <strong>bounded by \(O(\sqrt{T})\), ensuring FPL converges to the best decision in hindsight over time.</strong></p> <hr> <h5 id="extending-fpl-to-structured-problems-online-shortest-paths"><strong>Extending FPL to Structured Problems: Online Shortest Paths</strong></h5> <p>One of the major contributions of the paper is extending <strong>Follow the Perturbed Leader (FPL)</strong> beyond the <strong>experts setting</strong> to more <strong>structured problems</strong>, such as <strong>online shortest paths</strong>. This is important because treating every possible path as an independent expert is computationally infeasible when the number of paths is exponential in the number of edges.</p> <p><strong>1. The Online Shortest Path Problem</strong></p> <p><strong>Problem Setup</strong></p> <ul> <li>Given a <strong>directed graph</strong> with \(n\) nodes and \(m\) edges.</li> <li>Each edge \(e\) has a time cost \(c_t(e)\) at each time step \(t\).</li> <li>The goal is to <strong>select a path from source \(s\) to destination \(t\)</strong> at each time step without knowing future costs.</li> <li>After selecting a path, we observe the costs of all edges.</li> </ul> <p><strong>Objective</strong></p> <p>We want to ensure that the total travel time over \(T\) rounds is close to the best <strong>single</strong> path in hindsight.</p> <p><strong>Challenges</strong></p> <ul> <li>The number of possible paths grows exponentially with the number of nodes, making <strong>treating paths as experts infeasible</strong>.</li> <li>Instead of treating whole paths as independent decisions, we need a way to <strong>apply FPL efficiently at the edge level</strong>.</li> </ul> <p><strong>2. Applying FPL to Online Shortest Paths</strong></p> <p><strong>Naïve Approach (Infeasible)</strong></p> <ul> <li>If we were to apply <strong>vanilla FPL</strong> directly, we would: <ol> <li>Treat each <strong>entire path</strong> as an “expert.”</li> <li>Maintain cumulative travel time for every path.</li> <li>Apply perturbations to total path costs.</li> <li>Choose the best path.</li> </ol> </li> <li> <strong>Problem:</strong> The number of paths grows exponentially, making this computationally <strong>infeasible</strong>.</li> </ul> <p><strong>Efficient Approach: FPL at the Edge Level</strong></p> <p>To make FPL work efficiently, we <strong>apply perturbations to edges instead of whole paths</strong>:</p> <p><strong>Follow the Perturbed Leading Path (FPL for Paths)</strong></p> <p><strong>At each time step \(t\):</strong></p> <ol> <li> <strong>For each edge \(e\)</strong>, draw a random perturbation \(p_t(e)\) from an exponential distribution.</li> <li>Compute <strong>perturbed edge costs</strong>: \(\tilde{c}_t(e) = c_t(e) + p_t(e)\)</li> <li> <strong>Find the shortest path</strong> using these perturbed edge costs.</li> <li>Choose the <strong>shortest path</strong> in the graph based on the perturbed edge weights.</li> </ol> <p><strong>Why Does This Work?</strong></p> <ul> <li>Since perturbations are applied <strong>at the edge level</strong>, we avoid maintaining explicit costs for all paths.</li> <li>The standard <strong>shortest path algorithm (e.g., Dijkstra’s)</strong> can efficiently compute the best path at each step.</li> <li> <p>Theoretical guarantees remain valid: The expected regret is at most:</p> \[(1 + \epsilon) \times (\text{Best Path Cost}) + O(m n \log n)\] <p>where \(m\) is the number of edges and \(n\) is the number of nodes.</p> </li> </ul> <p><strong>3. Understanding the Regret Bound</strong></p> <p>We now derive the <strong>regret bound</strong> for online shortest paths.</p> <p><strong>Key Definitions</strong></p> <ul> <li>Let \(P_t\) be the path chosen by FPL at time \(t\).</li> <li> <p>Let \(P^*\) be the best fixed path in hindsight, i.e., the path with the lowest total cost over \(T\) rounds:</p> \[P^* = \arg\min_{P} \sum_{t=1}^{T} c_t(P).\] </li> <li> <p>The regret of FPL is:</p> \[\sum_{t=1}^{T} c_t(P_t) - \sum_{t=1}^{T} c_t(P^*).\] </li> </ul> <p><strong>Applying FPL Analysis</strong></p> <ul> <li>The perturbation ensures that <strong>bad paths are not chosen too often</strong> and <strong>good paths are discovered quickly</strong>.</li> <li>The additional regret due to perturbation grows as \(O(m n \log n)\), meaning it is still <strong>sublinear in \(T\)</strong>.</li> </ul> <p>Thus, FPL guarantees:</p> \[E[\text{Total Cost of FPL}] \leq (1 + \epsilon) \times \text{Best Path Cost} + O(m n \log n).\] <p><strong>4. Why is This Important?</strong></p> <p><strong>1. Generalization to Graph-Structured Problems</strong></p> <ul> <li>This method can be applied to <strong>any structured problem where the decision space is large</strong>.</li> <li>Example: Instead of treating each full <strong>decision tree</strong> as an expert, perturbations can be applied at <strong>the node level</strong>.</li> </ul> <p><strong>2. Computational Efficiency</strong></p> <ul> <li>Unlike <strong>exponential weighting algorithms</strong> (which require maintaining weights for each path), this approach <strong>only requires standard shortest-path computations</strong>.</li> <li> <strong>Time complexity:</strong> Runs in <strong>\(O(m)\)</strong> (if using Bellman-Ford) or <strong>\(O(m + n \log n)\)</strong> (if using Dijkstra’s).</li> </ul> <p><strong>3. Practical Use Cases</strong></p> <ul> <li> <strong>Network Routing:</strong> Selecting optimal paths in a <strong>dynamic network</strong>.</li> <li> <strong>Robot Navigation:</strong> Choosing paths in a changing environment.</li> <li> <strong>Traffic Prediction:</strong> Adjusting routes based on real-time conditions.</li> </ul> <p><strong>5. Summary</strong></p> <p><strong>Problem</strong>: Online shortest path selection where edge costs change over time.<br> <strong>FPL Extension</strong>: Instead of treating full paths as experts, <strong>apply perturbations to edges</strong>.<br> <strong>Algorithm</strong>:</p> <ol> <li>Add <strong>random noise</strong> to <strong>edge costs</strong>.</li> <li>Compute the <strong>shortest path</strong> with the perturbed costs.</li> <li>Follow that path.<br> <strong>Regret Bound</strong>: <ul> <li> <strong>Competitive with the best fixed path</strong> in hindsight.</li> <li> <strong>Extra cost</strong> due to perturbations is <strong>small</strong> (only \(O(m n \log n)\)).<br> <strong>Key Benefit</strong>: <strong>Works efficiently</strong> even when the number of paths is exponential.</li> </ul> </li> </ol> <hr> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>