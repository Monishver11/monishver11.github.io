<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> On-line to Batch Conversion | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2025/online-to-batch/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">On-line to Batch Conversion</h1> <p class="post-meta"> Created in January 30, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/adv-ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ADV-ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In previous sections, we explored various algorithms for online learning, such as the <strong>Perceptron</strong> and <strong>Winnow algorithms</strong>, analyzing their performance within the <strong>mistake model</strong>—a setting where no assumptions are made about how the training sequence is generated.</p> <p>A natural question arises:</p> <blockquote> <p>Can these algorithms be leveraged to derive hypotheses with <strong>small generalization error</strong> in a standard <strong>stochastic setting</strong>?<br> Moreover, how can the intermediate hypotheses they generate be combined to form an <strong>accurate predictor</strong>?</p> </blockquote> <p>These questions are the focus of this section.</p> <hr> <h5 id="problem-setup"><strong>Problem Setup</strong></h5> <p>We consider a <strong>supervised learning</strong> setting where we have a labeled sample:</p> \[S = \{(x_1, y_1), (x_2, y_2), \dots, (x_T, y_T)\} \subset (X \times Y)^T\] <p>drawn <strong>i.i.d.</strong> from some unknown but fixed distribution \(D\).</p> <p>Let \(H\) be a <strong>hypothesis class</strong> consisting of functions mapping \(X\) to \(Y'\), and let \(L: Y' \times Y \to \mathbb{R}^+\) be a <strong>bounded loss function</strong>, meaning there exists a constant \(M \geq 0\) such that:</p> \[L \leq M.\] <p>An <strong>online learning algorithm</strong> \(A\) sequentially processes the dataset \(S\), generating hypotheses:</p> \[h_1, h_2, \dots, h_T \in H.\] <p>It starts with an initial hypothesis \(h_1\) and updates it after processing each training example \((x_t, y_t)\), for \(t \in [T]\).</p> <hr> <h4 id="regret-and-generalization-error"><strong>Regret and Generalization Error</strong></h4> <p>Before we tackle the question we started with, let’s first clarify why it’s important to address it. Also, let’s define the key terms and concepts to ensure everything is clear and aligned as we proceed.</p> <h5 id="1-regret">1. <strong>Regret:</strong> </h5> <p>The <strong>regret</strong> of an algorithm is a measure of how much worse the algorithm’s performance is compared to the best possible hypothesis in the class \(H\), in hindsight.</p> <p>Formally, it’s defined as:</p> \[R_T = \sum_{t=1}^{T} L(h_t(x_t), y_t) - \min_{h \in H} \sum_{t=1}^{T} L(h(x_t), y_t)\] <p>Let’s break this down:</p> <ul> <li>\(h_t(x_t)\) is the prediction of the algorithm at time \(t\), after processing the \(t\)-th training example \((x_t, y_t)\).</li> <li>The term \(\sum_{t=1}^{T} L(h_t(x_t), y_t)\) is the total loss the algorithm incurs by predicting \(h_t(x_t)\) for each training example.</li> <li>\(\min_{h \in H} \sum_{t=1}^{T} L(h(x_t), y_t)\) represents the loss of the best hypothesis \(h \in H\) if it were chosen in advance and used to predict every \(x_t\) in the sequence.</li> </ul> <p><strong>Why regret matters:</strong><br> Regret tells us how far off the algorithm’s performance is compared to the best possible performance it could have had with a perfect hypothesis. If the regret is small, it means the algorithm is making predictions that are close to the best possible predictions, at least in terms of total loss.</p> <h5 id="2-generalization-error">2. <strong>Generalization Error:</strong> </h5> <p>The <strong>generalization error</strong> of a hypothesis \(h \in H\) refers to how well it performs on unseen data from the same distribution \(D\), and it’s typically measured by the <strong>expected loss</strong>:</p> \[R(h) = \mathbb{E}_{(x,y) \sim D} [L(h(x), y)]\] <p>In other words, the generalization error measures how well the hypothesis \(h\) performs on future data (or on data drawn from the same distribution) as opposed to just the training data. This is crucial because we want our hypothesis to not just perform well on the training set (which can be overfit) but also to generalize well to new, unseen examples.</p> <p><strong>How does this tie into the problem setup?</strong><br> The goal in this section is to bound the <strong>average generalization error</strong> of the sequence of hypotheses \(h_1, h_2, \dots, h_T\) generated by the algorithm \(A\). You want to know if the algorithm can generate hypotheses whose <strong>average loss</strong> on the training dataset \(S\) is a good predictor of their <strong>generalization error</strong>—meaning, will the algorithm’s performance on the training set reflect its performance on unseen data?</p> <h5 id="connecting-regret-to-generalization-error"><strong>Connecting Regret to Generalization Error:</strong></h5> <p>Now, let’s dive into the key insight of this section:</p> <ul> <li>The algorithm generates a sequence of hypotheses \(h_1, h_2, \dots, h_T\), but we don’t necessarily care about each individual hypothesis. Rather, we care about the <strong>average performance</strong> over all of them.</li> </ul> <p>Let’s assume the <strong>average loss</strong> of the hypotheses on the training data is:</p> \[\hat{L} = \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t)\] <p>The idea is that, if the average loss \(\hat{L}\) is small (i.e., the algorithm has done well on the training set), then the <strong>generalization error</strong> (i.e., the expected loss on unseen data) should also be small. The question here is how small, and whether we can bound this generalization error using the average loss on the training set.</p> <h5 id="key-insights-and-the-challenge"><strong>Key Insights and the Challenge:</strong></h5> <p>The big challenge in this problem setup is to establish a <strong>connection between regret and generalization error</strong>. Specifically, you are interested in showing that even though the algorithm has no prior knowledge of the distribution \(D\), the average loss of the hypotheses it generates will give you a good idea of their <strong>generalization performance</strong>.</p> <h5 id="how-do-we-approach-this"><strong>How do we approach this?</strong></h5> <p>To answer the question of how to use the average loss to bound the generalization error, we might use tools from <strong>PAC learning</strong> (Probably Approximately Correct) theory, where we can bound the generalization error of a hypothesis by looking at its empirical loss on the training set and how much variability exists due to the randomness in the sampling process.</p> <p>In simple terms:</p> <ul> <li>If the training loss is small, can we show that the hypothesis will likely also perform well on unseen data?</li> <li>Can we bound how much the training loss differs from the generalization error for the hypotheses \(h_1, h_2, \dots, h_T\)?</li> </ul> <p>This section would likely explore these questions in greater detail, using concepts such as <strong>concentration inequalities</strong> (e.g., Hoeffding’s inequality) or tools from statistical learning theory (e.g., uniform convergence) to establish these bounds.</p> <h5 id="key-takeaways"><strong>Key Takeaways:</strong></h5> <ul> <li> <strong>Regret</strong> measures how well the algorithm’s performance compares to the best possible hypothesis over the sequence.</li> <li> <strong>Generalization error</strong> measures how well a hypothesis performs on new, unseen data.</li> <li>The main goal is to relate the <strong>average loss</strong> of the hypotheses generated by the online algorithm to their <strong>generalization error</strong>, so that we can prove the algorithm produces good generalizers, not just good fitters to the training data.</li> </ul> <p>Alright, we’re all set! Now, let’s take a look at the actual bounds that are available.</p> <hr> <h4 id="generalization-error-bound"><strong>Generalization Error Bound</strong></h4> <p>The following lemma provides a bound on the average generalization error in terms of the empirical loss:</p> <h5 id="lemma-814"><strong>Lemma 8.14</strong></h5> <p>Let \(S = \{(x_1,y_1), ..., (x_T,y_T)\}\) be an i.i.d. sample from \(D\), and let \(h_1, ..., h_T\) be the sequence of hypotheses generated by an online algorithm \(A\) processing \(S\). If the loss function is bounded by \(M\), then for any \(\delta &gt; 0\), with probability at least \(1 - \delta\):</p> \[\frac{1}{T} \sum_{t=1}^{T} R(h_t) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log(1/\delta)}{T}}\] <h5 id="proof-sketch"><strong>Proof Sketch:</strong></h5> <p>We begin by defining the random variable:</p> \[V_t = R(h_t) - L(h_t(x_t), y_t)\] <p>This represents the difference between the <strong>true expected loss</strong> (generalization error) of \(h_t\) and the <strong>empirical loss</strong> observed on the sample \((x_t, y_t)\).</p> <p><strong>Step 1: Establishing Expectation Property</strong></p> <p>Since the data points \((x_t, y_t)\) are drawn <strong>i.i.d.</strong> from the distribution \(D\), the expectation of the empirical loss over the randomness in the sample should match the expected loss:</p> \[\mathbb{E}[L(h_t(x_t), y_t) | h_t] = R(h_t)\] <p>Rearranging this gives:</p> \[\mathbb{E}[V_t | x_1, ..., x_{t-1}] = R(h_t) - \mathbb{E}[L(h_t(x_t), y_t) | h_t] = 0\] <p>Thus, the sequence \(\{V_t\}_{t=1}^{T}\) forms a <strong>martingale difference sequence</strong>, meaning that the expected value of \(V_t\) at any step remains zero given all past observations. This ensures that there is no systematic drift in expectations.</p> <p>This means that knowing past values does not help predict the next value’s expectation—it remains centered around zero. No systematic drift means that the sequence does not consistently increase or decrease over time; it behaves like a fair game in probability.</p> <p><strong>Step 2: Bounding the Range of \(V_t\)</strong></p> <p>Since the loss function is <strong>bounded</strong> by \(M\), we get:</p> \[L(h_t(x_t), y_t) \in [0, M] \quad \Rightarrow \quad V_t = R(h_t) - L(h_t(x_t), y_t) \in [-M, +M]\] <p>This ensures that \(V_t\) is always within a fixed range.</p> <p><strong>Step 3: Applying Azuma’s Inequality</strong></p> <p>Azuma’s inequality states that for a martingale difference sequence \(V_t\) with bounded differences \(\vert V_t \vert \leq M\), the probability of the empirical mean deviating from its expectation satisfies:</p> \[P\left( \frac{1}{T} \sum_{t=1}^{T} V_t \geq \epsilon \right) \leq \exp \left( -\frac{2T\epsilon^2}{4M^2} \right)\] <p>Rearranging the denominator:</p> \[P\left( \frac{1}{T} \sum_{t=1}^{T} V_t \geq \epsilon \right) \leq \exp \left( -\frac{T\epsilon^2}{2M^2} \right)\] <p><strong>Step 4: Solving for \(\epsilon\)</strong></p> <p>We now set the right-hand side equal to \(\delta &gt; 0\):</p> \[\exp \left( -\frac{T\epsilon^2}{2M^2} \right) = \delta\] <p>Taking the natural logarithm on both sides:</p> \[-\frac{T\epsilon^2}{2M^2} = \log \delta\] <p>Solving for \(\epsilon\):</p> \[\epsilon = M \sqrt{\frac{2 \log(1/\delta)}{T}}\] <p><strong>Step 5: Concluding the Bound</strong></p> <p>Since \(\frac{1}{T} \sum_{t=1}^{T} V_t\) represents the difference between the <strong>average generalization error</strong> and the <strong>empirical average loss</strong>, we obtain:</p> \[\frac{1}{T} \sum_{t=1}^{T} R(h_t) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log(1/\delta)}{T}}\] <p>This completes the proof.</p> <hr> <p>Before we move forward, let’s break down what all of this means and how we’re using it to reach our desired outcome.</p> <h5 id="why-does-the-martingale-difference-property-allow-us-to-apply-azumas-inequality"><strong>Why does the martingale difference property allow us to apply Azuma’s inequality?</strong></h5> <ul> <li>The key idea of a <strong>martingale difference sequence</strong> is that at each step, the expectation of the difference \(V_t\) is <strong>zero</strong> given past observations.</li> <li>This means that on average, the sequence doesn’t drift systematically in any direction—it’s <strong>balanced</strong> in expectation.</li> <li>Moreover, since each \(V_t\) is <strong>bounded</strong> within \([-M, M]\), there are no extreme jumps.</li> <li> <strong>Azuma’s inequality</strong> applies precisely in such cases—it tells us that despite small fluctuations at each step, the <strong>cumulative deviation from zero</strong> remains <strong>small with high probability</strong>.</li> </ul> <p>💡 <strong>Analogy:</strong> Imagine you’re walking on a balance beam with a safety harness.</p> <ul> <li>You might take small steps to the left or right, but <strong>each step is unbiased</strong>, meaning you’re not favoring one direction over time.</li> <li>Also, your steps are limited in size (bounded).</li> <li><strong>Azuma’s inequality is like saying that, with high probability, you won’t drift too far from the center because of these properties.</strong></li> </ul> <h5 id="how-does-this-help-bound-generalization-error"><strong>How does this help bound generalization error?</strong></h5> <ul> <li>The empirical loss over the training sample is what we observe, but what we <strong>really care about</strong> is the generalization error (expected loss over unseen data).</li> <li>The sum \(\sum_{t=1}^{T} V_t\) captures how much our empirical loss <strong>deviates</strong> from the true generalization error.</li> <li>Since we established that this deviation behaves like a <strong>martingale difference sequence</strong>, we can now use Azuma’s inequality to show that this deviation is <strong>small with high probability</strong>.</li> <li><strong>This means that our empirical loss is a good estimate of the true generalization error, up to a small correction term!</strong></li> </ul> <p>💡 <strong>Analogy:</strong> Think of a weather forecast model trained on past temperature data.</p> <ul> <li>If the model is unbiased and doesn’t systematically overestimate or underestimate temperatures, then <strong>on average</strong>, its predictions should be close to reality.</li> <li><strong>Azuma’s inequality tells us that, with high probability, the difference between past observed temperatures (empirical loss) and future actual temperatures (generalization error) remains small.</strong></li> <li>This justifies why our training loss is a <strong>reliable estimate</strong> of test loss.</li> </ul> <hr> <h4 id="application-averaging-hypotheses"><strong>Application: Averaging Hypotheses</strong></h4> <p>When the loss function is <strong>convex</strong> in its first argument, we can bound the generalization error of the <strong>average hypothesis</strong>:</p> \[\bar{h} = \frac{1}{T} \sum_{t=1}^{T} h_t\] <p>Since expectation is <strong>linear</strong>, we can use <strong>Jensen’s inequality</strong>, which states that for a convex function \(f\):</p> \[f\left(\frac{1}{T} \sum_{t=1}^{T} x_t\right) \leq \frac{1}{T} \sum_{t=1}^{T} f(x_t)\] <p>Applying this to the generalization error:</p> \[R(\bar{h}) = \mathbb{E}_{(x,y) \sim D} [L(\bar{h}(x), y)]\] <p>By convexity of \(L\), we get:</p> \[R(\bar{h}) \leq \frac{1}{T} \sum_{t=1}^{T} R(h_t)\] <p>From our previous bound:</p> \[\frac{1}{T} \sum_{t=1}^{T} R(h_t) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log(1/\delta)}{T}}\] <p>Thus, we obtain:</p> \[R(\bar{h}) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log(1/\delta)}{T}}\] <p><strong>Key Insights:</strong></p> <ul> <li>The generalization error of the <strong>averaged</strong> hypothesis is controlled by the <strong>average loss</strong> of the individual hypotheses plus a small correction term.</li> <li>This shows that averaging hypotheses is a simple yet powerful way to obtain a <strong>low generalization error</strong> in an online learning setting.</li> </ul> <h5 id="connection-to-regret-minimization"><strong>Connection to Regret Minimization</strong></h5> <p>If the online learning algorithm has <strong>small regret</strong> \(R_T\), then:</p> \[\frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) \approx \inf_{h \in H} \frac{1}{T} \sum_{t=1}^{T} L(h(x_t), y_t)\] <p>Since the second term in our bound vanishes as \(T \to \infty\), we conclude:</p> \[R(\bar{h}) \approx \inf_{h \in H} R(h)\] <p>This means that the average hypothesis is nearly <strong>optimal</strong> in terms of generalization error.</p> <p>If the above result feels a bit half-baked for you to process, no worries—we’ll work through it and figure out what it means next.</p> <hr> <h4 id="theorem-815-generalization-error-of-averaged-hypotheses"><strong>Theorem 8.15: Generalization Error of Averaged Hypotheses</strong></h4> <p>Let \(S = ((x_1, y_1), \dots, (x_T, y_T))\) be a labeled sample drawn <strong>i.i.d.</strong> according to distribution \(D\). Let \(L\) be a <strong>loss function</strong> that is <strong>bounded</strong> by \(M\) and <strong>convex</strong> with respect to its first argument. Consider a sequence of hypotheses \(h_1, \dots, h_T\) generated by an online learning algorithm \(A\) processing \(S\) sequentially.</p> <p>Then, for any \(\delta &gt; 0\), with probability at least \(1 - \delta\), the following bounds hold:</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log \frac{1}{\delta}}{T}}.\] \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \inf_{h \in \mathcal{H}} R(h) + \frac{R_T}{T} + 2M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <hr> <h5 id="proof"><strong>Proof</strong></h5> <p><strong>Step 1: Bounding the Generalization Error of the Averaged Hypothesis</strong></p> <p>By the convexity of \(L\) in its first argument, for any \((x, y)\), we have (via <strong>Jensen’s inequality</strong>):</p> \[L\left(\frac{1}{T} \sum_{t=1}^{T} h_t(x), y\right) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x), y).\] <p>Taking expectations over the data distribution \(D\):</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) = \mathbb{E}_{(x,y) \sim D} \left[ L\left(\frac{1}{T} \sum_{t=1}^{T} h_t(x), y\right) \right] \leq \frac{1}{T} \sum_{t=1}^{T} R(h_t).\] <p>Using <strong>Lemma 8.14</strong>, we get:</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log \frac{1}{\delta}}{T}}.\] <p><strong>Step 2: Connection to Regret Minimization</strong></p> <p>By definition of the <strong>regret</strong> \(R_T\), for any \(\delta &gt; 0\), the following holds with probability at least \(1 - \delta/2\):</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) + M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <p>Since the learner attempts to minimize regret, we take the <strong>minimum</strong> over all hypotheses:</p> \[\frac{1}{T} \sum_{t=1}^{T} L(h_t(x_t), y_t) \leq \min_{h \in \mathcal{H}} \frac{1}{T} \sum_{t=1}^{T} L(h(x_t), y_t) + \frac{R_T}{T}.\] <p><strong>Note:</strong> Not sure how we got this. Actually, this is what we started with, right? The definition of regret as defined above when we started, just dividing by T across.</p> <p>Thus, with probability at least \(1 - \delta/2\):</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \min_{h \in \mathcal{H}} \frac{1}{T} \sum_{t=1}^{T} L(h(x_t), y_t) + \frac{R_T}{T} + M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <p>Using Hoeffding’s inequality, we can show that for any optimal hypothesis \(h^*\), with probability at least \(1 - \delta/2\):</p> \[\frac{1}{T} \sum_{t=1}^{T} L(h^*(x_t), y_t) \leq R(h^*) + M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <p>Combining these results:</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq R(h^*) + \frac{R_T}{T} + 2M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <p>By the definition of \(\inf_{h \in H} R(h)\), for any \(\epsilon &gt; 0\), there exists \(h^* \in H\) with</p> \[R(h^*) \leq \inf_{h \in H} R(h) + \epsilon.\] <p>Since \(h^*\) is chosen optimally, which gives:</p> \[R\left(\frac{1}{T} \sum_{t=1}^{T} h_t\right) \leq \inf_{h \in \mathcal{H}} R(h) + \frac{R_T}{T} + 2M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}.\] <p>Since the inequality holds for all \(\epsilon &gt; 0\), it proves the second statement.</p> <hr> <h5 id="application-to-online-learning-algorithms"><strong>Application to Online Learning Algorithms</strong></h5> <p>The theorem can be applied to a variety of <strong>online regret minimization algorithms</strong>. A key case is when:</p> \[\frac{R_T}{T} = O(1/\sqrt{T}).\] <p>In particular, we can apply this theorem to the <strong>exponentially weighted average algorithm</strong>. Assuming:</p> <ul> <li>The loss function \(L\) is <strong>bounded</strong> by \(M = 1\).</li> <li>The number of rounds \(T\) is <strong>known</strong> to the algorithm.</li> </ul> <p>Then, using the <strong>regret bound from Theorem 8.6</strong>, we obtain:</p> \[R_T \leq \sqrt{(T/2) \log N}\] <p>where \(N\) is the number of experts, or the <strong>dimension of the weight vectors</strong>.</p> <p>If \(T\) is <strong>not</strong> known in advance, we can apply the <strong>doubling trick</strong> (Theorem 8.7) to derive a similar bound.</p> <p>Before we wrap up, just one last thing: Hoeffding’s Inequality. I think I’ll split this blog into two parts for better readability—it’s a bit too much to cover in one post.</p> <h4 id="hoeffdings-inequality-and-its-application-in-generalization-bounds"><strong>Hoeffding’s Inequality and Its Application in Generalization Bounds</strong></h4> <p>Hoeffding’s inequality is a fundamental concentration inequality that provides a bound on the probability that the sum of bounded independent random variables deviates from its expected value.</p> <h5 id="statement-of-hoeffdings-inequality"><strong>Statement of Hoeffding’s Inequality</strong></h5> <p>Let \(X_1, X_2, \dots, X_T\) be independent random variables such that for each \(t\),</p> \[a_t \leq X_t \leq b_t.\] <p>Define the sum:</p> \[S_T = \sum_{t=1}^{T} X_t.\] <p>Then, the probability that \(S_T\) deviates from its expected value by more than \(\epsilon\) satisfies:</p> \[P\left( \left| S_T - \mathbb{E}[S_T] \right| \geq \epsilon \right) \leq 2 \exp \left( -\frac{2 \epsilon^2}{\sum_{t=1}^{T} (b_t - a_t)^2} \right).\] <p>If each \(X_t\) is bounded in the range \([a, b]\), this simplifies to:</p> \[P\left( \left| S_T - \mathbb{E}[S_T] \right| \geq \epsilon \right) \leq 2 \exp \left( -\frac{2 \epsilon^2}{T (b - a)^2} \right).\] <h5 id="application-in-generalization-bounds"><strong>Application in Generalization Bounds</strong></h5> <p>In our proof, we use Hoeffding’s inequality to control the deviation of the empirical risk from the expected risk.</p> <p>Define the random variables:</p> \[X_t = L(h^*(x_t), y_t),\] <p>where \(h^*\) is the optimal hypothesis. Since the loss function is <strong>bounded by</strong> \(M\), we have:</p> \[0 \leq X_t \leq M.\] <p>Thus, the sum:</p> \[\sum_{t=1}^{T} L(h^*(x_t), y_t)\] <p>is a sum of <strong>bounded</strong> independent random variables. Applying Hoeffding’s inequality, we get:</p> \[P\left( \left| \frac{1}{T} \sum_{t=1}^{T} L(h^*(x_t), y_t) - R(h^*) \right| \geq \epsilon \right) \leq 2 \exp \left( -\frac{2 T \epsilon^2}{M^2} \right).\] <p>Setting \(\epsilon = M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}}\) and solving for \(\delta\), we obtain the bound:</p> \[P\left( \frac{1}{T} \sum_{t=1}^{T} L(h^*(x_t), y_t) \geq R(h^*) + M \sqrt{\frac{2 \log \frac{2}{\delta}}{T}} \right) \leq \frac{\delta}{2}.\] <p>This bound ensures that, with high probability, the empirical loss is <strong>close</strong> to the expected loss, which is crucial for proving the generalization bounds of the learning algorithm.</p> <p>A few questions that i had and you might too:</p> <p><strong>Why Does the Optimal \(h^*\) Minimize the Loss?</strong><br> The optimal hypothesis \(h^*\) is defined as the one that minimizes the <strong>expected loss</strong> over the data distribution:</p> \[h^* = \arg\min_{h \in \mathcal{H}} R(h),\] <p>where the <strong>expected loss</strong> (also called the risk) is:</p> \[R(h) = \mathbb{E}_{(x,y) \sim D} [L(h(x), y)].\] <p>Since \(h^*\) minimizes this risk, it achieves the <strong>smallest possible expected loss</strong> over all hypotheses in \(\mathcal{H}\).</p> <hr> <p><strong>Why This Justifies Applying Hoeffding’s Inequality</strong></p> <p>We consider the empirical loss:</p> \[\frac{1}{T} \sum_{t=1}^{T} L(h^*(x_t), y_t),\] <p>which is an <strong>estimate</strong> of the true expected loss \(R(h^*)\). However, since the sample is drawn <strong>i.i.d.</strong>, there is some randomness, so the empirical loss might deviate from the expected loss.</p> <p>Since the loss function is <strong>bounded</strong> (i.e., \(0 \leq L(h^*(x), y) \leq M\)), we can apply <strong>Hoeffding’s inequality</strong> to bound this deviation. This ensures that, with high probability, the empirical loss <strong>does not stray too far</strong> from \(R(h^*)\), giving us a reliable way to estimate the generalization error.</p> <hr> <p><strong>Intuitive Explanation</strong></p> <p>Think of this as estimating the <strong>average height of people in a city</strong> by measuring the height of a random sample. The true average height (analogous to \(R(h^*)\)) is unknown, but if we take a large enough sample, the average height in our sample (empirical loss) will be <strong>close to the true average height</strong> with high probability.</p> <p>By Hoeffding’s inequality, the probability that our sample average <strong>significantly deviates</strong> from the true mean is exponentially small, ensuring our empirical loss is a good approximation of the expected loss.</p> <hr> <p><strong>Key Insights</strong></p> <ul> <li>\(h^*\) is optimal since it minimizes the expected loss.</li> <li>The empirical loss is a sample-based approximation of \(R(h^*)\).</li> <li>Hoeffding’s inequality guarantees that the empirical loss is close to the true loss with high probability.</li> <li>This allows us to <strong>confidently generalize from training data to unseen data</strong>.</li> </ul> <hr> <h5 id="conclusion"><strong>Conclusion</strong></h5> <p>This analysis demonstrates that online learning algorithms, despite being designed for <strong>adversarial settings</strong>, can be effectively converted into batch learning methods. By averaging the hypotheses they produce, we can derive strong <strong>generalization guarantees</strong>, bridging the gap between <strong>online and batch learning</strong> paradigms.</p> <p>This insight is particularly valuable in large-scale machine learning, where online learning methods can be computationally efficient while still ensuring strong predictive performance.</p> <h5 id="to-do">To-Do:</h5> <ul> <li>Split this Blog into two.</li> </ul> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://cs.nyu.edu/~mohri/mlbook/" rel="external nofollow noopener" target="_blank">Foundations of Machine Learning Book</a></li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-gpu-essentials-a-concise-technical-guide",title:"GPU Essentials - A Concise Technical Guide",description:"A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.",section:"Posts",handler:()=>{window.location.href="/blog/2025/GPU-Intro/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey-blog-2025-wrapping-ml-basics",title:"[Wrapping Up Our ML Foundations Journey](/blog/2025/wrapping-ml-basics/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>