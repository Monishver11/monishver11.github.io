<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding the Basics of Probability Theory for Machine Learning | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="This blog explores essential probability concepts and their significance in machine learning."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?e68e4955e21b20101db6e28a5a50abec"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2024/probability-1/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Understanding the Basics of Probability Theory for Machine Learning</h1> <p class="post-meta"> Created in December 22, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Probability theory forms the foundation of machine learning by enabling models to quantify uncertainty and make evidence-based predictions. This article delves into core probability concepts, providing explanations, examples, and analogies designed for clarity and practical sense.</p> <hr> <h4 id="probability-definition-interpretation-and-basic-axioms"><strong>Probability: Definition, Interpretation, and Basic Axioms</strong></h4> <p>In simple terms, <strong>probability</strong> is a measure of the likelihood of an event occurring. It quantifies uncertainty by assigning a number between 0 and 1, where:</p> <ul> <li>A probability of <strong>0</strong> means the event is impossible.</li> <li>A probability of <strong>1</strong> means the event is certain.</li> <li>Values between 0 and 1 represent the likelihood of an event, with higher values indicating greater likelihood.</li> </ul> <p>Mathematically, probability is defined as a function:\(P: \mathcal{F} \to [0, 1]\) where \(\mathcal{F}\) is a collection of events, and \(P\) assigns a value to each event representing its likelihood.</p> <h5 id="interpretation-of-probability"><strong>Interpretation of Probability</strong></h5> <ol> <li> <strong>Frequentist Interpretation</strong>: Probability is the long-run relative frequency of an event occurring after repeated trials. Example: In flipping a fair coin many times, the probability of getting heads is \(0.5\), as heads appear in roughly 50% of the flips.</li> <li> <strong>Bayesian Interpretation</strong>: Probability reflects a degree of belief or confidence in an event occurring, updated as new evidence becomes available. Example: If the chance of rain tomorrow is initially assigned as \(0.7\), this reflects 70% confidence based on current information.</li> </ol> <h5 id="basic-axioms-of-probability"><strong>Basic Axioms of Probability</strong></h5> <p>The three fundamental axioms govern how probabilities are assigned:</p> <ol> <li> <strong>Non-negativity</strong>: \(P(E) \geq 0 \quad \text{for all events } E\). Probabilities cannot be negative.</li> <li> <strong>Normalization</strong>: \(P(S) = 1\). Here, \(S\) is the <strong>sample space</strong> (all possible outcomes). The probability of \(S\) is 1, as one outcome must occur. Example: For a die roll, \(S = \{1, 2, 3, 4, 5, 6\}\), and \(P(S) = 1\).</li> <li> <p><strong>Additivity</strong>:</p> \[P(E_1 \cup E_2) = P(E_1) + P(E_2) \quad \text{if } E_1 \text{ and } E_2 \text{ are mutually exclusive}\] <p>For mutually exclusive events \(E_1\) and \(E_2\), the probability of either occurring is the sum of their individual probabilities. <strong>Example</strong>: For a die roll, let \(E_1 = \{1\}\) and \(E_2 = \{2\}\): \(P(E_1 \cup E_2) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3}\)</p> </li> </ol> <h5 id="consequences-of-the-axioms"><strong>Consequences of the Axioms</strong></h5> <ol> <li> <strong>Complementary Rule:</strong> \(P(E^c) = 1 - P(E)\). The probability of the complement of \(E\) (event not occurring) equals \(1\) minus \(P(E)\). Example: If \(P(\text{rain}) = 0.8\), then \(P(\text{no rain}) = 1 - 0.8 = 0.2\).</li> <li> <p><strong>Addition Rule for Non-Mutually Exclusive Events:</strong></p> \[P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)\] <p>For events that are not mutually exclusive, subtract the overlap probability. Example: Overlapping probabilities in surveys or data categorization.</p> </li> <li> <p><strong>Multiplication Rule for Independent Events:</strong></p> \[P(E_1 \cap E_2) = P(E_1) \cdot P(E_2) \quad \text{:if } E_1 \text{ and } E_2 \text{ are independent}\] <p>Example: Probability of flipping two heads with two coins:\(P(\text{heads on coin 1} \cap \text{heads on coin 2}) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}\)</p> </li> <li> <p><strong>Conditional Probability:</strong></p> \[P(E_1 \mid E_2) = \frac{P(E_1 \cap E_2)}{P(E_2)} \quad \text{if } P(E_2) &gt; 0\] <p>Conditional probability calculates \(E_1\)’s likelihood given \(E_2\) has occurred. Example: Used in models like <strong>Naive Bayes</strong> for predictions under given conditions.</p> </li> </ol> <h5 id="why-is-probability-important-in-machine-learning"><strong>Why is Probability Important in Machine Learning?</strong></h5> <p>Probability theory plays a critical role in machine learning by enabling:</p> <ol> <li> <strong>Modeling Uncertainty</strong>: Essential in probabilistic models like Bayesian networks and Hidden Markov Models.</li> <li> <strong>Decision Making</strong>: Used in reinforcement learning for action selection under uncertainty.</li> <li> <strong>Risk Assessment and Confidence</strong>: Provides confidence intervals and helps quantify prediction risks.</li> <li> <strong>Bayesian Inference</strong>: Updates beliefs about parameters or predictions using new data.</li> </ol> <hr> <h4 id="random-variables-discrete-and-continuous"><strong>Random Variables: Discrete and Continuous</strong></h4> <p>Building upon the foundational axioms of probability, understanding <strong>random variables</strong> is the next critical step. Random variables bridge the gap between abstract probabilistic events and numerical representation, enabling a deeper connection between data and mathematical models.</p> <p>A <strong>random variable</strong> is a variable whose possible values are outcomes of a random process or experiment. It maps outcomes from a probabilistic event to real numbers, playing a central role in probability theory and machine learning. Random variables are typically categorized into two types: <strong>discrete</strong> and <strong>continuous</strong>.</p> <p><strong>Intuition for Random Variables</strong></p> <p>Think of a random variable as a “number generator” that transforms outcomes of a random process into numbers. For instance:</p> <ul> <li>In a dice roll, the outcome (e.g., rolling a 4) is translated to the random variable \(X = 4\).</li> <li>In measuring rainfall, the amount (e.g., 12.5 mm) is assigned to the random variable \(X = 12.5\). This abstraction helps in applying mathematical operations and deriving distributions.</li> </ul> <h5 id="discrete-random-variables"><strong>Discrete Random Variables</strong></h5> <p>A <strong>discrete random variable</strong> takes on a countable number of distinct values. These values are often integers or counts, representing outcomes that are distinct and separate from one another. For example, the number of heads obtained when flipping a coin multiple times is a discrete random variable.</p> <p><strong>Characteristics</strong>:</p> <ul> <li> <strong>Countable Outcomes</strong>: The possible values can be listed, even if the list is infinite (e.g., the number of calls received by a call center).</li> <li> <strong>Probability Mass Function (PMF)</strong>: The probability distribution of a discrete random variable is described by a probability mass function (PMF), which assigns probabilities to each possible value. The PMF satisfies:</li> </ul> \[P(X = x) \geq 0 \quad \text{for all } x\] \[\sum_{x} P(X = x) = 1\] <p>Here, the sum is over all possible values \(x\) that the random variable can take.</p> <p><strong>Examples of Discrete Random Variables</strong>:</p> <ol> <li> <p><strong>Number of heads in coin flips</strong>: Let \(X\) be the number of heads in 3 flips of a fair coin. The possible values of \(X\) are \(0, 1, 2,\) and \(3\). The probabilities for each outcome can be computed using the binomial distribution.</p> </li> <li> <p><strong>Number of customers arriving at a store</strong>: Let \(X\) represent the number of customers who arrive at a store during a 1-hour period. If customers arrive independently with an average rate of 3 per hour, \(X\) could follow a <strong>Poisson distribution</strong>.</p> </li> </ol> <h5 id="continuous-random-variables"><strong>Continuous Random Variables</strong></h5> <p>A <strong>continuous random variable</strong> can take on an infinite number of possible values within a given range. These values are not countable but form a continuum. For example, the height of a person is a continuous random variable because it can take any value within a range, such as \(5.6\) feet, \(5.65\) feet, \(5.654\) feet, and so on.</p> <p><strong>Characteristics</strong>:</p> <ul> <li> <strong>Uncountably Infinite Outcomes</strong>: The possible values form a continuum, often represented by intervals on the real number line.</li> <li> <strong>Probability Density Function (PDF)</strong>: The probability distribution of a continuous random variable is described by a probability density function (PDF). Unlike a PMF, the PDF does not give the probability of any specific outcome but rather the probability of the random variable falling within a certain range. The total area under the PDF curve is equal to 1, and the probability of the variable falling in an interval \([a, b]\) is given by:</li> </ul> \[P(a \leq X \leq b) = \int_{a}^{b} f_X(x) \, dx\] <p>where \(f_X(x)\) is the PDF of \(X\).</p> <p><strong>Examples of Continuous Random Variables</strong>:</p> <ol> <li> <strong>Height of a person</strong>: The height \(X\) of a person can take any value within a realistic range (e.g., between 4 and 7 feet). The exact value is not countable, and it is typically modeled by a normal distribution.</li> <li> <strong>Time taken to complete a task</strong>: If you measure the time \(X\) taken by someone to complete a task, this can take any value (e.g., 2.5 minutes, 2.55 minutes, etc.). The distribution could be modeled using an exponential or normal distribution, depending on the scenario.</li> </ol> <h5 id="why-are-random-variables-important-in-machine-learning"><strong>Why are Random Variables Important in Machine Learning?</strong></h5> <ol> <li> <strong>Modeling Uncertainty</strong>: In machine learning, random variables allow us to model uncertainty in data and predictions. For instance, in regression models, the target variable is often modeled as a random variable with some uncertainty, usually represented as a continuous distribution.</li> <li> <strong>Bayesian Inference</strong>: In Bayesian models, parameters are treated as random variables, and their distributions are updated with new data. This allows for probabilistic reasoning and uncertainty quantification in predictions.</li> <li> <strong>Stochastic Processes</strong>: Many machine learning algorithms, such as those in reinforcement learning or Monte Carlo simulations, involve <strong>stochastic processes</strong>, where future states or outcomes are modeled as random variables with given distributions.</li> </ol> <hr> <h4 id="more-on-probability-distribution-and-types"><strong>More on Probability Distribution and Types</strong></h4> <p>A probability distribution describes how probabilities are assigned to different possible outcomes of a random variable. It provides a mathematical function that represents the likelihood of each possible value the random variable can take. In simpler terms, it tells us how likely each outcome of an experiment or process is.</p> <p>Formally, A <strong>probability distribution</strong> of a random variable \(X\) is a function that provides the probabilities of occurrence of different possible outcomes for the random variable. Depending on the nature of the random variable, the probability distribution can take different forms:</p> <ol> <li> <strong>Discrete Probability Distribution</strong>: This applies when the random variable can only take on a finite or countably infinite number of values (e.g., the number of heads in a coin flip). The distribution is described by a <strong>probability mass function (PMF)</strong>.</li> <li> <strong>Continuous Probability Distribution</strong>: This applies when the random variable can take on an infinite number of values within a range (e.g., the height of a person). The distribution is described by a <strong>probability density function (PDF)</strong>.</li> </ol> <p>For both types, the total probability across all possible outcomes must sum (or integrate) to 1:</p> <ul> <li> <p>For discrete distributions:</p> \[\sum_{x} P(X = x) = 1\] </li> <li> <p>For continuous distributions:</p> \[\int_{-\infty}^{\infty} f_X(x) \, dx = 1\] <p>where \(f_X(x)\) is the probability density function.</p> </li> </ul> <h5 id="1-discrete-probability-distributions"><strong>1. Discrete Probability Distributions</strong></h5> <p>Discrete distributions are used when the random variable can take a countable number of distinct values. Some common discrete probability distributions are:</p> <ul> <li> <strong>Bernoulli Distribution</strong>: Models a binary outcome (success/failure, \(1/0\)) of a single trial. The probability of success is \(p\), and the probability of failure is \(1 - p\). The PMF is: \(P(X = 1) = p, \quad P(X = 0) = 1 - p\) <ul> <li> <strong>Example</strong>: The outcome of a coin flip (Heads = 1, Tails = 0).</li> </ul> </li> <li> <p><strong>Binomial Distribution</strong>: Describes the number of successes in a fixed number of independent Bernoulli trials. The random variable \(X\) counts the number of successes. The PMF is:</p> \[P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}\] <p>where \(n\) is the number of trials, \(p\) is the probability of success, and \(k\) is the number of successes.</p> <ul> <li> <strong>Example</strong>: The number of heads in 10 coin flips.</li> </ul> </li> <li> <p><strong>Poisson Distribution</strong>: Models the number of events occurring in a fixed interval of time or space, given a constant average rate of occurrence. The PMF is:</p> \[P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}\] <p>where \(\lambda\) is the average rate of occurrence (mean), and \(k\) is the number of events.</p> <ul> <li> <strong>Example</strong>: The number of phone calls received by a call center in an hour.</li> </ul> </li> </ul> <h5 id="2-continuous-probability-distributions">2. <strong>Continuous Probability Distributions</strong> </h5> <p>Continuous distributions are used when the random variable can take any value within a given range or interval. These distributions are described by probability density functions (PDFs), where the probability of any single point is zero, and probabilities are calculated over intervals. Some common continuous probability distributions include:</p> <ul> <li> <p><strong>Uniform Distribution</strong>: A continuous distribution where all values within a given interval are equally likely. The PDF is:</p> \[f_X(x) = \frac{1}{b-a} \quad \text{for} \ a \leq x \leq b\] <ul> <li> <strong>Example</strong>: The time it takes for a bus to arrive, uniformly distributed between 5 and 15 minutes.</li> </ul> </li> <li> <p><strong>Normal (Gaussian) Distribution</strong>: A continuous distribution that is symmetric and bell-shaped. It is fully described by its mean \(\mu\) and standard deviation \(\sigma\). The PDF is:</p> \[f_X(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}\] <ul> <li> <strong>Example</strong>: The distribution of heights in a population.</li> </ul> </li> <li> <strong>Exponential Distribution</strong>: A continuous distribution often used to model the time between events in a Poisson process (events happening at a constant rate). The PDF is: \(f_X(x) = \lambda e^{-\lambda x} \quad \text{for} \ x \geq 0\) <ul> <li> <strong>Example</strong>: The time between arrivals of customers at a service station.</li> </ul> </li> <li> <p><strong>Gamma Distribution</strong>: A generalization of the exponential distribution, used for modeling the sum of multiple exponentially distributed random variables. Its PDF is:</p> \[f_X(x) = \frac{x^{k-1} e^{-x/\theta}}{\Gamma(k) \theta^k} \quad \text{for} \ x \geq 0\] <ul> <li> <strong>Example</strong>: The waiting time until a certain number of events occur in a Poisson process.</li> </ul> </li> <li> <p><strong>Beta Distribution</strong>: A continuous distribution on the interval \([0, 1]\), often used to model probabilities and proportions. Its PDF is:</p> \[f_X(x) = \frac{x^{\alpha-1} (1-x)^{\beta-1}}{B(\alpha, \beta)}\] <ul> <li> <strong>Example</strong>: Modeling the proportion of customers who prefer a certain product in a market research study.</li> </ul> </li> </ul> <h5 id="why-are-probability-distributions-important-in-machine-learning"><strong>Why are Probability Distributions Important in Machine Learning?</strong></h5> <ol> <li> <strong>Modeling Uncertainty</strong>: Many machine learning models assume that the data follows a certain probability distribution (e.g., Gaussian distribution in linear regression).</li> <li> <strong>Inference and Prediction</strong>: In probabilistic models, such as Bayesian inference or Hidden Markov Models, understanding the probability distributions of variables allows for reasoning about uncertainty and making predictions based on observed data.</li> <li> <strong>Risk Analysis</strong>: Distributions help quantify the risk or uncertainty in machine learning predictions. For example, a model’s output might be a probability distribution over potential outcomes, providing insights into the confidence of predictions.</li> </ol> <hr> <h4 id="cumulative-distribution-function-cdf"><strong>Cumulative Distribution Function (CDF)</strong></h4> <p>CDF is closely related to the <strong>Probability Density Function (PDF)</strong> in the case of continuous random variables and the <strong>Probability Mass Function (PMF)</strong> for discrete variables. The CDF gives the cumulative probability that a random variable takes a value less than or equal to a particular point.</p> <p>For a random variable \(X\), the <strong>Cumulative Distribution Function (CDF)</strong>, denoted by \(F_X(x)\), is defined as the probability that the random variable \(X\) takes a value less than or equal to \(x\). Formally:</p> \[F_X(x) = P(X \leq x)\] <p>The CDF is a function that provides the cumulative probability up to a point \(x\) and is computed by integrating (for continuous variables) or summing (for discrete variables) the corresponding probability distributions.</p> <h5 id="properties-of-the-cdf"><strong>Properties of the CDF</strong></h5> <ol> <li> <p><strong>Non-decreasing</strong>: The CDF is a non-decreasing function, meaning that the probability increases as \(x\) increases:</p> \[F_X(x_1) \leq F_X(x_2) \quad \text{if} \ x_1 \leq x_2\] </li> <li> <p><strong>Range</strong>: The CDF always lies within the range \([0, 1]\) : \(0 \leq F_X(x) \leq 1 \quad \text{for all} \ x\)</p> </li> <li> <strong>Limits</strong>: <ul> <li>As \(x \to -\infty\), the CDF tends to 0: \(\lim_{x \to -\infty} F_X(x) = 0\)</li> <li>As \(x \to \infty\), the CDF tends to 1: \(\lim_{x \to \infty} F_X(x) = 1\)</li> </ul> </li> <li> <strong>Continuity</strong>: <ul> <li>For <strong>continuous random variables</strong>, the CDF is continuous and smooth.</li> <li>For <strong>discrete random variables</strong>, the CDF is a step function, with jumps corresponding to the probabilities at specific points.</li> </ul> </li> </ol> <h5 id="cdf-for-discrete-random-variables"><strong>CDF for Discrete Random Variables</strong></h5> <p>For a <strong>discrete random variable</strong> \(X\), the CDF is computed by summing the probabilities given by the PMF. If the possible values of \(X\) are \(x_1, x_2, \dots\), the CDF is:</p> \[F_X(x) = P(X \leq x) = \sum_{x_i \leq x} P(X = x_i)\] <p><strong>Example</strong> Consider a discrete random variable \(X\) that represents the outcome of a fair 6-sided die. The possible values for \(X\) are \(1, 2, 3, 4, 5, 6\), each with a probability of \(\frac{1}{6}\). The CDF for \(X\) is:</p> \[F_X(x) = \begin{cases} 0 &amp; \text{for} \ x &lt; 1 \\ \frac{1}{6} &amp; \text{for} \ 1 \leq x &lt; 2 \\ \frac{2}{6} &amp; \text{for} \ 2 \leq x &lt; 3 \\ \frac{3}{6} &amp; \text{for} \ 3 \leq x &lt; 4 \\ \frac{4}{6} &amp; \text{for} \ 4 \leq x &lt; 5 \\ \frac{5}{6} &amp; \text{for} \ 5 \leq x &lt; 6 \\ 1 &amp; \text{for} \ x \geq 6 \end{cases}\] <h5 id="cdf-for-continuous-random-variables"><strong>CDF for Continuous Random Variables</strong></h5> <p>For a <strong>continuous random variable</strong> \(X\), the CDF is obtained by integrating the PDF:</p> \[F_X(x) = P(X \leq x) = \int_{-\infty}^{x} f_X(t) \, d t\] <p><strong>Example</strong> For a continuous random variable \(X\) that follows a <strong>uniform distribution</strong> on the interval \([0, 1]\), the PDF is:</p> \[f_X(x) = \begin{cases} 1 &amp; \text{for} \ 0 \leq x \leq 1 \\ 0 &amp; \text{otherwise} \end{cases}\] <p>The CDF is:</p> \[F_X(x) = \begin{cases} 0 &amp; \text{for} \ x &lt; 0 \\ x &amp; \text{for} \ 0 \leq x \leq 1 \\ 1 &amp; \text{for} \ x &gt; 1 \end{cases}\] <p>This shows that for values of \(x\) between 0 and 1, the probability increases linearly from 0 to 1.</p> <h5 id="relationship-between-pdf-and-cdf"><strong>Relationship Between PDF and CDF</strong></h5> <p>For a continuous random variable \(X\), the PDF is the derivative of the CDF:</p> \[f_X(x) = \frac{d}{dx} F_X(x)\] <p>Conversely, the CDF can be obtained by integrating the PDF:</p> \[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\] <h5 id="why-is-the-cdf-important-in-machine-learning"><strong>Why is the CDF Important in Machine Learning?</strong></h5> <ol> <li> <strong>Data Interpretation</strong>: The CDF provides a clear interpretation of the distribution of data and is useful for understanding the likelihood of a random variable being less than or equal to a specific value.</li> <li> <strong>Probabilistic Decision Making</strong>: The CDF helps integrate outcomes for decision-making in models like <strong>Naive Bayes</strong> or <strong>Bayesian networks</strong>.</li> </ol> <h5 id="how-pmf-pdf-and-cdf-interrelate"><strong>How PMF, PDF, and CDF Interrelate</strong></h5> <ul> <li> <strong>Discrete Variables</strong>: <ul> <li>PMF: \(P(X = x_i)\)</li> <li>CDF:</li> </ul> \[F_X(x) = \sum_{x_i \leq x} P(X = x_i)\] </li> <li> <strong>Continuous Variables</strong>: <ul> <li>PDF: \(f_X(x)\)</li> <li>CDF:</li> </ul> \[F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\] <ul> <li>PDF from CDF:</li> </ul> \[f_X(x) = \frac{d}{dx} F_X(x)\] </li> </ul> <hr> <h4 id="choosing-the-right-distribution-in-ml"><strong>Choosing the Right Distribution in ML</strong></h4> <p>Choosing the appropriate probability distribution for a given machine learning (ML) problem is crucial to making accurate predictions. Each probability distribution captures a unique set of characteristics regarding the randomness and uncertainty in the data. A proper understanding of these distributions can directly influence the performance and efficiency of your model. Here’s a more detailed look at the various distributions commonly used in ML:</p> <ul> <li> <p><strong>Bernoulli/Binomial Distributions:</strong> These are useful for binary or count outcomes. The <strong>Bernoulli distribution</strong> models a single trial with two possible outcomes, often labeled as success (1) or failure (0). The <strong>Binomial distribution</strong> generalizes this by modeling the number of successes in a fixed number of independent Bernoulli trials. These distributions are especially useful in classification tasks, such as predicting whether an email is spam or not.</p> </li> <li> <p><strong>Gaussian (Normal) Distribution:</strong> The Gaussian or <strong>normal distribution</strong> is one of the most widely used distributions in statistics and machine learning, especially when the data exhibits natural variability. It is characterized by its symmetric bell-shaped curve, defined by its mean and standard deviation. It’s particularly useful when you have continuous data that tends to cluster around a central value, such as the distribution of heights in a population or the error terms in regression models. Many machine learning algorithms, such as <strong>linear regression</strong> and <strong>k-nearest neighbors (KNN)</strong>, assume that the underlying data follows a Gaussian distribution.</p> </li> <li> <p><strong>Poisson/Exponential Distributions:</strong> These distributions model events that occur over time or space, where the events happen at a constant average rate. The <strong>Poisson distribution</strong> models the number of events happening in a fixed interval of time or space (e.g., the number of customer arrivals at a service station). The <strong>Exponential distribution</strong> is often used to model the time between events in a Poisson process. Both distributions are important in scenarios involving queues or event-based systems, like predicting the time between customer purchases or server failures in network systems.</p> </li> </ul> <h5 id="why-does-choosing-the-right-distribution-matter"><strong>Why Does Choosing the Right Distribution Matter?</strong></h5> <ol> <li> <strong>Tailoring Models to Data:</strong> Understanding the underlying distribution of the data helps in selecting the right model for your problem. For example, if the data is normally distributed, using models like <strong>linear regression</strong> (which assumes normality of residuals) can result in more accurate predictions. On the other hand, when data is binary (e.g., yes/no outcomes), a <strong>logistic regression</strong> or <strong>Bernoulli distribution</strong> approach would be more appropriate.</li> <li> <strong>Improving Model Efficiency:</strong> When we align the assumptions of a machine learning algorithm with the real-world distribution of the data, models tend to be more efficient and require less computation. For instance, algorithms that work with Gaussian-distributed data can be optimized to take advantage of the symmetry of the distribution, leading to faster convergence in training.</li> <li> <strong>Quantifying Uncertainty:</strong> Different distributions provide unique ways to handle uncertainty in predictions. For example, when working with <strong>Poisson distributions</strong>, we can predict the expected number of events in a fixed period with a known variance. In contrast, the <strong>Exponential distribution</strong> models waiting times, making it suitable for applications like survival analysis or reliability engineering.</li> </ol> <h5 id="further-exploration"><strong>Further Exploration</strong></h5> <p>While this overview has touched on the most commonly used distributions, the world of probability distributions in machine learning is vast. As you dive deeper into various ML topics, you’ll encounter additional distributions tailored to specific data types and problems. Understanding how these distributions behave allows you to refine your models for more accurate, effective predictions.</p> <p>If you’re eager to explore further, we will be diving deeper into these distributions as we continue our series on probability theory. The next section will introduce even more important concepts, so stay tuned for that!</p> <p><strong>See you in the next post!</strong></p> <h5 id="references"><strong>References:</strong></h5> <ul> <li><a href="https://benhay.es/posts/exploring-distributions/" rel="external nofollow noopener" target="_blank">Exploring Probability Distributions</a></li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>