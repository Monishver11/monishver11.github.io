<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multivariate Calculus - Prerequisites for Machine Learning | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2024/multivariate-calculus/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Multivariate Calculus - Prerequisites for Machine Learning</h1> <p class="post-meta"> Created in December 20, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Before we dive into the math-heavy world of machine learning, it’s crucial to build a solid foundation in multivariate calculus. This blog post covers the essentials you’ll need to understand and work with the mathematical concepts underpinning ML.</p> <p>If you’re already familiar with undergraduate-level topics like functions and vectors, this should feel like a natural progression. If not, I recommend revisiting those fundamentals first—they’ll make this much easier to grasp. While this list isn’t exhaustive, it’s sufficient to get started and serves as a reference as we build on these ideas.</p> <p>Let’s dive in!</p> <hr> <h4 id="what-is-calculus"><strong>What is Calculus?</strong></h4> <p>Calculus is the study of continuous change, centered on two main concepts:</p> <ol> <li> <strong>Differentiation</strong>: The process of finding rates of change or slopes of curves. It helps analyze how quantities change over time or in response to other variables.</li> <li> <strong>Integration</strong>: The reverse of differentiation, used to calculate areas under curves and accumulate quantities.</li> </ol> <p>These tools are applied in fields like physics, engineering, economics, and computer science to solve real-world problems involving continuous change.</p> <h5 id="what-is-multivariate-calculus"><strong>What is Multivariate Calculus?</strong></h5> <p>Multivariate calculus, or multivariable calculus, extends single-variable calculus to functions with multiple variables. Key components include:</p> <ul> <li> <strong>Partial Derivatives</strong>: Measure how a function changes with respect to one variable while keeping others constant.</li> <li> <strong>Multiple Integration</strong>: Used to calculate volumes and higher-dimensional quantities for functions of multiple variables.</li> <li> <strong>Vector Calculus</strong>: Explores vector fields, gradients, and theorems like Green’s and Stokes’ theorems.</li> </ul> <h5 id="why-multivariate-calculus-matters"><strong>Why Multivariate Calculus Matters</strong></h5> <p>Multivariate calculus is indispensable for:</p> <ul> <li>Analyzing systems with multiple inputs.</li> <li>Solving optimization problems in higher dimensions.</li> <li>Modeling physical phenomena in 3D or more dimensions.</li> <li>Understanding advanced concepts in physics, engineering, and data science.</li> </ul> <p>By extending the tools of calculus to multidimensional problems, multivariate calculus equips us to tackle complex systems and optimize machine learning models.</p> <hr> <h4 id="derivatives-a-fundamental-concept"><strong>Derivatives: A Fundamental Concept</strong></h4> <p>Derivatives are the backbone of calculus, measuring how a function’s output changes in response to its input. They play a critical role in understanding rates of change and optimizing functions.</p> <p>Here are some analogies to grasp the concept of derivatives:</p> <ol> <li> <strong>The Speedometer Analogy</strong>:<br> Imagine driving a car—the speedometer shows your instantaneous speed at any given moment. Similarly, a derivative tells you how fast a function is changing at a specific point.</li> <li> <strong>Slope of a Tangent Line</strong>:<br> Visually, the derivative represents the slope of the tangent line at a point on a graph. Think of it as placing a ruler on a curve to measure its tilt.</li> <li> <strong>Sensitivity Measure</strong>:<br> A derivative reveals how sensitive a function’s output is to small changes in its input. For example, it’s like determining how much the water level in a bathtub rises when you add a cup of water.</li> </ol> <h4 id="partial-derivatives-and-their-role-in-ml"><strong>Partial Derivatives and Their Role in ML</strong></h4> <p>Expanding on the concept of derivatives, <strong>partial derivatives</strong> allow us to analyze how functions of multiple variables change with respect to a single variable while keeping others constant. This is a cornerstone of multivariate calculus and a fundamental tool in machine learning.</p> <h5 id="what-are-partial-derivatives"><strong>What are Partial Derivatives?</strong></h5> <p>A partial derivative extends the idea of a derivative to multivariable functions. It isolates the effect of one variable while treating the others as fixed.</p> <p>Here are a couple of analogies to help visualize partial derivatives:</p> <ol> <li> <strong>Mountain Climbing</strong>:<br> Imagine standing on a mountain. A partial derivative measures the steepness in one specific direction (e.g., north-south or east-west), while you stay fixed in the same spot.</li> <li> <strong>Baking a Cake</strong>:<br> Consider a recipe where the outcome (taste) depends on multiple ingredients (variables). A partial derivative tells you how the taste changes when you adjust one ingredient (e.g., sugar) while keeping all others constant.</li> </ol> <h5 id="applications-in-machine-learning"><strong>Applications in Machine Learning</strong></h5> <p>Partial derivatives are indispensable in machine learning, especially when working with models that involve multiple parameters.</p> <ol> <li> <strong>Gradient Descent</strong>: <ul> <li>The <strong>gradient vector</strong>, composed of partial derivatives, points in the direction of the steepest increase of a function.</li> <li>Gradient descent uses this information to move in the opposite direction, minimizing the loss function and optimizing model parameters.</li> </ul> </li> <li> <strong>Back-propagation in Neural Networks</strong>: <ul> <li>Partial derivatives calculate how each weight in a neural network contributes to the overall error, enabling efficient training through backpropagation.</li> </ul> </li> <li> <strong>Optimization of Complex Loss Functions</strong>: <ul> <li>In high-dimensional parameter spaces, partial derivatives help navigate the loss landscape to find optimal solutions.</li> </ul> </li> <li> <strong>Sensitivity Analysis</strong>: <ul> <li>They provide insights into how sensitive a model’s output is to changes in individual input variables, aiding interpretability and robustness.</li> </ul> </li> <li> <strong>Second-Order Optimization</strong>: <ul> <li>Techniques like Newton’s method use the <strong>Hessian matrix</strong> (second-order partial derivatives) to achieve faster convergence.</li> </ul> </li> </ol> <p>By employing partial derivatives, machine learning algorithms can effectively optimize performance across multiple parameters and gain insights into intricate relationships within the data.</p> <hr> <h4 id="gradient-vectors-a-multivariable-power-tool"><strong>Gradient Vectors: A Multivariable Power Tool</strong></h4> <p>The <strong>gradient vector</strong> combines partial derivatives to form a powerful tool for analyzing multivariable functions. It indicates both the direction and magnitude of the steepest ascent at any given point.</p> <h5 id="what-is-a-gradient-vector"><strong>What is a Gradient Vector?</strong></h5> <p>The gradient vector generalizes the derivative to functions with multiple variables, providing a way to “sense” the terrain of the function.</p> <p>Here are some analogies to conceptualize gradient vectors:</p> <ol> <li> <strong>Mountain Climber’s Compass</strong>:<br> Imagine you’re on a mountain, and you have a compass that always points uphill in the steepest direction. That’s the gradient vector—it guides you to the quickest ascent.</li> <li> <strong>Water Flow on a Surface</strong>:<br> Think of water droplets on a curved surface. The gradient vector at any point shows the direction water would flow—the steepest descent.</li> <li> <strong>Heat-Seeking Missile</strong>:<br> The gradient works like a heat-seeking missile’s guidance system, constantly recalibrating to move toward the function’s maximum.</li> </ol> <h5 id="applications-in-machine-learning-1"><strong>Applications in Machine Learning</strong></h5> <p>Gradient vectors are pivotal in optimization and training algorithms:</p> <ol> <li> <strong>Gradient Descent</strong>: <ul> <li>The gradient vector points toward the steepest ascent, so moving in the opposite direction minimizes the loss function.</li> </ul> </li> <li> <strong>Adaptive Learning Rate Methods</strong>: <ul> <li>Advanced algorithms like AdaGrad and Adam utilize the gradient vector to adjust learning rates dynamically for each parameter.</li> </ul> </li> <li> <strong>Local Linear Approximation</strong>: <ul> <li>The gradient provides a local linear estimate of the function, helping algorithms make informed adjustments to parameters.</li> </ul> </li> </ol> <p>By leveraging the gradient vector, machine learning algorithms efficiently navigate complex parameter spaces, optimize models, and adapt to data characteristics. The gradient not only informs about the loss landscape but also helps shape strategies to improve performance.</p> <p>You might be wondering why the gradient points in the direction of steepest ascent—see the references below for more details.</p> <hr> <h4 id="hessian-and-jacobian-higher-order-tools"><strong>Hessian and Jacobian: Higher-Order Tools</strong></h4> <p>Building on partial derivatives and gradient vectors, <strong>Hessian</strong> and <strong>Jacobian matrices</strong> offer even deeper insights into multivariable functions. These higher-order constructs are essential for advanced optimization techniques and will be explored in detail next down.</p> <h5 id="jacobian-matrix"><strong>Jacobian Matrix</strong></h5> <p>The Jacobian matrix generalizes the gradient vector for vector-valued functions, capturing how changes in multiple inputs affect multiple outputs.</p> <h6 id="definition"><strong>Definition</strong></h6> \[\text{For a function } \mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m, \text{ the Jacobian matrix } \mathbf{J} \text{ is an } m \times n \text{ matrix defined as:}\] \[\mathbf{J} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \\ \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n} \end{bmatrix}\] \[\text{The entries of the Jacobian matrix are partial derivatives of the component functions } f_i\] \[\text{ with respect to the input variables } x_j.\] <h5 id="hessian-matrix"><strong>Hessian Matrix</strong></h5> <p>The Hessian matrix contains all second-order partial derivatives of a scalar-valued function. It provides information about the curvature of the function, making it essential for understanding the landscape of optimization problems.</p> <h6 id="definition-1"><strong>Definition</strong></h6> \[\text{For a function } f: \mathbb{R}^n \to \mathbb{R}, \text{ the Hessian matrix } \mathbf{H} \text{ is an } n \times n \text{ symmetric matrix defined as:}\] \[\mathbf{H} = \begin{bmatrix} \frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \\ \frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix}\] \[\text{The Hessian provides information about the curvature of the function in all directions.}\] <p>Both the Jacobian and Hessian matrices extend the concept of partial derivatives to offer deeper insights into multivariable functions. In machine learning, these tools enhance the ability to analyze and optimize models by providing detailed information about variable interdependencies and the curvature of the function’s landscape. The Jacobian is particularly useful for understanding transformations and sensitivities in vector-valued functions, while the Hessian aids in second-order optimization by characterizing curvature and guiding efficient convergence. Together, they enable sophisticated analysis and optimization techniques, making it possible to tackle the complexities of high-dimensional spaces typical in machine learning tasks.</p> <hr> <h4 id="taylor-series"><strong>Taylor Series</strong></h4> <p>The Taylor series is a powerful tool that approximates complex functions using simpler polynomial expressions. This approximation is widely used in optimization and machine learning.</p> <h6 id="definition-2"><strong>Definition</strong></h6> \[\text{For a function } f(x) \text{ that is infinitely differentiable at a point } a,\] \[\text{ the Taylor series is given by:}\] \[f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \cdots\] \[\text{And more generally the Taylor series is given by:}\] \[f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!} (x - a)^n\] <p>Quick analogies to idealize it:</p> <ol> <li> <p><strong>Function Microscope</strong>:<br> The Taylor series “zooms in” on a specific point of a function, describing its local behavior with increasing precision.</p> </li> <li> <p><strong>Prediction Machine</strong>:<br> Think of it as a step-by-step refinement of a guess about the function’s behavior. It starts with a constant, then linear, quadratic, cubic, and so on, improving accuracy at each step.</p> </li> </ol> <h5 id="applications-in-machine-learning-2"><strong>Applications in Machine Learning</strong></h5> <ol> <li> <strong>Function Approximation</strong>:<br> Simplifies complex functions into polynomials that are computationally easier to work with.</li> <li> <strong>Optimization Algorithms</strong>:<br> Techniques like Newton’s method use Taylor approximations to estimate minima or maxima.</li> <li> <strong>Gradient Estimation</strong>:<br> When direct computation of gradients is challenging, Taylor series can approximate them.</li> </ol> <p>The Taylor series provides a detailed local understanding of functions, facilitating optimization, gradient estimation, and model interpretation. This makes it a valuable tool for bridging the gap between continuous mathematical phenomena and practical computational algorithms, playing a crucial role in solving complex problems and enhancing the efficiency of machine learning workflows.</p> <p>This foundational knowledge from multivariate calculus sets the stage for deeper exploration. If you’ve made it this far, congratulations! You’ve taken an important step in understanding and internalizing key concepts in machine learning. Take a moment to reflect, and when you’re ready, let’s move forward to the next topic. See you there!</p> <h5 id="references"><strong>References</strong></h5> <ul> <li><a href="https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent" target="_blank" rel="external nofollow noopener">Why is gradient the direction of steepest ascent?</a></li> <li><a href="https://betterexplained.com/articles/calculus-building-intuition-for-the-derivative/" target="_blank" rel="external nofollow noopener">Calculus: Building Intuition for the Derivative – BetterExplained</a></li> <li><a href="https://photomath.com/articles/what-is-calculus-definition-applications-and-concepts/" target="_blank" rel="external nofollow noopener">What is Calculus? Definition, Applications, and Concepts – Photomath</a></li> </ul> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>