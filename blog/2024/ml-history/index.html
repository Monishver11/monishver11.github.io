<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Timeline of Machine Learning History | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="A concise timeline of machine learning's history, showcasing key milestones and breakthroughs that shaped the field."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2024/ml-history/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Timeline of Machine Learning History</h1> <p class="post-meta"> Created in December 23, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Machine learning has come a long way, evolving through decades of research and innovation. This timeline highlights the pivotal moments that have defined the field.</p> <h5 id="1910s1940s-early-computational-foundations"><strong>1910s–1940s: Early Computational Foundations</strong></h5> <ul> <li> <p><strong>1913:</strong> <code class="language-plaintext highlighter-rouge">Markov Chains</code> <br> Andrey Markov introduces techniques later known as Markov chains, fundamental to many machine learning algorithms.</p> </li> <li> <p><strong>1936:</strong> <code class="language-plaintext highlighter-rouge">Turing's Theory of Computation</code><br> Alan Turing proposes the theory of computation, forming the foundation for modern computing and machine learning.</p> </li> <li> <p><strong>1940:</strong> <code class="language-plaintext highlighter-rouge">ENIAC</code><br> The first electronic general-purpose computer is created, paving the way for future computational advancements.</p> </li> <li> <p><strong>1943:</strong> <code class="language-plaintext highlighter-rouge">McCulloch-Pitts Model</code><br> Walter Pitts and Warren McCulloch publish the first mathematical model of a neural network, laying the foundation for neural networks.</p> </li> <li> <p><strong>1949:</strong> <code class="language-plaintext highlighter-rouge">Hebbian Learning</code><br> Donald Hebb publishes “The Organization of Behavior,” introducing concepts crucial to neural network development.</p> </li> </ul> <hr> <h5 id="1950s1960s-foundations-of-artificial-intelligence"><strong>1950s–1960s: Foundations of Artificial Intelligence</strong></h5> <ul> <li> <p><strong>1950:</strong> <code class="language-plaintext highlighter-rouge">Turing Test</code><br> Alan Turing proposes the Turing Test, a benchmark for machine intelligence.</p> </li> <li> <p><strong>1951:</strong> <code class="language-plaintext highlighter-rouge">SNARC</code><br> Marvin Minsky and Dean Edmonds build SNARC, the first artificial neural network machine.</p> </li> <li> <p><strong>1952:</strong> <code class="language-plaintext highlighter-rouge">First Learning Program</code><br> Arthur Samuel writes the first computer program capable of learning, a checkers-playing program.</p> </li> <li> <p><strong>1956:</strong> <code class="language-plaintext highlighter-rouge">Dartmouth Conference</code><br> The term “Artificial Intelligence” is coined, marking the birth of AI as a field.</p> </li> <li> <p><strong>1957:</strong> <code class="language-plaintext highlighter-rouge">Perceptron</code><br> Frank Rosenblatt invents the perceptron, an early type of neural network capable of binary classification.</p> </li> <li> <p><strong>1963:</strong> <code class="language-plaintext highlighter-rouge">Machine Learning in Games</code><br> Donald Michie creates a machine that uses reinforcement learning to play Tic-tac-toe.</p> </li> <li> <p><strong>1967:</strong> <code class="language-plaintext highlighter-rouge">Nearest Neighbor Algorithm</code><br> The Nearest Neighbor algorithm is developed, marking the birth of pattern recognition in computers.</p> </li> <li> <p><strong>1969:</strong> <code class="language-plaintext highlighter-rouge">Limitations of Neural Networks</code><br> Marvin Minsky and Seymour Papert publish “Perceptrons,” highlighting limitations of early neural networks.</p> </li> </ul> <hr> <h5 id="1970s1980s-growth-and-challenges"><strong>1970s–1980s: Growth and Challenges</strong></h5> <ul> <li> <p><strong>1970s:</strong> <code class="language-plaintext highlighter-rouge">First AI Winter</code><br> Funding and interest in AI declined due to unmet expectations and computational limitations.</p> </li> <li> <p><strong>1979:</strong> <code class="language-plaintext highlighter-rouge">Stanford Cart</code><br> Stanford University invents the “Stanford Cart,” an early autonomous mobile robot.</p> </li> <li> <p><strong>1981:</strong> <code class="language-plaintext highlighter-rouge">Explanation-Based Learning</code><br> Gerald Dejong introduces the concept of explanation-based learning.</p> </li> <li> <p><strong>1985:</strong> <code class="language-plaintext highlighter-rouge">NetTalk</code><br> Terry Sejnowski invents NetTalk, demonstrating machine learning of pronunciation.</p> </li> <li> <p><strong>1988:</strong> <code class="language-plaintext highlighter-rouge">Universal Approximation Theorem</code><br> Kurt Hornik proves the universal approximation theorem for neural networks.</p> </li> <li> <p><strong>1989:</strong> <code class="language-plaintext highlighter-rouge">CNN for Handwriting Recognition</code><br> Yann LeCun, Yoshua Bengio, and Patrick Haffner demonstrate CNNs for handwriting recognition.</p> </li> <li> <p><strong>1989:</strong> <code class="language-plaintext highlighter-rouge">Q-learning</code><br> Christopher Watkins develops Q-learning, advancing reinforcement learning.</p> </li> </ul> <hr> <h5 id="1990s-statistical-learning-and-commercial-ai"><strong>1990s: Statistical Learning and Commercial AI</strong></h5> <ul> <li> <p><strong>1992:</strong> <code class="language-plaintext highlighter-rouge">TD-Gammon</code><br> Gerald Tesauro invents TD-Gammon, a backgammon program using neural networks.</p> </li> <li> <p><strong>1997:</strong> <code class="language-plaintext highlighter-rouge">Deep Blue Defeats Chess Champion</code><br> IBM’s Deep Blue defeats world chess champion Garry Kasparov, demonstrating AI in games.</p> </li> <li> <p><strong>1997:</strong> <code class="language-plaintext highlighter-rouge">LSTMs Introduced</code><br> Sepp Hochreiter and Jürgen Schmidhuber invent Long Short-Term Memory (LSTM) networks.</p> </li> <li> <p><strong>1998:</strong> <code class="language-plaintext highlighter-rouge">MNIST Database Released</code> Yann LeCun releases the MNIST database, a benchmark for handwriting recognition.</p> </li> <li> <p><strong>1998:</strong> <code class="language-plaintext highlighter-rouge">Furby Released</code><br> Tiger Electronics releases Furby, introducing simple AI to the mass market.</p> </li> <li> <p><strong>1999:</strong> <code class="language-plaintext highlighter-rouge">AIBO Robot Dog</code><br> Sony launches AIBO, showcasing AI in consumer robotics.</p> </li> </ul> <hr> <h5 id="2000s-big-data-and-ml-techniques"><strong>2000s: Big Data and ML Techniques</strong></h5> <ul> <li> <p><strong>2000:</strong> <code class="language-plaintext highlighter-rouge">Nomad Robot</code><br> The Nomad robot explores Antarctica, becoming the first robot to discover a meteorite.</p> </li> <li> <p><strong>2002:</strong> <code class="language-plaintext highlighter-rouge">Torch Library Released</code><br> The Torch machine learning library is first released, enabling research in ML.</p> </li> <li> <p><strong>2009:</strong> <code class="language-plaintext highlighter-rouge">Netflix Prize</code><br> Netflix awards $1 million for improving its recommendation system.</p> </li> </ul> <hr> <h5 id="2010s-the-deep-learning-revolution"><strong>2010s: The Deep Learning Revolution</strong></h5> <ul> <li> <p><strong>2010:</strong> <code class="language-plaintext highlighter-rouge">Kaggle Launch</code><br> Kaggle, a platform for machine learning competitions, is launched.</p> </li> <li> <p><strong>2010:</strong> <code class="language-plaintext highlighter-rouge">Kinect for Xbox</code><br> Microsoft releases Kinect, showcasing advanced computer vision capabilities.</p> </li> <li> <p><strong>2011:</strong> <code class="language-plaintext highlighter-rouge">IBM Watson Wins Jeopardy!</code><br> IBM Watson defeats human champions, showcasing NLP and ML capabilities.</p> </li> <li> <p><strong>2012:</strong> <code class="language-plaintext highlighter-rouge">AlexNet Wins ImageNet</code><br> Deep CNNs significantly outperformed traditional approaches, heralding the deep learning era.</p> </li> <li> <p><strong>2013:</strong> <code class="language-plaintext highlighter-rouge">Deep Reinforcement Learning</code><br> DeepMind introduces deep reinforcement learning, advancing RL applications.</p> </li> <li> <p><strong>2013:</strong> <code class="language-plaintext highlighter-rouge">Word2Vec</code><br> Google introduces Word2Vec, a tool for vectorizing natural language.</p> </li> <li> <p><strong>2018:</strong> <code class="language-plaintext highlighter-rouge">Alibaba's AI</code><br> Alibaba’s AI outscores humans on Stanford University’s reading comprehension test.</p> </li> </ul> <hr> <h5 id="2020s-large-scale-ai-and-generative-models"><strong>2020s: Large-Scale AI and Generative Models</strong></h5> <ul> <li> <p><strong>2020:</strong> <code class="language-plaintext highlighter-rouge">GPT-3 Released</code><br> OpenAI’s large-scale language model demonstrated the power of generative pre-trained transformers.</p> </li> <li> <p><strong>2020:</strong> <code class="language-plaintext highlighter-rouge">Turing NLG</code> Microsoft introduces Turing Natural Language Generation.</p> </li> <li> <p><strong>2022:</strong> <code class="language-plaintext highlighter-rouge">AlphaFold Breakthrough</code><br> DeepMind solved the protein folding problem, revolutionizing biology with ML.</p> </li> <li> <p><strong>2023:</strong> <code class="language-plaintext highlighter-rouge">Generative AI Adoption</code><br> Widespread use of diffusion models and ChatGPT showcased the practical impact of generative AI.</p> </li> </ul> <hr> <h5 id="2024s-cutting-edge-ai-innovations"><strong>2024s: Cutting-Edge AI Innovations</strong></h5> <ul> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">OpenAI's O1 Model</code><br> Advanced reasoning capabilities in mathematics and coding, enhancing AI’s problem-solving skills.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Google DeepMind's GenCast</code><br> Improved weather predictions to optimize agriculture and disaster preparedness.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Microsoft's Copilot Vision</code><br> AI integration with digital environments to boost productivity.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">AI Video Creation Tools</code><br> Transformation of content creation with tools like Google’s Veo and OpenAI’s Sora.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Anthropic's Claude Chatbot</code><br> Enhanced AI safety and reliability for critical applications like disaster response.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Multimodal AI Advancements</code><br> Integration of text, audio, and visual inputs in AI models like ChatGPT-4.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Small Language Models (SLMs) Rise</code><br> Increased popularity of efficient AI models that require fewer computing resources.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Customizable Generative AI</code><br> Development of tailored AI systems for niche markets and specific user needs.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">Geo-Llama</code><br> Advanced AI technique for generating realistic simulated data on human movement in urban settings.</p> </li> <li> <p><strong>2024:</strong> <code class="language-plaintext highlighter-rouge">GPT-4 Enhancements</code><br> Improved emotional recognition capabilities from a third-person perspective.</p> </li> </ul> <p>The list of discoveries/events mentioned is extensive i guess, and apologies if I’ve missed any significant developments. The field of AI is advancing at a rapid pace, and we are eagerly awaiting the first steps toward AGI. As my focus remains on machine learning, I aim to contribute to this vibrant community, and I hope you’re as excited about the future of AI as I am. That’s likely why you’re reading this now. I wish you all the best and invite you to dive deeper into the realm of supervised learning in my next blog.</p> <p><strong>Stay tuned, and I’ll see you there!</strong></p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>