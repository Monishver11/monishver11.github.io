<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Advanced Probability Concepts for Machine Learning | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="This blog explores key probability theory concepts, from distributions and Bayes' Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2024/probability-2/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">Advanced Probability Concepts for Machine Learning</h1> <p class="post-meta"> Created in December 22, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h4 id="bayes-rule-and-associated-properties-a-key-concept"><strong>Bayes’ Rule and Associated Properties: A Key Concept</strong></h4> <p>Bayes’ Rule is a foundational concept in probability theory that plays a critical role in machine learning, especially in tasks involving classification, decision-making, and model inference. It provides a mathematical framework to update our beliefs about a hypothesis based on new evidence.</p> <h5 id="what-is-bayes-rule"><strong>What is Bayes’ Rule?</strong></h5> <p>Bayes’ Rule describes the relationship between conditional probabilities. It allows us to reverse conditional probabilities, which can be very useful in machine learning when we need to compute the probability of a certain hypothesis given observed data.</p> <p>Mathematically, Bayes’ Rule is expressed as:</p> \[P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}\] <p>Where:</p> <ul> <li>\(P(A \mid B)\) is the <strong>posterior probability</strong>: the probability of the hypothesis \(A\) being true given the evidence \(B\).</li> <li>\(P(B \mid A)\) is the <strong>likelihood</strong>: the probability of observing the evidence \(B\) given that the hypothesis \(A\) is true.</li> <li>\(P(A)\) is the <strong>prior probability</strong>: the initial probability of the hypothesis \(A\) before seeing any evidence.</li> <li>\(P(B)\) is the <strong>evidence</strong> or <strong>normalizing constant</strong>: the total probability of observing the evidence across all possible hypotheses.</li> </ul> <p><strong>To follow along, consider this analogy:</strong></p> <p>Imagine you’re trying to diagnose whether someone has a certain disease. You have:</p> <ul> <li>The prior probability of having the disease (\(P(A)\)), which could be based on general statistics about the disease.</li> <li>The likelihood (\(P(B \mid A)\)), which is the probability that a person with the disease would test positive on a medical test.</li> <li>The evidence (\(P(B)\)), which is the total probability that anyone, sick or healthy, would test positive.</li> </ul> <p>Bayes’ Rule helps us combine this information to update our belief about the probability of the disease (hypothesis \(A\)) given the test result (evidence \(B\)).</p> <h5 id="why-is-bayes-rule-crucial-in-machine-learning"><strong>Why is Bayes’ Rule Crucial in Machine Learning?</strong></h5> <p>Bayes’ Rule is central to a variety of machine learning models, particularly in probabilistic and Bayesian approaches. Some key applications include:</p> <ol> <li> <p><strong>Naive Bayes’ Classifier</strong>: In supervised learning, the Naive Bayes’ classifier uses Bayes’ Rule to classify data based on conditional probabilities. It assumes independence between features, simplifying the computation of probabilities.</p> </li> <li> <p><strong>Model Inference and Parameter Estimation</strong>: Bayesian methods in machine learning, like Bayesian neural networks, use Bayes’ Rule to update the distribution of model parameters as new data is observed, instead of relying on point estimates.</p> </li> <li> <p><strong>Decision Theory</strong>: Bayes’ Rule helps in decision-making processes by quantifying the uncertainty associated with different outcomes, especially when there is a probabilistic component to the environment or model.</p> </li> </ol> <h5 id="associated-properties-of-bayes-rule"><strong>Associated Properties of Bayes’ Rule</strong></h5> <ol> <li> <p><strong>Bayes’ Theorem for Multiple Events</strong>: Bayes’ Rule can be extended to more complex situations, such as when dealing with multiple hypotheses or events. This is useful when making predictions over many possible outcomes or when dealing with complex models in machine learning.</p> </li> <li> <p><strong>Conjugacy</strong>: In some models, certain prior distributions are chosen because they lead to mathematical simplicity when combined with Bayes’ Rule. These priors are called <strong>conjugate priors</strong>. For example, in Gaussian processes, using a conjugate prior for the likelihood of Gaussian data results in a simpler update process.</p> </li> <li> <p><strong>The Law of Total Probability</strong>: Bayes’ Rule is closely related to the Law of Total Probability, which decomposes the total probability of an event into a sum of conditional probabilities. This can be useful when considering multiple sources of evidence or when performing integration in complex models.</p> </li> </ol> <p>Bayes’ Rule is an indispensable tool in machine learning for reasoning about uncertainty, updating beliefs with new evidence, and making decisions in the face of incomplete information.</p> <hr> <h4 id="joint-probability-and-independence"><strong>Joint Probability and Independence:</strong></h4> <p>In machine learning, understanding how different events relate to each other is critical. Two key concepts that help us analyze these relationships are <strong>joint probability</strong> and <strong>independence</strong>.</p> <h5 id="what-is-joint-probability"><strong>What is Joint Probability?</strong></h5> <p>Joint probability refers to the probability of two or more events occurring simultaneously. In other words, it is the likelihood that multiple events happen at the same time, and is often represented as \(P(A \cap B)\) for two events \(A\) and \(B\).</p> <p>Mathematically, the joint probability of events \(A\) and \(B\) is defined as:</p> \[P(A \cap B) = P(A \mid B) \cdot P(B) = P(B \mid A) \cdot P(A)\] <p>This equation shows that the joint probability of two events \(A\) and \(B\) can be computed by multiplying the conditional probability of one event given the other by the probability of the second event. The reverse relationship also holds true.</p> <p><strong>Now, consider this analogy:</strong></p> <p>Imagine you’re rolling two dice and are interested in the probability that the first die shows a 4 and the second die shows a 6. The joint probability, \(P(\text{Die 1 = 4 and Die 2 = 6})\), is the probability of both events happening at once. Since the dice rolls are independent, we can compute this as the product of the individual probabilities:</p> \[P(\text{Die 1 = 4}) \cdot P(\text{Die 2 = 6})\] <p>This product gives the likelihood that both dice will show these values simultaneously.</p> <h5 id="why-is-joint-probability-important-in-machine-learning"><strong>Why is Joint Probability Important in Machine Learning?</strong></h5> <p>Joint probability is used to model relationships between different features or variables in a dataset. Some applications include:</p> <ol> <li> <p><strong>Multivariate Probability Models</strong>: In many machine learning problems, we are dealing with multiple features simultaneously. Joint probability helps model the dependencies between these features, which is essential for tasks like classification or clustering. For example, in a classification task, joint probabilities allow us to compute the likelihood of a particular outcome given multiple features.</p> </li> <li> <p><strong>Markov Chains</strong>: In sequence-based tasks like time series forecasting, the joint probability of a sequence of events (e.g., states in a Markov Chain) is crucial in determining the probability distribution over future states based on previous ones.</p> </li> </ol> <h4 id="what-is-independence"><strong>What is Independence?</strong></h4> <p>Two events \(A\) and \(B\) are said to be <strong>independent</strong> if the occurrence of one event does not affect the probability of the other event occurring. Mathematically, two events are independent if:</p> \[P(A \cap B) = P(A) \cdot P(B)\] <p>This property is a key assumption in many machine learning algorithms, especially those based on probabilistic reasoning.</p> <p><strong>For Intuition, think it this way:</strong></p> <p>Think of tossing a coin and rolling a die. The outcome of the coin toss does not affect the outcome of the die roll. These two events are independent, meaning the joint probability can be computed as the product of their individual probabilities. So, the probability of getting heads on the coin toss and a 6 on the die roll is:</p> \[P(\text{Heads}) \cdot P(\text{Die = 6})\] <h5 id="why-is-independence-important-in-machine-learning"><strong>Why is Independence Important in Machine Learning?</strong></h5> <p>Independence is a simplifying assumption in many machine learning models and can significantly reduce the complexity of computations:</p> <ol> <li> <p><strong>Naive Bayes Classifier</strong>: The Naive Bayes classifier makes a strong independence assumption—that the features are conditionally independent given the class. This simplifies the computation of joint probabilities for multiple features and makes the model efficient even with high-dimensional data.</p> </li> <li> <p><strong>Factorization</strong>: In probabilistic models, assuming independence allows for factorizing the joint probability distribution into simpler, more manageable parts. This can be particularly useful in situations like generative models or in deep learning when modeling complex dependencies in large datasets.</p> </li> <li> <p><strong>Feature Independence</strong>: In feature engineering, assuming that features are independent can help simplify model design and speed up training. It’s often used as a heuristic, particularly when exploring models like Gaussian Mixture Models (GMM) or Hidden Markov Models (HMM).</p> </li> </ol> <h5 id="conditional-independence"><strong>Conditional Independence</strong></h5> <p>While not the same as plain independence, <strong>conditional independence</strong> is another important concept. Two events \(A\) and \(B\) are conditionally independent given a third event \(C\) if:</p> \[P(A \cap B \mid C) = P(A \mid C) \cdot P(B \mid C)\] <p>This property is widely used in Bayesian networks and machine learning models to break down complex dependencies into simpler conditional ones.</p> <hr> <h4 id="conditional-probability-and-conditional-distributions-building-blocks-for-predictive-models"><strong>Conditional Probability and Conditional Distributions: Building Blocks for Predictive Models</strong></h4> <p>Conditional probability and conditional distributions concepts help us refine predictions based on additional information and allow us to build more accurate, data-driven models by considering how the likelihood of one event changes when we know about the occurrence of another.</p> <h5 id="what-is-conditional-probability"><strong>What is Conditional Probability?</strong></h5> <p>Conditional probability is the probability of an event occurring given that another event has already occurred. In other words, it quantifies the likelihood of an event, assuming that certain information is known. It is expressed as:</p> \[P(A \mid B) = \frac{P(A \cap B)}{P(B)}\] <p>Where:</p> <ul> <li>\(P(A \mid B)\) is the <strong>conditional probability</strong> of event \(A\) given event \(B\).</li> <li>\(P(A \cap B)\) is the <strong>joint probability</strong> of both \(A\) and \(B\) occurring.</li> <li>\(P(B)\) is the <strong>probability</strong> of event \(B\).</li> </ul> <p>This formula helps us understand how the occurrence of one event (\(B\)) affects the likelihood of another event (\(A\)).</p> <p><strong>Intuition and Analogy for Conditional Probability</strong></p> <p>Imagine you’re at a concert and you’re interested in the probability that a person will be wearing a red T-shirt, given that they are in the front row. Without any information, the probability of someone wearing a red T-shirt might be 30%. But if you know that the person is in the front row (which could imply a certain type of concertgoer), this could affect the probability—perhaps fans who are in the front row are more likely to wear red.</p> <p>This situation is an example of <strong>conditional probability</strong>, where the event \(A\) (person wearing a red T-shirt) is conditioned on the event \(B\) (person being in the front row). Conditional probability helps refine your predictions based on new information.</p> <h5 id="why-is-conditional-probability-important-in-machine-learning"><strong>Why is Conditional Probability Important in Machine Learning?</strong></h5> <p>Conditional probability is essential in many machine learning models for predicting outcomes based on known data. Key applications include:</p> <ol> <li> <p><strong>Classification and Regression</strong>: In supervised learning, we use conditional probability to predict the class label (in classification) or continuous values (in regression) based on observed features. For instance, in logistic regression, we compute the conditional probability of a binary outcome given certain feature values.</p> </li> <li> <p><strong>Naive Bayes Classifier</strong>: The Naive Bayes algorithm, which assumes conditional independence of features, uses conditional probabilities to predict class labels. It calculates the probability of the class label given the observed features using Bayes’ Theorem.</p> </li> <li> <p><strong>Bayesian Inference</strong>: In Bayesian methods, we continuously update the probability of a hypothesis based on new data. This is done using conditional probabilities and allows for probabilistic reasoning in models such as Bayesian networks.</p> </li> </ol> <h4 id="what-are-conditional-distributions"><strong>What are Conditional Distributions?</strong></h4> <p>A <strong>conditional distribution</strong> is a probability distribution of a subset of variables given the values of other variables. It generalizes conditional probability to the case of multiple random variables and helps us understand how the distribution of one variable changes when the values of others are known.</p> <p>For example, if you have two variables \(X\) and \(Y\), the conditional distribution of \(X\) given \(Y = y\) is the probability distribution of \(X\) when you know that \(Y\) takes the specific value \(y\).</p> <p>Mathematically, a conditional distribution is denoted as:</p> \[P(X \mid Y = y)\] <p>Where:</p> <ul> <li>\(P(X \mid Y = y)\) is the conditional distribution of \(X\) given that \(Y = y\).</li> <li>This describes how the distribution of \(X\) changes when \(Y\) is fixed at a particular value.</li> </ul> <p><strong>Intuition and Analogy for Conditional Distributions</strong></p> <p>Consider a scenario where you’re trying to predict a person’s income (\(X\)) based on their level of education (\(Y\)). If you know that someone has a college degree, the distribution of their income (the possible range of incomes they could have) will be different than if you only know their high school education level.</p> <p>In this case, \(P(X \mid Y)\) would describe the distribution of income (\(X\)) conditional on a specific level of education (\(Y\)). The distribution will shift depending on the value of \(Y\), helping refine your predictions of income based on known education levels.</p> <h5 id="why-are-conditional-distributions-important-in-machine-learning"><strong>Why are Conditional Distributions Important in Machine Learning?</strong></h5> <p>Conditional distributions are vital in machine learning for understanding relationships between features and predicting outcomes. Some key uses include:</p> <ol> <li> <p><strong>Generative Models</strong>: In models like Gaussian Mixture Models (GMM) or Hidden Markov Models (HMM), conditional distributions are used to model how data points (such as observations or states) are generated given certain parameters.</p> </li> <li> <p><strong>Bayesian Networks</strong>: In Bayesian networks, the conditional distributions represent the probabilistic dependencies between variables. Each node (representing a random variable) has a conditional distribution based on its parent nodes, and the overall network structure allows us to compute the joint distribution of all variables.</p> </li> <li> <p><strong>Expectation-Maximization (EM) Algorithm</strong>: The EM algorithm, used for unsupervised learning and model fitting, relies on conditional distributions to estimate parameters in models with missing or incomplete data. The E-step computes the conditional distributions of hidden variables given the observed data.</p> </li> </ol> <p>While conditional probability allows us to adjust our expectations based on new information, conditional distributions give us a broader view of how data is distributed when specific conditions are known.</p> <hr> <h4 id="law-of-total-probability-a-fundamental-tool-for-dealing-with-uncertainty"><strong>Law of Total Probability: A Fundamental Tool for Dealing with Uncertainty</strong></h4> <p>The Law of Total Probability is a key principle that allows us to compute the probability of an event by considering all possible ways that event could occur, based on different conditions or scenarios. This law is often used in machine learning when dealing with complex models where outcomes depend on multiple factors, or when some information is missing or unknown.</p> <h5 id="what-is-the-law-of-total-probability"><strong>What is the Law of Total Probability?</strong></h5> <p>It helps us calculate the probability of an event by partitioning the sample space into different mutually exclusive events and then summing up the probabilities of the event occurring in each of these partitions.</p> <p>Mathematically, the law is expressed as:</p> \[P(A) = \sum_{i} P(A \mid B_i) P(B_i)\] <p>Where:</p> <ul> <li>\(P(A)\) is the total probability of event \(A\).</li> <li>\(B_1, B_2, \dots, B_n\) are a partition of the sample space, meaning these events are mutually exclusive and exhaustive (they cover all possible outcomes).</li> <li>\(P(A \mid B_i)\) is the <strong>conditional probability</strong> of \(A\) given \(B_i\), i.e., the probability of \(A\) occurring under the condition that \(B_i\) occurs.</li> <li>\(P(B_i)\) is the probability of event \(B_i\).</li> </ul> <p>The law essentially breaks down the probability of \(A\) into cases based on different conditions \(B_1, B_2, \dots, B_n\), and then combines them weighted by the likelihood of each condition.</p> <p><strong>So, how to internalize this idea:</strong></p> <p>Imagine you are trying to determine the probability that a customer will purchase a product \(A\), but you have different types of customers \(B_1, B_2, \dots, B_n\) (e.g., based on their age group, spending history, etc.). The probability that a customer purchases the product will vary depending on their group. The Law of Total Probability tells you to:</p> <ol> <li>Calculate the probability of purchasing given each customer type (e.g., \(P(A \mid B_1)\), \(P(A \mid B_2)\), etc.).</li> <li>Multiply these by the probability of each customer type occurring (e.g., \(P(B_1)\), \(P(B_2)\)).</li> <li>Sum these products to find the total probability of purchasing across all customer types.</li> </ol> <p>In this way, the law allows you to compute the overall probability by considering all relevant scenarios (customer types) and weighing them accordingly.</p> <h5 id="why-is-the-law-of-total-probability-important-in-machine-learning"><strong>Why is the Law of Total Probability Important in Machine Learning?</strong></h5> <p>In machine learning, the Law of Total Probability is widely used for various tasks, especially in probabilistic modeling, classification, and predictive analytics. Some key applications include:</p> <ol> <li> <p><strong>Bayesian Inference</strong>: When updating beliefs about a hypothesis (or class) based on new data, the total probability is calculated over all possible hypotheses or classes. This helps refine predictions and is foundational in models such as <strong>Naive Bayes</strong>.</p> </li> <li> <p><strong>Handling Missing Data</strong>: In models dealing with missing data, the law helps to marginalize over the unknown values by considering all possible ways the data could be missing. For example, in the <strong>Expectation-Maximization (EM)</strong> algorithm, the law is used to estimate the missing values based on the observed data.</p> </li> <li> <p><strong>Class Conditional Probability in Classification</strong>: In classification problems, especially when working with multiple classes, the law allows the decomposition of class probabilities into conditional probabilities based on different features, facilitating the calculation of total class probabilities.</p> </li> </ol> <p><strong>Example: Applying the Law of Total Probability</strong></p> <p>Let’s consider an example in a classification task. Suppose you are trying to predict whether a customer will buy a product \(A\) (event \(A\)), and you have two features that classify the customer: whether they are a <strong>new customer</strong> (\(B_1\)) or a <strong>returning customer</strong> (\(B_2\)).</p> <p>The Law of Total Probability helps you compute the total probability of purchasing the product, considering both new and returning customers:</p> \[P(\text{Buy}) = P(\text{Buy} \mid \text{New Customer}) \cdot P(\text{New Customer}) + P(\text{Buy} \mid \text{Returning Customer}) \cdot P(\text{Returning Customer})\] <p>Here:</p> <ul> <li>\(P(\text{Buy} \mid \text{New Customer})\) is the probability that a new customer buys the product.</li> <li>\(P(\text{New Customer})\) is the probability that the customer is new.</li> <li>Similarly, \(P(\text{Buy} \mid \text{Returning Customer})\) and \(P(\text{Returning Customer})\) are for the returning customers.</li> </ul> <p>This allows you to compute the total probability of a customer buying the product, considering both customer types.</p> <h5 id="connection-with-conditional-probability"><strong>Connection with Conditional Probability</strong></h5> <p>The Law of Total Probability is built on conditional probability. It helps us to marginalize over unknown or unobserved conditions, ensuring we account for all possible scenarios that could influence the event of interest.</p> <p>For example, in a machine learning model that makes predictions based on different feature values, the law allows us to break down the total probability of an outcome by conditioning on the feature values and summing over all possible feature combinations.</p> <p>Whether you are building a Bayesian model, dealing with missing data, or predicting outcomes in complex scenarios, the Law of Total Probability provides a systematic way to combine multiple probabilities and refine your model’s predictions.</p> <hr> <h4 id="expectation-and-variance-essential-measures"><strong>Expectation and Variance: Essential Measures</strong></h4> <p>They provide valuable insights into the behavior of data and are widely used in machine learning to understand the characteristics of models, assess uncertainty, and make predictions. Here’s a breakdown of each concept and its relevance to machine learning.</p> <h5 id="what-is-expectation"><strong>What is Expectation?</strong></h5> <p>The <strong>expectation</strong> (or <strong>mean</strong>) of a random variable represents its <strong>average</strong> or <strong>central tendency</strong>. It is the weighted average of all possible values that the variable can take, where the weights are given by the probabilities of these values.</p> <p>For a discrete random variable \(X\), the expectation \(E(X)\) is defined as:</p> \[E(X) = \sum_{i} x_i P(x_i)\] <p>Where:</p> <ul> <li>\(x_i\) are the possible values that \(X\) can take.</li> <li>\(P(x_i)\) is the probability of \(X\) taking the value \(x_i\).</li> </ul> <p>For a continuous random variable with probability density function \(f(x)\), the expectation is:</p> \[E(X) = \int_{-\infty}^{\infty} x f(x) \, dx\] <p><strong>Intuition for Expectation:</strong></p> <p>Think of expectation as the “balance point” of a distribution. For example, if you were to imagine a physical rod with different weights placed at various points, the <strong>center of mass</strong> of the rod would represent the expectation.</p> <p>In machine learning, the expectation helps us understand the <strong>average behavior</strong> of the data. For instance, in regression tasks, the expectation of the target variable provides a baseline prediction.</p> <h5 id="what-is-variance"><strong>What is Variance?</strong></h5> <p>The <strong>variance</strong> of a random variable quantifies the spread or dispersion of the variable around its expectation. A high variance indicates that the values are widely spread out, while a low variance indicates that the values are clustered around the mean.</p> <p>For a discrete random variable \(X\), the variance \(\text{Var}(X)\) is defined as:</p> \[\text{Var}(X) = E[(X - E(X))^2] = \sum_{i} (x_i - E(X))^2 P(x_i)\] <p>For a continuous random variable:</p> \[\text{Var}(X) = \int_{-\infty}^{\infty} (x - E(X))^2 f(x) \, dx\] <p>Alternatively, variance can also be computed as:</p> \[\text{Var}(X) = E(X^2) - (E(X))^2\] <p>Where \(E(X^2)\) is the expectation of \(X^2\), i.e., the expected value of the square of \(X\).</p> <p><strong>Intuition for Variance:</strong></p> <p>Variance tells us about the <strong>spread</strong> of the data. Imagine measuring the height of a group of people:</p> <ul> <li>If everyone has a similar height, the variance will be low.</li> <li>If the group includes both very short and very tall individuals, the variance will be high.</li> </ul> <p>In machine learning, variance provides insights into <strong>model uncertainty</strong>. High variance in a model’s predictions indicates overfitting, while low variance suggests underfitting.</p> <h5 id="why-are-expectation-and-variance-important-in-machine-learning"><strong>Why Are Expectation and Variance Important in Machine Learning?</strong></h5> <ol> <li> <strong>Expectation</strong>: <ul> <li> <strong>Model Evaluation</strong>: Used as a baseline for evaluating model predictions (e.g., in regression tasks).</li> <li> <strong>Loss Functions</strong>: Central to defining loss functions like Mean Squared Error (MSE).</li> <li> <strong>Feature Engineering</strong>: Understanding the average behavior of features aids in creating or selecting the most informative ones.</li> </ul> </li> <li> <strong>Variance</strong>: <ul> <li> <strong>Bias-Variance Tradeoff</strong>: Balancing model complexity to avoid overfitting (high variance) or underfitting (low variance).</li> <li> <strong>Model Complexity</strong>: Guides the choice of model complexity (e.g., simpler models like linear regression have lower variance).</li> <li> <strong>Uncertainty Estimation</strong>: Quantifies confidence in probabilistic models like Gaussian Processes.</li> <li> <strong>Performance Metrics</strong>: Used in cross-validation to measure consistency across datasets.</li> </ul> </li> </ol> <hr> <h4 id="covariance-and-correlation-measuring-relationships-between-variables"><strong>Covariance and Correlation: Measuring Relationships Between Variables</strong></h4> <p>Covariance and correlation are statistical tools used to understand the relationships between two random variables. In machine learning, these concepts are essential for identifying feature interactions, reducing dimensionality, and improving model performance.</p> <h5 id="what-is-covariance"><strong>What is Covariance?</strong></h5> <p>Covariance measures the <strong>direction</strong> of the linear relationship between two variables, indicating whether they increase or decrease together.</p> <p>For two random variables \(X\) and \(Y\), the covariance is defined as:</p> \[\text{Cov}(X, Y) = E\left[(X - E(X))(Y - E(Y))\right]\] <p>Where:</p> <ul> <li>\(E(X)\) and \(E(Y)\) are the expectations of \(X\) and \(Y\).</li> <li>\((X - E(X))\) and \((Y - E(Y))\) represent deviations from their means.</li> </ul> <p><strong>Interpretation</strong>:</p> <ul> <li>\(\text{Cov}(X, Y) &gt; 0\): Positive relationship (as \(X\) increases, \(Y\) tends to increase).</li> <li>\(\text{Cov}(X, Y) &lt; 0\): Negative relationship (as \(X\) increases, \(Y\) tends to decrease).</li> <li>\(\text{Cov}(X, Y) = 0\): No linear relationship.</li> </ul> <h5 id="what-is-correlation"><strong>What is Correlation?</strong></h5> <p>Correlation is a <strong>scaled version of covariance</strong> that provides the strength and direction of the relationship on a fixed scale \([-1, 1]\).</p> <p>The <strong>Pearson correlation coefficient</strong> is defined as:</p> \[\rho(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}\] <p>Where:</p> <ul> <li>\(\sigma_X\) and \(\sigma_Y\) are the standard deviations of \(X\) and \(Y\).</li> </ul> <p><strong>Interpretation</strong>:</p> <ul> <li>\(\rho(X, Y) = 1\): Perfect positive linear relationship.</li> <li>\(\rho(X, Y) = -1\): Perfect negative linear relationship.</li> <li>\(\rho(X, Y) = 0\): No linear relationship.</li> </ul> <h5 id="key-differences"><strong>Key Differences</strong></h5> <table> <thead> <tr> <th><strong>Aspect</strong></th> <th><strong>Covariance</strong></th> <th><strong>Correlation</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Scale</strong></td> <td>Depends on the units of variables</td> <td>Unitless, standardized</td> </tr> <tr> <td><strong>Range</strong></td> <td>\((-\infty, \infty)\)</td> <td>\([-1, 1]\)</td> </tr> <tr> <td><strong>Use Case</strong></td> <td>Direction of relationship</td> <td>Strength and direction combined</td> </tr> </tbody> </table> <p>//</p> <h5 id="applications-in-machine-learning"><strong>Applications in Machine Learning</strong></h5> <ol> <li> <strong>Feature Relationships</strong>: <ul> <li>Covariance highlights how features interact.</li> <li>Correlation quantifies redundancy or relevance.</li> </ul> </li> <li> <strong>Feature Selection</strong>: <ul> <li>Retain features with high correlation to the target.</li> <li>Remove features with high inter-correlation to reduce multicollinearity.</li> </ul> </li> <li> <strong>Dimensionality Reduction</strong>: <ul> <li> <strong>Principal Component Analysis (PCA)</strong> uses covariance or correlation matrices to identify directions of maximum variance.</li> </ul> </li> </ol> <hr> <h4 id="central-limit-theorem-the-foundation-of-statistical-inference"><strong>Central Limit Theorem: The Foundation of Statistical Inference</strong></h4> <p>The <strong>Central Limit Theorem (CLT)</strong> explains why normal distributions appear so frequently in practice and is key for making inferences about data.</p> <h5 id="what-is-the-central-limit-theorem"><strong>What is the Central Limit Theorem?</strong></h5> <p>The <strong>Central Limit Theorem</strong> states that for a population with a finite mean \(\mu\) and variance \(\sigma^2\), the distribution of the <strong>sample mean</strong> from sufficiently large random samples will approximate a <strong>normal distribution</strong>, regardless of the original distribution of the population.</p> <p>Mathematically, if \(X_1, X_2, \dots, X_n\) are i.i.d. random variables drawn from a population with mean \(\mu\) and variance \(\sigma^2\), the sample mean \(\bar{X}\) is defined as:</p> \[\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i\] <p>As the sample size \(n\) increases, the sample mean has the following properties:</p> <ul> <li>The <strong>mean</strong> of \(\bar{X}\) is \(\mu\) (the population mean).</li> <li>The <strong>variance</strong> of \(\bar{X}\) is \(\frac{\sigma^2}{n}\), meaning the variance decreases as \(n\) increases.</li> <li>The distribution of \(\bar{X}\) approaches a <strong>normal distribution</strong> with mean \(\mu\) and variance \(\frac{\sigma^2}{n}\), and the <strong>standard deviation</strong> becomes \(\frac{\sigma}{\sqrt{n}}\), called the <strong>standard error</strong>.</li> </ul> <p><strong>How do we remember this?</strong></p> <p>Imagine you are sampling from a non-normal distribution, such as the distribution of ages in a city. A small sample might produce a skewed or non-normal distribution. However, as you increase the sample size, the distribution of the sample mean will become increasingly normal, regardless of the original distribution shape.</p> <p>This phenomenon is like averaging noisy measurements in engineering. A single measurement might be noisy, but averaging multiple measurements reduces the noise, making the result more predictable and normally distributed.</p> <h5 id="why-is-the-central-limit-theorem-important-in-machine-learning"><strong>Why is the Central Limit Theorem Important in Machine Learning?</strong></h5> <p>The Central Limit Theorem is foundational in statistics and machine learning for the following reasons:</p> <ol> <li> <strong>Foundation for Inference</strong>: <ul> <li>The CLT enables statistical inference techniques like hypothesis testing and confidence intervals. When drawing random samples, the sample mean will follow a normal distribution, allowing for probabilistic statements about population parameters.</li> </ul> </li> <li> <strong>Simplifying Assumptions</strong>: <ul> <li>Many machine learning algorithms assume normality (e.g., linear regression). The CLT allows us to assume that, for sufficiently large datasets, estimators of model parameters will follow a normal distribution, making them easier to analyze.</li> </ul> </li> <li> <strong>Sample Size Considerations</strong>: <ul> <li>The CLT shows that, for large datasets, we can assume normality even if the underlying data is non-normal. As the sample size increases, algorithms become more stable and their performance becomes more predictable.</li> </ul> </li> </ol> <p><strong>Example of the Central Limit Theorem in Practice</strong></p> <p>Imagine you are analyzing <strong>house prices</strong> in a city, and the distribution of house prices is highly skewed due to a few luxury homes. You want to estimate the average house price.</p> <ul> <li> <p><strong>Without CLT</strong>: The highly skewed data would result in a mean that doesn’t reflect the typical house price.</p> </li> <li> <p><strong>With CLT</strong>: By taking random samples, computing the mean for each sample, and repeating the process many times, the distribution of sample means will become normal, even though the underlying distribution of house prices is skewed. The sample mean will be a more reliable estimator of the population mean, allowing for more accurate confidence intervals.</p> </li> </ul> <h5 id="central-limit-theorem-in-machine-learning"><strong>Central Limit Theorem in Machine Learning</strong></h5> <p>The CLT is useful in several machine learning contexts:</p> <ol> <li> <strong>Regression Models</strong>: <ul> <li>In linear regression, the CLT implies that, for large sample sizes, the distribution of estimated coefficients will be approximately normal. This enables the use of confidence intervals to assess uncertainty in the coefficients.</li> </ul> </li> <li> <strong>Bootstrap Methods</strong>: <ul> <li>The CLT is essential for bootstrap resampling methods, which estimate the variability of a statistic (like the mean) by repeatedly sampling from the data. Due to the CLT, the distribution of these sample statistics will be approximately normal.</li> </ul> </li> <li> <strong>Confidence Intervals and Hypothesis Testing</strong>: <ul> <li>Many machine learning techniques rely on the CLT to estimate confidence intervals and perform hypothesis testing. For example, in regression, the standard error of the coefficients is derived from the CLT.</li> </ul> </li> </ol> <h5 id="conditions-for-the-central-limit-theorem"><strong>Conditions for the Central Limit Theorem</strong></h5> <p>For the CLT to hold, the following conditions are necessary:</p> <ol> <li> <strong>Independence</strong>: The samples must be independent.</li> <li> <strong>Sample Size</strong>: The sample size should be large enough. Typically, a sample size of 30 or more is considered sufficient for the CLT to apply.</li> <li> <strong>Finite Variance</strong>: The population must have a finite variance.</li> </ol> <p>By leveraging the CLT, you can make reliable estimates, perform hypothesis testing, and create models that work well, even when the underlying data distribution is non-normal.</p> <hr> <p>Finally, we’ve explored key concepts in probability theory that are important to machine learning. From understanding the basics of probability distributions and Bayes’ Theorem to the relationships between variables through covariance and correlation, these concepts provide the mathematical layer for building robust models. Finally, the Central Limit Theorem ties everything together, offering insight into statistical inference and ensuring that predictions and model estimates are reliable.</p> <p>In the next post, we’ll continue diving deeper into the brief history of machine learning and what are we upto currently.</p> <p><strong>See you there!</strong></p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-distributed-systems-lecture-1",title:"Distributed Systems - Lecture 1",description:"Distributed Systems Course at NYU Courant - Personal Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2026/ds1/"}},{id:"post-llmr-a1",title:"Llmr A1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/llmr-a1/"}},{id:"post-llmr-lecture-1",title:"LLMR - Lecture 1",description:"LLM Reasoners Course at NYU Courant - Personal Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2026/llmr1/"}},{id:"post-apache-flink",title:"Apache Flink",description:"Realtime and Big Data Analytics Course at NYU Courant - Personal Notes 10",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-11-flink/"}},{id:"post-apache-kafka",title:"Apache Kafka",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 9",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-10-kafka/"}},{id:"post-apache-zookeeper",title:"Apache ZooKeeper",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 8",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-9-zookeeper/"}},{id:"post-apache-hbase",title:"Apache HBase",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 7",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-8-hbase/"}},{id:"post-hive-amp-trino",title:"Hive &amp; Trino",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 6",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-7-hive/"}},{id:"post-mapreduce-design-patterns",title:"MapReduce Design Patterns",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 5",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-5-mr-dp/"}},{id:"post-big-data-processing-concepts-amp-mapreduce",title:"Big Data Processing Concepts &amp; MapReduce",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-4-mapreduce/"}},{id:"post-hadoop-distributed-file-system-hdfs",title:"Hadoop Distributed File System (HDFS)",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-3-hdfs/"}},{id:"post-reading-notes-from-aleksa-gordic-39-s-gpu-blogpost",title:"Reading Notes from Aleksa Gordic&#39;s GPU BlogPost",description:"Reading notes for my reference from Aleksa Gordic&#39;s GPU BlogPost",section:"Posts",handler:()=>{window.location.href="/blog/2025/aleksagordic-gpu-blog-notes/"}},{id:"post-mlp-standard-derivatives-derivation",title:"MLP Standard Derivatives Derivation",description:"MLP Standard Derivatives Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-derivatives/"}},{id:"post-simple-mlp-forward-and-backward-pass-with-einsum-derivation",title:"Simple MLP - Forward and Backward Pass (With Einsum) Derivation",description:"Simple MLP - Forward and Backward Pass Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-fw-bwd/"}},{id:"post-gpu-notes",title:"GPU Notes",description:"GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes",section:"Posts",handler:()=>{window.location.href="/blog/2025/gpu-notes/"}},{id:"post-big-data-storage",title:"Big Data Storage",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 2",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-2-storage/"}},{id:"post-introduction-to-realtime-and-big-data-analytics",title:"Introduction to Realtime and Big Data Analytics",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-1-intro/"}},{id:"post-gpu-essentials-a-concise-technical-guide",title:"GPU Essentials - A Concise Technical Guide",description:"A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.",section:"Posts",handler:()=>{window.location.href="/blog/2025/GPU-Intro/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey-blog-2025-wrapping-ml-basics",title:"[Wrapping Up Our ML Foundations Journey](/blog/2025/wrapping-ml-basics/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-smallgraphgcn-accelerating-gnn-training-on-batched-small-graphs",title:"SmallGraphGCN - Accelerating GNN Training on Batched Small Graphs",description:"Discover how fused-edge-centric CUDA kernels dramatically accelerate Graph Neural Network training on molecular datasets. By rethinking parallelism strategies for batched small graphs, our approach achieves up to 3.1\xd7 faster forward execution and 1.3\xd7 end-to-end training speedup over PyTorch Geometric.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-from-baseline-to-deepseek-single-gpu-moe-training-efficiency",title:"From Baseline to DeepSeek - Single-GPU MoE Training Efficiency",description:"A systems-level analysis of training Mixture-of-Experts (MoE) Transformer models under single-GPU constraints. We compare naive PyTorch MoE, ScatterMoE, MegaBlocks, and DeepSeek-inspired architectures, revealing critical trade-offs between convergence behavior, memory footprint, and training throughput.",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>