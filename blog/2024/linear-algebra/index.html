<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Linear Algebra - Prerequisites for Machine Learning | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="This blog post covers the key linear algebra concepts and their applications in machine learning."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2024/linear-algebra/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Algebra - Prerequisites for Machine Learning</h1> <p class="post-meta"> Created in December 20, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> Math</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Linear algebra forms the backbone of modern machine learning. As a branch of mathematics, it deals with vector spaces and the linear transformations between them. This area of study allows for the manipulation and efficient computation of datasets, making it fundamental to various machine learning algorithms. Whether you’re working with deep learning, regression models, or optimization techniques, a solid understanding of linear algebra will be crucial to mastering machine learning.</p> <hr> <h4 id="core-components-of-linear-algebra"><strong>Core Components of Linear Algebra</strong></h4> <h5 id="1-vectors"><strong>1. Vectors</strong></h5> <p>A <strong>vector</strong> is a fundamental concept in linear algebra and is essentially a one-dimensional array of numbers. In machine learning, vectors can represent different elements, including features, weights, or data points.</p> <ul> <li> <strong>Definition:</strong> A vector is a set of numbers arranged in a specific order, and it can be represented either as a <strong>row vector</strong> or a <strong>column vector</strong>. <ul> <li>Row vector: \(\mathbf{v} = [v_1, v_2, \dots, v_n]\)</li> <li>Column vector: \(\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n\end{bmatrix}\)</li> </ul> </li> <li> <p><strong>Properties:</strong></p> <ul> <li> <strong>Magnitude (Norm):</strong> The magnitude of a vector, often referred to as its norm, measures the vector’s length. The most common norms used are the L2 norm (Euclidean norm) and L1 norm.</li> </ul> \[\|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2} \ \ ; \quad \|\mathbf{v}\|_1 = \sum_{i=1}^{n} |v_i|\] <ul> <li> <strong>Dot Product:</strong> The dot product of two vectors measures their similarity. The dot product between two vectors \(\mathbf{v}_1​\) and \(\mathbf{v}_2\)​ is computed as:</li> </ul> \[\mathbf{v}_1 \cdot \mathbf{v}_2 = \sum_{i=1}^{n} v_{1i} v_{2i}\] <ul> <li> <strong>Distance:</strong> The Euclidean distance is a common way to measure the difference between two vectors:</li> </ul> \[d(\mathbf{v}_1, \mathbf{v}_2) = \sqrt{\sum_{i=1}^{n} (v_{1i} - v_{2i})^2}\] </li> <li> <strong>Operations on Vectors:</strong> <ul> <li> <strong>Addition:</strong> Vectors of the same size can be added element-wise.</li> <li> <strong>Scalar Multiplication:</strong> Multiplying each element of a vector by a scalar.</li> <li> <strong>Dot Product:</strong> A fundamental operation for determining the similarity between two vectors.</li> </ul> </li> </ul> <h5 id="2-matrices"><strong>2. Matrices</strong></h5> <p>A <strong>matrix</strong> is a two-dimensional array of numbers, and it is widely used in machine learning for data storage, transformations, and solving systems of equations.</p> <ul> <li> <p><strong>Definition:</strong> A matrix consists of rows and columns and is denoted as \(A\), where \(A_{ij}\)​ represents the element in the i-th row and j-th column.</p> \[A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn} \end{bmatrix}\] </li> <li> <strong>Properties of Matrices:</strong> <ul> <li> <strong>Rank:</strong> The rank of a matrix is the maximum number of linearly independent rows or columns, indicating the number of independent dimensions in the matrix.</li> <li> <strong>Trace:</strong> The trace is the sum of the diagonal elements of a square matrix. It is often involved in optimization problems.</li> <li> <strong>Determinant:</strong> The determinant helps in determining whether a matrix is invertible. A non-zero determinant implies that the matrix is invertible.</li> <li> <strong>Invertibility:</strong> A matrix \(A\) is invertible if it has full rank and a non-zero determinant. The inverse of a matrix \(A\) is denoted by \(A^{-1}\), and it satisfies the equation: \(A A^{-1} = I\) where \(I\) is the identity matrix.</li> </ul> </li> <li> <strong>Operations on Matrices:</strong> <ul> <li> <strong>Matrix Addition/Subtraction:</strong> Matrices of the same dimension can be added or subtracted element-wise.</li> <li> <strong>Matrix Multiplication:</strong> Matrix multiplication is the dot product of rows and columns between two matrices. This operation is central to machine learning algorithms.</li> <li> <strong>Transpose:</strong> The transpose of a matrix \(A\) is denoted as \(A^T\) and involves flipping its rows and columns.</li> <li> <strong>Inverse:</strong> If a matrix is invertible, its inverse can be used to solve systems of linear equations.</li> </ul> </li> </ul> <h5 id="vectors-and-matrices-in-ml"><strong>Vectors and Matrices in ML</strong></h5> <p>Vectors and matrices play a pivotal role in representing both data and models in machine learning.</p> <p><strong>1. Data Representation</strong></p> <ul> <li>In supervised learning, each data point is typically represented as a feature vector. For example, if a dataset has \(m\) samples and \(n\) features, it can be represented as an \(m \times n\) matrix, where each row corresponds to a feature vector for a data point. The corresponding labels or target values are often stored in a vector.</li> </ul> <p><strong>2. Model Representation</strong></p> <ul> <li>In models like linear regression and neural networks, the weights that transform input data are stored in vectors or matrices. For example, in linear regression, the model is defined as: \(\hat{y} = Xw + b\) where \(X\) is the data matrix, \(w\) is the weight vector, and \(b\) is the bias term.</li> </ul> <p><strong>3. Operations in Machine Learning Algorithms</strong></p> <ul> <li> <strong>Linear Regression:</strong> In linear regression, matrix operations are used to solve for the optimal weights. The normal equation for linear regression is:</li> </ul> \[w=(X^TX)^{−1}X^Ty\] <p>where \(X\) is the matrix of input features and \(y\) is the vector of target values.</p> <ul> <li> <strong>Neural Networks:</strong> Each layer of a neural network applies a linear transformation to its input, which is represented by matrix multiplication:</li> </ul> \[y=XW+b\] <p>where \(X\) is the input matrix, \(W\) is the weight matrix, and \(b\) is the bias vector.</p> <ul> <li> <strong>Gradient Descent:</strong> The gradient descent optimization algorithm frequently uses vector and matrix operations to update model parameters iteratively. In deep learning, the gradient of the loss function with respect to the weights and biases is calculated using matrix operations during back-propagation.</li> </ul> <p><strong>4. Dimensionality Reduction</strong></p> <ul> <li>Principal Component Analysis (PCA) is a popular technique for dimensionality reduction. It involves finding the eigenvectors and eigenvalues of the covariance matrix of the data. Eigenvalues and Eigenvectors are explained down below.</li> </ul> <hr> <h4 id="eigenvalues-and-eigenvectors"><strong>Eigenvalues and Eigenvectors</strong></h4> <h5 id="definition"><strong>Definition:</strong></h5> <ul> <li> <p><strong>Eigenvector:</strong><br> An eigenvector of a square matrix \(\mathbf{A}\) is a non-zero vector \(\mathbf{v}\) that, when the matrix \(\mathbf{A}\) is applied to it, only scales the vector without changing its direction:</p> \[\mathbf{A} \mathbf{v} = \lambda \mathbf{v}\] <p>where:</p> <ul> <li>\(\mathbf{v}\) is the eigenvector,</li> <li>\(\lambda\) is the eigenvalue, the scalar that represents how much the eigenvector is scaled by the transformation.</li> </ul> </li> <li> <p><strong>Eigenvalue:</strong><br> The eigenvalue \(\lambda\) is the factor by which the eigenvector is scaled when the matrix \(\mathbf{A}\) acts on it.</p> </li> </ul> <p><strong>To build intuition, consider this analogy:</strong></p> <p>Imagine a squishy sheet of rubber (the matrix) and a point in space (the vector). If you apply a transformation (like stretching, rotating, or shearing) to the point using the rubber sheet, most points move to new locations. However, some special points, called <strong>eigenvectors</strong>, only get <strong>stretched</strong> or <strong>compressed</strong> but <strong>stay in the same direction</strong>. The amount of stretching or compression is determined by the <strong>eigenvalue</strong>.</p> <p><strong>Mathematical Properties of Eigenvalues and Eigenvectors</strong></p> <ul> <li>Eigenvectors must be <strong>non-zero vectors</strong>.</li> <li>Eigenvalues can be <strong>real</strong> or <strong>complex</strong> (but are often real in machine learning applications).</li> <li>A matrix can have multiple eigenvectors corresponding to the <strong>same eigenvalue</strong> (if it is <strong>degenerate</strong>) or distinct eigenvalues corresponding to distinct eigenvectors.</li> </ul> <h5 id="why-are-eigenvalues-and-eigenvectors-important-in-machine-learning"><strong>Why Are Eigenvalues and Eigenvectors Important in Machine Learning?</strong></h5> <p><strong>PCA</strong> is a widely used technique for <strong>dimensionality reduction</strong> in machine learning. It reduces the number of features while retaining the most important information in the dataset.</p> <ul> <li> <strong>Covariance Matrix:</strong> PCA begins by computing the covariance matrix to capture relationships between features.</li> <li> <strong>Eigenvectors of Covariance Matrix:</strong> The eigenvectors represent the directions of maximum variance in the data—these are the <strong>principal components</strong>.</li> <li> <strong>Eigenvalues:</strong> The corresponding eigenvalues indicate the magnitude of variance in each direction.</li> </ul> <p>Key steps in PCA:</p> <ul> <li>Sort eigenvectors in decreasing order of their eigenvalues.</li> <li>Select the top <strong>k</strong> eigenvectors to reduce dimensionality while preserving most of the variance.</li> </ul> <h5 id="key-takeaways-of-eigenvalues-and-eigenvectors-in-ml"><strong>Key takeaways of Eigenvalues and Eigenvectors in ML</strong></h5> <ul> <li> <strong>Diagonalizability:</strong><br> A matrix is diagonalizable if it has enough eigenvectors to form a full basis. This property is essential in PCA and <strong>Singular Value Decomposition (SVD)</strong>, enabling efficient computation and interpretation.</li> <li> <strong>Magnitude of Eigenvalues:</strong><br> The magnitude of eigenvalues corresponds to the <strong>variance captured</strong> by the associated eigenvectors (principal components). Larger eigenvalues imply more variance explained.</li> <li> <strong>Orthogonality of Eigenvectors (Symmetric Matrices):</strong><br> For <strong>symmetric matrices</strong>, eigenvectors are <strong>orthogonal</strong>. This is critical in PCA, where the principal components are orthogonal, ensuring that reduced dimensions remain <strong>uncorrelated</strong>.</li> </ul> <hr> <h4 id="a-few-more-key-matrices-types-relevant-to-ml"><strong>A Few More Key Matrices Types Relevant to ML</strong></h4> <h5 id="symmetric-matrix"><strong>Symmetric Matrix:</strong></h5> <ul> <li> <strong>Definition:</strong> A matrix \(A\) is <strong>symmetric</strong> if \(A = A^T\), meaning it is equal to its transpose.</li> <li> <strong>Properties:</strong> <ul> <li>A symmetric matrix always has real eigenvalues and orthogonal eigenvectors.</li> <li>If \(A\) is symmetric, it is always diagonalizable.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Covariance Matrices:</strong> Covariance matrices are always symmetric because the covariance between two features is the same regardless of the order.</li> <li> <strong>Optimization Problems:</strong> Many optimization problems in machine learning involve symmetric matrices (e.g., in second-order optimization methods like Newton’s method or in regularization).</li> </ul> </li> </ul> <h5 id="orthogonal-matrix"><strong>Orthogonal Matrix:</strong></h5> <ul> <li> <strong>Definition:</strong> A matrix \(A\) is <strong>orthogonal</strong> if \(A^T A = I\), where \(I\) is the identity matrix.</li> <li> <strong>Properties:</strong> <ul> <li>The rows and columns of an orthogonal matrix are orthonormal (i.e., they are both orthogonal and of unit length).</li> <li>The inverse of an orthogonal matrix is equal to its transpose \((A^{-1} = A^T)\).</li> <li>The determinant of an orthogonal matrix is either +1 or -1.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Rotation and Transformation:</strong> Orthogonal matrices are used in certain machine learning algorithms for transformations that preserve distances and angles. For example, in PCA, orthogonal transformation is used to create new orthogonal basis vectors.</li> </ul> </li> </ul> <h5 id="positive-definite-matrix-pd"><strong>Positive Definite Matrix (PD):</strong></h5> <ul> <li> <strong>Definition:</strong> A square matrix \(A\) is <strong>positive definite</strong> if for any non-zero vector \(\mathbf{v}\), \(\mathbf{v}^T A \mathbf{v} &gt; 0\). In simpler terms, it means that the matrix has strictly positive eigenvalues.</li> <li> <strong>Properties:</strong> <ul> <li>All eigenvalues are positive.</li> <li>The matrix is invertible (non-singular).</li> <li>It implies that the quadratic form is always positive.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Optimization Problems:</strong> In convex optimization, the Hessian matrix of a convex function is often positive definite. This ensures that a function has a unique local minimum, making optimization well-posed.</li> <li> <strong>Covariance Matrices:</strong> The covariance matrix of any dataset with multiple features is positive semi-definite. In special cases (e.g., full rank), it can be positive definite.</li> </ul> </li> </ul> <h5 id="positive-semi-definite-matrix-psd"><strong>Positive Semi-Definite Matrix (PSD):</strong></h5> <ul> <li> <strong>Definition:</strong> A matrix \(A\) is <strong>positive semi-definite</strong> if for any vector \(\mathbf{v}\), \(\mathbf{v}^T A \mathbf{v} \geq 0\). In other words, all eigenvalues are non-negative (i.e., zero or positive).</li> <li> <strong>Properties:</strong> <ul> <li>Eigenvalues are non-negative \((\lambda_i \geq 0)\).</li> <li>The matrix may not be invertible if it has zero eigenvalues.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Covariance Matrices:</strong> As mentioned, the covariance matrix of a dataset is positive semi-definite. This is essential because covariance cannot be negative and the matrix represents the relationship between features.</li> <li> <strong>Kernel Matrices (in SVMs, Gaussian Processes, etc.):</strong> The kernel matrix in algorithms like SVM and kernel PCA is always positive semi-definite. It measures similarity between data points in a transformed feature space.</li> </ul> </li> </ul> <h5 id="covariance-matrix"><strong>Covariance Matrix:</strong></h5> <ul> <li> <p><strong>Definition:</strong> A <strong>covariance matrix</strong> is a square matrix that contains the covariances between pairs of features in a dataset. If a dataset has \(n\) features, the covariance matrix will be an \(n \times n\) matrix, where each entry represents the covariance between two features.</p> </li> <li> <p><strong>Covariance of two variables X and Y:</strong></p> \[\text{Cov}(X, Y) = \frac{1}{m} \sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})\] <p>where \(m\) is the number of data points, and \(\bar{x}\) and \(\bar{y}\)​ are the means of the features \(X\) and \(Y\), respectively.</p> </li> <li> <p><strong>Covariance Matrix Definition:</strong> For a dataset with \(n\) features, the covariance matrix \(\Sigma\) is an \(n \times n\) matrix where each entry is:</p> \[\Sigma_{ij} = \text{Cov}(X_i, X_j)\] <p>where \(X_i\)​ and \(X_j\)​ are the \(i\)-th and \(j\)-th features, respectively.</p> </li> <li> <strong>Properties:</strong> <ul> <li> <strong>Symmetry:</strong> The covariance matrix is always symmetric, i.e., \(\Sigma_{ij} = \Sigma_{ji}\)</li> <li> <strong>Positive Semi-Definiteness (PSD):</strong> The covariance matrix is always positive semi-definite, meaning for any vector \(\mathbf{v}\), \(\mathbf{v}^T \Sigma \mathbf{v} \geq 0\).</li> <li> <strong>Diagonal Entries (Variance):</strong> The diagonal entries represent the variance of individual features.</li> <li> <strong>Off-Diagonal Entries (Covariance):</strong> The off-diagonal entries represent the covariance between different features. Positive covariance indicates that the features increase or decrease together, while negative covariance suggests they move inversely.</li> <li> <strong>Eigenvalues and Eigenvectors:</strong> The eigenvectors of the covariance matrix represent the directions of maximum variance, while the eigenvalues represent the magnitude of variance along these directions.</li> <li> <strong>Rank:</strong> The rank of the covariance matrix corresponds to the number of <strong>independent</strong> features. If the matrix is rank-deficient, it indicates linearly dependent features.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>PCA:</strong> In PCA, the covariance matrix is used to identify the directions (eigenvectors) of maximum variance in the dataset. Eigenvalues indicate how much variance is explained by each principal component. This helps in dimensionality reduction by selecting the most important components.</li> <li> <strong>Multivariate Gaussian Distribution:</strong> In probabilistic models like <strong>Gaussian Mixture Models (GMM)</strong>, the covariance matrix defines the shape of the data distribution. It is used to model the distribution of features in a multi-dimensional space.</li> <li> <strong>Feature Selection:</strong> Covariance matrices help identify correlated features. Features that show high covariance (i.e., strong correlation) can be dropped or combined to improve model performance and reduce dimensionality.</li> </ul> </li> </ul> <h5 id="full-rank-matrix"><strong>Full Rank Matrix:</strong></h5> <ul> <li> <strong>Definition:</strong> A matrix is <strong>full rank</strong> if its rank is equal to the smallest of its number of rows or columns. In other words, all rows (or columns) are linearly independent.</li> <li> <strong>Properties:</strong> <ul> <li>A matrix \(A\) with full rank has no redundant or dependent rows or columns.</li> <li>If \(A\) is an \(m \times n\) matrix, and \(\text{rank}(A) = \min(m, n)\), the matrix is full rank.</li> <li>A full rank matrix is <strong>invertible</strong> if it is square (i.e., if \(m = n\)).</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Linear Regression:</strong> In linear regression, the design matrix \(X\) must be full rank to ensure a unique solution. If \(X\) is not full rank, the matrix \(X^T X\) is singular and cannot be inverted.</li> </ul> </li> </ul> <h5 id="singular-matrix"><strong>Singular Matrix:</strong></h5> <ul> <li> <strong>Definition:</strong> A matrix is <strong>singular</strong> if it is not invertible, meaning its determinant is zero. A singular matrix has linearly dependent rows or columns.</li> <li> <strong>Properties:</strong> <ul> <li>The determinant of a singular matrix is 0.</li> <li>The matrix has at least one eigenvalue equal to 0.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Linear Dependence:</strong> If the feature matrix \(X\) in a linear model is singular, some features are perfectly correlated, and this leads to instability in training and difficulties in solving for the model parameters.</li> </ul> </li> </ul> <hr> <p>That wraps up the key linear algebra concepts for machine learning. This post is designed as a quick reference rather than an exhaustive guide. Don’t stress about memorizing everything—focus instead on understanding the concepts and knowing when to revisit them if needed.</p> <p>Math is a language, and like any language, it’s more about learning to use it than memorizing rules. Treat this as a foundation to build on, and come back to refresh your knowledge whenever necessary.</p> <p>Up next, we’ll explore the prerequisites of <strong>Probability Theory</strong> for machine learning. Since probability can often feel trickier, we’ll focus more on “what,” “why,” and “how” questions to make the concepts intuitive and approachable.</p> <p>See you in the next one!</p> <h5 id="references"><strong>References:</strong></h5> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>