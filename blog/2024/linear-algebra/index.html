<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Linear Algebra - Prerequisites for Machine Learning | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="This blog post covers the key linear algebra concepts and their applications in machine learning."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2024/linear-algebra/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Algebra - Prerequisites for Machine Learning</h1> <p class="post-meta"> Created in December 20, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   ·   <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Linear algebra forms the backbone of modern machine learning. As a branch of mathematics, it deals with vector spaces and the linear transformations between them. This area of study allows for the manipulation and efficient computation of datasets, making it fundamental to various machine learning algorithms. Whether you’re working with deep learning, regression models, or optimization techniques, a solid understanding of linear algebra will be crucial to mastering machine learning.</p> <hr> <h2 id="core-components-of-linear-algebra">Core Components of Linear Algebra</h2> <h3 id="vectors-and-matrices">Vectors and Matrices</h3> <h4 id="1-vectors">1. Vectors</h4> <p>A <strong>vector</strong> is a fundamental concept in linear algebra and is essentially a one-dimensional array of numbers. In machine learning, vectors can represent different elements, including features, weights, or data points.</p> <ul> <li> <strong>Definition:</strong> A vector is a set of numbers arranged in a specific order, and it can be represented either as a <strong>row vector</strong> or a <strong>column vector</strong>. <ul> <li>Row vector: \(\mathbf{v} = [v_1, v_2, \dots, v_n]\)</li> <li>Column vector:\(\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n\end{bmatrix}\)</li> </ul> </li> <li> <strong>Properties:</strong> <ul> <li> <table> <tbody> <tr> <td> <strong>Magnitude (Norm):</strong> The magnitude of a vector, often referred to as its norm, measures the vector’s length. The most common norms used are the L2 norm (Euclidean norm) and L1 norm. $$ |\mathbf{v}|<em>2 = \sqrt{\sum</em>{i=1}^{n} v_i^2} \ \ ; \quad |\mathbf{v}|<em>1 = \sum</em>{i=1}^{n}</td> <td>v_i</td> <td>$$</td> </tr> </tbody> </table> </li> <li> <strong>Dot Product:</strong> The dot product of two vectors measures their similarity. The dot product between two vectors \mathbf{v}_1​ and \mathbf{v}_2​ is computed as: \(\mathbf{v}_1 \cdot \mathbf{v}_2 = \sum_{i=1}^{n} v_{1i} v_{2i}\)</li> <li> <strong>Distance:</strong> The Euclidean distance is a common way to measure the difference between two vectors: \(d(\mathbf{v}_1, \mathbf{v}_2) = \sqrt{\sum_{i=1}^{n} (v_{1i} - v_{2i})^2}\)</li> </ul> </li> <li> <strong>Operations on Vectors:</strong> <ul> <li> <strong>Addition:</strong> Vectors of the same size can be added element-wise.</li> <li> <strong>Scalar Multiplication:</strong> Multiplying each element of a vector by a scalar.</li> <li> <strong>Dot Product:</strong> A fundamental operation for determining the similarity between two vectors.</li> </ul> </li> </ul> <h4 id="2-matrices">2. Matrices</h4> <p>A <strong>matrix</strong> is a two-dimensional array of numbers, and it is widely used in machine learning for data storage, transformations, and solving systems of equations.</p> <ul> <li> <strong>Definition:</strong> A matrix consists of rows and columns and is denoted as $ A $, where A_{ij}​ represents the element in the i-th row and j-th column.</li> <li> \[A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \dots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{m1} &amp; a_{m2} &amp; \dots &amp; a_{mn} \end{bmatrix}\] </li> <li> <strong>Properties of Matrices:</strong> <ul> <li> <strong>Rank:</strong> The rank of a matrix is the maximum number of linearly independent rows or columns, indicating the number of independent dimensions in the matrix.</li> <li> <strong>Trace:</strong> The trace is the sum of the diagonal elements of a square matrix. It is often involved in optimization problems.</li> <li> <strong>Determinant:</strong> The determinant helps in determining whether a matrix is invertible. A non-zero determinant implies that the matrix is invertible.</li> <li> <strong>Invertibility:</strong> A matrix $A$ is invertible if it has full rank and a non-zero determinant. The inverse of a matrix $A$ is denoted by $A^{-1}$, and it satisfies the equation: $A A^{-1} = I$ where $I$ is the identity matrix.</li> </ul> </li> <li> <strong>Operations on Matrices:</strong> <ul> <li> <strong>Matrix Addition/Subtraction:</strong> Matrices of the same dimension can be added or subtracted element-wise.</li> <li> <strong>Matrix Multiplication:</strong> Matrix multiplication is the dot product of rows and columns between two matrices. This operation is central to machine learning algorithms.</li> <li> <strong>Transpose:</strong> The transpose of a matrix $A$ is denoted as $A^T$ and involves flipping its rows and columns.</li> <li> <strong>Inverse:</strong> If a matrix is invertible, its inverse can be used to solve systems of linear equations.</li> </ul> </li> </ul> <h3 id="vectors-and-matrices-in-ml">Vectors and Matrices in ML</h3> <p>Vectors and matrices play a pivotal role in representing both data and models in machine learning.</p> <h4 id="1-data-representation">1. Data Representation</h4> <p>In supervised learning, each data point is typically represented as a feature vector. For example, if a dataset has mmm samples and n features, it can be represented as an $m \times n$ matrix, where each row corresponds to a feature vector for a data point. The corresponding labels or target values are often stored in a vector.</p> <h4 id="2-model-representation">2. Model Representation</h4> <ul> <li> <strong>Weight Vectors and Matrices:</strong> In models like linear regression and neural networks, the weights that transform input data are stored in vectors or matrices. For example, in linear regression, the model is defined as: \(\hat{y} = Xw + b\) where $X$ is the data matrix, $w$ is the weight vector, and $b$ is the bias term.</li> </ul> <h4 id="3-operations-in-machine-learning-algorithms">3. Operations in Machine Learning Algorithms</h4> <ul> <li> <strong>Linear Regression:</strong> In linear regression, matrix operations are used to solve for the optimal weights. The normal equation for linear regression is:\(w=(X^TX)^{−1}X^Ty\) where $X$ is the matrix of input features and $y$ is the vector of target values.</li> <li> <strong>Neural Networks:</strong> Each layer of a neural network applies a linear transformation to its input, which is represented by matrix multiplication: \(y=XW+b\)where $X$ is the input matrix, $W$ is the weight matrix, and $b$ is the bias vector.</li> <li> <strong>Gradient Descent:</strong> The gradient descent optimization algorithm frequently uses vector and matrix operations to update model parameters iteratively. In deep learning, the gradient of the loss function with respect to the weights and biases is calculated using matrix operations during back-propagation.</li> </ul> <h4 id="4-dimensionality-reduction">4. Dimensionality Reduction</h4> <ul> <li> <strong>Principal Component Analysis (PCA):</strong> PCA is a popular technique for dimensionality reduction. It involves finding the eigenvectors and eigenvalues of the covariance matrix of the data. The eigenvectors represent the directions of maximum variance, and the eigenvalues indicate the magnitude of the variance along those directions.</li> </ul> <h4 id="5-optimization">5. Optimization</h4> <ul> <li> <strong>Loss Functions:</strong> Machine learning models are optimized by minimizing a loss function, which often involves vectors and matrices. For instance, in deep learning, back-propagation uses gradients to update the weights by calculating the derivative of the loss function with respect to each weight matrix.</li> </ul> <hr> <h3 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h3> <h4 id="1-what-are-eigenvalues-and-eigenvectors"><strong>1. What Are Eigenvalues and Eigenvectors?</strong></h4> <ul> <li> <strong>Eigenvector:</strong><br> An <strong>eigenvector</strong> of a square matrix $\mathbf{A}$ is a non-zero vector $\mathbf{v}$ that, when the matrix $\mathbf{A}$ is applied to it, only scales the vector without changing its direction:\(\mathbf{A} \mathbf{v} = \lambda \mathbf{v}\) where: <ul> <li>$\mathbf{v}$ is the <strong>eigenvector</strong>,</li> <li>$\lambda$ is the <strong>eigenvalue</strong>, the scalar that represents how much the eigenvector is scaled by the transformation.</li> </ul> </li> <li> <strong>Eigenvalue:</strong><br> The <strong>eigenvalue</strong> $\lambda$ is the factor by which the eigenvector is scaled when the matrix $\mathbf{A}$ acts on it.</li> </ul> <p><strong>To build intuition, consider this analogy:</strong></p> <p>Imagine a <strong>squishy sheet</strong> of rubber (the matrix) and a <strong>point</strong> in space (the vector). If you apply a transformation (like stretching, rotating, or shearing) to the point using the rubber sheet, most points move to new locations. However, some special points, called <strong>eigenvectors</strong>, only get <strong>stretched</strong> or <strong>compressed</strong> but <strong>stay in the same direction</strong>. The amount of stretching or compression is determined by the <strong>eigenvalue</strong>.</p> <h4 id="2-mathematical-properties-of-eigenvalues-and-eigenvectors"><strong>2. Mathematical Properties of Eigenvalues and Eigenvectors</strong></h4> <ul> <li> <strong>Eigenvalue Equation:</strong><br> For a matrix $\mathbf{A}$ and a vector $\mathbf{v}$: \(\mathbf{A} \mathbf{v} = \lambda \mathbf{v}\) where $\mathbf{A}$ is an $n \times n$ matrix, $\mathbf{v}$ is the eigenvector, and $\lambda$ is the eigenvalue.</li> <li>Eigenvectors must be <strong>non-zero vectors</strong>.</li> <li>Eigenvalues can be <strong>real</strong> or <strong>complex</strong> (but are often real in machine learning applications).</li> <li>A matrix can have multiple eigenvectors corresponding to the <strong>same eigenvalue</strong> (if it is <strong>degenerate</strong>) or distinct eigenvalues corresponding to distinct eigenvectors.</li> </ul> <h4 id="3-why-are-eigenvalues-and-eigenvectors-important-in-machine-learning"><strong>3. Why Are Eigenvalues and Eigenvectors Important in Machine Learning?</strong></h4> <p><strong>Principal Component Analysis (PCA)</strong> is a widely used technique for <strong>dimensionality reduction</strong> in machine learning. It reduces the number of features while retaining the most important information in the dataset.</p> <ul> <li> <strong>Covariance Matrix:</strong> PCA begins by computing the <strong>covariance matrix</strong> to capture relationships between features.</li> <li> <strong>Eigenvectors of Covariance Matrix:</strong> The <strong>eigenvectors</strong> represent the directions of maximum variance in the data—these are the <strong>principal components</strong>.</li> <li> <strong>Eigenvalues:</strong> The corresponding <strong>eigenvalues</strong> indicate the magnitude of variance in each direction.</li> </ul> <p>Key steps in PCA:</p> <ul> <li>Sort eigenvectors in decreasing order of their eigenvalues.</li> <li>Select the top <strong>k</strong> eigenvectors to reduce dimensionality while preserving most of the variance.</li> </ul> <h4 id="4-key-properties-of-eigenvalues-and-eigenvectors-in-machine-learning"><strong>4. Key Properties of Eigenvalues and Eigenvectors in Machine Learning</strong></h4> <ul> <li> <strong>Diagonalizability:</strong><br> A matrix is <strong>diagonalizable</strong> if it has enough eigenvectors to form a full basis. This property is essential in PCA and <strong>Singular Value Decomposition (SVD)</strong>, enabling efficient computation and interpretation.</li> <li> <strong>Magnitude of Eigenvalues:</strong><br> The magnitude of eigenvalues corresponds to the <strong>variance captured</strong> by the associated eigenvectors (principal components). Larger eigenvalues imply more variance explained.</li> <li> <strong>Orthogonality of Eigenvectors (Symmetric Matrices):</strong><br> For <strong>symmetric matrices</strong>, eigenvectors are <strong>orthogonal</strong>. This is critical in PCA, where the principal components are orthogonal, ensuring that reduced dimensions remain <strong>uncorrelated</strong>.</li> </ul> <hr> <h3 id="a-few-more-key-matrices-relevant-to-ml">A Few More Key Matrices Relevant to ML</h3> <h4 id="1-symmetric-matrix">1. <strong>Symmetric Matrix:</strong> </h4> <ul> <li> <strong>Definition:</strong> A matrix $A$ is <strong>symmetric</strong> if $A = A^T$, meaning it is equal to its transpose.</li> <li> <strong>Properties:</strong> <ul> <li>A symmetric matrix always has real eigenvalues and orthogonal eigenvectors.</li> <li>If $A$ is symmetric, it is always diagonalizable.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Covariance Matrices:</strong> Covariance matrices are always symmetric because the covariance between two features is the same regardless of the order.</li> <li> <strong>Optimization Problems:</strong> Many optimization problems in machine learning involve symmetric matrices (e.g., in second-order optimization methods like Newton’s method or in regularization).</li> </ul> </li> </ul> <h4 id="2-orthogonal-matrix">2. <strong>Orthogonal Matrix:</strong> </h4> <ul> <li> <strong>Definition:</strong> A matrix $A$ is <strong>orthogonal</strong> if $A^T A = I$, where $I$ is the identity matrix.</li> <li> <strong>Properties:</strong> <ul> <li>The rows and columns of an orthogonal matrix are orthonormal (i.e., they are both orthogonal and of unit length).</li> <li>The inverse of an orthogonal matrix is equal to its transpose $(A^{-1} = A^T)$.</li> <li>The determinant of an orthogonal matrix is either +1 or -1.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Rotation and Transformation:</strong> Orthogonal matrices are used in certain machine learning algorithms for transformations that preserve distances and angles. For example, in PCA, orthogonal transformation is used to create new orthogonal basis vectors.</li> </ul> </li> </ul> <h4 id="3-positive-definite-matrix-pd">3. <strong>Positive Definite Matrix (PD):</strong> </h4> <ul> <li> <strong>Definition:</strong> A square matrix $A$ is <strong>positive definite</strong> if for any non-zero vector $\mathbf{v}$, $\mathbf{v}^T A \mathbf{v} &gt; 0$. In simpler terms, it means that the matrix has strictly positive eigenvalues.</li> <li> <strong>Properties:</strong> <ul> <li>All eigenvalues are positive.</li> <li>The matrix is invertible (non-singular).</li> <li>It implies that the quadratic form is always positive.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Optimization Problems:</strong> In convex optimization, the Hessian matrix of a convex function is often positive definite. This ensures that a function has a unique local minimum, making optimization well-posed.</li> <li> <strong>Covariance Matrices:</strong> The covariance matrix of any dataset with multiple features is positive semi-definite. In special cases (e.g., full rank), it can be positive definite.</li> </ul> </li> </ul> <h4 id="4-positive-semi-definite-matrix-psd">4. <strong>Positive Semi-Definite Matrix (PSD):</strong> </h4> <ul> <li> <strong>Definition:</strong> A matrix $A$ is <strong>positive semi-definite</strong> if for any vector $\mathbf{v}$, $\mathbf{v}^T A \mathbf{v} \geq 0$. In other words, all eigenvalues are non-negative (i.e., zero or positive).</li> <li> <strong>Properties:</strong> <ul> <li>Eigenvalues are non-negative $(\lambda_i \geq 0)$.</li> <li>The matrix may not be invertible if it has zero eigenvalues.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Covariance Matrices:</strong> As mentioned, the covariance matrix of a dataset is positive semi-definite. This is essential because covariance cannot be negative and the matrix represents the relationship between features.</li> <li> <strong>Kernel Matrices (in SVMs, Gaussian Processes, etc.):</strong> The kernel matrix in algorithms like SVM and kernel PCA is always positive semi-definite. It measures similarity between data points in a transformed feature space.</li> </ul> </li> </ul> <h4 id="5-covariance-matrix"><strong>5. Covariance Matrix:</strong></h4> <ul> <li> <p><strong>Definition:</strong> A <strong>covariance matrix</strong> is a square matrix that contains the covariances between pairs of features in a dataset. If a dataset has $n$ features, the covariance matrix will be an $n \times n$ matrix, where each entry represents the covariance between two features.</p> </li> <li> <strong>Covariance of two variables XXX and YYY:</strong> \(\text{Cov}(X, Y) = \frac{1}{m} \sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})\) where $m$ is the number of data points, and $\bar{x}$ and $\bar{y}$​ are the means of the features $X$ and $Y$, respectively.</li> <li> <strong>Covariance Matrix Definition:</strong> For a dataset with nnn features, the covariance matrix $\Sigma$ is an $n \times n$ matrix where each entry is: \(\Sigma_{ij} = \text{Cov}(X_i, X_j)\) where $X_i$​ and $X_j$​ are the $i$-th and $j$-th features, respectively.</li> <li> <strong>Properties:</strong> <ul> <li> <strong>Symmetry:</strong> The covariance matrix is always symmetric, i.e., $\Sigma_{ij} = \Sigma_{ji}$</li> <li> <strong>Positive Semi-Definiteness (PSD):</strong> The covariance matrix is always positive semi-definite, meaning for any vector $\mathbf{v}$, $\mathbf{v}^T \Sigma \mathbf{v} \geq 0$.</li> <li> <strong>Diagonal Entries (Variance):</strong> The diagonal entries represent the variance of individual features.</li> <li> <strong>Off-Diagonal Entries (Covariance):</strong> The off-diagonal entries represent the covariance between different features. Positive covariance indicates that the features increase or decrease together, while negative covariance suggests they move inversely.</li> <li> <strong>Eigenvalues and Eigenvectors:</strong> The eigenvectors of the covariance matrix represent the directions of maximum variance, while the eigenvalues represent the magnitude of variance along these directions.</li> <li> <strong>Rank:</strong> The rank of the covariance matrix corresponds to the number of <strong>independent</strong> features. If the matrix is rank-deficient, it indicates linearly dependent features.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Principal Component Analysis (PCA):</strong> In PCA, the covariance matrix is used to identify the directions (eigenvectors) of maximum variance in the dataset. Eigenvalues indicate how much variance is explained by each principal component. This helps in dimensionality reduction by selecting the most important components.</li> <li> <strong>Multivariate Gaussian Distribution:</strong> In probabilistic models like <strong>Gaussian Mixture Models (GMM)</strong>, the covariance matrix defines the shape of the data distribution. It is used to model the distribution of features in a multi-dimensional space.</li> <li> <strong>Feature Selection:</strong> Covariance matrices help identify correlated features. Features that show high covariance (i.e., strong correlation) can be dropped or combined to improve model performance and reduce dimensionality.</li> </ul> </li> </ul> <h4 id="6-full-rank-matrix">6. <strong>Full Rank Matrix:</strong> </h4> <ul> <li> <strong>Definition:</strong> A matrix is <strong>full rank</strong> if its rank is equal to the smallest of its number of rows or columns. In other words, all rows (or columns) are linearly independent.</li> <li> <strong>Properties:</strong> <ul> <li>A matrix $A$ with full rank has no redundant or dependent rows or columns.</li> <li>If $A$ is an $m \times n$ matrix, and $\text{rank}(A) = \min(m, n)$, the matrix is full rank.</li> <li>A full rank matrix is <strong>invertible</strong> if it is square (i.e., if $m = n$).</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Linear Regression:</strong> In linear regression, the design matrix $X$ must be full rank to ensure a unique solution. If $X$ is not full rank, the matrix $X^T X$ is singular and cannot be inverted.</li> <li> <strong>Solving Systems of Equations:</strong> Full rank matrices guarantee that a system of linear equations has a unique solution. This is important for models that involve solving for weights (like in linear regression or neural networks).</li> </ul> </li> </ul> <h4 id="7-singular-matrix">7. <strong>Singular Matrix:</strong> </h4> <ul> <li> <strong>Definition:</strong> A matrix is <strong>singular</strong> if it is not invertible, meaning its determinant is zero. A singular matrix has linearly dependent rows or columns.</li> <li> <strong>Properties:</strong> <ul> <li>The determinant of a singular matrix is 0.</li> <li>The matrix has at least one eigenvalue equal to 0.</li> </ul> </li> <li> <strong>Relevance in Machine Learning:</strong> <ul> <li> <strong>Linear Dependence:</strong> If the feature matrix $X$ in a linear model is singular, some features are perfectly correlated, and this leads to instability in training and difficulties in solving for the model parameters.</li> </ul> </li> </ul> <hr> <p>That wraps up the key linear algebra concepts for machine learning. This post is designed as a quick reference rather than an exhaustive guide. Don’t stress about memorizing everything—focus instead on understanding the concepts and knowing when to revisit them if needed.</p> <p>Math is a language, and like any language, it’s more about learning to use it than memorizing rules. Treat this as a foundation to build on, and come back to refresh your knowledge whenever necessary.</p> <p>Up next, we’ll explore the prerequisites of <strong>probability theory</strong> for machine learning. Since probability can often feel trickier, we’ll focus more on “what,” “why,” and “how” questions to make the concepts intuitive and approachable.</p> <p>See you in the next post!</p> <h3 id="references"> <strong>References</strong>:</h3> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface & Introduction",description:"First blog post\u2014more of a preface, setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"post-a-post-with-image-galleries",title:"a post with image galleries",description:"this is what included image galleries could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/photo-gallery/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"social-inspire",title:"Inspire HEP",section:"Socials",handler:()=>{window.open("https://inspirehep.net/authors/1010907","_blank")}},{id:"social-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"social-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"social-custom_social",title:"Custom_social",section:"Socials",handler:()=>{window.open("https://www.alberteinstein.com/","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>