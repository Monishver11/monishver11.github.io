<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Empirical Risk Minimization (ERM) | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2024/ERM/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Empirical Risk Minimization (ERM)</h1> <p class="post-meta"> Created in December 24, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> ¬† ¬∑ ¬† <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a> ¬† ¬∑ ¬† <a href="/blog/category/sample-posts"> <i class="fa-solid fa-tag fa-sm"></i> sample-posts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Continuing from our discussion of supervised learning, we now dive into <strong>Empirical Risk Minimization (ERM)</strong>. While the ultimate goal is to minimize the true risk, ERM provides a practical way to approximate this goal using finite data. Let‚Äôs break it down.</p> <hr> <h3 id="definition-empirical-risk-minimization"><strong>Definition: Empirical Risk Minimization</strong></h3> <p>A function \(\hat{f}\) is an <strong>empirical risk minimizer</strong> if:</p> \[\hat{f} \in \arg\min_f \hat{R}_n(f),\] <p>where \(\hat{R}_n(f)\) is the empirical risk, defined over a finite sample and the minimum is taken over all functions\(f: X \to Y\). In an ideal scenario, we would want to minimize the true risk \(R(f)\). This raises a critical question:</p> <p><strong>Is the empirical risk minimizer close enough to the true risk minimizer?</strong></p> <h4 id="example-of-erm-in-action"><strong>Example of ERM in Action</strong></h4> <p>Let‚Äôs consider a simple case:</p> <ul> <li>Let \(P_X = \text{Uniform}[0,1]\) and \(Y \equiv 1\) (i.e., \(Y\) is always 1).</li> <li>A proposed prediction function:</li> </ul> \[\hat{f}(x) = \begin{cases} 1 &amp; \text{if } x \in \{0.25, 0.5, 0.75\}, \\ 0 &amp; \text{otherwise.} \end{cases}\] <p><strong>Loss Analysis:</strong></p> <p>Under both the square loss and the 0-1 loss:</p> <ul> <li> <strong>Empirical Risk</strong>: 0</li> <li> <strong>True Risk</strong>: 1</li> </ul> <p><strong>Explanation</strong>:</p> <ul> <li>The <strong>empirical risk</strong> measures the loss on the training data. Since \(\hat{f}(x)\) perfectly predicts the labels for the training points \(x \in \{0.25, 0.5, 0.75\}\), the empirical risk is zero. There are no errors on the observed data points. - The <strong>true risk</strong>, however, measures the loss over the entire distribution of \(P_X\). For all \(x \notin \{0.25, 0.5, 0.75\}\), \(\hat{f}(x)\) incorrectly predicts 0 instead of the correct label 1, resulting in significant errors. Since \(P_X\) is uniform over \([0,1]\), this means \(\hat{f}(x)\) is incorrect for all others data points of the domain, leading to a true risk of 1.</li> </ul> <p>This illustrates a key problem: <strong>ERM can lead to overfitting by simply memorizing the training data.</strong></p> <hr> <h3 id="generalization-improving-erm"><strong>Generalization: Improving ERM</strong></h3> <p>In the above example, ERM failed to generalize to unseen data. To improve generalization, we must ‚Äúsmooth things out‚Äù‚Äîa process that spreads and extrapolates information from observed parts of the input space \(X\) to unobserved parts.</p> <p>One solution is <strong>Constrained ERM</strong>: Instead of minimizing empirical risk over all possible functions, we restrict our search to a subset of functions, known as a <strong>hypothesis space</strong>.</p> <h4 id="hypothesis-spaces"><strong>Hypothesis Spaces</strong></h4> <p>A <strong>hypothesis space</strong> \(\mathcal{F}\) is a set of prediction functions mapping \(X \to Y\) that we consider when applying ERM.</p> <p><strong>Desirable Properties of a Hypothesis Space</strong></p> <ul> <li>Includes only functions with the desired ‚Äúregularity‚Äù (e.g., smoothness or simplicity).</li> <li>Is computationally tractable (efficient algorithms exist for finding the best function in \(\mathcal{F}\)).</li> </ul> <p>In practice, much of machine learning involves designing appropriate hypothesis spaces for specific tasks.</p> <h4 id="constrained-erm"><strong>Constrained ERM</strong></h4> <p>Given a hypothesis space \(\mathcal{F}\), the empirical risk minimizer in \(\mathcal{F}\) is defined as:</p> \[\hat{f}_n \in \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i),\] <p>where \(\ell(f(x), y)\) is the loss function.</p> <p>Similarly, the true risk minimizer in \(\mathcal{F}\) is:</p> \[f^*_{\mathcal{F}} \in \arg\min_{f \in \mathcal{F}} \mathbb{E}[\ell(f(x), y)].\] <hr> <h3 id="excess-risk-decomposition"><strong>Excess Risk Decomposition</strong></h3> <p>We analyze the performance of ERM through <strong>excess risk decomposition</strong>, which breaks down the gap between the true risk(e.g., the function returned by ERM) and the risk of the Bayes optimal function:</p> <p><strong>Again, Definitions</strong></p> <ul> <li> <p><strong>Bayes Optimal Function</strong>:</p> \[f^* = \arg\min_f \mathbb{E}[\ell(f(x), y)]\] </li> <li> <p><strong>Risk Minimizer in \(\mathcal{F}\)</strong>:</p> \[f_{\mathcal{F}} = \arg\min_{f \in \mathcal{F}} \mathbb{E}[\ell(f(x), y)]\] </li> <li> <p><strong>ERM Solution</strong>:</p> \[\hat{f}_n = \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)\] </li> </ul> <h4 id="excess-risk-decomposition-for-erm"><strong>Excess Risk Decomposition for ERM</strong></h4> <p><strong>Definition</strong></p> <p>The <strong>excess risk</strong> measures how much worse the risk of a function \(f\) is compared to the risk of the Bayes optimal function \(f^*\), which minimizes the true risk. Mathematically, it is defined as:</p> \[\text{Excess Risk}(f) = R(f) - R(f^*)\] <p>where:</p> <ul> <li>\(R(f)\) is the true risk of the function \(f\).</li> <li>\(R(f^*)\) is the Bayes risk, i.e., the lowest achievable risk.</li> </ul> <p><strong>Can Excess Risk Be Negative?</strong> No, the excess risk can never be negative because the Bayes optimal function \(f^*\) achieves the minimum possible risk by definition. For any other function \(f\), the risk \(R(f)\) will be equal to or greater than \(R(f^*)\).</p> <h5 id="decomposition-of-excess-risk-for-erm"><strong>Decomposition of Excess Risk for ERM</strong></h5> <p>For the empirical risk minimizer \(\hat{f}_n\), the excess risk can be decomposed as follows:</p> \[\text{Excess Risk}(\hat{f}_n) = R(\hat{f}_n) - R(f^*) = \underbrace{R(\hat{f}_n) - R(f_\mathcal{F})}_{\text{Estimation Error}} + \underbrace{R(f_\mathcal{F}) - R(f^*)}_{\text{Approximation Error}}\] <p>where:</p> <ul> <li>\(f_\mathcal{F}\) is the best function within the chosen hypothesis space \(\mathcal{F}\).</li> <li> <strong>Estimation Error</strong>: This term captures the error due to estimating the best function \(f_\mathcal{F}\) using finite training data.</li> <li> <strong>Approximation Error</strong>: This term reflects the penalty for restricting the search to the hypothesis space \(\mathcal{F}\) instead of considering all possible functions.</li> </ul> <p><strong>Key Insight: Tradeoff Between Errors</strong> There is always a <strong>tradeoff</strong> between approximation and estimation errors:</p> <ul> <li>A larger hypothesis space \(\mathcal{F}\) reduces approximation error but increases estimation error (due to greater model complexity).</li> <li>A smaller hypothesis space \(\mathcal{F}\) reduces estimation error but increases approximation error (due to limited flexibility).</li> </ul> <p>This tradeoff is crucial when designing models and choosing hypothesis spaces.</p> <hr> <h3 id="erm-in-practice"><strong>ERM in Practice</strong></h3> <p>In real-world machine learning, finding the exact ERM solution is challenging. We often settle for an approximate solution:</p> <ul> <li>Let \(\tilde{f}_n\) be the function returned by an optimization algorithm.</li> <li>The <strong>optimization error</strong> is:</li> </ul> \[\text{Optimization Error} = R(\tilde{f}_n) - R(\hat{f}_n)\] <p>where:</p> <ul> <li>\(\tilde{f}_n\)is the function returned by the optimization method.</li> <li>\(\hat{f}_n\)is the empirical risk minimizer.</li> </ul> <h5 id="practical-decomposition"><strong>Practical Decomposition</strong></h5> <p>For \(\tilde{f}_n\), the excess risk can be further decomposed as:</p> \[\text{Excess Risk}(\tilde{f}_n) = \underbrace{R(\tilde{f}_n) - R(\hat{f}_n)}_{\text{Optimization Error}} + \underbrace{R(\hat{f}_n) - R(f_{\mathcal{F}})}_{\text{Estimation Error}} + \underbrace{R(f_{\mathcal{F}}) - R(f^*)}_{\text{Approximation Error}}.\] <hr> <h3 id="summary-erm-overview"><strong>Summary: ERM Overview</strong></h3> <p>To apply ERM in practice:</p> <ol> <li> <strong>Choose a loss function</strong> \(\ell(f(x), y)\).</li> <li> <strong>Define a hypothesis space</strong> \(\mathcal{F}\) that balances approximation and estimation error.</li> <li> <strong>Use an optimization method</strong> to find \(\hat{f}_n = \arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n \ell(f(x_i), y_i)\) (or an approximate solution \(\tilde{f}_n\)).</li> </ol> <p>As the size of training data increases, we can use larger hypothesis spaces to reduce approximation error while keeping estimation error manageable.</p> <hr> <h3 id="conclusion"><strong>Conclusion</strong></h3> <p>Empirical Risk Minimization (ERM) provides a foundational framework for supervised learning by optimizing a model‚Äôs performance on training data. However, achieving a balance between approximation, estimation, and optimization errors is key to building effective models. This naturally raises the question: <strong>How do we efficiently minimize empirical risk in practice, especially for complex models and large datasets?</strong></p> <p>In the next blog, we‚Äôll dive into <strong>Gradient Descent</strong>, one of the most powerful and widely used optimization algorithms for minimizing risk, and explore how it enables us to tackle the challenges of ERM. Stay tuned and see you! üëã</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2024 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning's history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes' Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface & Introduction",description:"First blog post\u2014more of a preface, setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"social-inspire",title:"Inspire HEP",section:"Socials",handler:()=>{window.open("https://inspirehep.net/authors/1010907","_blank")}},{id:"social-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"social-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"social-custom_social",title:"Custom_social",section:"Socials",handler:()=>{window.open("https://www.alberteinstein.com/","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>