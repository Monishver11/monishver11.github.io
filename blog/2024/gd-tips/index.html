<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gradient Descent and Second-Order Optimization - A Thorough Comparison | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2024/gd-tips/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gradient Descent and Second-Order Optimization - A Thorough Comparison</h1> <p class="post-meta"> Created in December 29, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The concept discussed below may already be familiar to you, but it might appear a bit different in this blog. This content is adapted from the reference material - <a href="https://leon.bottou.org/publications/pdf/tricks-2012.pdf" rel="external nofollow noopener" target="_blank">Stochastic Gradient Descent Tricks</a> we received for Gradient Descent (GD) and Stochastic Gradient Descent (SGD) tips, written by <a href="https://leon.bottou.org/" rel="external nofollow noopener" target="_blank">L´eon Bottou</a>. It’s a well-written piece with many theoretical aspects that are often overlooked when applying GD in machine learning. To be honest, I still don’t fully grasp all of it, but I hope that as I continue on this learning journey, I’ll come to understand most of it and be able to make sense of it.</p> <p>This blog post, along with the next one, will serve as my personal notes on the material. Another reason for sharing this content is to familiarize ourselves with the different notations commonly used in machine learning research. Before diving directly into the SGD tips and tricks from the material, I felt it was important to revisit Gradient Descent (GD) as described in the text. In machine learning, the same concepts are often presented using various notations, which can be confusing if you’re not prepared.</p> <p>Think of it this way: just as there is a base language with many different slangs or colloquialisms, in math, the core concepts remain the same, but the notations can vary depending on who’s explaining them or for what purpose. So, consider this as the same concept, expressed in a different notation—another perspective on the same idea. That’s it—let’s get started!</p> <hr> <h4 id="1-gradient-descent-gd"><strong>1. Gradient Descent (GD)</strong></h4> <h6 id="objective"><strong>Objective</strong></h6> <p>Minimize the empirical risk \(E_n(f_w)\).</p> <h6 id="update-rule"><strong>Update Rule</strong></h6> \[w_{t+1} = w_t - \gamma \frac{1}{n} \sum_{i=1}^n \nabla_w Q(z_i, w_t),\] <p>where:</p> <ul> <li>\(w_t\): Current weights at iteration \(t\).</li> <li>\(\gamma\): Learning rate (a small positive scalar).</li> <li>\(\nabla_w Q(z_i, w_t)\): Gradient of the loss function \(Q\) with respect to \(w_t\) for data point \(z_i\).</li> </ul> <p><strong>Convergence Requirements</strong>:</p> <ul> <li>\(w_0\) (initial weights) close to the optimum.</li> <li>\(\gamma\) small enough.</li> </ul> <p><strong>Performance</strong>:</p> <ul> <li>Achieves <strong>linear convergence</strong>, meaning the error decreases exponentially with iterations. The convergence rate is denoted as \(\rho\), so:</li> </ul> \[-\log \rho \sim t,\] <p>where \(\rho\) represents the residual error.</p> <hr> <h4 id="2-second-order-gradient-descent-2gd"><strong>2. Second-Order Gradient Descent (2GD)</strong></h4> <h6 id="improvement"><strong>Improvement</strong></h6> <p>Instead of using a scalar learning rate \(\gamma\), introduce a positive definite matrix \(\Gamma_t\):</p> \[w_{t+1} = w_t - \Gamma_t \frac{1}{n} \sum_{i=1}^n \nabla_w Q(z_i, w_t).\] <ul> <li>\(\Gamma_t\): Approximates the inverse of the Hessian matrix of the cost function at the optimum.</li> <li>The Hessian is the second derivative of the cost function, capturing curvature information.</li> </ul> <h6 id="advantages"><strong>Advantages</strong></h6> <ul> <li>The algorithm accounts for the curvature of the cost function, leading to more informed updates.</li> <li>When \(\Gamma_t\) is exactly the inverse of the Hessian:</li> <li> <strong>Convergence is quadratic</strong>, meaning:</li> </ul> \[-\log \log \rho \sim t,\] <p>where the error decreases much faster than linear convergence.</p> <ul> <li>If the cost function is quadratic and the scaling matrix \(\Gamma_t\) is exact, the optimum is reached in <strong>one iteration</strong>.</li> </ul> <h6 id="assumptions-for-quadratic-convergence"><strong>Assumptions for Quadratic Convergence</strong></h6> <ul> <li>Smoothness of the cost function.</li> <li>\(w_0\) close enough to the optimum.</li> </ul> <h6 id="intuition-behind-quadratic-convergence"><strong>Intuition Behind Quadratic Convergence</strong></h6> <ul> <li>In GD, the learning rate \(\gamma\) is fixed and doesn’t adapt to the problem’s geometry, leading to slower convergence in certain directions.</li> <li>In 2GD, the matrix \(\Gamma_t\) adapts to the curvature of the cost function:</li> <li>Allows larger steps in flat directions.</li> <li>Takes smaller steps in steep directions.</li> <li>This results in significantly faster convergence.</li> </ul> <hr> <h4 id="follow-up-gradient-descent-and-second-order-gradient-descent"><strong>Follow-Up: Gradient Descent and Second-Order Gradient Descent</strong></h4> <p>This below section answers follow-up questions about the convergence behavior of Gradient Descent (GD) and the role of the inverse Hessian in Second-Order Gradient Descent (2GD).</p> <h5 id="1-how-does-linear-convergence-in-gd-lead-to-exponential-error-reduction"><strong>1. How does linear convergence in GD lead to exponential error reduction?</strong></h5> <h6 id="recap-of-linear-convergence"><strong>Recap of Linear Convergence</strong></h6> <p>Linear convergence means that the error at iteration \(t\) is proportional to the error at iteration \(t-1\), scaled by a constant \(\rho\):</p> \[\|w_t - w^*\| \leq \rho \|w_{t-1} - w^*\|, \quad \text{where } 0 &lt; \rho &lt; 1.\] <h6 id="derivation-of-exponential-error-reduction">Derivation of Exponential Error Reduction</h6> <p>Let’s derive how the error becomes proportional to \(\rho^t\) after \(t\) iterations:</p> <ul> <li>From the recurrence relation:</li> </ul> \[\|w_t - w^*\| \leq \rho \|w_{t-1} - w^*\|.\] <p>Expanding this iteratively:</p> \[\|w_t - w^*\| \leq \rho (\|w_{t-2} - w^*\|) \leq \rho^2 \|w_{t-2} - w^*\|.\] <ul> <li>Generalizing this pattern:</li> </ul> \[\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|,\] <p>where \(\|w_0 - w^*\|\) is the initial error at \(t = 0\).</p> <ul> <li>Since \(\rho &lt; 1\), \(\rho^t\) decreases exponentially as \(t\) increases. This shows the error reduces at an exponential rate in terms of the number of iterations.</li> </ul> <hr> <h5 id="2-what-is-the-inverse-of-the-hessian-matrix"><strong>2. What is the inverse of the Hessian matrix?</strong></h5> <h6 id="hessian-matrix"><strong>Hessian Matrix</strong></h6> <p>The Hessian matrix is a second-order derivative matrix of the cost function \(Q(w)\), defined as:</p> \[H = \nabla^2_w Q(w),\] <p>where each entry \(H_{ij} = \frac{\partial^2 Q(w)}{\partial w_i \partial w_j}\) captures how the gradient changes with respect to each pair of weights.</p> <h6 id="inverse-of-the-hessian"><strong>Inverse of the Hessian</strong></h6> <p>The inverse of the Hessian, \(H^{-1}\), rescales the gradient updates based on the curvature of the cost function:</p> <ul> <li>In directions where the curvature is steep, \(H^{-1}\) reduces the step size.</li> <li>In flatter directions, \(H^{-1}\) increases the step size.</li> </ul> <p>This adjustment improves convergence by adapting the optimization step to the geometry of the cost function.</p> <hr> <h5 id="3-how-does-using-the-inverse-hessian-converge-faster"><strong>3. How does using the inverse Hessian converge faster?</strong></h5> <h6 id="faster-convergence-with-2gd"><strong>Faster Convergence with 2GD</strong></h6> <p>In <strong>Second-Order Gradient Descent (2GD)</strong>:</p> \[w_{t+1} = w_t - H^{-1} \nabla_w Q(w_t).\] <p>This update accounts for the curvature of the cost function.</p> <h6 id="quadratic-convergence"><strong>Quadratic Convergence</strong></h6> <ul> <li>Near the optimum \(w^*\), the cost function can be locally approximated as quadratic:</li> </ul> \[Q(w) \approx \frac{1}{2}(w - w^*)^T H (w - w^*),\] <p>where \(H\) is the Hessian at \(w^*\).</p> <ul> <li>The gradient of the cost is:</li> </ul> \[\nabla_w Q(w) = H (w - w^*).\] <ul> <li>Substituting this gradient into the 2GD update:</li> </ul> \[w_{t+1} = w_t - H^{-1} H (w_t - w^*).\] <ul> <li>Simplifies to:</li> </ul> \[w_{t+1} = w^*.\] <p>This shows that in the best case (when the cost is exactly quadratic and \(H^{-1}\) is exact), the algorithm converges in <strong>one iteration</strong>.</p> <hr> <h5 id="4-summary-of-convergence-behavior"><strong>4. Summary of Convergence Behavior</strong></h5> <ul> <li> <strong>Gradient Descent (GD)</strong>: <ul> <li>Linear convergence: \(\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|\).</li> <li>Error decreases exponentially at a rate \(\rho\), where \(\rho\) depends on the learning rate and the condition number of the Hessian.</li> </ul> </li> <li> <strong>Second-Order Gradient Descent (2GD)</strong>: <ul> <li>Quadratic convergence: \(\|w_t - w^*\| \sim (\text{error})^2\) at each iteration.</li> <li>When the cost is quadratic and \(H^{-1}\) is exact, the algorithm converges in one step.</li> </ul> </li> </ul> <hr> <h4 id="more-details-linear-vs-quadratic-convergence-in-optimization">More Details: Linear vs. Quadratic Convergence in Optimization</h4> <h5 id="1-linear-convergence-in-gradient-descent"><strong>1. Linear Convergence in Gradient Descent</strong></h5> <h6 id="key-idea"><strong>Key Idea:</strong></h6> <p>Gradient Descent (GD) decreases the error at a fixed proportion \(\rho\) per iteration:</p> \[\|w_t - w^*\| \leq \rho \|w_{t-1} - w^*\|, \quad \text{where } 0 &lt; \rho &lt; 1.\] <h6 id="step-by-step-derivation"><strong>Step-by-Step Derivation:</strong></h6> <ul> <li> <strong>Iterative Expansion</strong>: Expanding the recurrence:</li> </ul> \[\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|,\] <p>where \(\|w_0 - w^*\|\) is the initial error.</p> <ul> <li> <strong>Take the Logarithm</strong>: Apply the natural logarithm to both sides:</li> </ul> \[\log \|w_t - w^*\| \leq \log (\rho^t \|w_0 - w^*\|).\] <ul> <li> <strong>Simplify Using Logarithm Rules</strong>: Using \(\log (ab) = \log a + \log b\) and \(\log (\rho^t) = t \log \rho\), we get:</li> </ul> \[\log \|w_t - w^*\| \leq t \log \rho + \log \|w_0 - w^*\|.\] <ul> <li> <strong>Why Does \(t \log \rho\) Decrease Linearly?</strong> <ul> <li>The parameter \(\rho\) satisfies \(0 &lt; \rho &lt; 1\), so \(\log \rho &lt; 0\).</li> <li>As \(t\) increases, \(t \log \rho\) becomes a larger negative number, reducing the value of \(\log \|w_t - w^*\|\).</li> <li>Since \(\log \rho\) is a constant, the term \(t \log \rho\) depends <strong>linearly on \(t\)</strong>:</li> </ul> \[t \log \rho = (\text{constant}) \cdot t, \quad \text{where constant} = \log \rho.\] </li> <li> <strong>Interpretation of Convergence Rate</strong>: <ul> <li>From the error bound \(\|w_t - w^*\| \leq \rho^t \|w_0 - w^*\|\), we see exponential error decay with \(t\).</li> <li>Taking the logarithm leads to a linear relationship in \(t\):</li> </ul> </li> </ul> \[\log \|w_t - w^*\| \sim t \log \rho.\] <ul> <li>This behavior is summarized as:</li> </ul> \[-\log \rho \sim t.\] <h5 id="2-quadratic-convergence-in-second-order-gradient-descent"><strong>2. Quadratic Convergence in Second-Order Gradient Descent</strong></h5> <h6 id="key-idea-1"><strong>Key Idea:</strong></h6> <p>In Second-Order Gradient Descent (2GD), the error at each step is proportional to the <strong>square</strong> of the error at the previous step:</p> \[\|w_t - w^*\| \sim (\|w_{t-1} - w^*\|)^2.\] <h6 id="step-by-step-derivation-1"><strong>Step-by-Step Derivation:</strong></h6> <ul> <li> <strong>Iterative Expansion</strong>: Rewriting the error at step \(t\) in terms of the initial error \(\|w_0 - w^*\|\):</li> </ul> \[\|w_t - w^*\| \sim (\|w_{t-1} - w^*\|)^2 \sim \left((\|w_{t-2} - w^*\|)^2\right)^2 \sim \dots \sim (\|w_0 - w^*\|)^{2^t}.\] <p>Thus:</p> \[\|w_t - w^*\| \sim (\|w_0 - w^*\|)^{2^t}.\] <ul> <li> <strong>Take the Logarithm</strong>: Apply the natural logarithm:</li> </ul> \[\log \|w_t - w^*\| \sim 2^t \log \|w_0 - w^*\|.\] <ul> <li> <strong>Take Another Logarithm</strong>: To analyze the rate of convergence, take the logarithm again:</li> </ul> \[\log \log \|w_t - w^*\| \sim \log (2^t) + \log \log \|w_0 - w^*\|.\] <p>Using \(\log (2^t) = t \log 2\), we simplify:</p> \[\log \log \|w_t - w^*\| \sim t + \log \log \|w_0 - w^*\|.\] <ul> <li> <strong>Interpretation of Convergence Rate</strong>: <ul> <li>From the error bound \(\|w_t - w^*\| \sim (\|w_0 - w^*\|)^{2^t}\), we see super-exponential error decay.</li> <li>Taking the logarithm of the logarithm shows linear growth in \(t\):</li> </ul> </li> </ul> \[\log \log \|w_t - w^*\| \sim t.\] <ul> <li>This behavior is expressed as:</li> </ul> \[-\log \log \rho \sim t.\] <hr> <h6 id="summary-of-convergence-behavior"><strong>Summary of Convergence Behavior</strong></h6> <table> <thead> <tr> <th><strong>Convergence Type</strong></th> <th><strong>Error Decay</strong></th> <th><strong>Logarithmic Analysis</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Gradient Descent (GD)</strong></td> <td>\(|w_t - w^*| \sim \rho^t\)</td> <td>\(\log |w_t - w^*| \sim t\)</td> </tr> <tr> <td><strong>Second-Order Gradient Descent (2GD)</strong></td> <td>\(|w_t - w^*| \sim (|w_0 - w^*|)^{2^t}\)</td> <td>\(\log \log |w_t - w^*| \sim t\)</td> </tr> </tbody> </table> <hr> <p>The next blog continues from this one, where we’ll explore SGD (Stochastic Gradient Descent) along with some helpful tips and analogies. We will use the same notations as those introduced in this blog. Keep learning, and head on to the next post!</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>