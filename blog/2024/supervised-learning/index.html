<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding the Supervised Learning Setup | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2024/supervised-learning/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding the Supervised Learning Setup</h1> <p class="post-meta"> Created in December 24, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a>   ·   <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Supervised learning is a cornerstone of machine learning, enabling systems to learn from labeled data to make predictions or decisions. In this post, we will explore the various components and formalizations of supervised learning to build a solid foundation.</p> <h4 id="goals-in-supervised-learning"><strong>Goals in Supervised Learning</strong></h4> <p>In supervised learning problems, we typically aim to:</p> <ul> <li> <strong>Make a decision:</strong> For instance, determining whether to move an email to a spam folder.</li> <li> <strong>Take an action:</strong> As in self-driving cars, deciding when to make a right turn.</li> <li> <strong>Reject a hypothesis:</strong> Such as testing the hypothesis that \(\theta = 0\) in classical statistics.</li> <li> <strong>Produce some output:</strong> Examples include identifying whose face is in an image, translating a Japanese sentence into Hindi, or predicting the location of a storm an hour into the future.</li> </ul> <p>Each of these goals involves predicting or generating some form of output based on given inputs.</p> <h5 id="labels-the-key-to-supervised-learning"><strong>Labels: The Key to Supervised Learning</strong></h5> <p>Supervised learning involves pairing inputs with <strong>labels</strong>, which serve as the ground truth. Examples of labels include:</p> <ul> <li>Whether or not a picture contains an animal.</li> <li>The storm’s location one hour after a query.</li> <li>Which, if any, of the suggested URLs were selected.</li> </ul> <p>These labels allow us to evaluate the performance of our predictions systematically.</p> <hr> <h4 id="evaluation-criterion"><strong>Evaluation Criterion</strong></h4> <p>The next step in supervised learning is finding <strong>optimal outputs</strong> under various definitions of optimality. Some examples of evaluation criteria include:</p> <ul> <li> <strong>Classification Accuracy:</strong> Is the predicted class correct?</li> <li> <strong>Exact Match:</strong> Does the transcription exactly match the spoken words?</li> <li> <strong>Partial Credit:</strong> How do we account for partially correct answers (e.g., getting some words right)?</li> <li> <strong>Prediction Distance:</strong> How far is the storm’s actual location from the predicted one?</li> <li> <strong>Density Prediction:</strong> How likely is the storm’s actual location under the predicted distribution?</li> </ul> <p>These criteria ensure that we can quantitatively measure the performance of our models.</p> <hr> <h4 id="typical-sequence-of-events"><strong>Typical Sequence of Events</strong></h4> <p>Supervised learning problems can often be formalized through the following sequence:</p> <ol> <li> <strong>Observe Input (\(x\)):</strong> Receive an input data point.</li> <li> <strong>Predict Output (\(\hat{y}\)):</strong> Use a prediction function to generate an output.</li> <li> <strong>Observe Label (\(y\)):</strong> Compare the predicted output with the true label.</li> <li> <strong>Evaluate Output:</strong> Assess the prediction’s quality based on the label.</li> </ol> <p>This sequence is at the heart of most supervised learning frameworks.</p> <hr> <h4 id="formalizing-supervised-learning"><strong>Formalizing Supervised Learning</strong></h4> <p>A <strong>prediction function</strong> is a mathematical function \(f: X \to Y\) that takes an input \(x \in X\) and produces an output \(\hat{y} \in Y\).</p> <p>A <strong>loss function</strong> evaluates the discrepancy between the predicted output \(\hat{y}\) and the true outcome \(y\). It quantifies the “cost” of making incorrect predictions.</p> <h6 id="the-goal-optimal-prediction"><strong>The Goal: Optimal Prediction</strong></h6> <p>The primary goal is to find the <strong>optimal prediction function</strong>. The intuition is simple: If we can evaluate how good a prediction function is, we can turn this into an optimization problem.</p> <ul> <li>The loss function \(\ell\) evaluates a single output.</li> <li>To evaluate the prediction function as a whole, we need to formalize the concept of “average performance.”</li> </ul> <h6 id="data-generating-distribution"><strong>Data Generating Distribution</strong></h6> <p>Assume there exists a data-generating distribution \(P_{X \times Y}\). All input-output pairs \((x, y)\) are generated independently and identically distributed (i.i.d.) from this distribution.</p> <p>A common objective is to have a prediction function \(f(x)\) that performs well <strong>on average</strong>:</p> \[\ell(f(x), y)\] <p>is small, in some sense.</p> <h6 id="risk-definition"><strong>Risk Definition</strong></h6> <p>The <strong>risk</strong> of a prediction function \(f: X \to Y\) is defined as:</p> \[R(f) = \mathbb{E}_{(x, y) \sim P_{X \times Y}} [\ell(f(x), y)].\] <p>In words, this is the expected loss of \(f\) over the data-generating distribution \(P_{X \times Y}\). However, since we do not know \(P_{X \times Y}\), we cannot compute this expectation directly. Instead, we estimate it.</p> <hr> <h4 id="the-bayes-prediction-function"><strong>The Bayes Prediction Function</strong></h4> <p><strong>Definition</strong></p> <p>The <strong>Bayes prediction function</strong> \(f^*: X \to Y\) achieves the minimal risk among all possible functions:</p> \[f^* \in \underset{f}{\text{argmin}} \ R(f),\] <p>where the minimum is taken over all functions from \(X\) to \(Y\).</p> <p><strong>Bayes Risk</strong></p> <p>The risk associated with the Bayes prediction function is called the <strong>Bayes risk</strong>. This function is often referred to as the “target function” because it represents the best possible predictor.</p> <hr> <h4 id="example-multiclass-classification"><strong>Example: Multiclass Classification</strong></h4> <p>In multiclass classification, the output space is:</p> \[Y = \{1, 2, \dots, k\}.\] <p>The <strong>0-1 loss</strong> function is defined as:</p> \[\ell(\hat{y}, y) = \mathbb{1}[\hat{y} \neq y] := \begin{cases} 1 &amp; \text{if } \hat{y} \neq y, \\ 0 &amp; \text{otherwise.} \end{cases}\] <ul> <li>Here, \(\mathbb{1}[\hat{y} \neq y]\) is an <strong>indicator function</strong>. It returns a value of 1 when the condition \(\hat{y} \neq y\) is true (i.e., the prediction is incorrect) and 0 otherwise. This signifies whether the prediction is correct or incorrect and is commonly used to measure classification errors.</li> </ul> <p>The risk \(R(f)\) under the 0-1 loss can be expanded as follows:</p> \[R(f) = \mathbb{E}[\mathbb{1}[f(x) \neq y]] = \mathbb{P}(f(x) \neq y),\] <p>where:</p> <ul> <li>\(\mathbb{E}[\mathbb{1}[f(x) \neq y]]\) represents the expected value of the indicator function, which counts the proportion of incorrect predictions.</li> <li>\(\mathbb{P}(f(x) \neq y)\) is the probability of the prediction \(f(x)\) being different from the true label \(y\).</li> </ul> <p>Further, this can be rewritten using the decomposition of probabilities:</p> \[R(f) = 0 \cdot \mathbb{P}(f(x) = y) + 1 \cdot \mathbb{P}(f(x) \neq y),\] <p>which simplifies back to:</p> \[R(f) = \mathbb{P}(f(x) \neq y).\] <p><strong>Explanation</strong></p> <ul> <li>The term \(0 \cdot \mathbb{P}(f(x) = y)\) accounts for the cases where the prediction is correct (loss is 0).</li> <li>The term \(1 \cdot \mathbb{P}(f(x) \neq y)\) accounts for the cases where the prediction is incorrect (loss is 1).</li> </ul> <p>Thus, \(R(f)\) directly measures the <strong>misclassification error rate</strong>, which is the probability of the model making an incorrect prediction.</p> <p><strong>Bayes Prediction Function</strong></p> <p>The Bayes prediction function returns the most likely class:</p> \[f^*(x) \in \underset{1 \leq c \leq k}{\text{argmax}} \ P(y = c \mid x).\] <hr> <h4 id="estimating-risk"><strong>Estimating Risk</strong></h4> <p>We cannot compute the true risk \(R(f) = \mathbb{E}[\ell(f(x), y)]\) because the true distribution \(P_{X \times Y}\) is unknown. However, we can estimate it.</p> <p>Assume we have sample data:</p> \[D_n = \{(x_1, y_1), \dots, (x_n, y_n)\},\] <p>where the samples are i.i.d. from \(P_{X \times Y}\). By the strong law of large numbers, the empirical average of losses converges to the expected value. If \(z_1, . . . , z_n\) are i.i.d. with expected value \(\mathbb{E}[z]\), then</p> \[\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n z_i = \mathbb{E}[z],\] <p>with probability 1.</p> <p>This leads us to the concept of <strong>empirical risk</strong> and its minimization, which we will explore in the next post.</p> <hr> <p>In the next blog, we will dive into <strong>empirical risk minimization</strong> and how it helps solve supervised learning problems effectively.</p> <p><strong>Stay tuned!</strong></p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"To Add",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>