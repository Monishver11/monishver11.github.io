<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Stochastic Gradient Descent (SGD) | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD)."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2024/SGD/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Stochastic Gradient Descent (SGD)</h1> <p class="post-meta"> Created in December 28, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> ¬† ¬∑ ¬† <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ML</a> ¬† ¬∑ ¬† <a href="/blog/category/ml-nyu"> <i class="fa-solid fa-tag fa-sm"></i> ML-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In the last blog, we introduced <strong>Stochastic Gradient Descent (SGD)</strong> as a computationally efficient optimization method. In this post, we‚Äôll dive deeper into the mechanics of SGD, exploring its nuances, trade-offs, and how it compares to other gradient descent variants. Let‚Äôs unravel the details and gain a comprehensive understanding of these optimization techniques.</p> <h4 id="noisy-gradient-descent"><strong>‚ÄúNoisy‚Äù Gradient Descent</strong></h4> <p>Instead of computing the exact gradient at every step, <strong>noisy gradient descent</strong> estimates the gradient using random subsamples. Surprisingly, this approximation often works well.</p> <p><strong>Why Does It Work?</strong></p> <p>Gradient descent is inherently iterative, meaning it has the chance to recover from previous missteps at each step. Leveraging noisy estimates can speed up the process without significantly impacting the final results.</p> <hr> <h4 id="mini-batch-gradient-descent"><strong>Mini-batch Gradient Descent</strong></h4> <p>The <strong>full gradient</strong> for a dataset \(D_n = (x_1, y_1), \dots, (x_n, y_n)\) is given by:</p> \[\nabla \hat{R}_n(w) = \frac{1}{n} \sum_{i=1}^n \nabla_w \ell(f_w(x_i), y_i)\] <p>This requires the entire dataset, which can be computationally expensive. To mitigate this, we use a <strong>mini-batch</strong> of size \(N\), a random subset of the data:</p> \[\nabla \hat{R}_N(w) = \frac{1}{N} \sum_{i=1}^N \nabla_w \ell(f_w(x_{m_i}), y_{m_i})\] <p>Here, \((x_{m_1}, y_{m_1}), \dots, (x_{m_N}, y_{m_N})\) is the mini-batch.</p> <hr> <h4 id="batch-vs-stochastic-methods"><strong>Batch vs. Stochastic Methods</strong></h4> <p><strong>Rule of Thumb:</strong></p> <ul> <li> <strong>Stochastic methods</strong> perform well far from the optimum but struggle as we approach it.</li> <li> <strong>Batch methods</strong> excel near the optimum due to more precise gradient calculations.</li> </ul> <h4 id="mini-batch-gradient-properties"><strong>Mini-batch Gradient Properties</strong></h4> <ul> <li>The mini-batch gradient is an <strong>unbiased estimator</strong> of the full gradient, meaning on average, the gradient computed using a minibatch (a small, random subset of the dataset) gives the same direction of descent as the gradient computed using the entire dataset.</li> </ul> \[\mathbb{E}[\nabla \hat{R}_N(w)] = \nabla \hat{R}_n(w)\] <ul> <li> <p>This implies that while individual minibatch gradients may vary due to the randomness of the sample, their expected value matches the full batch gradient. This property allows Stochastic Gradient Descent (SGD) to make consistent progress toward the optimum without requiring computation over the entire dataset in each iteration.</p> </li> <li> <p>Larger mini-batches result in better estimates but are slower to compute:</p> </li> </ul> \[\text{Var}[\nabla \hat{R}_N(w)] = \frac{1}{N} \text{Var}[\nabla \hat{R}_i(w)]\] <ul> <li>This is because averaging over more samples reduces randomness. Specifically, the variance is scaled by \(1/ùëÅ\), meaning larger minibatches produce more accurate and stable gradient estimates, closer to the full batch gradient.</li> </ul> <p><strong>Tradeoffs of minibatch size:</strong></p> <ul> <li> <strong>Larger \(N\):</strong> Better gradient estimate, slower computation.</li> <li> <strong>Smaller \(N\):</strong> Faster computation, noisier gradient estimates.</li> </ul> <hr> <h4 id="convergence-of-sgd"><strong>Convergence of SGD</strong></h4> <p>To ensure convergence, <strong>diminishing step sizes</strong> like \(\eta_k = 1/k\) are often used. While gradient descent (GD) theoretically converges faster than SGD:</p> <ul> <li> <strong>GD</strong> is efficient near the minimum due to higher accuracy.</li> <li> <strong>SGD</strong> is more practical for large-scale problems where high accuracy is unnecessary.</li> </ul> <p>In practice, SGD with <strong>fixed step sizes</strong> works well and can be adjusted using techniques like <strong>staircase decay</strong> or <strong>inverse time decay</strong> (\(1/t\)).</p> <h6 id="sgd-algorithm-with-mini-batches"><strong>SGD Algorithm with Mini-batches</strong></h6> <ol> <li>Initialize \(w = 0\).</li> <li>Repeat: <ul> <li>Randomly sample \(N\) points from \(D_n\): \({(x_i, y_i)}_{i=1}^N\).</li> <li>Update weights:</li> </ul> \[w \leftarrow w - \eta \left( \frac{1}{N} \sum_{i=1}^N \nabla_w \ell(f_w(x_i), y_i) \right)\] </li> </ol> <hr> <h5 id="why-diminishing-step-sizes-theoretical-aspects"><strong>Why Diminishing Step Sizes? (Theoretical Aspects)</strong></h5> <p>If \(f\) is \(L\)-smooth and convex, and the variance of \(\nabla f(x^{(k)})\) is bounded</p> \[\text{Var}(\nabla f(x^{(k)})) \leq \sigma^2\] <p>, then SGD with step size</p> \[\eta \leq \frac{1}{L}\] <p>satisfies:</p> \[\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2] \leq \frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k} + \frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\] <p><strong>Breaking it Down:</strong></p> <ol> <li> <strong>L-Smooth and Convex Function</strong>: <ul> <li>A function ( f ) is <strong>smooth</strong> if its gradient doesn‚Äôt change too rapidly. Specifically, an \(L\)-smooth function means that the gradient‚Äôs rate of change is bounded by a constant \(L\).</li> <li>A <strong>convex</strong> function means that it has a single global minimum, making optimization easier because we don‚Äôt have to worry about getting stuck in local minima.</li> </ul> </li> <li> <strong>Variance of Gradient</strong>: <ul> <li>The gradient at each step of SGD might not be exact. The variance \(\text{Var}(\nabla f(x^{(k)}))\) measures the ‚Äúnoise‚Äù or fluctuations in the gradient estimate. A smaller variance means the gradient is more stable.</li> </ul> </li> </ol> <p><strong>What Does the Formula Mean?</strong></p> <p>The formula provides an upper bound on the expected squared magnitude of the gradient:</p> \[\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2] \leq \frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k} + \frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\] <ul> <li> <p><strong>Left Side</strong>: \(\min_k \mathbb{E}[\|\nabla f(x^{(k)})\|^2]\) represents the minimum expected squared gradient magnitude. A smaller value indicates that the gradient is approaching zero, meaning we‚Äôre getting closer to the optimal solution.</p> </li> <li> <p><strong>Right Side</strong>:</p> <ul> <li>The first term \(\frac{f(x^{(0)}) - f(x^*)}{\sum_k \eta_k}\) reflects how the gap between the initial point \(x^{(0)}\) and the optimal solution \(x^*\) decreases over time. The more steps we take (i.e., the larger the sum of the step sizes \(\eta_k\)), the smaller the gap becomes.</li> <li>The second term \(\frac{L\sigma^2}{2} \frac{\sum_k \eta_k^2}{\sum_k \eta_k}\) accounts for the variance in the gradient. If the step size doesn‚Äôt decrease over time, this variance term grows, which can destabilize the optimization process. The numerator \(\sum_k \eta_k^2\) grows faster than the denominator \(\sum_k \eta_k\), so increasing step sizes overall increases the second term. So, this term will dominate if the step size does not decrease.</li> </ul> </li> </ul> <p><strong>Intuition Behind Diminishing Step Sizes:</strong></p> <ul> <li> <p><strong>Without diminishing step sizes</strong> - If you keep taking large steps, especially when close to the minimum, you risk overshooting the optimal solution. Large gradients or noisy estimates can lead to erratic behavior.</p> </li> <li> <p><strong>With diminishing step sizes</strong> - As we get closer to the minimum, reducing the step size helps take smaller, more controlled steps. This reduces the variance (noise) in the gradient and makes the convergence process smoother and more stable.</p> </li> </ul> <p><strong>So, now why diminish step sizes?</strong></p> <p>Diminishing step sizes are important because:</p> <ul> <li>Early on, larger steps help explore the solution space and make significant progress.</li> <li>As you approach the optimal solution, smaller steps are needed to fine-tune the result and avoid overshooting. This balance helps the optimization process converge efficiently while maintaining stability.</li> </ul> <p>More on the mathematical details of convergence will be covered in a separate blog post. For now, the key intuition to keep in mind is that diminishing step sizes help strike a balance between exploration (larger steps) and stability (smaller steps), leading to smoother convergence.</p> <hr> <h4 id="summary"><strong>Summary</strong></h4> <p>Gradient descent variants provide trade-offs in speed, accuracy, and computational cost:</p> <ul> <li> <strong>Full-batch gradient descent:</strong> Uses the entire dataset for gradient computation, yielding precise updates but high computational cost.</li> <li> <strong>Mini-batch gradient descent:</strong> Balances computational efficiency and gradient accuracy by using subsets of data.</li> <li> <strong>Stochastic gradient descent (SGD):</strong> Uses a single data point (\(N = 1\)) for updates, making it highly efficient but noisy.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GD_Variations.webp" sizes="95vw"></source> <img src="/assets/img/GD_Variations.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="GD_Variations" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center"> Batch Vs Stochastic Vs Mini-Batch GD </div> <p>When referring to SGD, always clarify the batch size to avoid ambiguity. Modern machine learning heavily relies on SGD due to its time and memory efficiency, especially for large-scale problems.</p> <hr> <h5 id="example-logistic-regression-with-ell_2-regularization"><strong>Example: Logistic Regression with \(\ell_2\)-Regularization</strong></h5> <div class="row justify-content-sm-center"> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_1-480.webp 480w,/assets/img/SGD_Comp_1-800.webp 800w,/assets/img/SGD_Comp_1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/SGD_Comp_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_1" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_2-480.webp 480w,/assets/img/SGD_Comp_2-800.webp 800w,/assets/img/SGD_Comp_2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/SGD_Comp_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_2" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/SGD_Comp_3-480.webp 480w,/assets/img/SGD_Comp_3-800.webp 800w,/assets/img/SGD_Comp_3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/SGD_Comp_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="SGD_Comp_3" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Convergence Speed(1), Computational Efficiency(2) and Rate at Near Optimum(3) of different GD approaches </div> <ul> <li> <strong>Batch methods:</strong> Converge faster near the optimum.</li> <li> <strong>Stochastic methods:</strong> Are computationally efficient, especially for large datasets.</li> </ul> <p>Understanding these trade-offs helps in choosing the right approach for different scenarios.</p> <hr> <p>In the next blog, we‚Äôll explore <strong>Gradient Descent Convergence Theorems</strong> and how to intuitively make sense out of it! See you.</p> <h6 id="image-credits"><strong>Image Credits:</strong></h6> <ul> <li><a href="https://alwaysai.co/blog/what-is-gradient-descent" rel="external nofollow noopener" target="_blank">Batch Vs Stochastic Vs Mini-Batch GD</a></li> <li><a href="https://www.stat.berkeley.edu/~ryantibs/" rel="external nofollow noopener" target="_blank">Example from Ryan Tibshirani</a></li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"To Add",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>