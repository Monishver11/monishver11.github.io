<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="9O0EoPaLhgFjSIvAkDDoQK0gr49C2Wuxtgl3c0bXObM"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LLMR - Lecture 1 | Monishver Chandrasekaran </title> <meta name="author" content="Monishver Chandrasekaran"> <meta name="description" content="LLM Reasoners Course at NYU Courant - Personal Notes 1"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?b337fdf3fe456a8da16aab16e9a00f8c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://monishver11.github.io/blog/2026/llmr1/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class=" "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Monishver</span> Chandrasekaran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/thoughts/">Thoughts </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-weight: 400;">LLMR - Lecture 1</h1> <p class="post-meta"> Created in January 24, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/category/llmr-nyu"> <i class="fa-solid fa-tag fa-sm"></i> LLMR-NYU</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h4 id="course-overview"><strong>Course Overview</strong></h4> <p><strong>Goals of this course:</strong> Understand how a language model works all the way down. Build it from scratch to really learn this.</p> <p>According to Percy Liang, three important things to know:</p> <ul> <li> <strong>Mechanics:</strong> how things work (Transformer architecture, model parallelism on GPUs, etc.)</li> <li> <strong>Mindset:</strong> squeezing the most out of the hardware, taking scale seriously</li> <li> <strong>Intuitions:</strong> which data and modeling decisions yield good accuracy</li> </ul> <h4 id="language-models"><strong>Language Models</strong></h4> <p><strong>Language models:</strong> place a distribution \(P(w)\) over strings \(w\) in a language.</p> <p><strong>Autoregressive models:</strong></p> \[P(w) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1, w_2) \cdots\] <h5 id="n-gram-models"><strong>N-gram Models</strong></h5> <p><strong>N-gram models:</strong> distribution of next word is a categorical conditioned on previous \(n-1\) words</p> \[P(w_i|w_1, \ldots, w_{i-1}) = P(w_i|w_{i-n+1}, \ldots, w_{i-1})\] <p><strong>Markov property:</strong> only consider a few previous words</p> <p><strong>Example:</strong> I visited San _____</p> <ul> <li>2-gram: \(P(w \mid \text{San})\)</li> <li>3-gram: \(P(w \mid \text{visited San})\)</li> <li>4-gram: \(P(w \mid \text{I visited San})\)</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li>N-gram LLMs don’t generalize or abstract over related words</li> <li>Don’t handle contexts beyond a few words</li> </ul> <h5 id="neural-language-models"><strong>Neural Language Models</strong></h5> <p><strong>Feedforward networks:</strong> Language models based purely on feedforward networks can abstract over words (using embeddings), but still fail to use large context.</p> <p><strong>Solution:</strong> Need to handle more context</p> <ul> <li>RNNs or CNNs can do this</li> <li>Current best: <strong>Transformers using self-attention</strong> </li> </ul> <h4 id="transformers"><strong>Transformers</strong></h4> <h5 id="attention-mechanism"><strong>Attention Mechanism</strong></h5> <p><strong>Attention:</strong> method to access arbitrarily far back in context from this point.</p> <p><strong>Components:</strong></p> <ul> <li> <strong>Keys:</strong> embedded versions of the sentence</li> <li> <strong>Query:</strong> what we want to find</li> </ul> <p><strong>Attention Process:</strong></p> <p><strong>Step 1:</strong> Compute scores for each key \(s_i = k_i^T q\)</p> <p><strong>Step 2:</strong> Softmax the scores to get probabilities \(\alpha\)</p> <p><strong>Step 3:</strong> Compute output values by multiplying embeddings by alpha and summing \(\text{result} = \sum_i \alpha_i e_i\)</p> <h5 id="making-attention-more-flexible"><strong>Making Attention More Flexible</strong></h5> <p>We can make attention more peaked by not setting keys equal to embeddings. Introduce weight matrices for keys.</p> <h5 id="attention-formally"><strong>Attention, Formally</strong></h5> <p><strong>Original “dot product” attention:</strong> \(s_i = k_i^T q\)</p> <p><strong>Scaled dot product attention:</strong> \(s_i = k_i^T W q\)</p> <p><strong>Equivalent to having two weight matrices:</strong> \(s_i = (W^K k_i)^T (W^Q q)\)</p> <p><strong>Reference:</strong> Jay Alammar, <a href="https://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener" target="_blank">The Illustrated Transformer</a></p> <h5 id="self-attention"><strong>Self-Attention</strong></h5> <p><strong>Self-attention:</strong> every word is both a key and a query simultaneously</p> <p><strong>Matrices:</strong></p> <ul> <li>\(Q\): seq_len × \(d\) matrix (\(d\) = embedding dimension = 2 for these slides)</li> <li>\(K\): seq_len × \(d\) matrix</li> </ul> <p><strong>Scores:</strong> \(S = QK^T\) where \(S_{ij} = q_i \cdot k_j\)</p> <p>Dimensions: seq_len × seq_len = (seq_len × \(d\)) × (\(d\) × seq_len)</p> <p><strong>Final step:</strong> softmax to get attentions \(A\), then output is \(AE\)</p> <p>*Technically it’s \(A(EW^V)\), using a values matrix \(V = EW^V\)</p> <p>TODO: Add pics from slides 48, 49, 50, 51, 52</p> <h5 id="attention-formalization"><strong>Attention Formalization</strong></h5> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>where:</p> <ul> <li>\(Q = EW^Q\) (queries)</li> <li>\(K = EW^K\) (keys)</li> <li>\(V = EW^V\) (values)</li> </ul> <p><strong>Normalizing by \(\sqrt{d_k}\):</strong> helps control the scale of the softmax, makes it less peaked</p> <p><strong>Multi-head attention:</strong> This is just one head of self-attention — produce multiple heads via randomly initialized parameter matrices</p> <h4 id="architecture"><strong>Architecture</strong></h4> <p>TODO: Add pics for slides 53, 54</p> <h5 id="transformer-block-structure"><strong>Transformer Block Structure</strong></h5> <p><strong>Q: In general, a transformer block contains encoder and decoder right? Here what we’re discussing is the encoder part right?</strong></p> <p><strong>A:</strong> Not quite. What we’re discussing here is a <strong>decoder-only</strong> architecture used for language modeling. The original Transformer (Vaswani et al., 2017) had both encoder and decoder blocks, but modern LLMs like GPT use only decoder blocks stacked together. Each decoder block contains:</p> <ul> <li>Causal (masked) multi-head self-attention with RoPE</li> <li>Layer normalization</li> <li>Position-wise feed-forward network</li> <li>Residual connections (Add operations)</li> </ul> <p>The key difference from an encoder is the <strong>causal masking</strong> in attention, which prevents positions from attending to future positions.</p> <h5 id="transformer-block-components"><strong>Transformer Block Components</strong></h5> <p>A single Transformer block consists of:</p> <ol> <li> <p><strong>Input:</strong> Token embeddings (seq_len × \(d_{model}\))</p> </li> <li> <strong>Causal Multi-Head Self-Attention with RoPE:</strong> <ul> <li>Apply RoPE to queries and keys</li> <li>Compute attention with causal mask</li> <li>Multi-head attention allows different heads to learn different attention patterns</li> </ul> </li> <li> <p><strong>Add &amp; Norm:</strong> Residual connection + Layer Normalization</p> </li> <li> <strong>Position-Wise Feed-Forward Network:</strong> \(\text{FFN}(x) = \text{SwiGLU}(x, W_1, W_2, W_3) = W_2(\text{SiLU}(W_1 x) \odot W_3 x)\) <ul> <li>Takes each position independently</li> <li>Typically uses a larger hidden dimension \(d_{ff}\)</li> </ul> </li> <li> <p><strong>Add &amp; Norm:</strong> Another residual connection + Layer Normalization</p> </li> <li> <strong>Output:</strong> Transformed representations (seq_len × \(d_{model}\))</li> </ol> <p>These blocks are stacked <code class="language-plaintext highlighter-rouge">num_layers</code> times, with the output of one block feeding into the next.</p> <p>TODO: Add pics from slides 55, 56</p> <h5 id="dimensions"><strong>Dimensions</strong></h5> <p><strong>Main vector size:</strong> \(d_{model}\)</p> <p><strong>Queries/keys:</strong> \(d_k\), always smaller than \(d_{model}\), often \(d_{model}/h\) (number of heads)</p> <p><strong>Values:</strong> separate dimension \(d_v\), output is multiplied by \(W^O\) which is \((d_v \times h) \times d_{model}\) so we can get back to \(d_{model}\)</p> <p><strong>FFN:</strong> can use a higher latent dimension \(d_{ff}\)</p> \[\text{FFN}(x) = \text{SwiGLU}(x, W_1, W_2, W_3) = W_2(\text{SiLU}(W_1 x) \odot W_3 x)\] <p><strong>Typical configurations (from GPT-3):</strong></p> <table> <thead> <tr> <th>Model</th> <th>\(n_{params}\)</th> <th>\(n_{layers}\)</th> <th>\(d_{model}\)</th> <th>\(n_{heads}\)</th> <th>\(d_{head}\)</th> </tr> </thead> <tbody> <tr> <td>GPT-3 Small</td> <td>125M</td> <td>12</td> <td>768</td> <td>12</td> <td>64</td> </tr> <tr> <td>GPT-3 Medium</td> <td>350M</td> <td>24</td> <td>1024</td> <td>16</td> <td>64</td> </tr> <tr> <td>GPT-3 Large</td> <td>760M</td> <td>24</td> <td>1536</td> <td>16</td> <td>96</td> </tr> <tr> <td>GPT-3 XL</td> <td>1.3B</td> <td>24</td> <td>2048</td> <td>24</td> <td>128</td> </tr> <tr> <td>GPT-3 2.7B</td> <td>2.7B</td> <td>32</td> <td>2560</td> <td>32</td> <td>80</td> </tr> <tr> <td>GPT-3 6.7B</td> <td>6.7B</td> <td>32</td> <td>4096</td> <td>32</td> <td>128</td> </tr> <tr> <td>GPT-3 13B</td> <td>13.0B</td> <td>40</td> <td>5140</td> <td>40</td> <td>128</td> </tr> <tr> <td>GPT-3 175B</td> <td>175.0B</td> <td>96</td> <td>12288</td> <td>96</td> <td>128</td> </tr> </tbody> </table> <h5 id="flops-distribution"><strong>FLOPs Distribution</strong></h5> <p>As models scale, the proportion of FLOPs changes:</p> <ul> <li> <strong>Smaller models (760M):</strong> ~35% MHA, ~44% FFN, ~15% attention computation, ~6% logits</li> <li> <strong>Larger models (175B):</strong> ~17% MHA, ~80% FFN, ~3% attention computation, ~0.3% logits</li> </ul> <p>The FFN becomes increasingly dominant in larger models.</p> <h4 id="transformer-language-modeling"><strong>Transformer Language Modeling</strong></h4> <h5 id="training"><strong>Training</strong></h5> \[P(w|\text{context}) = \text{softmax}(Wh_i)\] <p>where \(W\) is a (vocab_size) × (hidden_size) matrix</p> <p><strong>Training setup:</strong></p> <ul> <li>Input is a sequence of words</li> <li>Output is those words shifted by one</li> <li>Allows us to train on predictions across several timesteps simultaneously (similar to batching but this is NOT what we refer to as batching)</li> </ul> <p><strong>Loss:</strong> \(\text{loss} = -\log P(w^*|\text{context})\)</p> <p><strong>Total loss:</strong> sum of negative log likelihoods at each position</p> <p><strong>Important:</strong> Parallel inference across several tokens at training time, but at decoding time, tokens are generated one at a time.</p> <p>TODO: Add pics from slide 61</p> <h5 id="batched-lm-training-detailed-explanation"><strong>Batched LM Training: Detailed Explanation</strong></h5> <p><strong>Batching</strong> in LM training refers to processing multiple sequences simultaneously, which is different from the parallel processing of tokens within a single sequence.</p> <p><strong>Mechanism:</strong></p> <ol> <li> <strong>Batch Dimension:</strong> We process multiple independent sequences at once <ul> <li>Each sequence in the batch is a separate training example</li> <li>Sequences are typically padded to the same length for efficient matrix operations</li> </ul> </li> <li> <strong>Why Batching?</strong> <ul> <li> <strong>Computational Efficiency:</strong> GPUs are optimized for parallel matrix operations. Processing one sequence at a time would waste GPU capacity</li> <li> <strong>Gradient Stability:</strong> Averaging gradients over multiple examples provides more stable updates</li> <li> <strong>Throughput:</strong> We can process many more tokens per second</li> </ul> </li> <li> <strong>The Flow:</strong> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>Input Batch: [seq1, seq2, seq3, ...]
Shape: (batch_size, seq_len, d_model)
   
→ Each sequence processes through transformer independently
→ Attention within each sequence (not across sequences)
→ Loss computed for each sequence
→ Losses averaged across batch
</code></pre></div> </div> </li> <li> <strong>Two Levels of Parallelism:</strong> <ul> <li> <strong>Within-sequence parallelism:</strong> All tokens in a sequence are processed simultaneously during forward pass (enabled by self-attention)</li> <li> <strong>Across-sequence parallelism:</strong> Multiple sequences processed at once (batching)</li> </ul> </li> <li> <strong>Practical Considerations:</strong> <ul> <li>Batch size is limited by GPU memory</li> <li>Larger batches → more stable gradients but slower iteration</li> <li>Gradient accumulation can simulate larger batches</li> </ul> </li> </ol> <p>This is distinct from the token-level parallelism during training where we predict all positions simultaneously using teacher forcing.</p> <h5 id="a-small-problem-with-transformer-lms"><strong>A Small Problem with Transformer LMs</strong></h5> <p>This Transformer LM as we’ve described it will easily achieve perfect accuracy. Why?</p> <p>With standard self-attention: “I” attends to “saw” and the model is “cheating”. How do we ensure that this doesn’t happen?</p> <p><strong>Solution: Attention Masking</strong></p> <p>We want to mask out everything in red (an upper triangular matrix)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Query words:     &lt;s&gt;  I  saw  the  dog
Key words:  &lt;s&gt;  [ ]  [X] [X]  [X]  [X]
            I    [ ]  [ ] [X]  [X]  [X]
            saw  [ ]  [ ] [ ]  [X]  [X]
            the  [ ]  [ ] [ ]  [ ]  [X]
            dog  [ ]  [ ] [ ]  [ ]  [ ]
</code></pre></div></div> <p>Where [X] represents masked (forbidden) attention positions.</p> <h4 id="positional-encodings"><strong>Positional Encodings</strong></h4> <h5 id="why-do-we-need-them"><strong>Why Do We Need Them?</strong></h5> <p><strong>Problem:</strong> Self-attention is permutation-invariant without positional information. We need to distinguish:</p> <ul> <li>“B followed by 3 As” from other arrangements</li> <li>Position-dependent patterns</li> </ul> <h5 id="absolute-position-encodings-bert-etc"><strong>Absolute Position Encodings (BERT, etc.)</strong></h5> <p>Encode each sequence position as an integer, add it to the word embedding vector:</p> \[\text{input}_i = \text{emb}(\text{word}_i) + \text{emb}(\text{position}_i)\] <p><strong>Why does this work?</strong> The model learns to use the positional information through training. The embeddings can learn to represent position-dependent features.</p> <h5 id="sinusoidal-position-encodings-vaswani-et-al-2017"><strong>Sinusoidal Position Encodings (Vaswani et al., 2017)</strong></h5> <p>Alternative from Vaswani et al.: sines/cosines of different frequencies</p> <p><strong>Property:</strong> Closer words get higher dot products by default</p> <p>The encoding uses different frequencies for different dimensions:</p> \[PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] \[PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)\] <p>TODO: Add pics from slide 66, 67</p> <h5 id="rope-rotary-position-embedding---jianlin-su-et-al-2021"><strong>RoPE (Rotary Position Embedding) - Jianlin Su et al., 2021</strong></h5> <p><strong>Core Idea:</strong> Encode positional information by rotating the embedding vectors by an amount that depends on their position.</p> <p><strong>Mechanism:</strong></p> <ol> <li> <p><strong>Break vector into 2D chunks:</strong> Take the \(d\)-dimensional vector and treat it as \(d/2\) pairs of 2D vectors</p> </li> <li> <p><strong>Rotation angle:</strong> For position \(i\) and dimension pair \(k\): \(\theta_{i,k} = \frac{i}{\Theta^{(2k-2)/d}}\)</p> <p>where \(\Theta\) is a base value (typically 10000)</p> </li> <li> <p><strong>Rotation matrix:</strong> For each 2D chunk: \(R_k^i = \begin{bmatrix} \cos(\theta_{i,k}) &amp; -\sin(\theta_{i,k}) \\ \sin(\theta_{i,k}) &amp; \cos(\theta_{i,k}) \end{bmatrix}\)</p> </li> <li> <p><strong>Apply rotation:</strong> Multiply each 2D chunk by its corresponding rotation matrix</p> </li> </ol> <p><strong>What happens as \(i\) increases?</strong> \(\theta\) increases → more rotation</p> <p><strong>What happens as \(k\) increases?</strong> \(\theta\) decreases → less rotation per position</p> <p><strong>Intuition:</strong></p> <ul> <li>Initial positions (small \(k\)) rotate heavily with each position change</li> <li>Later positions (large \(k\)) rotate slowly</li> <li>This creates a multi-scale positional encoding</li> </ul> <p>TODO: Add pics from slide 68, 69</p> <p>TODO: Add pics from slide 71, 72</p> <p><strong>How does RoPE help encode position?</strong></p> <p>The key insight is that RoPE encodes <strong>relative</strong> position information through the geometry of rotations:</p> <ol> <li> <p><strong>Relative position through rotation difference:</strong> When computing attention between positions \(i\) and \(j\), the dot product \(q_i^T k_j\) naturally incorporates the rotation difference \((i-j)\)</p> </li> <li> <strong>Scale-dependent locality:</strong> Different frequency bands (\(k\) values) capture different scales of positional relationships: <ul> <li>Low \(k\): sensitive to immediate neighbors</li> <li>High \(k\): captures long-range dependencies</li> </ul> </li> <li> <strong>Extrapolation capability:</strong> The rotational nature allows the model to generalize to positions beyond those seen during training (within limits)</li> </ol> <h5 id="where-are-pes-used"><strong>Where are PEs used?</strong></h5> <p><strong>Classical Vaswani et al. Transformer (2017):</strong> Added to input</p> <ul> <li>PE applied once at the bottom of the network</li> <li>Affects all subsequent layers</li> </ul> <p><strong>Modern practice:</strong> Apply RoPE to Qs and Ks right before self-attention</p> <ul> <li>PE applied within each attention layer</li> <li>Only affects the attention computation, not the value vectors</li> <li>Better preserves semantic information in embeddings</li> </ul> <p><strong>How do these methods differ?</strong></p> <ul> <li>Absolute PEs: single application, affects all operations</li> <li>RoPE: applied multiple times, only affects attention mechanism, encodes relative positions</li> </ul> <h5 id="rope-interpolation-and-extrapolation-properties"><strong>RoPE Interpolation and Extrapolation Properties</strong></h5> <p><strong>Position Interpolation (PI):</strong> If RoPE is trained with encodings up to token \(L\), you can expand it to \(L'\) by scaling:</p> \[\theta_{i,k} = \frac{i \cdot (L/L')}{\Theta^{(2k-2)/d}}\] <p>This compresses the positional space, making the model think longer sequences are actually shorter.</p> <p><strong>Wavelength Concept:</strong> The number of tokens needed to do a full rotation at dimension pair \(k\):</p> \[\lambda_k = \frac{2\pi}{\theta_k} = 2\pi \Theta^{2k/|D|}\] <p><strong>YaRN (Yet another RoPE extensioN method) - Bowen Peng et al., 2023:</strong></p> <p>Two key ideas:</p> <ol> <li> <strong>Frequency-dependent interpolation:</strong> <ul> <li>If wavelength is small (\(k\) is large): rotations are fast, don’t use position interpolation</li> <li>If wavelength is large (\(k\) is small): rotations are slow, use position interpolation</li> <li>This preserves high-frequency (local) information while extending low-frequency (global) range</li> </ul> </li> <li> <p><strong>Temperature scaling:</strong> Introduce a temperature \(t\) on the attention computation: \(\text{Attention} = \text{softmax}\left(\frac{QK^T}{t\sqrt{d_k}}\right)V\)</p> <p>This compensates for distribution shifts when extending context length</p> </li> </ol> <p><strong>Why does this work?</strong></p> <ul> <li>Different frequency bands in RoPE capture different types of positional relationships</li> <li>Low frequencies (small \(k\)): long-range dependencies, can be interpolated</li> <li>High frequencies (large \(k\)): local patterns, should be preserved</li> <li>Temperature adjustment maintains attention entropy at extended lengths</li> </ul> <h5 id="nope-no-position-embedding---kazemnejad-et-al-2023"><strong>NoPE (No Position Embedding) - Kazemnejad et al., 2023</strong></h5> <p><strong>Question:</strong> Do we <em>actually</em> need positional encodings?</p> <p><strong>Surprising finding:</strong> With causal masking, transformers can learn positional information implicitly!</p> <p><strong>Key difference:</strong></p> <ul> <li> <strong>Full attention:</strong> No inherent positional information → requires explicit PEs</li> <li> <strong>Causal mask:</strong> The mask structure itself provides positional information <ul> <li>Each position can only attend to earlier positions</li> <li>The attention pattern reveals relative positions</li> </ul> </li> </ul> <p><strong>Result:</strong> In some settings, models can work without explicit positional encodings, though most modern LLMs still use them for better performance.</p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"Monishver11/monishver11.github.io","data-repo-id":"R_kgDONe9Wkw","data-category":"General","data-category-id":"DIC_kwDONe9Wk84Cp-bh","data-mapping":"title","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Monishver Chandrasekaran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-1HD0LJE1KY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1HD0LJE1KY");</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-resume",title:"Resume",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-thoughts",title:"Thoughts",description:"",section:"Navigation",handler:()=>{window.location.href="/thoughts/"}},{id:"post-distributed-systems-lecture-1",title:"Distributed Systems - Lecture 1",description:"Distributed Systems Course at NYU Courant - Personal Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2026/ds1/"}},{id:"post-llmr-a1",title:"Llmr A1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2026/llmr-a1/"}},{id:"post-llmr-lecture-1",title:"LLMR - Lecture 1",description:"LLM Reasoners Course at NYU Courant - Personal Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2026/llmr1/"}},{id:"post-apache-flink",title:"Apache Flink",description:"Realtime and Big Data Analytics Course at NYU Courant - Personal Notes 10",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-11-flink/"}},{id:"post-apache-kafka",title:"Apache Kafka",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 9",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-10-kafka/"}},{id:"post-apache-zookeeper",title:"Apache ZooKeeper",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 8",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-9-zookeeper/"}},{id:"post-apache-hbase",title:"Apache HBase",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 7",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-8-hbase/"}},{id:"post-hive-amp-trino",title:"Hive &amp; Trino",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 6",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-7-hive/"}},{id:"post-mapreduce-design-patterns",title:"MapReduce Design Patterns",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 5",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-5-mr-dp/"}},{id:"post-big-data-processing-concepts-amp-mapreduce",title:"Big Data Processing Concepts &amp; MapReduce",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 4",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-4-mapreduce/"}},{id:"post-hadoop-distributed-file-system-hdfs",title:"Hadoop Distributed File System (HDFS)",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 3",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-3-hdfs/"}},{id:"post-reading-notes-from-aleksa-gordic-39-s-gpu-blogpost",title:"Reading Notes from Aleksa Gordic&#39;s GPU BlogPost",description:"Reading notes for my reference from Aleksa Gordic&#39;s GPU BlogPost",section:"Posts",handler:()=>{window.location.href="/blog/2025/aleksagordic-gpu-blog-notes/"}},{id:"post-mlp-standard-derivatives-derivation",title:"MLP Standard Derivatives Derivation",description:"MLP Standard Derivatives Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-derivatives/"}},{id:"post-simple-mlp-forward-and-backward-pass-with-einsum-derivation",title:"Simple MLP - Forward and Backward Pass (With Einsum) Derivation",description:"Simple MLP - Forward and Backward Pass Derivation",section:"Posts",handler:()=>{window.location.href="/blog/2025/mlp-fw-bwd/"}},{id:"post-gpu-notes",title:"GPU Notes",description:"GPU Architecture and Programming Course at NYU Courant - Lecture and Conceptual Notes",section:"Posts",handler:()=>{window.location.href="/blog/2025/gpu-notes/"}},{id:"post-big-data-storage",title:"Big Data Storage",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 2",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-2-storage/"}},{id:"post-introduction-to-realtime-and-big-data-analytics",title:"Introduction to Realtime and Big Data Analytics",description:"Realtime and Big Data Analytics Course at NYU Courant - Conceptual Notes 1",section:"Posts",handler:()=>{window.location.href="/blog/2025/big-data-1-intro/"}},{id:"post-gpu-essentials-a-concise-technical-guide",title:"GPU Essentials - A Concise Technical Guide",description:"A concise, technical guide to GPU architecture and CUDA, showing how massive parallelism is achieved through threads, blocks, SMs, and memory hierarchies.",section:"Posts",handler:()=>{window.location.href="/blog/2025/GPU-Intro/"}},{id:"post-wrapping-up-our-ml-foundations-journey",title:"Wrapping Up Our ML Foundations Journey",description:"A reflection on our exploration of machine learning fundamentals, from mathematical prerequisites to gradient boosting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/wrapping-ml-basics/"}},{id:"post-gradient-boosting-in-practice",title:"Gradient Boosting in Practice",description:"Practical insights and regularization techniques to make gradient boosting robust, efficient, and generalize well in real-world applications.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gb-in-practice/"}},{id:"post-binomialboost",title:"BinomialBoost",description:"See how the gradient boosting framework naturally extends to binary classification using the logistic loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/binomial-boost/"}},{id:"post-gradient-boosting-quot-anyboost-quot",title:"Gradient Boosting / &quot;Anyboost&quot;",description:"A clear and intuitive walkthrough of gradient boosting as functional gradient descent, with detailed explanations of residuals, step directions, and algorithmic structure.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gradient-boosting/"}},{id:"post-forward-stagewise-additive-modeling",title:"Forward Stagewise Additive Modeling",description:"A clear walkthrough of FSAM and its role in boosting with exponential loss.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FSAM/"}},{id:"post-introduction-to-gradient-boosting",title:"Introduction to Gradient Boosting",description:"A beginner-friendly introduction to gradient boosting, connecting empirical risk minimization, adaptive basis functions, and the challenges of non-differentiable models like decision trees.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-gradient-boosting/"}},{id:"post-boosting-and-adaboost",title:"Boosting and AdaBoost",description:"This blog post provides an in-depth overview of boosting techniques, focusing on AdaBoost, explaining its key concepts, algorithm steps, and real-world applications in classification tasks.",section:"Posts",handler:()=>{window.location.href="/blog/2025/adaboost/"}},{id:"post-random-forests",title:"Random Forests",description:"Explore how Random Forests enhance Bagging by introducing randomness at each tree split, reducing correlation, and increasing diversity to build more accurate and stable prediction models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/random-forest/"}},{id:"post-bagging-bootstrap-aggregation",title:"Bagging - Bootstrap Aggregation",description:"Bagging (Bootstrap Aggregating) combines multiple high-variance models trained on different bootstrap samples to create a more stable, accurate, and lower-variance ensemble predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bagging/"}},{id:"post-introduction-to-ensemble-methods",title:"Introduction to Ensemble Methods",description:"A beginner&#39;s guide to ensemble methods in machine learning, explaining how averaging and bootstrapping reduce variance and improve model performance.",section:"Posts",handler:()=>{window.location.href="/blog/2025/intro-to-ensemble-methods/"}},{id:"post-decision-trees-for-classification",title:"Decision Trees for Classification",description:"Explains what makes a good split, how impurity is quantified using Gini, Entropy, and misclassification error, and why trees are both powerful and interpretable.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees-classification/"}},{id:"post-decision-trees-our-first-non-linear-classifier",title:"Decision Trees - Our First Non-Linear Classifier",description:"Learn how decision trees work for regression, including split criteria, overfitting control, and intuitive examples.",section:"Posts",handler:()=>{window.location.href="/blog/2025/decision-trees/"}},{id:"post-structured-perceptron-amp-structured-svm",title:"Structured Perceptron &amp; Structured SVM",description:"Understanding how Structured Perceptron and Structured SVM learn to predict structured outputs with interdependent components.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-perceptron-svm/"}},{id:"post-structured-prediction-and-multiclass-svm",title:"Structured Prediction and Multiclass SVM",description:"An in-depth yet intuitive walkthrough of structured prediction, covering sequence labeling, feature engineering, and scoring methods for complex outputs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/structured-prediction/"}},{id:"post-multiclass-classification-with-svm",title:"Multiclass Classification with SVM",description:"Learn how Support Vector Machines extend to multiclass classification with an intuitive breakdown of margin concepts, loss derivation, and the multiclass hinge loss formulation.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-svm/"}},{id:"post-multiclass-logistic-regression-amp-multiclass-perceptron-algorithm",title:"Multiclass Logistic Regression &amp; Multiclass Perceptron Algorithm",description:"Learn the essentials of multiclass classification, focusing on logistic regression, perceptron algorithms, and efficient model building techniques.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass-loss/"}},{id:"post-multiclass-classification-overview",title:"Multiclass Classification - Overview",description:"Learn how One-vs-All and One-vs-One extend binary classification to multiclass problems, their key differences, and best use cases.",section:"Posts",handler:()=>{window.location.href="/blog/2025/multiclass/"}},{id:"post-gaussian-regression-a-bayesian-approach-to-linear-regression",title:"Gaussian Regression - A Bayesian Approach to Linear Regression",description:"This guide explores Gaussian regression, deriving its closed-form posterior, linking MAP estimation to ridge regression, and explaining predictive uncertainty for Bayesian inference.",section:"Posts",handler:()=>{window.location.href="/blog/2025/gaussian-regression/"}},{id:"post-my-understanding-of-quot-efficient-algorithms-for-online-decision-problems-quot-paper",title:"My Understanding of &quot;Efficient Algorithms for Online Decision Problems&quot; Paper",description:"A breakdown of Follow the Perturbed Leader (FPL) from Kalai &amp; Vempala\u2019s (2005) paper, &quot;Efficient Algorithms for Online Decision Problems.&quot; This blog explores how FPL improves online decision-making, minimizes regret, and extends to structured problems like shortest paths and adaptive Huffman coding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL-proof/"}},{id:"post-follow-the-leader-fl-and-follow-the-perturbed-leader-fpl-in-online-learning",title:"Follow the Leader (FL) and Follow the Perturbed Leader (FPL) in Online Learning...",description:"Discover how Follow the Leader (FL) and Follow the Perturbed Leader (FPL) work in online learning, their mathematical foundations, and how perturbations help achieve better stability and regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/FPL/"}},{id:"post-bayesian-conditional-models",title:"Bayesian Conditional Models",description:"Learn how Bayesian conditional models leverage prior knowledge, posterior updates, and predictive distributions to make principled, uncertainty-aware predictions in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-conditional-models/"}},{id:"post-on-line-to-batch-conversion",title:"On-line to Batch Conversion",description:"Understanding how online learning algorithms can be used to derive hypotheses with small generalization error in a stochastic setting.",section:"Posts",handler:()=>{window.location.href="/blog/2025/online-to-batch/"}},{id:"post-randomized-weighted-majority-algorithm",title:"Randomized Weighted Majority Algorithm",description:"Learn how the Randomized Weighted Majority (RWM) Algorithm leverages probabilistic prediction to minimize regret and defend against adversarial strategies in online learning environments.",section:"Posts",handler:()=>{window.location.href="/blog/2025/RWM/"}},{id:"post-bayesian-decision-theory-concepts-and-recap",title:"Bayesian Decision Theory - Concepts and Recap",description:"A comprehensive guide to Bayesian decision theory, exploring its key components, point estimation, loss functions, and connections to classical probability modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-decision-theory/"}},{id:"post-reinforcement-learning-an-introductory-guide",title:"Reinforcement Learning - An Introductory Guide",description:"Explore the foundations of intelligence, decision-making principles, and their application in reinforcement learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/rl-intro/"}},{id:"post-conjugate-priors-and-bayes-point-estimates",title:"Conjugate Priors and Bayes Point Estimates",description:"Learn how conjugate priors streamline Bayesian inference and discover  ways to summarize posterior distributions using Bayes point estimates.",section:"Posts",handler:()=>{window.location.href="/blog/2025/bayes-point-estimate/"}},{id:"post-doubling-trick-a-clever-strategy-to-handle-unknown-horizons",title:"Doubling Trick - A Clever Strategy to Handle Unknown Horizons",description:"Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds.",section:"Posts",handler:()=>{window.location.href="/blog/2025/doubling-trick/"}},{id:"post-exponential-weighted-average-algorithm",title:"Exponential Weighted Average Algorithm",description:"Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/EWA/"}},{id:"post-bayesian-machine-learning-mathematical-foundations",title:"Bayesian Machine Learning - Mathematical Foundations",description:"A beginner-friendly guide to Bayesian statistics, explaining priors, likelihoods, posteriors, and real-world examples like coin-flipping to build a clear and intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Bayesian-ML/"}},{id:"post-understanding-the-weighted-majority-algorithm-in-online-learning",title:"Understanding the Weighted Majority Algorithm in Online Learning",description:"Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.",section:"Posts",handler:()=>{window.location.href="/blog/2025/WMA/"}},{id:"post-online-learning-in-ml-a-beginner-s-guide-to-adaptive-learning",title:"Online Learning in ML - A Beginner\u2019s Guide to Adaptive Learning",description:"Learn how online learning transforms machine learning by handling dynamic, real-time data and adversarial scenarios. Explore its advantages, real-world applications, and key concepts like regret minimization and the Halving Algorithm in this beginner-friendly guide to adaptive AI.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Online-Learning/"}},{id:"post-multivariate-gaussian-distribution-and-naive-bayes",title:"Multivariate Gaussian Distribution and Naive Bayes",description:"Dive into the multivariate Gaussian distribution, its role in probabilistic modeling, and how it powers Naive Bayes classifiers with practical insights and mathematical intuition.",section:"Posts",handler:()=>{window.location.href="/blog/2025/Multivariate-GNB/"}},{id:"post-gaussian-naive-bayes-a-natural-extension",title:"Gaussian Naive Bayes - A Natural Extension",description:"Explore how Gaussian Naive Bayes adapts to continuous inputs, including parameter estimation, decision boundaries, and its relation to logistic regression.",section:"Posts",handler:()=>{window.location.href="/blog/2025/NB-continuous-features/"}},{id:"post-an-introduction-to-generative-models-naive-bayes-for-binary-features",title:"An Introduction to Generative Models - Naive Bayes for Binary Features",description:"Learn the fundamentals of Naive Bayes, from its conditional independence assumption to the maximum likelihood estimation (MLE) of parameters, using a binary feature example.",section:"Posts",handler:()=>{window.location.href="/blog/2025/generative-models/"}},{id:"post-generalized-linear-models-explained-leveraging-mle-for-regression-and-classification",title:"Generalized Linear Models Explained - Leveraging MLE for Regression and Classification",description:"Explore how Maximum Likelihood Estimation (MLE) forms the backbone of generalized linear models, enabling robust solutions for regression, classification, and beyond.",section:"Posts",handler:()=>{window.location.href="/blog/2025/MLE/"}},{id:"post-unveiling-probabilistic-modeling",title:"Unveiling Probabilistic Modeling",description:"Explore the fundamentals of probabilistic modeling and how it enhances our understanding of linear regression, from parameter estimation to error distribution.",section:"Posts",handler:()=>{window.location.href="/blog/2025/probabilistic-modeling/"}},{id:"post-svm-solution-in-the-span-of-the-data",title:"SVM Solution in the Span of the Data",description:"This blog explores how the span property simplifies optimization in SVM and ridge regression, introduces the Representer Theorem, and highlights the computational benefits of kernelization.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-solution-span-of-data/"}},{id:"post-understanding-the-kernel-trick",title:"Understanding the Kernel Trick",description:"A step-by-step exploration of kernel methods, unraveling their role in enabling powerful nonlinear modeling through the elegance of the kernel trick.",section:"Posts",handler:()=>{window.location.href="/blog/2025/kernel-trick/"}},{id:"post-unleashing-the-power-of-linear-models-tackling-nonlinearity-with-feature-maps",title:"Unleashing the Power of Linear Models - Tackling Nonlinearity with Feature Maps",description:"Explore how feature maps transform inputs, handle nonlinearities, and expand the expressiveness of linear models with practical examples and intuitive solutions.",section:"Posts",handler:()=>{window.location.href="/blog/2025/feature-maps/"}},{id:"post-demystifying-svms-understanding-complementary-slackness-and-support-vectors",title:"Demystifying SVMs - Understanding Complementary Slackness and Support Vectors",description:"A deep dive into the complementary slackness conditions in SVMs, exploring their connection to margins, support vectors, and kernelized optimization for powerful classification.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm-dual-problem/"}},{id:"post-the-dual-problem-of-svm",title:"The Dual Problem of SVM",description:"An in-depth exploration of the dual problem in SVMs, covering its mathematical foundation, Lagrangian formulation, duality principles, and complementary slackness for intuitive understanding.",section:"Posts",handler:()=>{window.location.href="/blog/2025/dual-problem/"}},{id:"post-subgradient-and-subgradient-descent",title:"Subgradient and Subgradient Descent",description:"An deep dive into subgradients, subgradient descent, and their application in optimizing non-differentiable functions like SVMs.",section:"Posts",handler:()=>{window.location.href="/blog/2025/subgradient/"}},{id:"post-support-vector-machines-svm-from-hinge-loss-to-optimization",title:"Support Vector Machines(SVM) - From Hinge Loss to Optimization",description:"Demystifying Support Vector Machines (SVM) - A step-by-step exploration of hinge loss, optimization, and gradient mechanics.",section:"Posts",handler:()=>{window.location.href="/blog/2025/svm/"}},{id:"post-understanding-the-maximum-margin-classifier",title:"Understanding the Maximum Margin Classifier",description:"An engaging walkthrough of maximum margin classifiers, exploring their foundations, geometric insights, and the transition to support vector machines.",section:"Posts",handler:()=>{window.location.href="/blog/2025/max-margin-classifier/"}},{id:"post-l1-and-l2-regularization-nuanced-details",title:"L1 and L2 Regularization - Nuanced Details",description:"A detailed explanation of L1 and L2 regularization, focusing on their theoretical insights, geometric interpretations, and practical implications for machine learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2025/l1-l2-reg-indepth/"}},{id:"post-regularization-balancing-model-complexity-and-overfitting",title:"Regularization - Balancing Model Complexity and Overfitting",description:"Discover how regularization controls model complexity, reduces overfitting, and enhances generalization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/regularization/"}},{id:"post-loss-functions-regression-and-classification",title:"Loss Functions - Regression and Classification",description:"Exploring regression and classification loss functions, with a deep dive into logistic regression and its role in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2025/loss-functions/"}},{id:"post-optimizing-stochastic-gradient-descent-key-recommendations-for-effective-training",title:"Optimizing Stochastic Gradient Descent - Key Recommendations for Effective Training",description:"A comprehensive collection of expert recommendations to enhance the performance and reliability of Stochastic Gradient Descent, ensuring smoother and faster convergence during training.",section:"Posts",handler:()=>{window.location.href="/blog/2025/sgd-tips/"}},{id:"post-gradient-descent-and-second-order-optimization-a-thorough-comparison",title:"Gradient Descent and Second-Order Optimization - A Thorough Comparison",description:"An in-depth exploration of Gradient Descent (GD) and Second-Order Gradient Descent (2GD), focusing on convergence behavior, mathematical derivations, and performance differences.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-tips/"}},{id:"post-gradient-descent-convergence-prerequisites-and-detailed-derivation",title:"Gradient Descent Convergence - Prerequisites and Detailed Derivation",description:"Understanding the convergence of gradient descent with a fixed step size and proving its rate of convergence for convex, differentiable functions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gd-convergence/"}},{id:"post-understanding-stochastic-gradient-descent-sgd",title:"Understanding Stochastic Gradient Descent (SGD)",description:"A detailed guide to gradient descent variants, highlighting the mechanics, trade-offs, and practical insights of Stochastic Gradient Descent (SGD).",section:"Posts",handler:()=>{window.location.href="/blog/2024/SGD/"}},{id:"post-gradient-descent-a-detailed-walkthrough",title:"Gradient Descent - A Detailed Walkthrough",description:"An in-depth exploration of gradient descent, including its convergence and step size considerations.",section:"Posts",handler:()=>{window.location.href="/blog/2024/gradient-descent/"}},{id:"post-empirical-risk-minimization-erm",title:"Empirical Risk Minimization (ERM)",description:"Exploring Empirical Risk Minimization - Balancing approximation, estimation, and optimization errors to build effective supervised learning models.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ERM/"}},{id:"post-understanding-the-supervised-learning-setup",title:"Understanding the Supervised Learning Setup",description:"An in-depth exploration of the supervised learning setup, covering key concepts like prediction functions, loss functions, risk evaluation, and the Bayes optimal predictor.",section:"Posts",handler:()=>{window.location.href="/blog/2024/supervised-learning/"}},{id:"post-timeline-of-machine-learning-history",title:"Timeline of Machine Learning History",description:"A concise timeline of machine learning&#39;s history, showcasing key milestones and breakthroughs that shaped the field.",section:"Posts",handler:()=>{window.location.href="/blog/2024/ml-history/"}},{id:"post-advanced-probability-concepts-for-machine-learning",title:"Advanced Probability Concepts for Machine Learning",description:"This blog explores key probability theory concepts, from distributions and Bayes&#39; Theorem to covariance and the Central Limit Theorem, emphasizing their critical application in machine learning and statistical modeling.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-2/"}},{id:"post-understanding-the-basics-of-probability-theory-for-machine-learning",title:"Understanding the Basics of Probability Theory for Machine Learning",description:"This blog explores essential probability concepts and their significance in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/probability-1/"}},{id:"post-linear-algebra-prerequisites-for-machine-learning",title:"Linear Algebra - Prerequisites for Machine Learning",description:"This blog post covers the key linear algebra concepts and their applications in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/linear-algebra/"}},{id:"post-multivariate-calculus-prerequisites-for-machine-learning",title:"Multivariate Calculus - Prerequisites for Machine Learning",description:"This blog post explores key multivariate calculus concepts essential for understanding optimization in machine learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/multivariate-calculus/"}},{id:"post-introduction-to-machine-learning-ml",title:"Introduction to Machine Learning(ML)",description:"An easy guide to machine learning, its applications, and how it connects to AI and human learning.",section:"Posts",handler:()=>{window.location.href="/blog/2024/intro-to-ml/"}},{id:"post-preface-amp-introduction",title:"Preface &amp; Introduction",description:"First blog post\u2014setting the stage for the journey ahead.",section:"Posts",handler:()=>{window.location.href="/blog/2024/preface-ml/"}},{id:"news-spring-2025-semester-update",title:"Spring 2025 Semester Update",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_1/"}},{id:"news-sharing-personal-reflections-thoughts-tab-thoughts",title:"Sharing personal reflections - [Thoughts Tab](/thoughts/)",description:"",section:"News"},{id:"news-wrapping-up-our-ml-foundations-journey-blog-2025-wrapping-ml-basics",title:"[Wrapping Up Our ML Foundations Journey](/blog/2025/wrapping-ml-basics/)",description:"",section:"News"},{id:"projects-mta-transit-time-prediction",title:"MTA Transit Time Prediction",description:"Leveraging real-time data and machine learning to predict bus arrival times in New York City with route-based and grid-based approaches.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-gaze-guided-reinforcement-learning-for-visual-search",title:"Gaze-Guided Reinforcement Learning for Visual Search",description:"Discover how gaze prediction from human eye-tracking enhances AI agents in object search tasks. By integrating visual attention into reinforcement learning through three novel methods, our approach enables faster, more effective navigation in simulated environments.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-understanding-swap-regret-2-0",title:"Understanding Swap Regret 2.0",description:"This blog post unpacks the &quot;Swap Regret 2.0&quot; paper through slide-by-slide insights, showing how the TreeSwap algorithm closes the gap between external and swap regret, advancing equilibrium computation in game theory and reinforcement learning.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-smallgraphgcn-accelerating-gnn-training-on-batched-small-graphs",title:"SmallGraphGCN - Accelerating GNN Training on Batched Small Graphs",description:"Discover how fused-edge-centric CUDA kernels dramatically accelerate Graph Neural Network training on molecular datasets. By rethinking parallelism strategies for batched small graphs, our approach achieves up to 3.1\xd7 faster forward execution and 1.3\xd7 end-to-end training speedup over PyTorch Geometric.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-from-baseline-to-deepseek-single-gpu-moe-training-efficiency",title:"From Baseline to DeepSeek - Single-GPU MoE Training Efficiency",description:"A systems-level analysis of training Mixture-of-Experts (MoE) Transformer models under single-GPU constraints. We compare naive PyTorch MoE, ScatterMoE, MegaBlocks, and DeepSeek-inspired architectures, revealing critical trade-offs between convergence behavior, memory footprint, and training throughput.",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%6D%6F%6E%69%73%68%76%65%72%63%68%61%6E%64%72%61%73%65%6B%61%72%61%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"social-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Monishver11","_blank")}},{id:"social-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/monishver","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>