---
layout: post
title: LLMR - Lecture 1(WIP)
date: 2026-01-24 18:30:00-0400
featured: false
description: LLM Reasoners Course at NYU Courant - Personal Notes 1
tags: 
categories: LLMR-NYU
giscus_comments: true
related_posts: false

---

[This is a WIP]

Goals of this course: Understand how a language model works all the way down. Build it from scratch to really learn this.

According to Percy Liang: three important things to know:
- Mechanics: how things work (Transformer arch., model parallelism on GPUs, etc.)
- Mindset: squeezing the most out of the hardware, taking scale seriously
- Intui.ons: which data and modeling decisions yield good accuracy

Language models: place a distribution P(w) over strings w in a language.

Autoregressive models P (w) = P (w1).P(w2|w1).P(w3|w1, w2). . .

n-gram models: distribution of next word is a categorical conditioned on previous n-1 words

P (wi|w1, . . . , wi-1) = P (wi|wi-n+1, . . . , wi-1)

Markov property: only consider a few previous words

I visited San _____; put a distribution over the next word

2-gram: P(w | San)
3-gram: P(w | visited San)
4-gram: P(w | I visited San)

n-gram LLMs don’t generalize or abstract over related words, and don’t handle contexts beyond a few words.

Language models based purely on feedforward networks can abstract over words (using embeddings), but still fail to use large context

Need to handle more context: RNNs or CNNs can do this, but current best is Transformers using self-attention

Transformers;

Attention: method to access arbitrarily* far back in context from this point.

Keys: embedded versions of the sentence; query: what we want to find

Attention:
- Step 1: Compute scores for each key
- Step 2: softmax the scores to get probabilities α
- Step 3: compute output values by multiplying embeddings by alpha + summing

We can make attention more peaked by not setting keys equal to embeddings. Introduce the weight matrices for keys.

Attention, Formally:
-  Original “dot product” attention: si = kiTq
- Scaled dot product attention: si = kiT W q
- Equivalent to having two weight matrices: si = (WK ki)T(WQ q)
  
Self-attention: every word is both a key and a query simultaneously

Q: seq len x d matrix (d = embedding dimension = 2 for these slides)
K: seq len x d matrix

Scores S = QKT; Sij = qi.kj
len x len = (len x d) x (d x len)

Final step: softmax to get attentions A, then output is AE
*technically it’s A (EWV), using a values matrix V = EWV

Slide 47; <add the contents of this slide here>

TODO: Add pics from slide 48, 49, 50, 51, 52

<Add a link to Jay Alammar, The Illustrated Transformer for reference;>

Architecture:

TODO: Add pics for slide 53, 54;

Doubt; In general, a transformer block contains encode and decoder right, here what we're discussing is the encoder part right? <Answer it, by framing it as a question and answer, in a clean, precise and concise way>.

Slide 54; <add the contents of this slide here>

TODO: Add pics from slide 55, 56

Transformer Language Modeling;

P (w|context) = softmax(W hi); W is a (vocab size) x (hidden size) matrix

Training Transformer LMs

Input is a sequence of words, output is those words shifted by one, Allows us to train on predictions across several timesteps simultaneously (similar to batching but this is NOT what we refer to as batching)

loss = — log P(w*|context)
Total loss = sum of negative log likelihoods at each position

Parallel inference across several tokens at training .me, but at decoding time, tokens are generated one at a time

TODO: Add pics from slide 61

Batched LM Training: <add a short note on what it means, as per this flow and image>

A Small Problem with Transformer LMs

This Transformer LM as we’ve described it will easily achieve perfect accuracy. Why?

With standard self-a9en.on: “I” a9ends to “saw” and the model is “cheating”. How do we ensure that this doesn’t happen?

TODO: Add pics from slide 63

Attention Masking: We want to mask out everyting in red (an upper triangular matrix)

Positional Encodings

Absolute Posi.on Encodings (BERT, etc.)

Sinusoidal PE RoPE (Vaswani et al., 2017)

Alternative from Vaswani et al.: sines/cosines of diﬀerent frequencies
(closer words get higher dot products by default)

TODO: Add pics from slide 66, 67

RoPE (Jianlin Su et al., 2021)

TODO: Add pics from slide 68, 69

'k' is a chunked piece in the d-dim vector of a token

What happens as i increases? Theta increases

What happens as k increases? Theta decrases

<Link to RoPE Angle Visualizer>

TODO: Add pics from slide 71, 72

Rixi: rotate initial positions heavily, further-on positions less so

How does this help encode position for a Transformer?
- <Answer it>

And with RoPE, As the context doubles, you can divide the k/2 and add the embeddings is one benefit. <expand more on this and explain>

Where are PEs used?

Classical Vaswani et al. Transformer (2017): added to input

Modern practice: Apply RoPE to Qs and Ks right before self-attention

NoPE (Kazemnejad et al., 2023)

Full attention vs Causal mask
<Explain causal mask and this versus>

YaRN (Bowen Peng et al., 2023)


