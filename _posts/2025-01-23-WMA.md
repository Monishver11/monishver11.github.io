---
layout: post
title: Understanding the Weighted Majority Algorithm in Online Learning
date: 2025-01-23 12:33:00-0400
featured: false
description: Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.
tags: ML Math
categories: ADV-ML-NYU
giscus_comments: false
related_posts: false
# toc:
#   sidebar: left
---



**The Weighted Majority Algorithm: A Powerful Online Learning Technique**

In the continuation of our exploration into online learning, we turn to the **Weighted Majority Algorithm (WMA)**, an influential approach introduced by Littlestone and Warmuth in 1988. This algorithm builds upon the foundational principles of online learning and offers remarkable theoretical guarantees for handling adversarial scenarios.

Let’s dive into the workings of the Weighted Majority Algorithm, analyze its performance, and understand its strengths and limitations.


#### **The Weighted Majority Algorithm**

The Weighted Majority Algorithm operates in a framework where predictions are made by combining the advice of multiple experts. Unlike the Halving Algorithm, which outright eliminates incorrect hypotheses, WMA assigns and updates weights to experts based on their performance, ensuring a more adaptive approach.

##### **The Algorithm Steps**

1. **Initialization**:
   - Start with $$N$$ experts, each assigned an initial weight of 1:  
     $$w_{1,i} = 1 \quad \text{for } i = 1, 2, \dots, N$$

2. **Prediction**:
   - At each time step $$t$$:
     - Receive the instance $$x_t$$.
     - Predict the label $$\hat{y}_t$$ using a **weighted majority vote**:
       $$\hat{y}_t = 
       \begin{cases} 
       1 & \text{if } \sum_{i: y_{t,i}=1} w_{t,i} > \sum_{i: y_{t,i}=0} w_{t,i}, \\
       0 & \text{otherwise.} 
       \end{cases}$$

3. **Update Weights**:
   - After receiving the true label $$y_t$$, update the weights of the experts:
     - For each expert $$i$$:
       $$
       w_{t+1,i} =
       \begin{cases} 
       \beta w_{t,i} & \text{if } y_{t,i} \neq y_t, \\
       w_{t,i} & \text{otherwise,}
       \end{cases}
       $$
       where $$\beta \in [0,1)$$ is a parameter that reduces the weight of experts who make incorrect predictions.

4. **Termination**:
   - After $$T$$ iterations, return the final weights of all experts.


#### **Theoretical Performance of the Weighted Majority Algorithm**

##### **Mistake Bound**

One of the key results of the Weighted Majority Algorithm is its **mistake bound**, which ensures that the algorithm performs nearly as well as the best expert in hindsight.

**Theorem**:  
Let $$m_t$$ denote the total number of mistakes made by the Weighted Majority Algorithm up to time $$t$$, and let $$m_t^*$$ denote the number of mistakes made by the best expert. Then:
$$
m_t \leq m_t^* \log\left(\frac{1}{\beta}\right) + \log(N)
$$

### Special Cases
- **Realizable Case**: When there exists an expert who makes zero mistakes, the mistake bound simplifies to:
  $$
  m_t \leq \log(N)
  $$
  This matches the bound of the Halving Algorithm.

- **General Case**: The additional term involving $$m_t^*$$ reflects the cost of adapting to the worst-case scenario, where no expert perfectly predicts all labels.


##### **Proof of the Mistake Bound**

The analysis of WMA involves defining a **potential function** to measure the aggregate weight of all experts:
$$
W_t = \sum_{i=1}^N w_{t,i}
$$

**Upper Bound on Potential**
- After each mistake, the potential decreases by at least a factor of $$\beta$$:
  $$
  W_{t+1} \leq \beta W_t
  $$

**Lower Bound on Potential**
- The weight of the best expert, $$w_{t,i}^*$$, remains unchanged if they predict correctly:
  $$
  w_{t,i}^* \leq W_t
  $$

**Combining Bounds**
By comparing these upper and lower bounds, it can be shown that:
$$
m_t \leq m_t^* \log\left(\frac{1}{\beta}\right) + \log(N)
$$

This logarithmic dependence on the number of experts, $$N$$, ensures that WMA performs efficiently even in the presence of many experts.


##### **Strengths and Weaknesses of the Weighted Majority Algorithm**

**Advantages**
- **Remarkable Theoretical Guarantees**:
  - The logarithmic mistake bound requires no assumptions about the data distribution or expert performance.
- **Flexibility**:
  - WMA can be applied in a variety of adversarial and dynamic environments.

**Disadvantages**
- **Binary Loss**:
  - No deterministic algorithm, including WMA, can achieve zero regret with binary loss.
- **Improvements with Randomization**:
  - Randomized versions of WMA can achieve better regret bounds.
- **Extensions for Convex Losses**:
  - When applied to convex loss functions, WMA can yield improved theoretical guarantees.

---

##### **Conclusion**

The Weighted Majority Algorithm is a cornerstone of online learning, offering an elegant and efficient approach to handling adversarial settings. By adaptively updating the weights of experts, it ensures robust performance with minimal assumptions. While it has certain limitations, such as its inability to guarantee zero regret with binary loss, these can often be addressed through randomization or extensions to convex losses.

In the next post, we’ll explore randomized versions of WMA and delve into other powerful algorithms for online learning. Stay tuned!


---

**Exponential Weighted Average Algorithm: A Regret-Minimization Powerhouse**

The **Exponential Weighted Average Algorithm (EWAA)** is a fundamental online learning algorithm that provides elegant guarantees for minimizing regret in adversarial settings. It extends the principles of the Weighted Majority Algorithm by incorporating exponential weight updates, making it particularly effective for handling convex loss functions.


#### **How the Exponential Weighted Average Algorithm Works**

At its core, the EWAA maintains and updates weights for a set of experts, similar to the Weighted Majority Algorithm. However, it uses an **exponential weighting scheme** to achieve better bounds on regret, especially for convex losses.

##### **Steps of the Algorithm**

1. **Weight Update**:  
   At each time step $$t$$, the weights are updated as follows:  
   $$
   w_{t+1,i} = w_{t,i} \cdot e^{-\eta L(\hat{y}_{t,i}, y_t)}
   $$
   where:
   - $$\eta > 0$$ is the learning rate.
   - $$L(\hat{y}_{t,i}, y_t)$$ is the loss incurred by expert $$i$$ at time $$t$$.

   This update exponentially penalizes experts based on the loss they incur, ensuring that poorly performing experts are rapidly down-weighted.

2. **Prediction**:  
   The algorithm predicts by averaging the predictions of all experts, weighted by their current weights:
   $$
   \hat{y}_t = \sum_{i=1}^N w_{t,i} \cdot \hat{y}_{t,i}
   $$


##### **Theorem: Regret Bound for EWAA**

Let $$L(y, y')$$ be a convex loss function in its first argument, taking values in $$[0, 1]$$. For any $$\eta > 0$$ and any sequence of labels $$y_1, \dots, y_T \in \mathcal{Y}$$, the regret of the Exponential Weighted Average Algorithm satisfies:
$$
\text{Regret}(T) \leq \frac{\log N}{\eta} + \frac{\eta T}{8}.
$$

Here, regret is defined as the difference between the total loss of the algorithm and the loss of the best expert:
$$
\text{Regret}(T) = \sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i \in [N]} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t).
$$

##### **Optimizing the Learning Rate**
Choosing $$\eta = \sqrt{\frac{8 \log N}{T}}$$ minimizes the regret bound, yielding:
$$
\text{Regret}(T) \leq \sqrt{\frac{T}{2} \log N}.
$$

This result demonstrates the efficiency of the EWAA, with regret growing only logarithmically with the number of experts $$N$$ and sublinearly with the number of time steps $$T$$.


#### **Proof Sketch: Regret Bound of EWAA**

##### **Key Idea: Potential Function**
The proof relies on analyzing the potential function:
$$
\Phi_t = \log \left( \sum_{i=1}^N w_{t,i} \right).
$$

**Upper Bound**
Using the weight update rule and the convexity of the loss function, the following upper bound is derived:
$$
\Phi_{t+1} - \Phi_t \leq -\eta L(\hat{y}_t, y_t) + \frac{\eta^2}{8}.
$$

**Lower Bound**
By considering the contribution of the best expert, the potential function satisfies:
$$
\Phi_T - \Phi_0 \geq -\eta \min_{i \in [N]} L_{T,i} - \log N,
$$
where $$L_{T,i} = \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)$$ is the cumulative loss of expert $$i$$.

**Combining Bounds**
Summing the inequalities and comparing the two expressions for $$\Phi_T - \Phi_0$$ gives the regret bound:
$$
\sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i \in [N]} L_{T,i} \leq \frac{\log N}{\eta} + \frac{\eta T}{8}.
$$


##### **Advantages and Disadvantages of EWAA**

**Advantages**
1. **Strong Theoretical Guarantees**:
   - The regret bound is logarithmic in the number of experts $$N$$ and sublinear in the number of time steps $$T$$.
2. **Applicability to Convex Losses**:
   - Unlike binary loss-focused algorithms, EWAA works seamlessly with convex loss functions, making it more versatile.
3. **Weight Adaptivity**:
   - Exponential weight updates ensure that poor-performing experts are penalized efficiently.

**Disadvantages**
1. **Dependence on Horizon**:
   - Choosing the optimal learning rate $$\eta$$ requires prior knowledge of the time horizon $$T$$, which is not always feasible in practice.
2. **Increased Computational Complexity**:
   - Exponential weight updates and averaging can be computationally expensive for a large number of experts.


##### **Conclusion**

The Exponential Weighted Average Algorithm is a cornerstone of online learning, offering robust guarantees for regret minimization under convex loss functions. Its reliance on exponential weight updates makes it adaptable and theoretically elegant, but its dependence on the time horizon $$T$$ can pose practical challenges. 

In the next post, we’ll explore randomized extensions and their impact on improving regret bounds. Stay tuned for more insights into online learning!

---


**Doubling Trick: A Clever Strategy to Handle Unknown Horizons**

In many online learning algorithms, selecting the learning rate ($$\eta$$) or other parameters often requires knowledge of the total time horizon $$T$$. However, in practical scenarios, $$T$$ may not be known in advance. The **Doubling Trick** is a simple yet powerful method to overcome this limitation by dividing the time horizon into exponentially growing intervals and resetting parameters at the start of each interval.


#### **The Idea Behind the Doubling Trick**

The Doubling Trick works by splitting time into **periods of exponentially increasing length**. For example, the $$k$$-th period spans the range:
$$
I_k = [2^k, 2^{k+1} - 1].
$$

In each period:
1. A new learning rate or parameter is chosen based on the length of the period.
2. The algorithm is reset for that period.

By progressively increasing the size of the intervals, the algorithm ensures that the total regret remains within acceptable bounds.


#### **Regret Bound with the Doubling Trick**

##### **Theorem**
Assume the same conditions as those in the Exponential Weighted Average Algorithm. For any total time horizon $$T$$, the regret achieved by the Doubling Trick satisfies:
$$
\text{Regret}(T) \leq \sqrt{2} \cdot \sqrt{\frac{T}{2} \log N} + \sqrt{\log N / 2}.
$$

This result demonstrates that the Doubling Trick achieves regret bounds that are only slightly worse (by a constant factor) than if the total time horizon $$T$$ were known in advance.


##### **Proof Sketch**

To derive the regret bound, let:
- $$L_{I_k}$$ be the cumulative loss of the algorithm in interval $$I_k$$.
- $$L_{I_k, i}$$ be the cumulative loss of expert $$i$$ in interval $$I_k$$.

By the regret bound for a single interval, we know:
$$
L_{I_k} - \min_{i \in [N]} L_{I_k, i} \leq \sqrt{\frac{2^k}{2} \log N}.
$$

**Total Loss Over All Periods**
Summing the regret over all intervals $$I_k$$ for $$k = 0, \dots, n$$, where $$n$$ is the number of intervals up to time $$T$$, gives:
$$
L_T = \sum_{k=0}^n L_{I_k} \leq \sum_{k=0}^n \min_{i \in [N]} L_{I_k, i} + \sum_{k=0}^n \sqrt{\frac{2^k}{2} \log N}.
$$

Using the geometric sum $$\sum_{k=0}^n \sqrt{2^k}$$, we bound the second term:
$$
\sum_{k=0}^n \sqrt{2^k} \leq \sqrt{2} \cdot \sqrt{T}.
$$

Thus, the total regret is bounded as:
$$
\text{Regret}(T) \leq \sqrt{2} \cdot \sqrt{\frac{T}{2} \log N} + \sqrt{\log N / 2}.
$$


##### **Why the Doubling Trick Works**

The beauty of the Doubling Trick lies in its ability to simulate knowing $$T$$ without actually requiring it:
- The intervals grow exponentially, so the algorithm spends more time in later intervals, where the length of the period aligns better with the total horizon $$T$$.
- By resetting parameters for each interval, the algorithm effectively adapts its learning rate to the length of the interval.


##### **Applications of the Doubling Trick**

The Doubling Trick is not limited to regret minimization. It finds applications in various areas of machine learning and optimization:
1. **Hyperparameter Tuning**:
   - Learning rates or regularization parameters can be adjusted dynamically using the Doubling Trick.
2. **Bandit Problems**:
   - It helps adapt exploration-exploitation strategies over unknown time horizons.
3. **General Parameter Updates**:
   - Any scenario requiring parameter tuning over an unknown horizon can leverage this method.


##### **Advantages and Limitations**

**Advantages**
1. **Handles Unknown Horizons**:
   - No prior knowledge of $$T$$ is needed, making it highly practical.
2. **Minimal Regret Penalty**:
   - The regret bound is only worse by a constant factor compared to the known $$T$$ case.

**Limitations**
1. **Reset Overhead**:
   - Resetting parameters at the start of each interval may introduce computational overhead.
2. **Suboptimal for Short Horizons**:
   - For small $$T$$, the initial intervals may dominate, leading to suboptimal performance.

---

##### **Conclusion**

The Doubling Trick is an ingenious and versatile technique that alleviates the need to know the time horizon in advance. By dividing time into exponentially growing intervals, it achieves regret bounds that are competitive with the best possible bounds for known $$T$$. Its applicability to various domains makes it a fundamental tool in the arsenal of machine learning practitioners and theorists.

In the next section, we’ll explore how this idea extends to randomized algorithms and convex loss functions for improved guarantees. Stay tuned for more!


---


For Content Piece 1: Halving Algorithm
Title: "Understanding the Halving Algorithm: A Foundational Tool in Online Learning"
Description: "Explore the Halving Algorithm, its mistake bounds, and how it minimizes regret in adversarial online learning scenarios."

For Content Piece 2: Weighted Majority Algorithm
Title: "The Weighted Majority Algorithm: Combining Expert Predictions Effectively"
Description: "Learn how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake."

For Content Piece 3: Exponential Weighted Average Algorithm
Title: "Exponential Weighted Average Algorithm: Optimizing Online Learning with Convex Losses"
Description: "Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization."

For Content Piece 4: Doubling Trick
Title: "Doubling Trick in Online Learning: Tackling Unknown Time Horizons"
Description: "Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds."









