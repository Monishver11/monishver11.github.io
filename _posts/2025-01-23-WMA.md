---
layout: post
title: Understanding the Weighted Majority Algorithm in Online Learning
date: 2025-01-23 12:33:00-0400
featured: false
description: Explore how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake.
tags: ML Math
categories: ADV-ML-NYU
giscus_comments: false
related_posts: false
# toc:
#   sidebar: left
---



**The Weighted Majority Algorithm: A Powerful Online Learning Technique**

In the continuation of our exploration into online learning, we turn to the **Weighted Majority Algorithm (WMA)**, an influential approach introduced by Littlestone and Warmuth in 1988. This algorithm builds upon the foundational principles of online learning and offers remarkable theoretical guarantees for handling adversarial scenarios.

Let’s dive into the workings of the Weighted Majority Algorithm, analyze its performance, and understand its strengths and limitations.


#### **The Weighted Majority Algorithm**

The Weighted Majority Algorithm operates in a framework where predictions are made by combining the advice of multiple experts. Unlike the Halving Algorithm, which outright eliminates incorrect hypotheses, WMA assigns and updates weights to experts based on their performance, ensuring a more adaptive approach.

##### **The Algorithm Steps**

1. **Initialization**:
   - Start with $$N$$ experts, each assigned an initial weight of 1:  
     $$w_{1,i} = 1 \quad \text{for } i = 1, 2, \dots, N$$

2. **Prediction**:
   - At each time step $$t$$:
     - Receive the instance $$x_t$$.
     - Predict the label $$\hat{y}_t$$ using a **weighted majority vote**:
       $$\hat{y}_t = 
       \begin{cases} 
       1 & \text{if } \sum_{i: y_{t,i}=1}^{N} w_{t,i} > \sum_{i: y_{t,i}=0}^{N} w_{t,i}, \\
       0 & \text{otherwise.} 
       \end{cases}$$


3. **Update Weights**:
   - After receiving the true label $$y_t$$, update the weights of the experts:
     - For each expert $$i$$:
       $$
       w_{t+1,i} =
       \begin{cases} 
       \beta w_{t,i} & \text{if } y_{t,i} \neq y_t, \\
       w_{t,i} & \text{otherwise,}
       \end{cases}
       $$
       where $$\beta \in [0,1)$$ is a parameter that reduces the weight of experts who make incorrect predictions.

[The condition check matters here right??, 2 cases: 1. if the majority vote is correct, then no update for the experts that did mistake. 2. If the majoriy vote is wrong, then we update correctly for experts that are wrong. So, its better to update without checking if the majority vote gives correct result right? ]

4. **Termination**:
   - After $$T$$ iterations, return the final weights of all experts.


#### **Theoretical Performance of the Weighted Majority Algorithm**

##### **Mistake Bound**

**Theorem:**
The Weighted Majority (WM) algorithm guarantees a bound on the number of mistakes it makes compared to the best expert. Let:
- $$ m_t $$: Total number of mistakes made by the WM algorithm up to time $$ t $$,
- $$ m_t^* $$: Number of mistakes made by the best expert up to time $$ t $$,
- $$ N $$: Total number of experts,
- $$ \beta $$: Parameter controlling the weight decay for incorrect experts.

The mistake bound is given by:

$$
m_t \leq \frac{\log N + m_t^* \log \frac{1}{\beta}}{\log \frac{2}{1+\beta}}
$$


**Interpretation of the Bound:**
1. **First Term ($$ \log N $$)**:
   - This term accounts for the initial uncertainty due to having $$ N $$ experts. The algorithm needs to explore to identify the best expert, and the logarithmic dependence on $$ N $$ ensures scalability.

2. **Second Term ($$ m_t^* \log \frac{1}{\beta} $$)**:
   - This term reflects the cost of following the best expert, scaled by $$ \log \frac{1}{\beta} $$. A smaller $$ \beta $$ increases the penalty for mistakes, leading to slower adaptation but potentially fewer overall mistakes. [**Think it through, Why?**]

3. **Denominator ($$ \log \frac{2}{1+\beta} $$)**:
   - This represents the efficiency of the weight adjustment. When $$ \beta $$ is close to 0 (as in the Halving Algorithm), the denominator becomes larger, leading to tighter bounds.


**Special Cases:**
1. **Realizable Case ($$ m_t^* = 0 $$)**:
   - If there exists an expert with zero mistakes, the bound simplifies to:
     $$
     m_t \leq \frac{\log N}{\log \frac{2}{1+\beta}}
     $$
     - For the Halving Algorithm ($$ \beta = 0 $$), this further simplifies to:
       $$
       m_t \leq \log N
       $$

2. **General Case**:
   - When no expert is perfect ($$ m_t^* > 0 $$), the algorithm incurs an additional cost proportional to $$ m_t^* \log \frac{1}{\beta} $$. This reflects the cost of distributing weights among experts.


**Intuition:**
- The Weighted Majority Algorithm balances exploration (trying all experts) and exploitation (focusing on the best expert). The logarithmic terms indicate that the algorithm adapts efficiently, even with a large number of experts.
- By tuning $$ \beta $$, the algorithm can trade off between faster adaptation and resilience to noise. A smaller $$ \beta $$ (e.g., $$ \beta = 0 $$) emphasizes rapid adaptation, while a larger $$ \beta $$ smoothens weight updates.


**Note:** If this isn’t clear, I can simplify it further, but this time, I encourage you to try and understand it on your own. There’s a reason behind this—it helps you build a mental model of how the algorithm works and strengthens your intuition. This kind of reinforcement is essential as we dive deeper into advanced ML concepts. I hope this makes sense. If you’re still struggling, consider using tools like ChatGPT or Perplexity to gain additional clarity.


---

#### **Proof of the Mistake Bound**

<mark>A general method for deducing bounds and guarantees involves defining a potential function, establishing both upper and lower bounds for it, and deriving results from the resulting inequality.</mark> This powerful approach is central to deducing several proofs. Specifically, the proof of the mistake bound relies on defining a potential function that tracks the total weight of all experts over time.

##### **Potential Function**

The potential function at time $$ t $$ is defined as:

$$
\Phi_t = \sum_{i=1}^N w_{t,i},
$$

where $$ w_{t,i} $$ is the weight of expert $$ i $$ at time $$ t $$.


##### **Upper Bound on Potential**

Initially, the weights of all $$ N $$ experts sum up to $$ \Phi_0 = N $$ since each expert starts with a weight of 1.

**Step 1: Effect of a Mistake**

When the Weighted Majority Algorithm makes a mistake, the weights of the experts who predicted incorrectly are reduced by a factor of $$ \beta $$, where $$ 0 < \beta < 1 $$. This means the total weight at the next time step, $$ \Phi_{t+1} $$, will be less than or equal to the weighted average of the weights of the correct and incorrect experts.

**Step 2: Fraction of Correct and Incorrect Experts**

Let’s say a fraction $$ p $$ of the total weight belongs to the correct experts, and a fraction $$ 1 - p $$ belongs to the incorrect experts. After the mistake, the incorrect experts’ weights are scaled by $$ \beta $$. Thus, the new potential is:

$$
\Phi_{t+1} = p \Phi_t + \beta (1 - p) \Phi_t
$$

**Step 3: Simplifying the Expression**

Factor out $$ \Phi_t $$ from the equation:

$$
\Phi_{t+1} = \Phi_t \left[ p + \beta (1 - p) \right]
$$

Rewriting $$ p + \beta (1 - p) $$:

$$
\Phi_{t+1} = \Phi_t \left[ 1 - (1 - \beta)(1 - p) \right]
$$

Since $$ p + (1 - p) = 1 $$, this simplifies to:

$$
\Phi_{t+1} \leq \Phi_t \left[ 1 + \frac{\beta}{2} \right]
$$

This inequality holds because the worst-case scenario assumes $$ p = \frac{1}{2} $$, where half the weight comes from correct predictions and half from incorrect predictions.

**Step 4: Over Multiple Mistakes**

If the algorithm makes $$ m_t $$ mistakes, the inequality applies iteratively. After $$ m_t $$ mistakes, the potential becomes:

$$
\Phi_t \leq \Phi_0 \left[ 1 + \frac{\beta}{2} \right]^{m_t}
$$

Substituting $$ \Phi_0 = N $$ (the initial total weight):

$$
\Phi_t \leq N \left[ 1 + \frac{\beta}{2} \right]^{m_t}
$$

**Intuition:**
The factor $$ 1 + \frac{\beta}{2} $$ reflects the reduction in potential after each mistake. As $$ \beta $$ decreases, the penalty for incorrect experts increases, leading to a faster reduction in $$ \Phi_t $$. This bound shows how the potential decreases exponentially with the number of mistakes $$ m_t $$.


--- 

##### **Lower Bound on Potential**

The **lower bound** on the potential is based on the performance of the best expert in the algorithm. 

Let’s define $$ w_{t,i} $$ as the weight of expert $$ i $$ at time $$ t $$. The total potential $$ \Phi_t $$ is the sum of the weights of all experts. Since the best expert is the one with the highest weight at any time, we can state that:

$$
\Phi_t \geq w_{t,i^*}
$$

where $$ i^* $$ is the index of the best expert. 

Now, the key point is that the weight of the best expert, $$ w_{t,i^*} $$, decays over time as it makes mistakes. Let $$ m_t^* $$ be the number of mistakes made by the best expert up to time $$ t $$. Since each mistake reduces the weight of the best expert by a factor of $$ \beta $$, the weight of the best expert at time $$ t $$ is given by:

$$
w_{t,i^*} = \beta^{m_t^*}
$$

Therefore, the potential at time $$ t $$ is at least the weight of the best expert:

$$
\Phi_t \geq \beta^{m_t^*}
$$

This provides a **lower bound** on the potential.

##### **Combining the Upper and Lower Bounds**

Now that we have both an upper bound and a lower bound on the potential, we can combine them to get a more useful inequality.

From the upper bound, we know:

$$
\Phi_t \leq \left[\frac{1 + \beta}{2}\right]^{m_t} N
$$

From the lower bound, we know:

$$
\Phi_t \geq \beta^{m_t^*}
$$

By combining these two inequalities, we get:

$$
\beta^{m_t^*} \leq \left[\frac{1 + \beta}{2}\right]^{m_t} N
$$

This inequality tells us that the weight of the best expert at time $$ t $$, $$ \beta^{m_t^*} $$, is less than or equal to the total potential $$ \Phi_t $$ after $$ m_t $$ mistakes.


To solve for $$ m_t $$, we take the logarithm of both sides of the inequality:

$$
\log \left( \beta^{m_t^*} \right) \leq \log \left( \left[\frac{1 + \beta}{2}\right]^{m_t} N \right)
$$

Using the logarithmic property $$ \log(a^b) = b \log(a) $$, we simplify both sides:

$$
m_t^* \log \beta \leq m_t \log \left[\frac{1 + \beta}{2}\right] + \log N
$$



Now, we want to isolate $$ m_t $$ on one side of the inequality. First, subtract $$ \log N $$ from both sides:

$$
m_t^* \log \beta - \log N \leq m_t \log \left[\frac{1 + \beta}{2}\right].
$$

Now, divide both sides by $$ \log \left[\frac{1 + \beta}{2}\right] $$. Note that $$ \log \left[\frac{1 + \beta}{2}\right] $$ is negative because $$ \frac{1 + \beta}{2} < 1 $$, so dividing by it reverses the inequality:

$$
m_t \geq \frac{m_t^* \log \beta - \log N}{\log \left[\frac{1 + \beta}{2}\right]}.
$$

We can simplify the expression further by recognizing that $$ \log \frac{1}{\beta} = -\log \beta $$. This gives us:

$$
m_t \leq \frac{\log N + m_t^* \log \frac{1}{\beta}}{\log \frac{2}{1 + \beta}}.
$$

This is the final inequality, which gives a bound on the number of mistakes $$ m_t $$ made by the algorithm in terms of the number of mistakes $$ m_t^* $$ made by the best expert, the total number of experts $$ N $$, and the factor $$ \beta $$.

[How the last inequality conversion happened or the log(2/(1+beta)) change?]

This completes the proof of the mistake bound. The inequality shows that the number of mistakes $$ m_t $$ made by the algorithm is related to the mistakes $$ m_t^* $$ made by the best expert, and that the algorithm’s performance improves as $$ \beta $$ decreases.

[Self - Understand it better, think it through]

---

##### **Strengths and Weaknesses of the Weighted Majority Algorithm**

**Advantages**
- **Remarkable Theoretical Guarantees**:
  - The logarithmic mistake bound requires no assumptions about the data distribution or expert performance.
- **Flexibility**:
  - WMA can be applied in a variety of adversarial and dynamic environments.

**Disadvantages**
- **Binary Loss**:
  - No deterministic algorithm, including WMA, can achieve zero regret with binary loss.
- **Improvements with Randomization**:
  - Randomized versions of WMA can achieve better regret bounds.
- **Extensions for Convex Losses**:
  - When applied to convex loss functions, WMA can yield improved theoretical guarantees.

[Analyse and explain it in a better way to form intuition]

---

##### **Conclusion**

The Weighted Majority Algorithm is a cornerstone of online learning, offering an elegant and efficient approach to handling adversarial settings. By adaptively updating the weights of experts, it ensures robust performance with minimal assumptions. While it has certain limitations, such as its inability to guarantee zero regret with binary loss, these can often be addressed through randomization or extensions to convex losses.

In the next post, we’ll explore randomized versions of WMA and delve into other powerful algorithms for online learning. Stay tuned!


---

**Exponential Weighted Average Algorithm: A Regret-Minimization Powerhouse**

The **Exponential Weighted Average Algorithm (EWAA)** is a fundamental online learning algorithm that provides elegant guarantees for minimizing regret in adversarial settings. It extends the principles of the Weighted Majority Algorithm by incorporating exponential weight updates, making it particularly effective for handling convex loss functions.


#### **How the Exponential Weighted Average Algorithm Works**

At its core, the EWAA maintains and updates weights for a set of experts, similar to the Weighted Majority Algorithm. However, it uses an **exponential weighting scheme** to achieve better bounds on regret, especially for convex losses.

##### **Steps of the Algorithm**

1. **Weight Update**:  
   At each time step $$t$$, the weights are updated as follows:  
   $$
   w_{t+1,i} = w_{t,i} \cdot e^{-\eta L(\hat{y}_{t,i}, y_t)}
   $$
   where:
   - $$\eta > 0$$ is the learning rate.
   - $$L(\hat{y}_{t,i}, y_t)$$ is the loss incurred by expert $$i$$ at time $$t$$.

   This update exponentially penalizes experts based on the loss they incur, ensuring that poorly performing experts are rapidly down-weighted.

[How in slides, did we equate w(t+1) to just e-x form]

[Add a point to plot e^(-x) to show that it decrease as x increases rapidly.]

2. **Prediction**:  
   The algorithm predicts by averaging the predictions of all experts, weighted by their current weights:
   $$
   \hat{y}_t = \sum_{i=1}^N w_{t,i} \cdot \hat{y}_{t,i}
   $$


##### **Theorem: Regret Bound for EWAA**

Let $$L(y, y')$$ be a convex loss function in its first argument, taking values in $$[0, 1]$$. For any $$\eta > 0$$ and any sequence of labels $$y_1, \dots, y_T \in \mathcal{Y}$$, the regret of the Exponential Weighted Average Algorithm satisfies:
$$
\text{Regret}(T) \leq \frac{\log N}{\eta} + \frac{\eta T}{8}.
$$

Here, regret is defined as the difference between the total loss of the algorithm and the loss of the best expert:
$$
\text{Regret}(T) = \sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i \in [N]} \sum_{t=1}^T L(\hat{y}_{t,i}, y_t).
$$

##### **Optimizing the Learning Rate**
Choosing $$\eta = \sqrt{\frac{8 \log N}{T}}$$ minimizes the regret bound, yielding:
$$
\text{Regret}(T) \leq \sqrt{\frac{T}{2} \log N}.
$$

This result demonstrates the efficiency of the EWAA, with regret growing only logarithmically with the number of experts $$N$$ and sublinearly with the number of time steps $$T$$.


#### **Proof Sketch: Regret Bound of EWAA**

##### **Key Idea: Potential Function**
The proof relies on analyzing the potential function:
$$
\Phi_t = \log \left( \sum_{i=1}^N w_{t,i} \right).
$$

**Upper Bound**
Using the weight update rule and the convexity of the loss function, the following upper bound is derived:
$$
\Phi_{t+1} - \Phi_t \leq -\eta L(\hat{y}_t, y_t) + \frac{\eta^2}{8}.
$$

**Lower Bound**
By considering the contribution of the best expert, the potential function satisfies:
$$
\Phi_T - \Phi_0 \geq -\eta \min_{i \in [N]} L_{T,i} - \log N,
$$
where $$L_{T,i} = \sum_{t=1}^T L(\hat{y}_{t,i}, y_t)$$ is the cumulative loss of expert $$i$$.

**Combining Bounds**
Summing the inequalities and comparing the two expressions for $$\Phi_T - \Phi_0$$ gives the regret bound:
$$
\sum_{t=1}^T L(\hat{y}_t, y_t) - \min_{i \in [N]} L_{T,i} \leq \frac{\log N}{\eta} + \frac{\eta T}{8}.
$$


##### **Advantages and Disadvantages of EWAA**

**Advantages**
1. **Strong Theoretical Guarantees**:
   - The regret bound is logarithmic in the number of experts $$N$$ and sublinear in the number of time steps $$T$$.
2. **Applicability to Convex Losses**:
   - Unlike binary loss-focused algorithms, EWAA works seamlessly with convex loss functions, making it more versatile.
3. **Weight Adaptivity**:
   - Exponential weight updates ensure that poor-performing experts are penalized efficiently.

**Disadvantages**
1. **Dependence on Horizon**:
   - Choosing the optimal learning rate $$\eta$$ requires prior knowledge of the time horizon $$T$$, which is not always feasible in practice.
2. **Increased Computational Complexity**:
   - Exponential weight updates and averaging can be computationally expensive for a large number of experts.


##### **Conclusion**

The Exponential Weighted Average Algorithm is a cornerstone of online learning, offering robust guarantees for regret minimization under convex loss functions. Its reliance on exponential weight updates makes it adaptable and theoretically elegant, but its dependence on the time horizon $$T$$ can pose practical challenges. 

In the next post, we’ll explore randomized extensions and their impact on improving regret bounds. Stay tuned for more insights into online learning!

---


**Doubling Trick: A Clever Strategy to Handle Unknown Horizons**

In many online learning algorithms, selecting the learning rate ($$\eta$$) or other parameters often requires knowledge of the total time horizon $$T$$. However, in practical scenarios, $$T$$ may not be known in advance. The **Doubling Trick** is a simple yet powerful method to overcome this limitation by dividing the time horizon into exponentially growing intervals and resetting parameters at the start of each interval.


#### **The Idea Behind the Doubling Trick**

The Doubling Trick works by splitting time into **periods of exponentially increasing length**. For example, the $$k$$-th period spans the range:
$$
I_k = [2^k, 2^{k+1} - 1].
$$

In each period:
1. A new learning rate or parameter is chosen based on the length of the period.
2. The algorithm is reset for that period.

By progressively increasing the size of the intervals, the algorithm ensures that the total regret remains within acceptable bounds.


#### **Regret Bound with the Doubling Trick**

##### **Theorem**
Assume the same conditions as those in the Exponential Weighted Average Algorithm. For any total time horizon $$T$$, the regret achieved by the Doubling Trick satisfies:
$$
\text{Regret}(T) \leq \sqrt{2} \cdot \sqrt{\frac{T}{2} \log N} + \sqrt{\log N / 2}.
$$

This result demonstrates that the Doubling Trick achieves regret bounds that are only slightly worse (by a constant factor) than if the total time horizon $$T$$ were known in advance.


##### **Proof Sketch**

To derive the regret bound, let:
- $$L_{I_k}$$ be the cumulative loss of the algorithm in interval $$I_k$$.
- $$L_{I_k, i}$$ be the cumulative loss of expert $$i$$ in interval $$I_k$$.

By the regret bound for a single interval, we know:
$$
L_{I_k} - \min_{i \in [N]} L_{I_k, i} \leq \sqrt{\frac{2^k}{2} \log N}.
$$

**Total Loss Over All Periods**
Summing the regret over all intervals $$I_k$$ for $$k = 0, \dots, n$$, where $$n$$ is the number of intervals up to time $$T$$, gives:
$$
L_T = \sum_{k=0}^n L_{I_k} \leq \sum_{k=0}^n \min_{i \in [N]} L_{I_k, i} + \sum_{k=0}^n \sqrt{\frac{2^k}{2} \log N}.
$$

Using the geometric sum $$\sum_{k=0}^n \sqrt{2^k}$$, we bound the second term:
$$
\sum_{k=0}^n \sqrt{2^k} \leq \sqrt{2} \cdot \sqrt{T}.
$$

Thus, the total regret is bounded as:
$$
\text{Regret}(T) \leq \sqrt{2} \cdot \sqrt{\frac{T}{2} \log N} + \sqrt{\log N / 2}.
$$


##### **Why the Doubling Trick Works**

The beauty of the Doubling Trick lies in its ability to simulate knowing $$T$$ without actually requiring it:
- The intervals grow exponentially, so the algorithm spends more time in later intervals, where the length of the period aligns better with the total horizon $$T$$.
- By resetting parameters for each interval, the algorithm effectively adapts its learning rate to the length of the interval.


##### **Applications of the Doubling Trick**

The Doubling Trick is not limited to regret minimization. It finds applications in various areas of machine learning and optimization:
1. **Hyperparameter Tuning**:
   - Learning rates or regularization parameters can be adjusted dynamically using the Doubling Trick.
2. **Bandit Problems**:
   - It helps adapt exploration-exploitation strategies over unknown time horizons.
3. **General Parameter Updates**:
   - Any scenario requiring parameter tuning over an unknown horizon can leverage this method.


##### **Advantages and Limitations**

**Advantages**
1. **Handles Unknown Horizons**:
   - No prior knowledge of $$T$$ is needed, making it highly practical.
2. **Minimal Regret Penalty**:
   - The regret bound is only worse by a constant factor compared to the known $$T$$ case.

**Limitations**
1. **Reset Overhead**:
   - Resetting parameters at the start of each interval may introduce computational overhead.
2. **Suboptimal for Short Horizons**:
   - For small $$T$$, the initial intervals may dominate, leading to suboptimal performance.

---

##### **Conclusion**

The Doubling Trick is an ingenious and versatile technique that alleviates the need to know the time horizon in advance. By dividing time into exponentially growing intervals, it achieves regret bounds that are competitive with the best possible bounds for known $$T$$. Its applicability to various domains makes it a fundamental tool in the arsenal of machine learning practitioners and theorists.

In the next section, we’ll explore how this idea extends to randomized algorithms and convex loss functions for improved guarantees. Stay tuned for more!


---


For Content Piece 1: Halving Algorithm
Title: "Understanding the Halving Algorithm: A Foundational Tool in Online Learning"
Description: "Explore the Halving Algorithm, its mistake bounds, and how it minimizes regret in adversarial online learning scenarios."

For Content Piece 2: Weighted Majority Algorithm
Title: "The Weighted Majority Algorithm: Combining Expert Predictions Effectively"
Description: "Learn how the Weighted Majority Algorithm achieves robust bounds for adversarial settings by adapting expert weights with every mistake."

For Content Piece 3: Exponential Weighted Average Algorithm
Title: "Exponential Weighted Average Algorithm: Optimizing Online Learning with Convex Losses"
Description: "Delve into the Exponential Weighted Average Algorithm, its regret bounds, and the mathematical proof ensuring efficient loss minimization."

For Content Piece 4: Doubling Trick
Title: "Doubling Trick in Online Learning: Tackling Unknown Time Horizons"
Description: "Discover how the Doubling Trick enables online algorithms to adapt to unknown horizons, maintaining competitive regret bounds."









