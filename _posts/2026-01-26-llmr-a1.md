Building LLM Reasoners Assignment 1: Building a Transformer LM

This is going to be my worklog for the first LLMR assignment. I'll be noting down the learning, notes, my understandings and how i want to remember things from this. The coding portion will be done on the assignment repo only, and will add it later after the course. 

#### **Starters**

What we're supposed to build;
1. Byte-pair encoding (BPE) tokenizer 
2. Transformer language model (LM) 
3. The cross-entropy loss function and the AdamW optimizer 
4. The training loop, with support for serializing and loading model and optimizer state 

What we'll be running;
1. Train a BPE tokenizer on the TinyStories dataset.
2. Run your trained tokenizer on the dataset to convert it into a sequence of integer IDs.
3. Train a Transformer LM on the TinyStories dataset.
4. Generate samples and evaluate perplexity using the trained Transformer LM.
5. Perform ablations of the architecture

What we're allowed to use and how we're allowed to use them;
- Expected to build these components from scratch
- Not allowed to use any definitions from torch.nn, torch.nn.functional, or torch.optim except for the following (which you may use):
  - torch.nn.Parameter
  - Container classes in torch.nn (e.g., Module, ModuleList, Sequential, etc.)
  - The torch.optim.Optimizer base class
- Allowed to use any other PyTorch definitions. When in doubt, consider if using it compromises the "from scratch" ethos of the assignment.

Statement on AI tools;
- Prompting LLMs such as ChatGPT is permitted for low-level programming questions or high-level conceptual questions about language models, but using it directly to solve the problem is prohibited.
- We strongly encourage you to disable AI autocomplete (e.g., Cursor Tab, GitHub CoPilot) in your IDE when completing assignments (though non-AI autocomplete, e.g., autocompleting function names is totally fine). We have found that AI autocomplete makes it much harder to engage deeply with the content.

Low-Resource/Downscaling Tip: Init;
Throughout the course’s assignment handouts, we will give advice for working through parts of the assignment with fewer or no GPU resources. For example, we will sometimes suggest downscaling your dataset or model size, or explain how to run training code on a MacOS integrated GPU or CPU. You’ll find these “low-resource tips” in a blue box (like this one). These tips may help you iterate faster and save time.

Cloned the Repo;

#### **Byte-Pair Encoding (BPE) Tokenizer**

In the first part of the assignment, we will train and implement a byte-level byte-pair encoding (BPE) tokenizer. In particular, we will represent arbitrary (Unicode) strings as a sequence of bytes and train our BPE tokenizer on this byte sequence. Later, we will use this tokenizer to encode text (a string) into tokens (a sequence of integers) for language modeling.

##### **The Unicode Standard**